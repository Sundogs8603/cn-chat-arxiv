<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01499</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#35780;&#20272;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing and Evaluating a Design Method for Positive Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01499
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#65292;&#24182;&#36890;&#36807;&#25345;&#32493;&#30340;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#36827;&#34892;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23545;&#31038;&#20250;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#20010;&#26041;&#38754;&#26085;&#30410;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#8220;AI for good&#8221;&#22312;&#19982;&#22797;&#26434;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#32570;&#20047;&#25104;&#29087;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#27491;&#21521;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27969;&#31243;&#65292;&#23558;&#24184;&#31119;&#24895;&#26223;&#36716;&#21270;&#20026;&#20855;&#20307;&#23454;&#36341;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#35813;&#26041;&#27861;&#30340;&#22235;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#24773;&#22659;&#21270;&#12289;&#25805;&#20316;&#21270;&#12289;&#20248;&#21270;&#21270;&#21644;&#23454;&#29616;&#31119;&#31049;&#65292;&#21516;&#26102;&#25903;&#25345;&#25345;&#32493;&#27979;&#37327;&#21644;&#21453;&#39304;&#24490;&#29615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#20854;&#20013;&#21021;&#23398;&#32773;&#35774;&#35745;&#24072;&#24212;&#29992;&#20102;&#35813;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#19982;&#25928;&#21147;&#21644;&#21487;&#29992;&#24615;&#30456;&#20851;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#39033;&#19987;&#23478;&#35780;&#20272;&#30740;&#31350;&#35780;&#20272;&#20102;&#25152;&#24471;&#27010;&#24565;&#30340;&#36136;&#37327;&#65292;&#23558;&#20854;&#35780;&#20026;&#20013;&#31561;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) continues advancing, ensuring positive societal impacts becomes critical, especially as AI systems become increasingly ubiquitous in various aspects of life. However, developing "AI for good" poses substantial challenges around aligning systems with complex human values. Presently, we lack mature methods for addressing these challenges. This article presents and evaluates the Positive AI design method aimed at addressing this gap. The method provides a human-centered process to translate wellbeing aspirations into concrete practices. First, we explain the method's four key steps: contextualizing, operationalizing, optimizing, and implementing wellbeing supported by continuous measurement for feedback cycles. We then present a multiple case study where novice designers applied the method, revealing strengths and weaknesses related to efficacy and usability. Next, an expert evaluation study assessed the quality of the resulting concepts, rating them modera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://rss.arxiv.org/abs/2311.05112</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#65306;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#21307;&#23398;&#20013;&#65292;LLMs&#22312;&#21327;&#21161;&#21307;&#29983;&#36827;&#34892;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;LLMs&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#20855;&#20307;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;LLMs&#65311;2&#65289;&#20160;&#20040;&#26159;&#21307;&#23398;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65311;3&#65289;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;4&#65289;&#20351;&#29992;&#21307;&#23398;LLMs&#20250;&#20986;&#29616;&#21738;&#20123;&#25361;&#25112;&#65311;5&#65289;&#22914;&#20309;&#26356;&#22909;&#22320;&#26500;&#24314;&#21644;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#21307;&#23398;&#20013;LLMs&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#30340;&#23454;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#32500;&#25252;&#24182;&#23450;&#26399;&#26356;&#26032;&#19968;&#20010;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;</title><link>https://arxiv.org/abs/2403.02338</link><description>&lt;p&gt;
&#29992;&#21452;&#25163;&#25197;&#24320;&#30422;&#23376;
&lt;/p&gt;
&lt;p&gt;
Twisting Lids Off with Two Hands
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02338
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20004;&#21482;&#22810;&#25351;&#25163;&#33218;&#25805;&#32437;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#30340;&#20016;&#23500;&#25509;&#35302;&#24615;&#36136;&#20197;&#21450;&#21327;&#35843;&#39640;&#32500;&#24230;&#21452;&#25163;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#20004;&#21482;&#25163;&#25197;&#24320;&#21508;&#31181;&#29942;&#23376;&#30422;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#36890;&#36807;&#23545;&#29289;&#29702;&#24314;&#27169;&#12289;&#23454;&#26102;&#24863;&#30693;&#21644;&#22870;&#21169;&#35774;&#35745;&#30340;&#26032;&#24037;&#31243;&#35265;&#35299;&#65292;&#35813;&#31574;&#30053;&#23637;&#31034;&#20102;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#36143;&#31359;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#65292;&#23637;&#31034;&#20986;&#21160;&#24577;&#21644;&#28789;&#24039;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20173;&#28982;&#26159;&#35299;&#20915;&#21069;&#25152;&#26410;&#26377;&#22797;&#26434;&#38382;&#39064;&#30340;&#25805;&#32437;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#26631;&#24535;&#26816;&#27979;&#12289;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#21644;&#26631;&#24535;&#20301;&#32622;&#20998;&#26512;&#65292;&#29992;&#20110;&#34913;&#37327;&#21253;&#35013;&#35774;&#35745;&#20013;&#21697;&#29260;&#26631;&#24535;&#30340;&#20851;&#27880;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02336</link><description>&lt;p&gt;
&#21253;&#35013;&#20013;&#30340;&#21697;&#29260;&#21487;&#35265;&#24230;&#65306;&#19968;&#31181;&#29992;&#20110;&#26631;&#24535;&#26816;&#27979;&#12289;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#21644;&#26631;&#24535;&#20301;&#32622;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Brand Visibility in Packaging: A Deep Learning Approach for Logo Detection, Saliency-Map Prediction, and Logo Placement Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#26631;&#24535;&#26816;&#27979;&#12289;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#21644;&#26631;&#24535;&#20301;&#32622;&#20998;&#26512;&#65292;&#29992;&#20110;&#34913;&#37327;&#21253;&#35013;&#35774;&#35745;&#20013;&#21697;&#29260;&#26631;&#24535;&#30340;&#20851;&#27880;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20135;&#21697;&#33829;&#38144;&#39046;&#22495;&#30340;&#28608;&#28872;&#31454;&#20105;&#20013;&#65292;&#21253;&#35013;&#19978;&#21697;&#29260;&#26631;&#24535;&#30340;&#21487;&#35265;&#24230;&#22312;&#22609;&#36896;&#28040;&#36153;&#32773;&#35748;&#30693;&#12289;&#30452;&#25509;&#24433;&#21709;&#20135;&#21697;&#25104;&#21151;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21253;&#35013;&#35774;&#35745;&#20013;&#21697;&#29260;&#26631;&#24535;&#30340;&#20851;&#27880;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#21033;&#29992;YOLOv8&#22312;&#26174;&#33879;&#25968;&#25454;&#38598;FoodLogoDet-1500&#21644;LogoDet-3K&#19978;&#36827;&#34892;&#31934;&#30830;&#30340;&#26631;&#24535;&#26816;&#27979;&#12290;&#31532;&#20108;&#27493;&#28041;&#21450;&#20351;&#29992;&#19968;&#31181;&#19987;&#20026;&#21253;&#35013;&#19978;&#19979;&#25991;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#26174;&#33879;&#24615;&#39044;&#27979;&#27169;&#22411;&#24314;&#27169;&#29992;&#25143;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26174;&#33879;&#24615;&#27169;&#22411;&#32467;&#21512;&#20102;&#35270;&#35273;&#20803;&#32032;&#19982;&#25991;&#26412;&#26144;&#23556;&#65292;&#21033;&#29992;&#22522;&#20110;transformers&#30340;&#26550;&#26500;&#39044;&#27979;&#29992;&#25143;&#27880;&#24847;&#21147;&#22270;&#12290;&#22312;&#31532;&#19977;&#27493;&#20013;&#65292;&#36890;&#36807;&#23558;&#26631;&#24535;&#26816;&#27979;&#19982;&#26174;&#33879;&#24615;&#22270;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21697;&#29260;&#20851;&#27880;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02336v1 Announce Type: cross  Abstract: In the highly competitive area of product marketing, the visibility of brand logos on packaging plays a crucial role in shaping consumer perception, directly influencing the success of the product. This paper introduces a comprehensive framework to measure the brand logo's attention on a packaging design. The proposed method consists of three steps. The first step leverages YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500 and LogoDet-3K. The second step involves modeling the user's visual attention with a novel saliency prediction model tailored for the packaging context. The proposed saliency model combines the visual elements with text maps employing a transformers-based architecture to predict user attention maps. In the third step, by integrating logo detection with a saliency map generation, the framework provides a comprehensive brand attention score. The effectiveness of the proposed method is assess
&lt;/p&gt;</description></item><item><title>GCSL&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#21033;&#29992;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#26469;&#35757;&#32451;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.02334</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20851;&#23376;&#31354;&#38388;&#23398;&#20064;&#25269;&#25239;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Gradient Correlation Subspace Learning against Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02334
&lt;/p&gt;
&lt;p&gt;
GCSL&#26159;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#21033;&#29992;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#26469;&#35757;&#32451;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#26679;&#30340;&#23398;&#20064;&#38754;&#20020;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#20063;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#22686;&#37327;&#31867;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21517;&#20026;&#26799;&#24230;&#30456;&#20851;&#23376;&#31354;&#38388;&#23398;&#20064;&#65288;GCSL&#65289;&#12290;&#35813;&#26041;&#27861;&#26816;&#27979;&#21040;&#26368;&#19981;&#21463;&#20197;&#21069;&#20219;&#21153;&#24433;&#21709;&#30340;&#26435;&#37325;&#23376;&#31354;&#38388;&#65292;&#24182;&#23558;&#26435;&#37325;&#25237;&#24433;&#21040;&#35813;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32473;&#23450;&#32593;&#32476;&#26550;&#26500;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#23618;&#65292;&#24182;&#19988;&#25152;&#20351;&#29992;&#30340;&#23376;&#31354;&#38388;&#22823;&#23567;&#21487;&#20197;&#20174;&#23618;&#21040;&#23618;&#12289;&#20219;&#21153;&#21040;&#20219;&#21153;&#36827;&#34892;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02334v1 Announce Type: cross  Abstract: Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#28246;&#30340;&#27010;&#24565;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#31649;&#29702;&#20013;&#30340;&#22522;&#30784;&#30740;&#31350;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.02327</link><description>&lt;p&gt;
&#27169;&#22411;&#28246;
&lt;/p&gt;
&lt;p&gt;
Model Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#28246;&#30340;&#27010;&#24565;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#27169;&#22411;&#31649;&#29702;&#20013;&#30340;&#22522;&#30784;&#30740;&#31350;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23547;&#25214;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12289;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#24182;&#21306;&#20998;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#30446;&#21069;&#65292;&#20174;&#19994;&#32773;&#20381;&#38752;&#25163;&#24037;&#32534;&#20889;&#30340;&#25991;&#26723;&#26469;&#29702;&#35299;&#21644;&#36873;&#25321;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#27169;&#22411;&#37117;&#26377;&#23436;&#25972;&#21487;&#38752;&#30340;&#25991;&#26723;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21457;&#29616;&#12289;&#21306;&#20998;&#21644;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#21463;&#25968;&#25454;&#28246;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#23450;&#20041;&#20102;&#27169;&#22411;&#28246;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#27169;&#22411;&#31649;&#29702;&#20013;&#30340;&#22522;&#26412;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#21738;&#20123;&#22522;&#26412;&#30340;&#25968;&#25454;&#31649;&#29702;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#31649;&#29702;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02327v1 Announce Type: cross  Abstract: Given a set of deep learning models, it can be hard to find models appropriate to a task, understand the models, and characterize how models are different one from another. Currently, practitioners rely on manually-written documentation to understand and choose models. However, not all models have complete and reliable documentation. As the number of machine learning models increases, this issue of finding, differentiating, and understanding models is becoming more crucial. Inspired from research on data lakes, we introduce and define the concept of model lakes. We discuss fundamental research challenges in the management of large models. And we discuss what principled data management techniques can be brought to bear on the study of large model management.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02325</link><description>&lt;p&gt;
&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65306;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#23450;&#20301;&#20934;&#30830;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02325
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#20351;&#27169;&#22411;&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#24182;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#22270;&#20687;&#20013;&#29305;&#21035;&#30456;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#23548;&#27169;&#22411;&#26356;&#23494;&#20999;&#22320;&#20851;&#27880;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#21306;&#22495;&#24341;&#23548;&#65288;CRG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#24320;&#28304;&#30340;VLMs&#21709;&#24212;&#35270;&#35273;&#25552;&#31034;&#65292;&#24182;&#22312;&#21508;&#31181;VL&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02325v1 Announce Type: cross  Abstract: Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a "visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#22312;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;&#20174;&#32780;&#26500;&#24314;&#20004;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#20256;&#32479;&#26041;&#31243;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02290</link><description>&lt;p&gt;
Koopman&#36741;&#21161;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Koopman-Assisted Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#22312;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;&#20174;&#32780;&#26500;&#24314;&#20004;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#20256;&#32479;&#26041;&#31243;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40077;&#26364;&#26041;&#31243;&#21450;&#20854;&#36830;&#32493;&#24418;&#24335;&#65292;&#21363;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#39640;&#32500;&#29366;&#24577;&#21644;&#38750;&#32447;&#24615;&#30340;&#31995;&#32479;&#65292;&#36825;&#20123;&#26041;&#31243;&#24456;&#24555;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;Koopman&#31639;&#23376;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#20004;&#31181;&#26032;&#30340;RL&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;Koopman&#31639;&#23376;&#25216;&#26415;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#25552;&#21319;&#21040;&#26032;&#22352;&#26631;&#31995;&#65292;&#20854;&#20013;&#21160;&#21147;&#23398;&#21464;&#24471;&#36817;&#20284;&#32447;&#24615;&#65292;HJB&#26041;&#27861;&#26356;&#26131;&#22788;&#29702;&#12290;&#29305;&#21035;&#22320;&#65292;Koopman&#31639;&#23376;&#33021;&#22815;&#36890;&#36807;&#25552;&#21319;&#21040;&#30340;&#22352;&#26631;&#31995;&#20013;&#30340;&#32447;&#24615;&#21160;&#24577;&#26469;&#25429;&#33719;&#32473;&#23450;&#31995;&#32479;&#20540;&#20989;&#25968;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#26399;&#26395;&#12290;&#36890;&#36807;&#29992;&#25511;&#21046;&#21160;&#20316;&#21442;&#25968;&#21270;Koopman&#31639;&#23376;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;Koopman&#24352;&#37327;&#8221;&#65292;&#20197;&#20415;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02290v1 Announce Type: new  Abstract: The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the es
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25351;&#20986;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#65292;&#28151;&#28102;&#20167;&#24680;&#21644;&#20882;&#29359;&#21487;&#33021;&#20250;&#20351;&#30740;&#31350;&#32467;&#26524;&#22833;&#25928;&#65292;&#21628;&#21505;&#26410;&#26469;&#24037;&#20316;&#38656;&#35201;&#20174;&#29702;&#35770;&#19978;&#23558;&#20167;&#24680;&#19982;&#20882;&#29359;&#27010;&#24565;&#36827;&#34892;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.02268</link><description>&lt;p&gt;
&#20027;&#35266; $\textit{Isms}$? &#35770;&#28151;&#28102;&#20167;&#24680;&#19982;&#20882;&#29359;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#21361;&#38505;
&lt;/p&gt;
&lt;p&gt;
Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence in Abusive Language Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02268
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25351;&#20986;&#22312;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20013;&#65292;&#28151;&#28102;&#20167;&#24680;&#21644;&#20882;&#29359;&#21487;&#33021;&#20250;&#20351;&#30740;&#31350;&#32467;&#26524;&#22833;&#25928;&#65292;&#21628;&#21505;&#26410;&#26469;&#24037;&#20316;&#38656;&#35201;&#20174;&#29702;&#35770;&#19978;&#23558;&#20167;&#24680;&#19982;&#20882;&#29359;&#27010;&#24565;&#36827;&#34892;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02268v1 &#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25509;&#21463;&#27880;&#37322;&#32773;&#20027;&#35266;&#24615;&#30340;&#27010;&#24565;&#65292;&#36825;&#20027;&#35201;&#21463;&#21040;&#26631;&#35760;&#24046;&#24322;&#30340;&#39537;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#27599;&#20010;&#27880;&#37322;&#32773;&#30340;&#35266;&#28857;&#35270;&#20026;&#26377;&#25928;&#65292;&#36825;&#23545;&#20110;&#23884;&#20837;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;(&#22914;&#24773;&#24863;&#20998;&#26512;)&#21487;&#33021;&#38750;&#24120;&#36866;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20219;&#21153;&#65292;&#36825;&#31181;&#26500;&#36896;&#21487;&#33021;&#26159;&#19981;&#24688;&#24403;&#30340;&#65292;&#22240;&#20026;&#23427;&#36171;&#20104;&#20102;&#25152;&#26377;&#20851;&#20110;&#24615;&#21035;&#27495;&#35270;&#25110;&#31181;&#26063;&#20027;&#20041;&#31561;&#35758;&#39064;&#30340;&#31435;&#22330;&#30456;&#21516;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#20167;&#24680;&#21644;&#20882;&#29359;&#30340;&#28151;&#28102;&#21487;&#33021;&#20250;&#20351;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#32467;&#26524;&#26080;&#25928;&#65292;&#24182;&#21628;&#21505;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#32622;&#20110;&#29702;&#35770;&#26694;&#26550;&#20013;&#65292;&#23558;&#20167;&#24680;&#19982;&#20854;&#27491;&#20132;&#27010;&#24565;&#20882;&#29359;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02268v1 Announce Type: cross  Abstract: Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator's view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis. However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02249</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Sequence-to-Sequence Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#29983;&#25104;&#39044;&#27979;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#24310;&#36831;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24182;&#34892;&#35299;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;Query-CTC&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;&#38480;&#21046;&#22312;&#26465;&#20214;&#20998;&#24067;&#19978;&#12290;&#32467;&#26524;&#27169;&#22411;NARVL&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#26356;&#24555;&#65292;&#20174;&#19982;&#39034;&#24207;&#29983;&#25104;&#26631;&#35760;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20943;&#23569;&#21040;&#24120;&#37327;&#26102;&#38388;&#32852;&#21512;&#25512;&#29702;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02249v1 Announce Type: cross  Abstract: Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#29992;&#20110;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;&#35843;&#24230;&#30340;&#26356;&#22909;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.02243</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#26356;&#22909;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Better Schedules for Low Precision Training of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#29992;&#20110;&#20302;&#31934;&#24230;&#35757;&#32451;&#30340;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;&#35843;&#24230;&#30340;&#26356;&#22909;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#25216;&#26415;&#65292;&#20294;&#24490;&#29615;&#31934;&#24230;&#35757;&#32451;(CPT)&#65292;&#26681;&#25454;&#24490;&#29615;&#35843;&#24230;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31934;&#24230;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25928;&#29575;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;DNN&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;CPT&#23454;&#29616;&#37319;&#29992;&#24120;&#35265;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#65288;&#20363;&#22914;&#65292;&#21608;&#26399;&#20313;&#24358;&#35843;&#24230;&#65289;&#65292;&#24182;&#22312;&#20302;&#31934;&#24230;&#35757;&#32451;&#20013;&#20351;&#29992;&#23427;&#20204;&#65292;&#20294;&#26410;&#19982;&#20854;&#20182;&#35843;&#24230;&#36873;&#39033;&#36827;&#34892;&#20805;&#20998;&#27604;&#36739;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#22871;&#22810;&#26679;&#21270;&#30340;CPT&#35843;&#24230;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#21508;&#31181;DNN&#35757;&#32451;&#26041;&#26696;&#20013;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#20302;&#31934;&#24230;&#35757;&#32451;&#25991;&#29486;&#20013;&#23578;&#26410;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65289;&#12290;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#25552;&#20379;&#36827;&#19968;&#27493;&#25552;&#21319;&#35757;&#32451;&#25928;&#29575;&#30340;&#26367;&#20195;CPT&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02243v1 Announce Type: cross  Abstract: Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout train- ing according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine sched- ules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.02238</link><description>&lt;p&gt;
&#26397;&#21521;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#31649;&#29702;&#65306;5G&#26680;&#24515;&#32593;&#32476;&#20013;&#29992;&#20110;&#24847;&#22270;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;ML/AI&#65289;&#19982;&#31532;&#20116;&#20195;&#65288;5G&#65289;&#32593;&#32476;&#30340;&#25972;&#21512;&#65292;&#20984;&#26174;&#20986;&#20102;&#32593;&#32476;&#26234;&#33021;&#19982;&#24403;&#21069;&#21644;&#19979;&#19968;&#20195;&#35774;&#22791;&#23545;&#36234;&#26469;&#36234;&#20005;&#26684;&#30340;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290; &#36825;&#31181;&#21521;&#26222;&#36941;&#26234;&#33021;&#30340;&#36807;&#28193;&#35201;&#27714;&#29992;&#25143;&#21644;&#32593;&#32476;&#36816;&#33829;&#21830;&#20043;&#38388;&#39640;&#24230;&#30340;&#36830;&#25509;&#24615;&#12289;&#21516;&#27493;&#24615;&#21644;&#31471;&#21040;&#31471;&#30340;&#36890;&#20449;&#65292;&#24182;&#23558;&#20026;&#23454;&#29616;&#23436;&#20840;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#20840;&#33258;&#21160;&#32593;&#32476;&#38138;&#24179;&#36947;&#36335;&#12290;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#26159;&#20943;&#23569;&#20154;&#31867;&#34892;&#20026;&#12289;&#35282;&#33394;&#21644;&#36131;&#20219;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21516;&#26102;&#36716;&#21521;&#33258;&#21160;&#21270;&#32593;&#32476;&#31649;&#29702;&#30340;&#26032;&#22411;&#25552;&#21462;&#21644;&#35299;&#37322;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#20026;5G&#21644;&#19979;&#19968;&#20195;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#24320;&#21457;&#33258;&#23450;&#20041;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;LLM&#21457;&#23637;&#21644;&#38598;&#25104;&#30340;&#35265;&#35299;&#65292;&#20197;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#32593;&#32476;&#20013;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#24847;&#22270;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02238v1 Announce Type: cross  Abstract: The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices. This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without human intervention. Intent-based networking is a key factor in the reduction of human actions, roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management. This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network in
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02232</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#23545;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#35752;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;&#20351;&#29992;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#12290;&#26088;&#22312;&#36890;&#36807;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#32531;&#35299;&#23041;&#32961;&#26469;&#25512;&#36827;&#32593;&#32476;&#23433;&#20840;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38598;&#25104;&#21644;&#38750;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;TF-IDF&#34920;&#31034;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
&lt;/p&gt;</description></item><item><title>&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#21338;&#24328;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02227</link><description>&lt;p&gt;
&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Policy Space Response Oracles: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02227
&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#21338;&#24328;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#21644;&#25552;&#39640;&#25928;&#29575;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21338;&#24328;&#35770;&#20013;&#65292;&#28216;&#25103;&#25351;&#30340;&#26159;&#29702;&#24615;&#20915;&#31574;&#32773;&#25110;&#29609;&#23478;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#22411;&#65292;&#20182;&#20204;&#36890;&#36807;&#36873;&#25321;&#26469;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#12290;&#20102;&#35299;&#20182;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#36890;&#24120;&#34987;&#31216;&#20026;&#21338;&#24328;&#25512;&#29702;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#28216;&#25103;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#21338;&#24328;&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#25919;&#31574;&#31354;&#38388;&#21709;&#24212;&#31070;&#35861;&#65288;PSRO&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#28608;&#21457;PSRO&#30340;&#21160;&#26426;&#65292;&#25552;&#20379;&#21382;&#21490;&#32972;&#26223;&#65292;&#24182;&#23558;PSRO&#23450;&#20301;&#22312;&#21338;&#24328;&#25512;&#29702;&#26041;&#27861;&#20013;&#12290;&#28982;&#21518;&#25105;&#20204;&#20851;&#27880;PSRO&#30340;&#31574;&#30053;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#26377;&#25928;&#32452;&#21512;&#31574;&#30053;&#32452;&#21512;&#20197;&#22312;&#27169;&#25311;&#28508;&#22312;&#28216;&#25103;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#30446;&#21069;&#29992;&#20110;&#22686;&#24378;PSRO&#25928;&#29575;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25506;&#35752;&#20102;PSRO&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02227v1 Announce Type: cross  Abstract: In game theory, a game refers to a model of interaction among rational decision-makers or players, making choices with the goal of achieving their individual objectives. Understanding their behavior in games is often referred to as game reasoning. This survey provides a comprehensive overview of a fast-developing game-reasoning framework for large games, known as Policy Space Response Oracles (PSRO). We first motivate PSRO, provide historical context, and position PSRO within game-reasoning approaches. We then focus on the strategy exploration issue for PSRO, the challenge of assembling an effective strategy portfolio for modeling the underlying game with minimum computational cost. We also survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.02178</link><description>&lt;p&gt;
&#25513;&#38754;&#24605;&#24819;:&#31616;&#21333;&#22320;&#25513;&#30422;&#37096;&#20998;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02178
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#36731;&#24494;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#38543;&#26426;&#25513;&#30422;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#36825;&#31181;&#25216;&#26415;&#23545;&#25512;&#29702;&#20219;&#21153;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;</title><link>https://arxiv.org/abs/2403.02167</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#35782;&#21035;&#35821;&#38899;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition from voice messages recorded in the wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02167
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#34920;&#28436;&#25110;&#24341;&#21457;&#30340;&#35821;&#38899;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Emotional Voice Messages&#65288;EMOVOME&#65289;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#22312;&#28040;&#24687;&#24212;&#29992;&#20013;&#30340;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#30001;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#26631;&#27880;&#32773;&#20197;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#24773;&#24863;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;eGeMAPS&#29305;&#24449;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#35762;&#35805;&#32773;&#26080;&#20851;&#30340;SER&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#21442;&#32771;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20998;&#26512;&#20102;&#26631;&#27880;&#32773;&#21644;&#24615;&#21035;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#30340;Unispeech-L&#27169;&#22411;&#21450;&#20854;&#19982;eGeMAPS&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#22312;3&#31867;valence&#21644;arousal&#39044;&#27979;&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;61.64%&#21644;55.57%&#30340;Unweighted Accuracy&#65288;UA&#65289;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;&#23545;&#20110;&#24773;&#24863;&#31867;&#21035;&#65292;&#33719;&#24471;&#20102;42.58%&#30340;UA&#12290;EMOVOME&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#65292;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.02164</link><description>&lt;p&gt;
&#35748;&#30693;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999; - &#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#19979;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cognition is All You Need - The Next Layer of AI Above Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#65292;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#30740;&#31350;&#65292;&#27604;&#22914;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#30693;&#35782;&#24037;&#20316;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#19982;&#25512;&#29702;&#21644;&#22810;&#27493;&#38382;&#39064;&#35299;&#20915;&#30456;&#20851;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#21644;&#20043;&#22806;&#23454;&#29616;&#20197;&#31243;&#24207;&#23450;&#20041;&#30340;&#31070;&#32463;&#31526;&#21495;&#35748;&#30693;&#30340;&#39640;&#32423;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35748;&#30693;&#20154;&#24037;&#26234;&#33021;&#30340;&#21452;&#23618;&#21151;&#33021;&#26550;&#26500;&#65292;&#20316;&#20026;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#22810;&#27493;&#30693;&#35782;&#24037;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02164v1 Announce Type: new  Abstract: Recent studies of the applications of conversational AI tools, such as chatbots powered by large language models, to complex real-world knowledge work have shown limitations related to reasoning and multi-step problem solving. Specifically, while existing chatbots simulate shallow reasoning and understanding they are prone to errors as problem complexity increases. The failure of these systems to address complex knowledge work is due to the fact that they do not perform any actual cognition. In this position paper, we present Cognitive AI, a higher-level framework for implementing programmatically defined neuro-symbolic cognition above and outside of large language models. Specifically, we propose a dual-layer functional architecture for Cognitive AI that serves as a roadmap for AI systems that can perform complex multi-step knowledge work. We propose that Cognitive AI is a necessary precursor for the evolution of higher forms of AI, suc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02131</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#65306;&#20197;&#24494;&#20998;&#36827;&#21270;&#20026;&#20363;&#30340;&#21407;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#31639;&#27861;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#65292;&#22914;&#24494;&#20998;&#36827;&#21270;&#65292;&#22312;&#35299;&#20915;&#23454;&#25968;&#21442;&#25968;&#20248;&#21270;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19981;&#21516;&#38382;&#39064;&#23454;&#20363;&#19978;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#31639;&#27861;&#36873;&#25321;&#25110;&#37197;&#32622;&#26041;&#38754;&#25237;&#20837;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19968;&#32452;&#31639;&#27861;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#20026;&#29305;&#23450;&#38382;&#39064;&#21160;&#24577;&#35843;&#24230;&#23427;&#20204;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#26694;&#26550;&#26469;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#24577;&#31639;&#27861;&#36873;&#25321;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#31574;&#30053;&#26799;&#24230;&#26041;&#24335;&#35757;&#32451;&#20195;&#29702;&#36873;&#25321;&#26681;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#20351;&#20195;&#29702;&#20855;&#22791;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#20010;&#32463;&#36807;&#28145;&#24605;&#29087;&#34385;&#30340;&#26223;&#35266;&#21644;&#31639;&#27861;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02131v1 Announce Type: cross  Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorith
&lt;/p&gt;</description></item><item><title>LOCR&#26159;&#19968;&#31181;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#20013;&#22312;transformer&#26550;&#26500;&#20013;&#38598;&#25104;&#20301;&#32622;&#24341;&#23548;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23398;&#26415;&#25991;&#26723;&#20013;&#30340;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.02127</link><description>&lt;p&gt;
LOCR&#65306;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#20301;&#32622;&#24341;&#23548;Transformer
&lt;/p&gt;
&lt;p&gt;
LOCR: Location-Guided Transformer for Optical Character Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02127
&lt;/p&gt;
&lt;p&gt;
LOCR&#26159;&#19968;&#31181;&#38754;&#21521;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33258;&#22238;&#24402;&#36807;&#31243;&#20013;&#22312;transformer&#26550;&#26500;&#20013;&#38598;&#25104;&#20301;&#32622;&#24341;&#23548;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#23398;&#26415;&#25991;&#26723;&#20013;&#30340;&#37325;&#22797;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#26723;&#20805;&#26021;&#30528;&#25991;&#26412;&#12289;&#26041;&#31243;&#24335;&#12289;&#34920;&#26684;&#21644;&#22270;&#24418;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#25165;&#33021;&#20934;&#30830;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#12290;&#23613;&#31649;&#31471;&#21040;&#31471;OCR&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#22522;&#20110;&#24067;&#23616;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#22788;&#29702;&#37325;&#22797;&#24615;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#8220;&#39046;&#22495;&#22806;&#8221;&#65288;OOD&#65289;&#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#24067;&#23616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LOCR&#65292;&#19968;&#31181;&#23558;&#20301;&#32622;&#24341;&#23548;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;125K&#20010;&#23398;&#26415;&#25991;&#26723;&#39029;&#38754;&#30340;&#36229;&#36807;7700&#19975;&#20010;&#25991;&#26412;-&#20301;&#32622;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#21253;&#25324;&#21333;&#35789;&#12289;&#34920;&#26684;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#36793;&#30028;&#26694;&#12290;LOCR&#33021;&#29087;&#32451;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#20803;&#32032;&#24182;&#20197;Markdown&#35821;&#35328;&#29983;&#25104;&#20869;&#23481;&#12290;&#22312;&#25105;&#20204;&#20174;arXiv&#26500;&#24314;&#30340;&#27979;&#35797;&#38598;&#20013;&#65292;&#34913;&#37327;&#26041;&#24335;&#20026;&#32534;&#36753;&#36317;&#31163;&#12289;BLEU&#12289;METEOR&#21644;F-measure&#65292;LOCR&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;LOCR&#36824;&#23558;&#37325;&#22797;&#39057;&#29575;&#20174;4
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02127v1 Announce Type: cross  Abstract: Academic documents are packed with texts, equations, tables, and figures, requiring comprehensive understanding for accurate Optical Character Recognition (OCR). While end-to-end OCR methods offer improved accuracy over layout-based approaches, they often grapple with significant repetition issues, especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this issue, we propose LOCR, a model that integrates location guiding into the transformer architecture during autoregression. We train the model on a dataset comprising over 77M text-location pairs from 125K academic document pages, including bounding boxes for words, tables and mathematical symbols. LOCR adeptly handles various formatting elements and generates content in Markdown language. It outperforms all existing methods in our test set constructed from arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also reduces repetition frequency from 4
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#25506;&#32034;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02121</link><description>&lt;p&gt;
&#22312;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#20013;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#21487;&#34892;&#24615;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24369;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#36827;&#34892;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#25506;&#32034;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#25512;&#21160;&#20102;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#26631;&#27880;&#21644;&#35757;&#32451;&#37117;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#26368;&#36817;&#65292;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#25104;&#20026;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;&#22312;&#28151;&#21512;&#20195;&#30721;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#38382;&#39064;&#39046;&#22495;&#65292;LLM&#30340;&#20351;&#29992;&#22312;&#36825;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;100&#26465;YouTube&#35780;&#35770;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#28151;&#21512;&#20195;&#30721;&#30340;&#21360;&#22320;&#35821;&#20013;&#30340;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#24615;&#21035;&#27495;&#35270;&#36827;&#34892;&#20102;&#24369;&#26631;&#27880;&#12290;&#30001;&#20110;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#27880;&#37322;&#65292;&#22240;&#27492;&#37319;&#29992;&#20102;&#24369;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#20102;&#38646;&#27425;&#23398;&#20064;&#12289;&#19968;&#27425;&#23398;&#20064;&#21644;&#23569;&#27425;&#23398;&#20064;&#20197;&#21450;&#25552;&#31034;&#26041;&#27861;&#26469;&#20026;&#35780;&#35770;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#24037;&#26631;&#35760;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02121v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has advanced the benchmark in various Natural Language Processing (NLP) tasks. However, large amounts of labelled training data are required to train LLMs. Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and few-shot learning have recently emerged as viable options for labelling data using large pre-trained models. Hate speech detection in mix-code low-resource languages is an active problem area where the use of LLMs has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. Zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have then been applied to assign labels to the comments and compare them to human-ass
&lt;/p&gt;</description></item><item><title>&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.02118</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#38754;&#21521;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#38544;&#24335;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Towards Implicit Prompt For Text-To-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#20301;&#32622;&#35770;&#25991;&#35752;&#35770;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#26041;&#38754;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#21517;&#20026;ImplicitBench&#30340;&#26032;&#22522;&#20934;&#65292;&#24182;&#23545; T2I &#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#34920;&#29616;&#21450;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26174;&#24335;&#25552;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#38544;&#24335;&#25552;&#31034;&#65288;&#26263;&#31034;&#30446;&#26631;&#32780;&#19981;&#26126;&#30830;&#25552;&#21040;&#65289;&#12290;&#36825;&#20123;&#25552;&#31034;&#21487;&#33021;&#28040;&#38500;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24212;&#29992;&#26500;&#25104;&#28508;&#22312;&#23041;&#32961;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24403;&#19979;T2I&#27169;&#22411;&#26397;&#30528;&#38544;&#24335;&#25552;&#31034;&#30340;&#29616;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImplicitBench&#30340;&#22522;&#20934;&#65292;&#24182;&#23545;&#27969;&#34892;&#30340;T2I&#27169;&#22411;&#22312;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#24615;&#33021;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25910;&#38598;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#36229;&#36807;2,000&#20010;&#38544;&#24335;&#25552;&#31034;&#65306;&#36890;&#29992;&#31526;&#21495;&#12289;&#21517;&#20154;&#38544;&#31169;&#21644;&#19981;&#23433;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20845;&#20010;&#30693;&#21517;T2I&#27169;&#22411;&#22312;&#36825;&#20123;&#38544;&#24335;&#25552;&#31034;&#19979;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#65288;1&#65289;T2I&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#21019;&#24314;&#21508;&#31181;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02118v1 Announce Type: cross  Abstract: Recent text-to-image (T2I) models have had great success, and many benchmarks have been proposed to evaluate their performance and safety. However, they only consider explicit prompts while neglecting implicit prompts (hint at a target without explicitly mentioning it). These prompts may get rid of safety constraints and pose potential threats to the applications of these models. This position paper highlights the current state of T2I models toward implicit prompts. We present a benchmark named ImplicitBench and conduct an investigation on the performance and impacts of implicit prompts with popular T2I models. Specifically, we design and collect more than 2,000 implicit prompts of three aspects: General Symbols, Celebrity Privacy, and Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models' capabilities under these implicit prompts. Experiment results show that (1) T2I models are able to accurately create various targe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#38646;&#35843;&#20248;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#26041;&#27861;VTG-GPT&#65292;&#36890;&#36807;&#29983;&#25104;&#26080;&#20559;&#26597;&#35810;&#21644;&#26356;&#31934;&#30830;&#30340;&#35270;&#35273;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.02076</link><description>&lt;p&gt;
VTG-GPT&#65306;&#20351;&#29992;GPT&#23454;&#29616;&#20813;&#35843;&#20248;&#38646;&#26679;&#26412;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#38646;&#35843;&#20248;&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#26041;&#27861;VTG-GPT&#65292;&#36890;&#36807;&#29983;&#25104;&#26080;&#20559;&#26597;&#35810;&#21644;&#26356;&#31934;&#30830;&#30340;&#35270;&#35273;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26102;&#38388;&#23450;&#20301;&#65288;VTG&#65289;&#26088;&#22312;&#26681;&#25454;&#35821;&#35328;&#26597;&#35810;&#20174;&#26410;&#32463;&#21098;&#36753;&#30340;&#35270;&#39057;&#20013;&#23450;&#20301;&#29305;&#23450;&#30340;&#26102;&#38388;&#27573;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;VTG&#27169;&#22411;&#37117;&#26159;&#22312;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#35270;&#39057;&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#20165;&#24341;&#20837;&#20102;&#26469;&#33258;&#26597;&#35810;&#30340;&#20154;&#20026;&#20559;&#35265;&#65292;&#36824;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VTG-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#30340;&#38646;&#35843;&#20248;VTG&#26041;&#27861;&#12290;&#20026;&#20102;&#20943;&#23569;&#21407;&#22987;&#26597;&#35810;&#20013;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#20351;&#29992;Baichuan2&#29983;&#25104;&#26080;&#20559;&#26597;&#35810;&#12290;&#20026;&#20102;&#20943;&#23569;&#35270;&#39057;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#25105;&#20204;&#24212;&#29992;MiniGPT-v2&#23558;&#35270;&#35273;&#20869;&#23481;&#36716;&#25442;&#20026;&#26356;&#31934;&#30830;&#30340;&#23383;&#24149;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#26696;&#29983;&#25104;&#22120;&#21644;&#21518;&#22788;&#29702;&#26469;&#20174;&#26080;&#20559;&#26597;&#35810;&#21644;&#22270;&#20687;&#23383;&#24149;&#20013;&#29983;&#25104;&#20934;&#30830;&#30340;&#27573;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;VTG-GPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#26126;&#26174;&#20248;&#20110;SOTA&#26041;&#27861;&#65292;&#24182;&#36229;&#36234;&#20102;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02076v1 Announce Type: cross  Abstract: Video temporal grounding (VTG) aims to locate specific temporal segments from an untrimmed video based on a linguistic query. Most existing VTG models are trained on extensive annotated video-text pairs, a process that not only introduces human biases from the queries but also incurs significant computational costs. To tackle these challenges, we propose VTG-GPT, a GPT-based method for zero-shot VTG without training or fine-tuning. To reduce prejudice in the original query, we employ Baichuan2 to generate debiased queries. To lessen redundant information in videos, we apply MiniGPT-v2 to transform visual content into more precise captions. Finally, we devise the proposal generator and post-processing to produce accurate segments from debiased queries and image captions. Extensive experiments demonstrate that VTG-GPT significantly outperforms SOTA methods in zero-shot settings and surpasses unsupervised approaches. More notably, it achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24577;&#24863;&#30693;&#21644;&#20301;&#31227;&#28151;&#21512;&#22120;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#22312;&#33041;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02074</link><description>&lt;p&gt;
&#27169;&#24577;&#24863;&#30693;&#21644;&#20301;&#31227;&#28151;&#21512;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24577;&#24863;&#30693;&#21644;&#20301;&#31227;&#28151;&#21512;&#22120;&#65292;&#29992;&#20110;&#34701;&#21512;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#23454;&#29616;&#22312;&#33041;&#32959;&#30244;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#22270;&#20687;&#36827;&#34892;&#32452;&#21512;&#26377;&#21033;&#20110;&#25506;&#32034;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#20449;&#24687;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#24577;&#24863;&#30693;&#21644;&#20301;&#31227;&#28151;&#21512;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#32780;&#31283;&#20581;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#65292;&#35813;&#28151;&#21512;&#22120;&#25972;&#21512;&#20102;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#20869;&#27169;&#24577;&#21644;&#20132;&#21449;&#27169;&#24577;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#24863;&#30693;&#27169;&#22359;&#65292;&#26681;&#25454;&#31070;&#32463;&#24433;&#20687;&#30740;&#31350;&#26469;&#24314;&#27169;&#20302;&#23618;&#27425;&#30340;&#29305;&#23450;&#27169;&#24577;&#23545;&#20851;&#31995;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#29305;&#23450;&#39532;&#36187;&#20811;&#22270;&#26696;&#30340;&#27169;&#24577;&#20301;&#31227;&#27169;&#22359;&#65292;&#20197;&#25506;&#32034;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02074v1 Announce Type: cross  Abstract: Combining images from multi-modalities is beneficial to explore various information in computer vision, especially in the medical domain. As an essential part of clinical diagnosis, multi-modal brain tumor segmentation aims to delineate the malignant entity involving multiple modalities. Although existing methods have shown remarkable performance in the task, the information exchange for cross-scale and high-level representations fusion in spatial and modality are limited in these methods. In this paper, we present a novel Modality Aware and Shift Mixer that integrates intra-modality and inter-modality dependencies of multi-modal images for effective and robust brain tumor segmentation. Specifically, we introduce a Modality-Aware module according to neuroimaging studies for modeling the specific modality pair relationships at low levels, and a Modality-Shift module with specific mosaic patterns is developed to explore the complex relat
&lt;/p&gt;</description></item><item><title>LEO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#38646;-shot&#20248;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#30446;&#26631;&#21644;&#39640;&#32500;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#24819;&#35937;&#21147;&#21644;&#24187;&#35273;&#20542;&#21521;&#38656;&#35201;&#35880;&#24910;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.02054</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#20248;&#21270;&#22120;&#65306;&#31934;&#33521;&#20027;&#20041;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02054
&lt;/p&gt;
&lt;p&gt;
LEO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#20248;&#21270;&#22120;&#65292;&#20855;&#26377;&#38646;-shot&#20248;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#30446;&#26631;&#21644;&#39640;&#32500;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#24819;&#35937;&#21147;&#21644;&#24187;&#35273;&#20542;&#21521;&#38656;&#35201;&#35880;&#24910;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20854;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#24212;&#29992;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#22768;&#31216;LLMs&#20855;&#26377;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#38646;-shot&#20248;&#21270;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#21644;&#39640;&#32500;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#20540;&#20248;&#21270;&#30340;&#26032;&#22411;&#22522;&#20110;&#31181;&#32676;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#21270;&#20248;&#21270;&#22120;(LEO)&#12290;&#36890;&#36807;&#36328;&#22522;&#20934;&#21644;&#24037;&#19994;&#24037;&#31243;&#38382;&#39064;&#30340;&#25968;&#20540;&#31034;&#20363;&#65292;&#22914;&#36229;&#38899;&#36895;&#21943;&#31649;&#24418;&#29366;&#20248;&#21270;&#12289;&#28909;&#20256;&#36882;&#21644;&#39118;&#22330;&#24067;&#23616;&#20248;&#21270;&#65292;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#34429;&#28982;LLMs&#20135;&#29983;&#30340;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#65292;&#20294;&#23427;&#20204;&#30340;&#24819;&#20687;&#33021;&#21147;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#38656;&#35201;&#35880;&#24910;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#33719;&#24471;&#21487;&#38752;&#31572;&#26696;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02054v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#25506;&#35752;&#20102;&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#34892;&#20026;&#21644;&#26368;&#20808;&#36827;AI&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#24433;&#21709;&#39550;&#39542;&#34892;&#20026;&#30340;&#21313;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.02053</link><description>&lt;p&gt;
&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#34892;&#20026;&#21450;&#26368;&#26032;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Scoping Review of Energy-Efficient Driving Behaviors and Applied State-of-the-Art AI Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#22260;&#23457;&#26597;&#25506;&#35752;&#20102;&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#34892;&#20026;&#21644;&#26368;&#20808;&#36827;AI&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#24433;&#21709;&#39550;&#39542;&#34892;&#20026;&#30340;&#21313;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#34892;&#19994;&#20173;&#28982;&#26159;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#29702;&#35299;&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#34892;&#20026;&#21644;&#24212;&#29992;&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#31574;&#30053;&#23545;&#20110;&#20943;&#23569;&#27773;&#36710;&#29123;&#26009;&#28040;&#32791;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#33021;&#28304;&#39640;&#25928;&#39550;&#39542;&#34892;&#20026;&#21644;&#31574;&#30053;&#23578;&#26080;&#32508;&#21512;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24050;&#24212;&#29992;&#20110;&#20998;&#26512;&#29615;&#20445;&#39550;&#39542;&#39118;&#26684;&#65292;&#20294;&#23578;&#26080;&#27010;&#36848;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#23545;&#29983;&#24577;&#39550;&#39542;&#34892;&#20026;&#21644;&#39118;&#26684;&#36827;&#34892;&#20102;&#24443;&#24213;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#33021;&#28304;&#28040;&#32791;&#30340;&#39550;&#39542;&#22240;&#32032;&#21644;&#26368;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#36807;&#31243;&#65292;&#23545;&#26041;&#27861;&#35770;&#21644;&#30456;&#20851;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#24433;&#21709;&#39550;&#39542;&#34892;&#20026;&#30340;&#22240;&#32032;&#21487;&#20197;&#24635;&#32467;&#20026;&#21253;&#25324;&#36895;&#24230;&#12289;&#21152;&#36895;&#12289;&#20943;&#36895;&#12289;&#36367;&#26495;&#31561;&#21313;&#19968;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02053v1 Announce Type: new  Abstract: The transportation sector remains a major contributor to greenhouse gas emissions. The understanding of energy-efficient driving behaviors and utilization of energy-efficient driving strategies are essential to reduce vehicles' fuel consumption. However, there is no comprehensive investigation into energy-efficient driving behaviors and strategies. Furthermore, many state-of-the-art AI models have been applied for the analysis of eco-friendly driving styles, but no overview is available. To fill the gap, this paper conducts a thorough literature review on ecological driving behaviors and styles and analyzes the driving factors influencing energy consumption and state-of-the-art methodologies. With a thorough scoping review process, the methodological and related data are compared. The results show that the factors that impact driving behaviors can be summarized into eleven features including speed, acceleration, deceleration, pedal, and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#31574;&#30053;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.02018</link><description>&lt;p&gt;
&#36328;&#22495;&#31574;&#30053;&#36716;&#31227;&#19982;&#25928;&#26524;&#24490;&#29615;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross Domain Policy Transfer with Effect Cycle-Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02018
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#31574;&#30053;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#26426;&#22120;&#20154;&#31574;&#30053;&#21487;&#33021;&#22240;&#20026;&#26679;&#26412;&#25928;&#29575;&#20302;&#32780;&#25104;&#26412;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#23558;&#22312;&#28304;&#22495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#21464;&#24471;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30456;&#20284;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#19981;&#21516;&#30340;&#22495;&#19978;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#22312;&#20110;&#20855;&#26377;&#19981;&#21516;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#39046;&#22495;&#65292;&#36825;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#23454;&#38469;&#24847;&#20041;&#65292;&#21363;&#20174;&#26426;&#22120;&#20154;A&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;B&#30340;&#31574;&#30053;&#36716;&#31227;&#12290;&#19982;&#20381;&#36182;&#37197;&#23545;&#25968;&#25454;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26410;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;&#36328;&#39046;&#22495;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25928;&#26524;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#36890;&#36807;&#23545;&#31216;&#20248;&#21270;&#32467;&#26500;&#26469;&#23545;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#36807;&#28193;&#25928;&#26524;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#32780;&#23398;&#20064;&#36825;&#20123;&#26144;&#23556;&#20989;&#25968;&#12290;&#19968;&#26086;&#26144;&#23556;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02018v1 Announce Type: cross  Abstract: Training a robotic policy from scratch using deep reinforcement learning methods can be prohibitively expensive due to sample inefficiency. To address this challenge, transferring policies trained in the source domain to the target domain becomes an attractive paradigm. Previous research has typically focused on domains with similar state and action spaces but differing in other aspects. In this paper, our primary focus lies in domains with different state and action spaces, which has broader practical implications, i.e. transfer the policy from robot A to robot B. Unlike prior methods that rely on paired data, we propose a novel approach for learning the mapping functions between state and action spaces across domains using unpaired data. We propose effect cycle consistency, which aligns the effects of transitions across two domains through a symmetrical optimization structure for learning these mapping functions. Once the mapping fun
&lt;/p&gt;</description></item><item><title>VulnScopper&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#36719;&#20214;&#28431;&#27934;&#20998;&#26512;&#65292;&#26377;&#25928;&#22788;&#29702;&#26410;&#35265;&#23454;&#20307;&#65292;&#24182;&#22312;NVD&#21644;Red Hat CVE&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02014</link><description>&lt;p&gt;
&#25581;&#31034;&#26410;&#35265;&#23433;&#20840;&#23454;&#20307;&#20043;&#38388;&#38544;&#34255;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Unveiling Hidden Links Between Unseen Security Entities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02014
&lt;/p&gt;
&lt;p&gt;
VulnScopper&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#36719;&#20214;&#28431;&#27934;&#20998;&#26512;&#65292;&#26377;&#25928;&#22788;&#29702;&#26410;&#35265;&#23454;&#20307;&#65292;&#24182;&#22312;NVD&#21644;Red Hat CVE&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28431;&#27934;&#30340;&#19981;&#26029;&#22686;&#22810;&#32473;&#23433;&#20840;&#25968;&#25454;&#24211;&#21644;&#20998;&#26512;&#20154;&#21592;&#21450;&#26102;&#35782;&#21035;&#12289;&#20998;&#31867;&#21644;&#20462;&#22797;&#28431;&#27934;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VulnScopper&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#65292;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;&#36719;&#20214;&#28431;&#27934;&#20998;&#26512;&#12290;VulnScopper&#21033;&#29992;ULTRA&#20316;&#20026;&#30693;&#35782;&#22270;&#35889;&#22522;&#30784;&#27169;&#22411;&#65292;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26377;&#25928;&#22788;&#29702;&#26410;&#35265;&#23454;&#20307;&#65292;&#20811;&#26381;&#20102;&#20197;&#24448;&#30693;&#35782;&#22270;&#35889;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#23433;&#20840;&#25968;&#25454;&#38598;&#65292;&#21363;NVD&#21644;&#32418;&#24125;CVE&#25968;&#25454;&#24211;&#19978;&#35780;&#20272;&#20102;VulnScopper&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02014v1 Announce Type: cross  Abstract: The proliferation of software vulnerabilities poses a significant challenge for security databases and analysts tasked with their timely identification, classification, and remediation. With the National Vulnerability Database (NVD) reporting an ever-increasing number of vulnerabilities, the traditional manual analysis becomes untenably time-consuming and prone to errors. This paper introduces VulnScopper, an innovative approach that utilizes multi-modal representation learning, combining Knowledge Graphs (KG) and Natural Language Processing (NLP), to automate and enhance the analysis of software vulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined with a Large Language Model (LLM), VulnScopper effectively handles unseen entities, overcoming the limitations of previous KG approaches. We evaluate VulnScopper on two major security datasets, the NVD and the Red Hat CVE database. Our method significantly improves
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340; Transformer &#27169;&#22411;&#65292;&#21457;&#29616;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#26159;&#32763;&#35793;&#24615;&#33021;&#30340;&#26368;&#22823;&#39537;&#21160;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.01985</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21464;&#21387;&#22120;&#65306;Is F\'eidir Linn&#65281;
&lt;/p&gt;
&lt;p&gt;
Transformers for Low-Resource Languages:Is F\'eidir Linn!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340; Transformer &#27169;&#22411;&#65292;&#21457;&#29616;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#26159;&#32763;&#35793;&#24615;&#33021;&#30340;&#26368;&#22823;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#27169;&#22411;&#26159;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#31070;&#32463;&#32763;&#35793;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#35821;&#35328;&#23545;&#19978;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#65292;&#20351;&#29992;&#35813;&#32467;&#26500;&#36827;&#34892;&#23454;&#39564;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#20197;&#32763;&#35793;&#20302;&#36164;&#28304;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#21442;&#25968;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#27491;&#30830;&#36873;&#25321;&#23376;&#35789;&#27169;&#22411;&#34987;&#35777;&#26126;&#26159;&#32763;&#35793;&#24615;&#33021;&#26368;&#22823;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#35780;&#20272;&#20102;&#20351;&#29992; unigram &#21644; BPE &#26041;&#27861;&#30340; SentencePiece &#27169;&#22411;&#12290;&#23545;&#27169;&#22411;&#26550;&#26500;&#30340;&#21464;&#21270;&#21253;&#25324;&#20462;&#25913;&#23618;&#25968;&#12289;&#27979;&#35797;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#21450;&#35780;&#20272;&#29992;&#20110;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#22836;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01985v1 Announce Type: cross  Abstract: The Transformer model is the state-of-the-art in Machine Translation. However, in general, neural translation models often under perform on language pairs with insufficient training data. As a consequence, relatively few experiments have been carried out using this architecture on low-resource language pairs. In this study, hyperparameter optimization of Transformer models in translating the low-resource English-Irish language pair is evaluated. We demonstrate that choosing appropriate parameters leads to considerable performance improvements. Most importantly, the correct choice of subword model is shown to be the biggest driver of translation performance. SentencePiece models using both unigram and BPE approaches were appraised. Variations on model architectures included modifying the number of layers, testing various regularisation techniques and evaluating the optimal number of heads for attention. A generic 55k DGT corpus and an i
&lt;/p&gt;</description></item><item><title>TTA-Nav&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#65292;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28857;&#30446;&#26631;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01977</link><description>&lt;p&gt;
TTA-Nav: &#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#37325;&#24314;&#29992;&#20110;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#28857;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01977
&lt;/p&gt;
&lt;p&gt;
TTA-Nav&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#65292;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28857;&#30446;&#26631;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01977v1 &#20844;&#21578;&#31867;&#22411;: &#36328;  &#25688;&#35201;: &#22312;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TTA-Nav&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;&#25439;&#22351;&#19979;&#30340;&#28857;&#30446;&#26631;&#23548;&#33322;&#12290;&#25105;&#20204;&#30340;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#26041;&#27861;&#23558;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#19982;&#39044;&#35757;&#32451;&#30340;&#23548;&#33322;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#30340;&#23548;&#33322;&#27169;&#22411;&#25509;&#25910;&#19968;&#20010;&#25439;&#22351;&#30340;&#22270;&#20687;&#24182;&#25552;&#21462;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#26681;&#25454;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#30340;&#39640;&#32423;&#29305;&#24449;&#29983;&#25104;&#37325;&#24314;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#23558;&#25439;&#22351;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#39304;&#36865;&#22238;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20877;&#27425;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#20197;&#36755;&#20986;&#21160;&#20316;&#12290;&#23613;&#31649;&#20165;&#22312;&#28165;&#26224;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#33258;&#39030;&#21521;&#19979;&#30340;&#35299;&#30721;&#22120;&#21487;&#20197;&#20174;&#25439;&#22351;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#65292;&#26080;&#38656;&#22522;&#20110;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#12290;&#20855;&#26377;&#25105;&#20204;&#33258;&#39030;&#21521;&#19979;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#23548;&#33322;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01977v1 Announce Type: cross  Abstract: Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our "plug-and-play" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre-trained model. Then, it feeds the reconstruction of a corrupted image back to the pre-trained model. Finally, the pre-trained model does forward pass again to output action. Despite being trained solely on clean images, the top-down decoder can reconstruct cleaner images from corrupted ones without the need for gradient-based adaptation. The pre-trained navigation model with our top-down decoder significantly enhances navigation performance acr
&lt;/p&gt;</description></item><item><title>&#24847;&#22823;&#21033;&#23545;ChatGPT&#23454;&#26045;&#31105;&#20196;&#21518;&#65292;&#19981;&#21516;&#32463;&#39564;&#30340;&#29992;&#25143;&#29983;&#20135;&#21147;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#22312;&#30701;&#26399;&#20869;&#20135;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#22343;&#26377;&#25552;&#21319;&#65292;&#32780;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#29983;&#20135;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.01964</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24322;&#36136;&#29983;&#20135;&#21147;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
The Heterogeneous Productivity Effects of Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01964
&lt;/p&gt;
&lt;p&gt;
&#24847;&#22823;&#21033;&#23545;ChatGPT&#23454;&#26045;&#31105;&#20196;&#21518;&#65292;&#19981;&#21516;&#32463;&#39564;&#30340;&#29992;&#25143;&#29983;&#20135;&#21147;&#34920;&#29616;&#20986;&#24046;&#24322;&#65292;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#22312;&#30701;&#26399;&#20869;&#20135;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#22343;&#26377;&#25552;&#21319;&#65292;&#32780;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#29983;&#20135;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#24847;&#22823;&#21033;&#23545;ChatGPT&#65288;&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#30340;&#31105;&#20196;&#23545;&#20010;&#20154;&#29983;&#20135;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#24847;&#22823;&#21033;&#21450;&#20854;&#20182;&#27431;&#27954;&#22269;&#23478;&#36229;&#36807;36,000&#21517;GitHub&#29992;&#25143;&#30340;&#27599;&#26085;&#32534;&#30721;&#36755;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#25968;&#25454;&#65292;&#24182;&#23558;&#36825;&#20123;&#25968;&#25454;&#19982;&#35813;&#31105;&#20196;&#30340;&#31361;&#28982;&#23459;&#24067;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24046;&#24322;&#24615;&#24046;&#24322;&#26694;&#26550;&#12290;&#22312;&#21463;&#24433;&#21709;&#30340;&#24847;&#22823;&#21033;&#29992;&#25143;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#32463;&#39564;&#36739;&#23569;&#30340;&#29992;&#25143;&#65292;&#36755;&#20986;&#25968;&#37327;&#21644;&#36136;&#37327;&#30701;&#26399;&#20869;&#22686;&#21152;&#65292;&#32780;&#23545;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#22312;&#26356;&#24120;&#35268;&#30340;&#20219;&#21153;&#19978;&#29983;&#20135;&#21147;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01964v1 Announce Type: cross  Abstract: We analyse the individual productivity effects of Italy's ban on ChatGPT, a generative pretrained transformer chatbot. We compile data on the daily coding output quantity and quality of over 36,000 GitHub users in Italy and other European countries and combine these data with the sudden announcement of the ban in a difference-in-differences framework. Among the affected users in Italy, we find a short-term increase in output quantity and quality for less experienced users and a decrease in productivity on more routine tasks for experienced users.
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.01924</link><description>&lt;p&gt;
&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#65311;&#20851;&#20110;&#20154;&#24037;&#29615;&#22659;&#22312;&#21307;&#23398;&#24320;&#25918;&#22495;&#38382;&#31572;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;&#36817;&#26399;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#30693;&#35782;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#23545;&#25239;&#26550;&#26500;&#35268;&#27169;&#21270;&#65292;&#24182;&#20801;&#35768;&#22312;&#24120;&#35265;&#30340;&#20302;&#36164;&#28304;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26816;&#32034;&#28982;&#21518;&#38405;&#35835;&#30340;&#33539;&#24335;&#24050;&#21464;&#24471;&#26222;&#36941;&#65292;&#27169;&#22411;&#39044;&#27979;&#20381;&#36182;&#20110;&#26469;&#33258;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;PubMed&#12289;&#25945;&#31185;&#20070;&#21644;UMLS&#65289;&#30340;&#30456;&#20851;&#30693;&#35782;&#29255;&#27573;&#12290;&#21478;&#19968;&#26465;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#20294;&#30001;&#20110;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#21487;&#33021;&#30340;&#36335;&#24452;&#26159;&#36890;&#36807;&#25552;&#31034;&#26500;&#24314;&#20154;&#24037;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#8220;&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#8221;&#25104;&#20026;&#20102;&#29616;&#20195;&#29256;&#30340;&#21704;&#22982;&#38647;&#29305;&#22256;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#29983;&#25104;&#28982;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;MedQA-USMLE&#12289;MedMCQA&#21644;MMLU&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20551;&#35774;&#26368;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim
&lt;/p&gt;</description></item><item><title>xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01915</link><description>&lt;p&gt;
xT&#65306;&#29992;&#20110;&#22823;&#22270;&#20687;&#20013;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
xT: Nested Tokenization for Larger Context in Large Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01915
&lt;/p&gt;
&lt;p&gt;
xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#27969;&#27700;&#32447;&#20197;&#20004;&#31181;&#27425;&#20248;&#26041;&#24335;&#22788;&#29702;&#22823;&#22270;&#20687;&#65306;&#19979;&#37319;&#26679;&#25110;&#35009;&#21098;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#23548;&#33268;&#22270;&#20687;&#20013;&#20449;&#24687;&#21644;&#32972;&#26223;&#30340;&#20002;&#22833;&#12290;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#20840;&#23616;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#19982;&#39640;&#39057;&#32454;&#33410;&#19968;&#26679;&#65292;&#20363;&#22914;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#65307;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#24517;&#39035;&#20570;&#20986;&#33293;&#24323;&#21738;&#20123;&#20449;&#24687;&#30340;&#22256;&#25200;&#36873;&#25321;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;xT&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#35273;Transformer&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32858;&#21512;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#21487;&#20197;&#22312;&#24403;&#20195;GPU&#19978;&#31471;&#23545;&#31471;&#22320;&#23545;&#22823;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#32452;&#36328;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#35270;&#35273;&#27169;&#22411;&#29702;&#35299;&#30495;&#27491;&#22823;&#22411;&#22270;&#20687;&#24182;&#22312;&#22823;&#33539;&#22260;&#20869;&#34701;&#21512;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#19978;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#22823;&#22270;&#20687;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01915v1 Announce Type: cross  Abstract: Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. By introducing a nested tokenization scheme for large images in 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.01909</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#22522;&#20110;&#35821;&#20041;&#23545;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36880;&#20687;&#32032;&#26631;&#35760;&#22270;&#20687;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#20013;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#39318;&#27425;&#32508;&#21512;&#21644;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01909v1 Announce Type: cross  Abstract: Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;FCM-wDTW&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01895</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;FCM-wDTW&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#30456;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#29305;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#24191;&#27867;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#23545;&#22122;&#22768;&#25935;&#24863;&#12290;&#34429;&#28982;&#29616;&#26377;&#24037;&#20316;&#24050;&#32463;&#25506;&#35752;&#20102;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#20197;&#22686;&#24378;&#20854;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#20165;&#25903;&#25345;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#19978;&#30340;&#30417;&#30563;&#20219;&#21153;&#65292;&#32570;&#20047;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FCM-wDTW&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#36317;&#31163;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#21407;&#22987;&#25968;&#25454;&#32534;&#30721;&#25104;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20013;&#24515;&#25581;&#31034;&#27491;&#24120;&#32500;&#24230;&#20851;&#31995;&#12290;FCM-wDTW&#23558;&#23616;&#37096;&#21152;&#26435;DTW&#24341;&#20837;&#21040;&#27169;&#31946;C&#22343;&#20540;&#32858;&#31867;&#20013;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#36890;&#36807;&#25968;&#25454;&#37325;&#24314;&#23454;&#29616;&#24322;&#24120;&#35782;&#21035;&#12290;&#38024;&#23545;11&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01895v1 Announce Type: cross  Abstract: Distance-based time series anomaly detection methods are prevalent due to their relative non-parametric nature and interpretability. However, the commonly used Euclidean distance is sensitive to noise. While existing works have explored dynamic time warping (DTW) for its robustness, they only support supervised tasks over multivariate time series (MTS), leaving a scarcity of unsupervised methods. In this work, we propose FCM-wDTW, an unsupervised distance metric learning method for anomaly detection over MTS, which encodes raw data into latent space and reveals normal dimension relationships through cluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means clustering and learns the optimal latent space efficiently, enabling anomaly identification via data reconstruction. Experiments with 11 different types of benchmarks demonstrate our method's competitive accuracy and efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26723;&#20013;&#30340;&#20016;&#23500;&#35821;&#27861;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.01886</link><description>&lt;p&gt;
FCDS: &#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;
&lt;/p&gt;
&lt;p&gt;
FCDS: Fusing Constituency and Dependency Syntax into Document-Level Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26723;&#20013;&#30340;&#20016;&#23500;&#35821;&#27861;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26088;&#22312;&#35782;&#21035;&#21333;&#20010;&#25991;&#26723;&#20869;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#21477;&#27861;&#34701;&#21512;&#21040;DocRE&#20013;&#65292;&#21033;&#29992;&#30701;&#35821;&#32467;&#26500;&#32858;&#21512;&#25972;&#20010;&#21477;&#23376;&#20449;&#24687;&#24182;&#36873;&#25321;&#30446;&#26631;&#23545;&#30340;&#25351;&#23548;&#24615;&#21477;&#23376;&#65292;&#21033;&#29992;&#20381;&#23384;&#21477;&#27861;&#22312;&#22270;&#32467;&#26500;&#20013;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#26681;&#25454;&#20381;&#23384;&#22270;&#36873;&#25321;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01886v1 Announce Type: cross  Abstract: Document-level Relation Extraction (DocRE) aims to identify relation labels between entities within a single document. It requires handling several sentences and reasoning over them. State-of-the-art DocRE methods use a graph structure to connect entities across the document to capture dependency syntax information. However, this is insufficient to fully exploit the rich syntax information in the document. In this work, we propose to fuse constituency and dependency syntax into DocRE. It uses constituency syntax to aggregate the whole sentence information and select the instructive sentences for the pairs of targets. It exploits the dependency syntax in a graph structure with constituency syntax enhancement and chooses the path between entity pairs based on the dependency graph. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed method. The code is publicly available at this url.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23460;&#20869;&#22330;&#26223;&#30340;&#32467;&#26500;&#24863;&#30693;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#20122;&#29305;&#20848;&#22823;&#32467;&#26500;&#20272;&#35745;&#24179;&#38754;surfel&#21306;&#22495;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01861</link><description>&lt;p&gt;
AiSDF&#65306;&#23460;&#20869;&#22330;&#26223;&#20013;&#30340;&#32467;&#26500;&#24863;&#30693;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;
&lt;/p&gt;
&lt;p&gt;
AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23460;&#20869;&#22330;&#26223;&#30340;&#32467;&#26500;&#24863;&#30693;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#20122;&#29305;&#20848;&#22823;&#32467;&#26500;&#20272;&#35745;&#24179;&#38754;surfel&#21306;&#22495;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#29983;&#27963;&#30340;&#23460;&#20869;&#22330;&#26223;&#22312;&#35270;&#35273;&#19978;&#26159;&#22343;&#36136;&#25110;&#26080;&#32441;&#29702;&#30340;&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#24418;&#24335;&#65292;&#24182;&#20026;3D&#22330;&#26223;&#37325;&#24314;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#32467;&#26500;&#20808;&#39564;&#12290;&#21463;&#36825;&#19968;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23460;&#20869;&#22330;&#26223;&#29305;&#21035;&#26159;&#22312;&#20122;&#29305;&#20848;&#22823;&#19990;&#30028;&#65288;AW&#65289;&#20551;&#35774;&#19979;&#30340;&#32467;&#26500;&#24863;&#30693;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#37325;&#24314;&#26694;&#26550;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#22686;&#37327;SDF&#37325;&#24314;&#31216;&#20026;AiSDF&#12290;&#22312;&#22312;&#32447;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25512;&#26029;&#32473;&#23450;&#22330;&#26223;&#30340;&#28508;&#22312;&#20122;&#29305;&#20848;&#22823;&#32467;&#26500;&#65292;&#28982;&#21518;&#20272;&#35745;&#25903;&#25345;&#20122;&#29305;&#20848;&#22823;&#32467;&#26500;&#30340;&#24179;&#38754;surfel&#21306;&#22495;&#12290;&#36825;&#31181;&#20122;&#29305;&#20848;&#22823;&#24863;&#30693;&#30340;surfel&#34920;&#31034;&#20026;&#32473;&#23450;&#22330;&#26223;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#24179;&#38754;&#22320;&#22270;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36825;&#20123;&#20122;&#29305;&#20848;&#22823;&#24179;&#38754;surfel&#21306;&#22495;&#65292;&#25105;&#20204;&#33258;&#36866;&#24212;&#22320;&#37319;&#26679;&#21644;&#32422;&#26463;SDF&#37325;&#24314;&#20013;&#30340;&#32467;&#26500;&#35268;&#24459;&#65292;&#20174;&#32780;&#36890;&#36807;&#20445;&#25345;&#39640;&#27700;&#24179;&#32467;&#26500;&#21516;&#26102;&#22686;&#24378;&#32454;&#33410;&#26469;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01861v1 Announce Type: cross  Abstract: Indoor scenes we are living in are visually homogenous or textureless, while they inherently have structural forms and provide enough structural priors for 3D scene reconstruction. Motivated by this fact, we propose a structure-aware online signed distance fields (SDF) reconstruction framework in indoor scenes, especially under the Atlanta world (AW) assumption. Thus, we dub this incremental SDF reconstruction for AW as AiSDF. Within the online framework, we infer the underlying Atlanta structure of a given scene and then estimate planar surfel regions supporting the Atlanta structure. This Atlanta-aware surfel representation provides an explicit planar map for a given scene. In addition, based on these Atlanta planar surfel regions, we adaptively sample and constrain the structural regularity in the SDF reconstruction, which enables us to improve the reconstruction quality by maintaining a high-level structure while enhancing the deta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#20013;&#25991;Mixtral&#20026;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#30340;Mixtral&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01851</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;LLM&#35821;&#35328;&#36866;&#24212;&#24615;&#65306;&#20197;&#20013;&#25991;Mixtral&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#20013;&#25991;Mixtral&#20026;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#30340;Mixtral&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixtral&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;(SMoE)&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20197;Mixtral-8x7B-v0.1&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#20013;&#25991;-Mixtral&#21644;&#20013;&#25991;-Mixtral-Instruct&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#25552;&#39640;&#20102;&#20013;&#25991;&#35821;&#35328;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20013;&#25991;-Mixtral&#21644;&#20013;&#25991;-Mixtral-Instruct&#25104;&#21151;&#25552;&#21319;&#20102;&#20013;&#25991;&#29702;&#35299;&#21644;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21407;&#22987;&#30340;&#33521;&#25991;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#26102;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#25193;&#23637;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#30340;&#24517;&#35201;&#24615;&#20197;&#21450;&#21021;&#22987;&#21270;&#27169;&#22411;&#30340;&#36873;&#25321;&#65288;&#22522;&#30784;&#27169;&#22411;vs.&#25351;&#23548;&#27169;&#22411;&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#23454;&#35777;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#27599;&#20010;&#19987;&#23478;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#20197;&#26816;&#39564;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01851v1 Announce Type: cross  Abstract: Mixtral, a representative sparse mixture of experts (SMoE) language model, has received significant attention due to its unique model design and superior performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language abilities by adopting further pre-training and instruction fine-tuning. Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct successfully improve Chinese understanding and generation performance while retaining the original English abilities. Then, we discuss several key questions when performing language adaptation on large language models, including the necessity of extending the language-specific vocabulary and the choice of the initialization model (foundation model v.s. instruction model), by providing empirical results and analysis. We also present the visualizations of each expert to examine their importance on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25913;&#21892;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01849</link><description>&lt;p&gt;
&#19968;&#20010;&#25552;&#31034;&#35789;&#23601;&#36275;&#20197;&#25552;&#21319;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25913;&#21892;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;CLIP&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#33030;&#24369;&#12290;&#26412;&#25991;&#20174;&#25991;&#26412;&#25552;&#31034;&#30340;&#26032;&#39062;&#35270;&#35282;&#32780;&#38750;&#20256;&#32479;&#30740;&#31350;&#30340;&#27169;&#22411;&#26435;&#37325;&#65288;&#22312;&#26412;&#25991;&#20013;&#20923;&#32467;&#65289;&#30740;&#31350;&#20102;VLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#23545;&#20351;&#29992;&#30340;&#25991;&#26412;&#25552;&#31034;&#25935;&#24863;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;VLMs&#30340;&#24378;&#38887;&#25991;&#26412;&#25552;&#31034;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Adversarial Prompt Tuning&#65288;APT&#65289;&#65292;&#22312;&#35745;&#31639;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#22312;15&#20010;&#25968;&#25454;&#38598;&#21644;4&#31181;&#25968;&#25454;&#31232;&#30095;&#26041;&#26696;&#65288;&#20174;1-shot&#21040;&#23436;&#20840;&#35757;&#32451;&#25968;&#25454;&#35774;&#32622;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;APT&#30456;&#23545;&#20110;&#25163;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;APT&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01849v1 Announce Type: cross  Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent 
&lt;/p&gt;</description></item><item><title>NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01845</link><description>&lt;p&gt;
NASH&#65306;&#29992;&#20110;&#30828;&#20214;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01845
&lt;/p&gt;
&lt;p&gt;
NASH&#26159;&#19968;&#31181;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#30828;&#20214;&#35774;&#35745;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#12289;&#20302;&#24310;&#36831;&#21644;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#37096;&#32626;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22312;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NASH&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#12290;&#20351;&#29992;NASH&#65292;&#30828;&#20214;&#35774;&#35745;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#36824;&#21487;&#20197;&#23454;&#29616;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#29256;&#26412;&#30340;NASH&#31574;&#30053;&#65292;&#25152;&#26377;&#36825;&#20123;&#31574;&#30053;&#26174;&#31034;&#20986;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#20247;&#22810;&#27169;&#22411;&#25805;&#20316;&#20013;&#36873;&#25321;&#29305;&#23450;&#25805;&#20316;&#65292;&#24341;&#23548;&#35757;&#32451;&#36807;&#31243;&#26397;&#21521;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312; ResNet18 &#25110; ResNet34 &#19978;&#24212;&#29992;NASH&#65292;&#19982;&#38750;NASH&#29256;&#26412;&#30456;&#27604;&#65292;&#21487;&#20351;Top1&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;3.1%&#65292;Top5&#20934;&#30830;&#29575;&#25552;&#39640;&#39640;&#36798;2.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01840</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30001;&#27880;&#37322;&#26631;&#31614;&#36827;&#34892;&#20154;-&#29289;&#20114;&#21160;&#26816;&#27979;&#30340;FreeA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FreeA: Human-object Interaction Detection using Free Annotation Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;-&#29289;&#20114;&#21160;&#65288;HOI&#65289;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#21171;&#21160;&#21147;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#38656;&#35201;&#20840;&#38754;&#27880;&#37322;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;CLIP&#30340;&#36866;&#24212;&#24615;&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#26080;&#38656;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FreeA&#23558;&#20154;-&#29289;&#23545;&#30340;&#22270;&#20687;&#29305;&#24449;&#19982;HOI&#25991;&#26412;&#27169;&#26495;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25513;&#27169;&#26041;&#27861;&#26469;&#25233;&#21046;&#19981;&#22826;&#21487;&#33021;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;FreeA&#21033;&#29992;&#20102;&#25552;&#20986;&#30340;&#20132;&#20114;&#30456;&#20851;&#24615;&#21305;&#37197;&#26041;&#27861;&#26469;&#22686;&#24378;&#19982;&#25351;&#23450;&#21160;&#20316;&#30456;&#20851;&#30340;&#21160;&#20316;&#30340;&#21487;&#33021;&#24615;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#29983;&#25104;&#30340;HOI&#26631;&#31614;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreeA&#22312;&#24369;&#30417;&#30563;HOI&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HICO-DET&#19978;&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mAP&#65289;&#25552;&#39640;&#20102;+8.58&#65292;&#22312;V-COCO&#19978;&#25552;&#39640;&#20102;+1.23&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01840v1 Announce Type: cross  Abstract: Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;-Based Data-Centric AI &#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#26415;&#30028;&#25968;&#25454;&#36136;&#37327;&#21644;&#24037;&#19994;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#21512;&#27169;&#22411;&#32771;&#34385;&#21040;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.01832</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65306;&#24357;&#21512;&#23398;&#26415;&#29702;&#24819;&#19982;&#24037;&#19994;&#23454;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;
&lt;/p&gt;
&lt;p&gt;
Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals and Industrial Pragmatism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;-Based Data-Centric AI &#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#23398;&#26415;&#30028;&#25968;&#25454;&#36136;&#37327;&#21644;&#24037;&#19994;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#21512;&#27169;&#22411;&#32771;&#34385;&#21040;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20869;&#25968;&#25454;&#30340;&#23545;&#27604;&#35282;&#33394;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#21644;&#27169;&#22411;&#26080;&#20851;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#20391;&#37325;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39318;&#35201;&#24615;&#65292;&#32780;&#27169;&#22411;&#26080;&#20851;&#20154;&#24037;&#26234;&#33021;&#21017;&#20248;&#20808;&#32771;&#34385;&#31639;&#27861;&#28789;&#27963;&#24615;&#65292;&#24448;&#24448;&#20197;&#29306;&#29298;&#25968;&#25454;&#36136;&#37327;&#32771;&#34385;&#20026;&#20195;&#20215;&#12290;&#36825;&#31181;&#21306;&#21035;&#26174;&#31034;&#65292;&#23398;&#26415;&#30028;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#26631;&#20934;&#24448;&#24448;&#19981;&#33021;&#28385;&#36275;&#24037;&#19994;&#24212;&#29992;&#30340;&#20005;&#26684;&#35201;&#27714;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#37096;&#32626;&#23398;&#26415;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#24102;&#26469;&#30340;&#25361;&#25112;&#20197;&#21450;&#24357;&#21512;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#27169;&#22411;&#32771;&#34385;&#34701;&#20837;&#25968;&#25454;&#20248;&#21270;&#36807;&#31243;&#26469;&#35843;&#21644;&#36825;&#20123;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01832v1 Announce Type: new  Abstract: This paper delves into the contrasting roles of data within academic and industrial spheres, highlighting the divergence between Data-Centric AI and Model-Agnostic AI approaches. We argue that while Data-Centric AI focuses on the primacy of high-quality data for model performance, Model-Agnostic AI prioritizes algorithmic flexibility, often at the expense of data quality considerations. This distinction reveals that academic standards for data quality frequently do not meet the rigorous demands of industrial applications, leading to potential pitfalls in deploying academic models in real-world settings. Through a comprehensive analysis, we address these disparities, presenting both the challenges they pose and strategies for bridging the gap. Furthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which aims to reconcile these differences by integrating model considerations into data optimization processes. This approach u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.01827</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#22522;&#20110;&#20840; memristor &#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#30340;&#20648;&#23618;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Analysis and Fully Memristor-based Reservoir Computing for Temporal Data Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#36890;&#36807;&#38598;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340; memristor&#65292;&#24182;&#22312;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#20648;&#23618;&#35745;&#31639;&#65288;RC&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#20449;&#21495;&#30340;&#31070;&#32463;&#24418;&#24577;&#23398;&#26694;&#26550;&#12290;RC&#20197;&#20854;&#26102;&#38388;&#22788;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#19982;&#20256;&#32479;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20854;&#30828;&#20214;&#37096;&#32626;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33021;&#22815;&#29983;&#25104;&#21160;&#24577;&#20648;&#23618;&#29366;&#24577;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23384;&#20648;&#22120; RC &#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#31181;&#22522;&#20110; WOx &#30340; memristor &#30340;&#30701;&#26399;&#23384;&#20648;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616; 16 &#20010;&#19981;&#21516;&#29366;&#24577;&#30340;&#32534;&#30721;&#36229;&#36807; 4 &#20010;&#27604;&#29305;&#65292;&#24182;&#22312;&#35835;&#20986;&#23618;&#20013;&#20351;&#29992; TiOx-based memristor &#30340;&#38271;&#26399;&#23384;&#20648;&#22120;&#32452;&#20214;&#12290;&#25105;&#20204;&#24443;&#24213;&#30740;&#31350;&#20102;&#20004;&#31181; memristor &#31867;&#22411;&#65292;&#24182;&#21033;&#29992; RC &#31995;&#32479;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340; RC &#31995;&#32479;&#30340;&#24615;&#33021;&#36890;&#36807;&#20004;&#20010;&#22522;&#20934;&#20219;&#21153;&#36827;&#34892;&#20102;&#39564;&#35777;: &#23545;&#20855;&#26377;&#19981;&#23436;&#25972;&#36755;&#20837;&#30340;&#23396;&#31435;&#21475;&#36848;&#25968;&#23383;&#35782;&#21035;&#21644; Mackey-Glass &#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340; 98.84% &#20934;&#30830;&#29575;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01827v1 Announce Type: cross  Abstract: Reservoir computing (RC) offers a neuromorphic framework that is particularly effective for processing spatiotemporal signals. Known for its temporal processing prowess, RC significantly lowers training costs compared to conventional recurrent neural networks. A key component in its hardware deployment is the ability to generate dynamic reservoir states. Our research introduces a novel dual-memory RC system, integrating a short-term memory via a WOx-based memristor, capable of achieving 16 distinct states encoded over 4 bits, and a long-term memory component using a TiOx-based memristor within the readout layer. We thoroughly examine both memristor types and leverage the RC system to process temporal data sets. The performance of the proposed RC system is validated through two benchmark tasks: isolated spoken digit recognition with incomplete inputs and Mackey-Glass time series prediction. The system delivered an impressive 98.84% accu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#24182;&#23558;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20419;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.01823</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#30340;RT-H&#21160;&#20316;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
RT-H: Action Hierarchies Using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01823
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#24182;&#23558;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20419;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20026;&#23558;&#22797;&#26434;&#27010;&#24565;&#20998;&#35299;&#20026;&#21487;&#28040;&#21270;&#30340;&#37096;&#20998;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#24335;&#12290;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24037;&#20316;&#20351;&#29992;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#31574;&#30053;&#65292;&#26681;&#25454;&#35270;&#35273;&#35266;&#23519;&#21644;&#35821;&#35328;&#20013;&#25351;&#23450;&#30340;&#39640;&#32423;&#20219;&#21153;&#26469;&#39044;&#27979;&#21160;&#20316;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#32467;&#26500;&#22312;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#20013;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#8220;&#25343;&#21487;&#20048;&#32592;&#8221;&#21644;&#8220;&#25688;&#33529;&#26524;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#22312;&#35821;&#20041;&#19978;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65288;&#20363;&#22914;&#65292;&#8220;&#25343;&#21487;&#20048;&#32592;&#8221;&#21644;&#8220;&#20498;&#26479;&#23376;&#8221;&#65289;&#65292;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#25968;&#25454;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#23398;&#20064;&#23558;&#39640;&#32423;&#20219;&#21153;&#26144;&#23556;&#21040;&#21160;&#20316;&#38656;&#35201;&#26356;&#22810;&#30340;&#28436;&#31034;&#25968;&#25454;&#12290;&#20026;&#20102;&#26550;&#36215;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#25105;&#20204;&#30340;idea&#26159;&#25945;&#20250;&#26426;&#22120;&#20154;&#21160;&#20316;&#35821;&#35328;&#65292;&#29992;&#26356;&#31934;&#32454;&#30340;&#30701;&#35821;&#25551;&#36848;&#20302;&#32423;&#36816;&#21160;&#65292;&#20363;&#22914;&#8220;&#21521;&#21069;&#31227;&#21160;&#25163;&#33218;&#8221;&#12290;&#23558;&#36825;&#20123;&#35821;&#35328;&#21160;&#20316;&#20316;&#20026;&#20219;&#21153;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#26469;&#39044;&#27979;&#36843;&#20351;&#31574;&#30053;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01823v1 Announce Type: cross  Abstract: Language provides a way to break down complex concepts into digestible pieces. Recent works in robot imitation learning use language-conditioned policies that predict actions given visual observations and the high-level task specified in language. These methods leverage the structure of natural language to share data between semantically similar tasks (e.g., "pick coke can" and "pick an apple") in multi-task datasets. However, as tasks become more semantically diverse (e.g., "pick coke can" and "pour cup"), sharing data between tasks becomes harder, so learning to map high-level tasks to actions requires much more demonstration data. To bridge tasks and actions, our insight is to teach the robot the language of actions, describing low-level motions with more fine-grained phrases like "move arm forward". Predicting these language motions as an intermediate step between tasks and actions forces the policy to learn the shared structure of
&lt;/p&gt;</description></item><item><title>AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01818</link><description>&lt;p&gt;
AllSpark: &#21033;&#29992;Transformer&#20013;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01818
&lt;/p&gt;
&lt;p&gt;
AllSpark&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;SSSS&#65289;&#26088;&#22312;&#20943;&#36731;&#32791;&#26102;&#30340;&#20687;&#32032;&#32423;&#25163;&#21160;&#26631;&#27880;&#36127;&#25285;&#65292;&#23427;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#20197;&#21450;&#26356;&#22810;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20934;&#30495;&#20540;&#35757;&#32451;&#26631;&#35760;&#25968;&#25454;&#21644;&#20351;&#29992;&#20266;&#26631;&#31614;&#35757;&#32451;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#35757;&#32451;&#27969;&#31243;&#26159;&#20998;&#24320;&#30340;&#65292;&#36825;&#20351;&#24471;&#26631;&#35760;&#25968;&#25454;&#20027;&#23548;&#35757;&#32451;&#36807;&#31243;&#65292;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#21644;&#20174;&#32780;&#27425;&#20248;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AllSpark&#65292;&#21033;&#29992;&#36890;&#36947;&#32423;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#20174;&#26410;&#26631;&#35760;&#30340;&#29305;&#24449;&#20013;&#37325;&#26032;&#29983;&#25104;&#26631;&#35760;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#35821;&#20041;&#35760;&#24518;&#21644;&#36890;&#36947;&#35821;&#20041;&#20998;&#32452;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#26410;&#26631;&#35760;&#29305;&#24449;&#20805;&#20998;&#20195;&#34920;&#26631;&#35760;&#29305;&#24449;&#12290;AllSpark&#20026;SSSS&#30340;&#26550;&#26500;&#32423;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#32780;&#38750;&#26694;&#26550;&#32423;&#21035;&#65292;&#36991;&#20813;&#20102;&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01818v1 Announce Type: cross  Abstract: Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate the burden of time-consuming pixel-level manual labeling, which leverages limited labeled data along with larger amounts of unlabeled data. Current state-of-the-art methods train the labeled data with ground truths and unlabeled data with pseudo labels. However, the two training flows are separate, which allows labeled data to dominate the training process, resulting in low-quality pseudo labels and, consequently, sub-optimal results. To alleviate this issue, we present AllSpark, which reborns the labeled features from unlabeled ones with the channel-wise cross-attention mechanism. We further introduce a Semantic Memory along with a Channel Semantic Grouping strategy to ensure that unlabeled features adequately represent labeled features. The AllSpark shed new light on the architecture level designs of SSSS rather than framework level, which avoids increasingly
&lt;/p&gt;</description></item><item><title>SMAUG&#26694;&#26550;&#25552;&#20986;&#20102;&#22522;&#20110;&#28369;&#21160;&#22810;&#32500;&#20219;&#21153;&#31383;&#21475;&#30340;&#36866;&#24212;&#24615;&#23454;&#26102;&#23376;&#20219;&#21153;&#35782;&#21035;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01816</link><description>&lt;p&gt;
SMAUG&#65306;&#22522;&#20110;&#28369;&#21160;&#22810;&#32500;&#20219;&#21153;&#31383;&#21475;&#30340;&#36866;&#24212;&#24615;&#23454;&#26102;&#23376;&#20219;&#21153;&#35782;&#21035;MARL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for Adaptive Real-Time Subtask Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01816
&lt;/p&gt;
&lt;p&gt;
SMAUG&#26694;&#26550;&#25552;&#20986;&#20102;&#22522;&#20110;&#28369;&#21160;&#22810;&#32500;&#20219;&#21153;&#31383;&#21475;&#30340;&#36866;&#24212;&#24615;&#23454;&#26102;&#23376;&#20219;&#21153;&#35782;&#21035;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#39640;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#30452;&#25509;&#20174;&#25351;&#25968;&#32423;&#25193;&#23637;&#30340;&#32852;&#21512;&#35266;&#23519;-&#21160;&#20316;&#31354;&#38388;&#20013;&#20316;&#20986;&#34892;&#20026;&#20915;&#31574;&#19981;&#21516;&#65292;&#22522;&#20110;&#23376;&#20219;&#21153;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#22522;&#20110;&#23376;&#20219;&#21153;&#30340;MARL&#26041;&#27861;&#22522;&#20110;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38480;&#21046;&#23376;&#20219;&#21153;&#30340;&#25968;&#37327;&#65292;&#21608;&#26399;&#24615;&#25191;&#34892;&#23376;&#20219;&#21153;&#35782;&#21035;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#26102;&#38388;&#27573;&#20869;&#35782;&#21035;&#21644;&#25191;&#34892;&#29305;&#23450;&#23376;&#20219;&#21153;&#65292;&#36825;&#20351;&#23427;&#20204;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#23376;&#20219;&#21153;&#30340;&#22810;&#26679;&#21270;&#21160;&#24577;&#22330;&#26223;&#12290;&#20026;&#20102;&#31361;&#30772;&#19978;&#36848;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28369;&#21160;&#22810;&#32500;&#20219;&#21153;&#31383;&#21475;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65288;SMAUG&#65289;&#29992;&#20110;&#33258;&#36866;&#24212;&#23454;&#26102;&#23376;&#20219;&#21153;&#35782;&#21035;&#12290;&#23427;&#21033;&#29992;&#28369;&#21160;&#22810;&#32500;&#20219;&#21153;&#31383;&#21475;&#26469;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01816v1 Announce Type: new  Abstract: Instead of making behavioral decisions directly from the exponentially expanding joint observational-action space, subtask-based multi-agent reinforcement learning (MARL) methods enable agents to learn how to tackle different subtasks. Most existing subtask-based MARL methods are based on hierarchical reinforcement learning (HRL). However, these approaches often limit the number of subtasks, perform subtask recognition periodically, and can only identify and execute a specific subtask within the predefined fixed time period, which makes them inflexible and not suitable for diverse and dynamic scenarios with constantly changing subtasks. To break through above restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window based m\textbf{U}ti-agent reinforcement learnin\textbf{G} framework (SMAUG) is proposed for adaptive real-time subtask recognition. It leverages a sliding multidimensional task window to extract essentia
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#19982;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#36328;&#22478;&#24066;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36716;&#31227;&#20013;&#30340;&#36866;&#24212;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01801</link><description>&lt;p&gt;
COLA: &#36328;&#22478;&#24066;&#31227;&#21160;&#24615;&#36716;&#25442;&#22120;&#29992;&#20110;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
COLA: Cross-city Mobility Transformer for Human Trajectory Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01801
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#19982;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#36328;&#22478;&#24066;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36716;&#31227;&#20013;&#30340;&#36866;&#24212;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26085;&#24120;&#31227;&#21160;&#35774;&#22791;&#20135;&#29983;&#30340;&#20154;&#31867;&#36712;&#36857;&#25968;&#25454;&#22312;&#22478;&#24066;&#35268;&#21010;&#21644;&#30123;&#24773;&#38450;&#25511;&#31561;&#37325;&#35201;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#38024;&#23545;&#20010;&#20154;&#38544;&#31169;&#38382;&#39064;&#65292;&#20154;&#31867;&#36712;&#36857;&#27169;&#25311;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#22823;&#37327;&#36924;&#30495;&#30340;&#31227;&#21160;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#30340;&#26222;&#36941;&#38382;&#39064;&#26080;&#30097;&#38477;&#20302;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#36328;&#22478;&#24066;&#31227;&#21160;&#24615;&#36716;&#31227;&#30340;&#38382;&#39064;&#65292;&#25226;&#25569;&#20154;&#31867;&#36712;&#36857;&#30340;&#26222;&#36941;&#27169;&#24335;&#65292;&#20026;Transformer&#27169;&#22411;&#22686;&#21152;&#22806;&#37096;&#31227;&#21160;&#25968;&#25454;&#12290;&#22312;&#36328;&#22478;&#24066;&#30693;&#35782;&#36716;&#31227;&#20013;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#20351;Transformer&#36866;&#24212;&#39046;&#22495;&#30340;&#24322;&#36136;&#24615;&#65307;2&#65289;&#22914;&#20309;&#26657;&#20934;Transformer&#20197;&#36866;&#24212;&#32454;&#24494;&#19981;&#21516;&#30340;&#38271;&#23614;&#39057;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01801v1 Announce Type: cross  Abstract: Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention. In terms of the individual privacy concern, human trajectory simulation has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks. Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models. In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful Transformer with external mobility data. There are two crucial challenges arising in the knowledge transfer across cities: 1) how to transfer the Transformer to adapt for domain heterogeneity; 2) how to calibrate the Transformer to adapt for subtly different long-tail frequency distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#19981;&#21516;AI&#35282;&#33394;&#65288;&#25512;&#33616;&#31995;&#32479;&#12289;&#20998;&#26512;&#32773;&#21644;&#39764;&#39740;&#30340;&#36777;&#25252;&#32773;&#65289;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#20219;&#21153;&#34920;&#29616;&#12289;&#20381;&#36182;&#36866;&#24403;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01791</link><description>&lt;p&gt;
&#36229;&#36234;&#25512;&#33616;&#31995;&#32479;&#65306;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#19981;&#21516;AI&#35282;&#33394;&#24433;&#21709;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#19981;&#21516;AI&#35282;&#33394;&#65288;&#25512;&#33616;&#31995;&#32479;&#12289;&#20998;&#26512;&#32773;&#21644;&#39764;&#39740;&#30340;&#36777;&#25252;&#32773;&#65289;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#20219;&#21153;&#34920;&#29616;&#12289;&#20381;&#36182;&#36866;&#24403;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#21508;&#26377;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#20316;&#20026;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;&#65292;&#25552;&#20379;AI&#35748;&#20026;&#27491;&#30830;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#20154;&#31867;&#20998;&#26512;&#24605;&#32500;&#65292;&#24182;&#23548;&#33268;&#20154;&#31867;&#19981;&#24688;&#24403;&#22320;&#20381;&#36182;AI&#65292;&#25439;&#23475;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#32676;&#20307;&#20915;&#31574;&#20013;&#30340;&#20154;&#31867;&#39038;&#38382;&#25198;&#28436;&#21508;&#31181;&#35282;&#33394;&#65292;&#20363;&#22914;&#20998;&#26512;&#26367;&#20195;&#36873;&#39033;&#25110;&#25209;&#35780;&#20915;&#31574;&#32773;&#20197;&#40723;&#21169;&#20182;&#20204;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;&#36825;&#31181;&#35282;&#33394;&#30340;&#22810;&#26679;&#24615;&#23578;&#26410;&#22312;AI&#36741;&#21161;&#20013;&#24471;&#21040;&#23454;&#35777;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#19977;&#31181;AI&#35282;&#33394;&#65306;&#25512;&#33616;&#31995;&#32479;&#12289;&#20998;&#26512;&#32773;&#21644;&#39764;&#39740;&#30340;&#36777;&#25252;&#32773;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20004;&#31181;AI&#24615;&#33021;&#27700;&#24179;&#19979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#27599;&#31181;&#35282;&#33394;&#22312;&#20219;&#21153;&#34920;&#29616;&#12289;&#20381;&#36182;&#36866;&#24403;&#24615;&#21644;&#29992;&#25143;&#20307;&#39564;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25512;&#33616;&#31995;&#32479;&#35282;&#33394;&#24182;&#38750;&#22987;&#32456;&#26368;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01791v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. In this paper, we examine three AI roles: Recommender, Analyzer, and Devil's Advocate, and evaluate their effects across two AI performance levels. Our results show each role's distinct strengths and limitations in task performance, reliance appropriateness, and user experience. Notably, the Recommender role is not always the most effective, especi
&lt;/p&gt;</description></item><item><title>CatCode&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01784</link><description>&lt;p&gt;
CatCode&#65306;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#26041;&#38754;&#30340;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01784
&lt;/p&gt;
&lt;p&gt;
CatCode&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33539;&#30068;&#35770;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;LLMs&#22312;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20195;&#30721;&#21644;&#25991;&#26412;&#28151;&#21512;&#26041;&#38754;&#36234;&#26469;&#36234;&#31934;&#36890;&#12290;&#22522;&#20110;&#36825;&#31181;&#28151;&#21512;&#30340;&#35780;&#20272;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#20102;&#35299;&#27169;&#22411;&#22312;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#22312;&#20219;&#21153;&#35206;&#30422;&#33539;&#22260;&#19978;&#30340;&#38480;&#21046;&#25110;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33539;&#30068;&#35770;&#20316;&#20026;&#35780;&#20272;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20195;&#30721;&#33539;&#30068;&#20013;&#30340;&#24577;&#23556;&#21487;&#20197;&#34920;&#31034;&#20195;&#30721;&#35843;&#35797;&#21644;&#36716;&#25442;&#65292;&#20004;&#20010;&#33539;&#30068;&#20043;&#38388;&#30340;&#20989;&#23376;&#34920;&#31034;&#20195;&#30721;&#32763;&#35793;&#65292;&#20195;&#30721;&#33539;&#30068;&#21644;&#33258;&#28982;&#35821;&#35328;&#33539;&#30068;&#20043;&#38388;&#30340;&#20989;&#23376;&#34920;&#31034;&#20195;&#30721;&#29983;&#25104;&#12289;&#35299;&#37322;&#21644;&#20877;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CatCode&#65288;Category Code&#65289;&#30340;&#33258;&#21160;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20840;&#38754;&#35780;&#20272;LLMs&#65288;&#21253;&#25324;ChatGPT&#65289;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01784v1 Announce Type: new  Abstract: Large language models (LLMs) such as ChatGPT are increasingly proficient in understanding and generating a mixture of code and text. Evaluation based on such $\textit{mixture}$ can lead to a more comprehensive understanding of the models' abilities in solving coding problems. However, in this context, current evaluation methods are either limited in task coverage or lack standardization. To address this issue, we propose using category theory as a framework for evaluation. Specifically, morphisms within a code category can represent code debugging and transformation, functors between two categories represent code translation, and functors between a code category and a natural language category represent code generation, explanation, and reproduction. We present an automatic evaluation framework called $\textbf{CatCode}$ ($\textbf{Cat}$egory $\textbf{Code}$) that can comprehensively assess the coding abilities of LLMs, including ChatGPT, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#20809;&#35889;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65288;SWD&#65289;&#36827;&#34892;&#26368;&#20248;&#36755;&#36816;&#65292;&#22312;&#26080;&#30417;&#30563;&#24418;&#29366;&#21305;&#37197;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#21151;&#33021;&#26144;&#23556;&#21644;&#26368;&#20248;&#36755;&#36816;&#30340;&#39640;&#25928;&#25972;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.01781</link><description>&lt;p&gt;
&#25972;&#21512;&#39640;&#25928;&#30340;&#26368;&#20248;&#36755;&#36816;&#21644;&#21151;&#33021;&#26144;&#23556;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#24418;&#29366;&#23545;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#20809;&#35889;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20351;&#29992;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65288;SWD&#65289;&#36827;&#34892;&#26368;&#20248;&#36755;&#36816;&#65292;&#22312;&#26080;&#30417;&#30563;&#24418;&#29366;&#21305;&#37197;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#21151;&#33021;&#26144;&#23556;&#21644;&#26368;&#20248;&#36755;&#36816;&#30340;&#39640;&#25928;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#24314;&#31435;&#20960;&#20309;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#23545;&#24212;&#23545;&#20110;&#23545;&#35937;&#36319;&#36394;&#12289;&#37197;&#20934;&#12289;&#32441;&#29702;&#36716;&#31227;&#21644;&#32479;&#35745;&#24418;&#29366;&#20998;&#26512;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#20809;&#35889;&#26041;&#27861;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#32858;&#28966;&#20110;&#21151;&#33021;&#26144;&#23556;&#65288;FMs&#65289;&#21644;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#21644;&#25968;&#25454;&#39537;&#21160;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#37319;&#29992;&#20999;&#29255;Wasserstein&#36317;&#31163;&#65288;SWD&#65289;&#29992;&#20110;OT&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26080;&#30417;&#30563;&#24418;&#29366;&#21305;&#37197;&#26694;&#26550;&#20013;&#26377;&#25928;&#19988;&#24555;&#36895;&#30340;&#26368;&#20248;&#36755;&#36816;&#24230;&#37327;&#12290;&#35813;&#26080;&#30417;&#30563;&#26694;&#26550;&#23558;&#21151;&#33021;&#26144;&#23556;&#27491;&#21017;&#21270;&#22120;&#19982;&#20174;SWD&#23548;&#20986;&#30340;&#26032;&#22411;OT&#25439;&#22833;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#22686;&#24378;&#20102;&#34987;&#35270;&#20026;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#30340;&#24418;&#29366;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01781v1 Announce Type: cross  Abstract: In the realm of computer vision and graphics, accurately establishing correspondences between geometric 3D shapes is pivotal for applications like object tracking, registration, texture transfer, and statistical shape analysis. Moving beyond traditional hand-crafted and data-driven feature learning methods, we incorporate spectral methods with deep learning, focusing on functional maps (FMs) and optimal transport (OT). Traditional OT-based approaches, often reliant on entropy regularization OT in learning-based framework, face computational challenges due to their quadratic cost. Our key contribution is to employ the sliced Wasserstein distance (SWD) for OT, which is a valid fast optimal transport metric in an unsupervised shape matching framework. This unsupervised framework integrates functional map regularizers with a novel OT-based loss derived from SWD, enhancing feature alignment between shapes treated as discrete probability mea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.01773</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#25913;&#21892;&#22270;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving out-of-distribution generalization in graphs via hierarchical semantic environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01773
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20998;&#24067;&#36716;&#31227;&#21644;&#32570;&#20047;&#29615;&#22659;&#32972;&#26223;&#65292;&#22270;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#29983;&#25104;&#24179;&#38754;&#29615;&#22659;&#26469;&#22686;&#24378;&#22270;&#30340;OOD&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24179;&#38754;&#29615;&#22659;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#21253;&#21547;&#21508;&#31181;&#35757;&#32451;&#29615;&#22659;&#65288;&#22914;&#39592;&#26550;&#12289;&#22823;&#23567;&#31561;&#65289;&#30340;DrugOOD&#25968;&#25454;&#38598;&#65292;&#24179;&#38754;&#29615;&#22659;&#26080;&#27861;&#20805;&#20998;&#35299;&#20915;&#20854;&#39640;&#24322;&#36136;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#21363;&#29983;&#25104;&#26356;&#20855;&#35821;&#20041;&#20016;&#23500;&#30340;&#29615;&#22659;&#65292;&#20197;&#22686;&#24378;&#22270;&#30340;&#19981;&#21464;&#23398;&#20064;&#20197;&#22788;&#29702;&#20998;&#24067;&#36716;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#22270;&#29983;&#25104;&#20998;&#23618;&#35821;&#20041;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#36755;&#20837;&#22270;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#25552;&#21462;&#36755;&#20837;&#22270;&#20013;&#30340;&#21464;&#20307;&#23376;&#22270;&#65292;&#20197;&#22312;&#26412;&#22320;&#29615;&#22659;&#19978;&#29983;&#25104;&#20195;&#29702;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#38543;&#26426;&#27880;&#24847;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#26679;&#26412;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01769</link><description>&lt;p&gt;
&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Safe Screening Rule with Bi-level Optimization of $\nu$ Support Vector Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01769
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#30340;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#26679;&#26412;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#22312;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#12290;&#20316;&#20026;&#20256;&#32479;SVM&#30340;&#19968;&#20010;&#33879;&#21517;&#25193;&#23637;&#65292;$\nu$&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;$\nu$-SVM&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#32780;&#34920;&#29616;&#21331;&#36234;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#23427;&#20173;&#38754;&#20020;&#35757;&#32451;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21452;&#23618;&#20248;&#21270;&#30340;$\nu$-SVM&#23433;&#20840;&#31579;&#36873;&#35268;&#21017;&#65288;SRBO-$\nu$-SVM&#65289;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#31579;&#36873;&#20986;&#19981;&#27963;&#36291;&#30340;&#26679;&#26412;&#65292;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;SRBO-$\nu$-SVM&#20005;&#26684;&#22320;&#36890;&#36807;&#25972;&#21512;KKT&#26465;&#20214;&#12289;&#20984;&#38382;&#39064;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;$\nu$&#23646;&#24615;&#25512;&#23548;&#32780;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#20598;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;&#65288;DCDM&#65289;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;SRBO&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01769v1 Announce Type: cross  Abstract: Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\nu$ support vector machine ($\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\nu$-SVM (SRBO-$\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\nu$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#35268;&#33539;&#25968;&#25454;&#24418;&#24335;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#22320;&#35774;&#35745;&#25968;&#25454;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.01768</link><description>&lt;p&gt;
&#25511;&#21046;&#31995;&#32479;&#20013;&#25968;&#25454;&#25551;&#36848;&#30340;&#35268;&#33539;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Canonical Form of Datatic Description in Control Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#35268;&#33539;&#25968;&#25454;&#24418;&#24335;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#22320;&#35774;&#35745;&#25968;&#25454;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#39304;&#25511;&#21046;&#22120;&#30340;&#35774;&#35745;&#27491;&#32463;&#21382;&#20174;&#27169;&#22411;&#39537;&#21160;&#25511;&#21046;&#21040;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35268;&#33539;&#24418;&#24335;&#26159;&#27169;&#22411;&#39537;&#21160;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#65292;&#22914;Jordan&#24418;&#24335;&#12289;&#21487;&#25511;&#24418;&#24335;&#21644;&#21487;&#35266;&#23519;&#24418;&#24335;&#65292;&#20854;&#30446;&#30340;&#26159;&#20419;&#36827;&#31995;&#32479;&#20998;&#26512;&#21644;&#25511;&#21046;&#22120;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#39046;&#22495;&#65292;&#32570;&#20047;&#25968;&#25454;&#31995;&#32479;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#35268;&#33539;&#25968;&#25454;&#24418;&#24335;&#30340;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#22320;&#35774;&#35745;&#25968;&#25454;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01768v1 Announce Type: cross  Abstract: The design of feedback controllers is undergoing a paradigm shift from modelic (i.e., model-driven) control to datatic (i.e., data-driven) control. Canonical form of state space model is an important concept in modelic control systems, exemplified by Jordan form, controllable form and observable form, whose purpose is to facilitate system analysis and controller synthesis. In the realm of datatic control, there is a notable absence in the standardization of data-based system representation. This paper for the first time introduces the concept of canonical data form for the purpose of achieving more effective design of datatic controllers. In a control system, the data sample in canonical form consists of a transition component and an attribute component. The former encapsulates the plant dynamics at the sampling time independently, which is a tuple containing three elements: a state, an action and their corresponding next state. The la
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.01757</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#38598;&#25104;&#22914;&#20309;&#25552;&#21319;LLM&#22312;&#20248;&#21270;&#20013;&#30340;&#24615;&#33021;&#65306;&#20197;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#36827;&#34892;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#26126;&#26174;&#22320;&#23558;&#23427;&#20204;&#23450;&#20301;&#20026;&#35299;&#20915;&#22797;&#26434;&#20248;&#21270;&#25361;&#25112;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#34987;&#35748;&#21487;&#65292;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#20248;&#21270;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#65292;&#22312;&#20165;&#20381;&#36182;&#20110;&#25968;&#23383;&#25991;&#26412;&#25552;&#31034;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#38590;&#20197;&#25429;&#25417;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;LLM&#26469;&#22686;&#24378;&#20248;&#21270;&#24615;&#33021;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#25991;&#26412;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#28145;&#20837;&#20102;&#35299;&#22788;&#29702;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#31181;&#38598;&#25104;&#20801;&#35768;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;LLM&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#35299;&#20915;&#38382;&#39064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#25193;&#23637;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01757v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01748</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#20449;&#21495;&#35299;&#30721;&#20026;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Decode Neural signal as Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#25506;&#32034;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#20197;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#12289;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#20197;&#21450;&#26410;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33041;&#21160;&#24577;&#35299;&#30721;&#35821;&#35328;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#26041;&#21521;&#65292;&#23588;&#20854;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#30456;&#23545;&#20110;&#38656;&#35201;&#30005;&#26497;&#26893;&#20837;&#25163;&#26415;&#30340;&#20405;&#20837;&#24615;&#20449;&#21495;&#65292;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#20449;&#21495;&#65288;&#22914;EEG&#12289;MEG&#65289;&#30001;&#20110;&#20854;&#23433;&#20840;&#24615;&#21644;&#26222;&#36866;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#20010;&#26041;&#38754;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#65306;1&#65289;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;EEG&#19978;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#20808;&#21069;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;MEG&#20449;&#21495;&#36136;&#37327;&#26356;&#22909;&#30340;&#38382;&#39064;&#65307;2&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22312;&#29983;&#25104;&#35299;&#30721;&#36807;&#31243;&#20013;&#20351;&#29992;&#8220;teacher-forcing&#8221;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65307;3&#65289;&#20197;&#21069;&#30340;&#24037;&#20316;&#22823;&#22810;&#26159;&#22522;&#20110;&#8220;BART&#8221;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#22312;&#20854;&#20182;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MEG&#20449;&#21495;&#30340;&#33041;&#21040;&#25991;&#26412;&#36716;&#25442;&#22312;&#35821;&#38899;&#35299;&#30721;&#24418;&#24335;&#20013;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#20013;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01748v1 Announce Type: cross  Abstract: Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used ``teacher-forcing" during generative decoding, which is impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attentio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RbSL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26679;&#32422;&#26463;&#26102;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.01734</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#22312;&#20855;&#26377;&#24674;&#22797;&#31574;&#30053;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01734
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RbSL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22788;&#29702;&#22810;&#26679;&#32422;&#26463;&#26102;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#26088;&#22312;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#38598;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#19979;&#30340;&#31163;&#32447;GCRL&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#24674;&#22797;&#30340;&#30417;&#30563;&#23398;&#20064;&#65288;RbSL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23436;&#25104;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01734v1 Announce Type: cross  Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23581;&#35797;&#24212;&#29992;&#20110;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.01709</link><description>&lt;p&gt;
LLMs&#33021;&#21542;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#65311;-&#19968;&#39033;&#25506;&#32034;&#24615;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#31569;&#35774;&#35745;&#20915;&#31574;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23581;&#35797;&#24212;&#29992;&#20110;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#30693;&#35782;&#31649;&#29702;&#65288;AKM&#65289;&#28041;&#21450;&#23545;&#39033;&#30446;&#25110;&#32452;&#32455;&#20013;&#19982;&#24314;&#31569;&#20915;&#31574;&#21644;&#35774;&#35745;&#30456;&#20851;&#20449;&#24687;&#30340;&#26377;&#32452;&#32455;&#22788;&#29702;&#12290;AKM&#30340;&#19968;&#20010;&#37325;&#35201;&#20135;&#29289;&#26159;&#26550;&#26500;&#20915;&#31574;&#35760;&#24405;&#65288;ADR&#65289;&#65292;&#23427;&#35760;&#24405;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#12290; ADR&#26159;&#25429;&#25417;&#20915;&#31574;&#32972;&#26223;&#12289;&#24050;&#20570;&#20986;&#30340;&#20915;&#31574;&#20197;&#21450;&#19982;&#35774;&#35745;&#20915;&#31574;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#25991;&#20214;&#65292;&#20174;&#32780;&#20419;&#36827;&#36879;&#26126;&#24230;&#12289;&#21327;&#20316;&#21644;&#29702;&#35299;&#12290; &#23613;&#31649;&#23427;&#20204;&#26377;&#30410;&#22788;&#65292;&#20294;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21442;&#19982;&#24230;&#19981;&#19968;&#33268;&#31561;&#25361;&#25112;&#65292;ADR&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#37319;&#29992;&#36895;&#24230;&#36739;&#24930;&#12290; &#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#26377;&#21161;&#20110;&#24357;&#21512;&#36825;&#31181;&#37319;&#29992;&#24046;&#36317;&#65292;&#36890;&#36807;&#20419;&#36827;ADR&#30340;&#29983;&#25104;&#12290; &#20294;&#26159;&#65292;LLM&#29992;&#20110;ADR&#29983;&#25104;&#25110;&#29702;&#35299;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290; &#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#26088;&#22312;&#35843;&#26597;&#20351;&#29992;LLM&#36827;&#34892;&#30340;&#21487;&#34892;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01709v1 Announce Type: cross  Abstract: Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artifact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in Large Language Models (LLMs) may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of LLM for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using LLM for the 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01699</link><description>&lt;p&gt;
Brilla AI: &#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;
&lt;/p&gt;
&lt;p&gt;
Brilla AI: AI Contestant for the National Science and Maths Quiz
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01699
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21442;&#36187;&#32773;Brilla AI&#22312;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#32570;&#20047;&#21512;&#26684;&#25945;&#24072;&#30340;&#38750;&#27954;&#25552;&#20379;&#20102;&#23398;&#20064;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#22823;&#38470;&#32570;&#20047;&#36275;&#22815;&#30340;&#21512;&#26684;&#25945;&#24072;&#65292;&#36825;&#38459;&#30861;&#20102;&#25552;&#20379;&#36275;&#22815;&#30340;&#23398;&#20064;&#25903;&#25345;&#12290;&#20154;&#24037;&#26234;&#33021;&#26377;&#21487;&#33021;&#22686;&#24378;&#26377;&#38480;&#25968;&#37327;&#25945;&#24072;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#24102;&#26469;&#26356;&#22909;&#30340;&#23398;&#20064;&#25104;&#26524;&#12290;&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;NSMQ AI Grand Challenge&#30340;&#39318;&#35201;&#25104;&#26524;&#65292;&#35813;&#25361;&#25112;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29616;&#23454;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#65306;&#8220;&#24314;&#31435;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#21152;&#32435;&#30340;&#20840;&#22269;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65288;NSMQ&#65289;&#65292;&#24182;&#33719;&#32988;&#8212;&#8212;&#22312;&#27604;&#36187;&#30340;&#25152;&#26377;&#36718;&#27425;&#21644;&#38454;&#27573;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20248;&#31168;&#30340;&#21442;&#36187;&#32773;&#8221;&#12290;NSMQ&#26159;&#21152;&#32435;&#30340;&#39640;&#20013;&#23398;&#29983;&#27599;&#24180;&#20030;&#34892;&#30340;&#29616;&#22330;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#65292;3&#38431;2&#21517;&#23398;&#29983;&#36890;&#36807;&#22238;&#31572;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#29289;&#29702;&#21644;&#25968;&#23398;&#38382;&#39064;&#22312;5&#36718;&#27604;&#36187;&#20013;&#31454;&#20105;&#65292;&#36880;&#28176;&#26187;&#32423;&#33267;&#26368;&#32456;&#20896;&#20891;&#30340;&#38431;&#20237;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;Brilla AI&#65292;&#19968;&#20010;&#21442;&#21152;NSMQ&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01699v1 Announce Type: cross  Abstract: The African continent lacks enough qualified teachers which hampers the provision of adequate learning support. An AI could potentially augment the efforts of the limited number of teachers, leading to better learning outcomes. Towards that end, this work describes and evaluates the first key output for the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for such an AI: "Build an AI to compete live in Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing better than the best contestants in all rounds and stages of the competition". The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. In this work, we built Brilla AI, an AI contestant that we de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;HEED&#21644;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;MoEEF&#65292;&#26377;&#25928;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01698</link><description>&lt;p&gt;
&#32593;&#39029;&#20013;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Hypertext Entity Extraction in Webpage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;HEED&#21644;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;MoEEF&#65292;&#26377;&#25928;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#23454;&#20307;&#25552;&#21462;&#26159;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#20170;&#22823;&#22810;&#25968;&#32593;&#39029;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#37117;&#26159;&#22312;&#21147;&#27714;&#20445;&#30041;&#25991;&#26412;&#20869;&#23481;&#21450;&#20854;&#32467;&#26500;&#20449;&#24687;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HEED&#30340;&#36229;&#25991;&#26412;&#23454;&#20307;&#25552;&#21462;&#25968;&#25454;&#38598;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#26174;&#24335;&#36229;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#36827;&#34892;&#20102;&#39640;&#36136;&#37327;&#25163;&#21160;&#23454;&#20307;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;MoE&#30340;&#23454;&#20307;&#25552;&#21462;&#26694;&#26550;(MoEEF)&#65292;&#36890;&#36807;&#22810;&#19987;&#23478;&#28151;&#21512;&#26377;&#25928;&#22320;&#25972;&#21512;&#22810;&#20010;&#29305;&#24449;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01698v1 Announce Type: cross  Abstract: Webpage entity extraction is a fundamental natural language processing task in both research and applications. Nowadays, the majority of webpage entity extraction models are trained on structured datasets which strive to retain textual content and its structure information. However, existing datasets all overlook the rich hypertext features (e.g., font color, font size) which show their effectiveness in previous works. To this end, we first collect a \textbf{H}ypertext \textbf{E}ntity \textbf{E}xtraction \textbf{D}ataset (\textit{HEED}) from the e-commerce domains, scraping both the text and the corresponding explicit hypertext features with high-quality manual entity annotations. Furthermore, we present the \textbf{Mo}E-based \textbf{E}ntity \textbf{E}xtraction \textbf{F}ramework (\textit{MoEEF}), which efficiently integrates multiple features to enhance model performance by Mixture of Experts and outperforms strong baselines, includi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;</title><link>https://arxiv.org/abs/2403.01695</link><description>&lt;p&gt;
DyCE&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#21644;&#25193;&#23637;&#30340;&#21160;&#24577;&#21487;&#37197;&#32622;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01695
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#26377;&#25928;&#37096;&#32626;&#26102;&#65292;&#20351;&#29992;&#32553;&#25918;&#21644;&#21387;&#32553;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21160;&#24577;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#25552;&#21069;&#36864;&#20986;&#65289;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#26679;&#26412;&#30340;&#22256;&#38590;&#31243;&#24230;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#35745;&#31639;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#21160;&#24577;&#26041;&#27861;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#19982;&#38745;&#24577;&#26041;&#27861;&#20849;&#23384;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#29616;&#19978;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21160;&#24577;&#37096;&#20998;&#30340;&#20219;&#20309;&#21464;&#21270;&#37117;&#20250;&#24433;&#21709;&#21518;&#32493;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21160;&#24577;&#21387;&#32553;&#35774;&#35745;&#37117;&#26159;&#21333;&#29255;&#30340;&#65292;&#19982;&#22522;&#30784;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#65292;&#20174;&#32780;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#26032;&#39062;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#31181;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#35774;&#35745;&#32771;&#34385;&#30456;&#20114;&#35299;&#32806;&#20197;&#21450;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
&lt;/p&gt;</description></item><item><title>HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01693</link><description>&lt;p&gt;
HanDiffuser: &#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#22806;&#35266;&#30340;&#25991;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01693
&lt;/p&gt;
&lt;p&gt;
HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25991;&#25688;: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#24418;&#35937;&#65292;&#20294;&#22312;&#29983;&#25104;&#25163;&#37096;&#26102;&#20250;&#22833;&#21435;&#36924;&#30495;&#24230;&#12290;&#24120;&#35265;&#30340;&#32570;&#38519;&#21253;&#25324;&#19981;&#35268;&#21017;&#30340;&#25163;&#37096;&#23039;&#21183;&#12289;&#24418;&#29366;&#12289;&#38169;&#35823;&#30340;&#25163;&#25351;&#25968;&#37327;&#20197;&#21450;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#25163;&#25351;&#26041;&#21521;&#12290;&#20026;&#20102;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#31216;&#20026;HanDiffuser&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#24230;&#12290;HanDiffuser&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;:Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;SMPL-&#36523;&#20307;&#21644;MANO-&#25163;&#37096;&#21442;&#25968;&#65292;&#20197;&#21450;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19978;&#19968;&#37096;&#20214;&#29983;&#25104;&#30340;&#25552;&#31034;&#21644;&#25163;&#37096;&#21442;&#25968;&#19978;&#36827;&#34892;&#35843;&#33410;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#21512;&#24182;&#20102;&#25163;&#37096;&#34920;&#31034;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;3D&#24418;&#29366;&#21644;&#20851;&#33410;&#32423;&#25163;&#25351;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#20851;&#33410;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#21487;&#38752;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 Announce Type: cross  Abstract: Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We
&lt;/p&gt;</description></item><item><title>CATS&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#25972;&#21512;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.01673</link><description>&lt;p&gt;
CATS&#65306;&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01673
&lt;/p&gt;
&lt;p&gt;
CATS&#36890;&#36807;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#22806;&#29983;&#21464;&#37327;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#25972;&#21512;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#27169;&#22411;&#22823;&#24133;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;MTSF&#65289;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#26174;&#31034;&#65292;&#21333;&#21464;&#37327;&#27169;&#22411;&#32463;&#24120;&#20248;&#20110;&#22810;&#20803;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#20803;&#27169;&#22411;&#30340;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26500;&#24314;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#65288;CATS&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;2D&#26102;&#38388;&#19978;&#19979;&#25991;&#20851;&#27880;&#26426;&#21046;&#65292;&#20174;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#65288;OTS&#65289;&#29983;&#25104;&#36741;&#21161;&#26102;&#38388;&#24207;&#21015;&#65288;ATS&#65289;&#65292;&#20197;&#26377;&#25928;&#34920;&#31034;&#21644;&#25972;&#21512;&#31995;&#21015;&#38388;&#20851;&#31995;&#29992;&#20110;&#39044;&#27979;&#12290;ATS&#30340;&#20851;&#38190;&#21407;&#21017;-&#36830;&#32493;&#24615;&#65292;&#31232;&#30095;&#24615;&#21644;&#21464;&#24322;&#24615;-&#36890;&#36807;&#19981;&#21516;&#27169;&#22359;&#36827;&#34892;&#35782;&#21035;&#21644;&#23454;&#29616;&#12290;&#21363;&#20351;&#26159;&#22522;&#26412;&#30340;2&#23618;MLP&#20316;&#20026;&#26680;&#24515;&#39044;&#27979;&#22120;&#65292;CATS&#20063;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#22797;&#26434;&#24615;&#21644;&#21442;&#25968;&#65292;&#20351;&#20854;&#25104;&#20026;&#39640;&#25928;&#19988;&#21487;&#36716;&#31227;&#30340;MTSF&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01673v1 Announce Type: cross  Abstract: For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.
&lt;/p&gt;</description></item><item><title>Contestability&#23545;&#20110;&#25919;&#24220;&#20915;&#23450;&#20010;&#20154;&#20107;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#30740;&#35752;&#20250;&#25506;&#35752;&#20102;&#36890;&#36807;&#31454;&#20105;&#24615;&#26469;&#21457;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#24182;&#23545;&#31995;&#32479;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01649</link><description>&lt;p&gt;
&#25919;&#24220;&#21457;&#23637;&#21644;&#20351;&#29992;&#20808;&#36827;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#20915;&#23450;&#20010;&#20154;&#20107;&#21153;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Recommendations for Government Development and Use of Advanced Automated Systems to Make Decisions about Individuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01649
&lt;/p&gt;
&lt;p&gt;
Contestability&#23545;&#20110;&#25919;&#24220;&#20915;&#23450;&#20010;&#20154;&#20107;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#30740;&#35752;&#20250;&#25506;&#35752;&#20102;&#36890;&#36807;&#31454;&#20105;&#24615;&#26469;&#21457;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#24182;&#23545;&#31995;&#32479;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Contestability&#8212;&#8212;&#21363;&#26377;&#25928;&#25361;&#25112;&#20915;&#23450;&#30340;&#33021;&#21147;&#8212;&#8212;&#23545;&#20110;&#20844;&#24179;&#23454;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20851;&#20110;&#20010;&#20154;&#30340;&#25919;&#24220;&#20915;&#31574;&#32972;&#26223;&#19979;&#65292;&#31454;&#20105;&#24615;&#24448;&#24448;&#20316;&#20026;&#27491;&#24403;&#31243;&#24207;&#30340;&#35201;&#32032;&#21463;&#23466;&#27861;&#35201;&#27714;&#65307;&#29305;&#23450;&#31243;&#24207;&#21487;&#33021;&#20250;&#34987;&#24030;&#25110;&#32852;&#37030;&#27861;&#24459;&#35201;&#27714;&#30340;&#30456;&#20851;&#29305;&#23450;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#31454;&#20105;&#24615;&#20063;&#21487;&#20197;&#25104;&#20026;&#21457;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#23453;&#36149;&#26041;&#24335;&#65292;&#26377;&#21161;&#20110;&#25345;&#32493;&#35780;&#20272;&#21644;&#31995;&#32479;&#25913;&#36827;&#12290;2024&#24180;1&#26376;24-25&#26085;&#65292;&#22312;&#22269;&#23478;&#31185;&#23398;&#22522;&#37329;&#20250;&#21644;&#23041;&#24265;&#21644;&#24343;&#27931;&#25289;&#183;&#20241;&#21033;&#29305;&#22522;&#37329;&#20250;&#30340;&#25903;&#25345;&#19979;&#65292;&#25105;&#20204;&#21484;&#38598;&#20102;&#19968;&#32676;&#22810;&#26679;&#21270;&#30340;&#25919;&#24220;&#23448;&#21592;&#12289;&#39046;&#20808;&#31185;&#25216;&#20844;&#21496;&#30340;&#20195;&#34920;&#12289;&#23398;&#26415;&#30028;&#21644;&#38750;&#33829;&#21033;&#37096;&#38376;&#30340;&#25216;&#26415;&#21644;&#25919;&#31574;&#19987;&#23478;&#12289;&#20513;&#23548;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#23601;&#20808;&#36827;&#33258;&#21160;&#21270;&#20915;&#31574;&#12289;&#31454;&#20105;&#24615;&#21644;&#27861;&#24459;&#20030;&#21150;&#20102;&#19968;&#20010;&#30740;&#35752;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01649v1 Announce Type: cross  Abstract: Contestability -- the ability to effectively challenge a decision -- is critical to the implementation of fairness. In the context of governmental decision making about individuals, contestability is often constitutionally required as an element of due process; specific procedures may be required by state or federal law relevant to a particular program. In addition, contestability can be a valuable way to discover systemic errors, contributing to ongoing assessments and system improvement.   On January 24-25, 2024, with support from the National Science Foundation and the William and Flora Hewlett Foundation, we convened a diverse group of government officials, representatives of leading technology companies, technology and policy experts from academia and the non-profit sector, advocates, and stakeholders for a workshop on advanced automated decision making, contestability, and the law. Informed by the workshop's rich and wide-ranging
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#27867;&#21270;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.01621</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#19982;&#28145;&#24230;&#23398;&#20064;&#65306;&#27867;&#21270;&#38382;&#39064;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning vs Deep Learning: The Generalization Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01621
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#27867;&#21270;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#19982;&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#40065;&#26834;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#22312;&#22806;&#25512;&#26041;&#38754;&#30340;&#27604;&#36739;&#33021;&#21147;&#8212;&#8212;&#36825;&#26159;&#27867;&#21270;&#30340;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#23545;&#19981;&#22312;&#20854;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01621v1 Announce Type: cross  Abstract: The capacity to generalize beyond the range of training data is a pivotal challenge, often synonymous with a model's utility and robustness. This study investigates the comparative abilities of traditional machine learning (ML) models and deep learning (DL) algorithms in terms of extrapolation -- a more challenging aspect of generalization because it requires the model to make inferences about data points that lie outside the domain it has been trained on. We present an empirical analysis where both ML and DL models are trained on an exponentially growing function and then tested on values outside the training domain. The choice of this function allows us to distinctly showcase the divergence in performance when models are required to predict beyond the scope of their training data. Our findings suggest that deep learning models possess inherent capabilities to generalize beyond the training scope, an essential feature for real-world a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#33258;&#21160;&#25512;&#26029;&#36816;&#21160;&#32452;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.01606</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Unified Model Selection Technique for Spectral Clustering Based Motion Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#33258;&#21160;&#25512;&#26029;&#36816;&#21160;&#32452;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#20154;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#26041;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36816;&#21160;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#36816;&#21160;&#20851;&#31995;&#30697;&#38453;&#25191;&#34892;&#35889;&#32858;&#31867;&#65292;&#23558;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#25110;&#28857;&#36712;&#36857;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#36816;&#21160;&#32452;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#30693;&#36947;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#25968;&#37327;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#30340;&#29616;&#26377;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#33258;&#21160;&#25512;&#26029;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#36816;&#21160;&#20998;&#21106;&#26041;&#27861;&#30340;&#36816;&#21160;&#32452;&#25968;&#12290;&#25105;&#20204;&#22312;KT3DMoSeg&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#22522;&#20934;&#32467;&#26524;&#36827;&#34892;&#20102;&#31454;&#20105;&#24615;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01606v1 Announce Type: cross  Abstract: Motion segmentation is a fundamental problem in computer vision and is crucial in various applications such as robotics, autonomous driving and action recognition. Recently, spectral clustering based methods have shown impressive results on motion segmentation in dynamic environments. These methods perform spectral clustering on motion affinity matrices to cluster objects or point trajectories in the scene into different motion groups. However, existing methods often need the number of motions present in the scene to be known, which significantly reduces their practicality. In this paper, we propose a unified model selection technique to automatically infer the number of motion groups for spectral clustering based motion segmentation methods by combining different existing model selection techniques together. We evaluate our method on the KT3DMoSeg dataset and achieve competitve results comparing to the baseline where the number of clu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#20462;&#27491;&#27531;&#24046;&#35823;&#24046;&#65292;&#26377;&#26395;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.01605</link><description>&lt;p&gt;
&#26397;&#21521;&#21487;&#35777;&#26126;&#30340;&#23545;&#25968;&#23494;&#24230;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Provable Log Density Policy Gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#20462;&#27491;&#27531;&#24046;&#35823;&#24046;&#65292;&#26377;&#26395;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26159;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#29616;&#20195;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#34429;&#28982;&#25104;&#21151;&#65292;&#20294;&#22312;&#26799;&#24230;&#20272;&#35745;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27531;&#24046;&#35823;&#24046;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#20010;&#27531;&#24046;&#39033;&#24456;&#37325;&#35201;&#65292;&#32416;&#27491;&#23427;&#26377;&#21487;&#33021;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#65292;&#21487;&#20197;&#32416;&#27491;&#36825;&#20010;&#27531;&#24046;&#35823;&#24046;&#39033;&#12290;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29366;&#24577;-&#21160;&#20316;&#25240;&#25187;&#20998;&#24067;&#24418;&#24335;&#26469;&#35745;&#31639;&#31574;&#30053;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#32473;&#20986;&#20102;&#20934;&#30830;&#25214;&#21040;&#26631;&#31614;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#25152;&#38656;&#30340;&#26041;&#31243;&#24335;&#12290;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21518;&#21521;&#21363;&#26102;&#65288;TD&#65289;&#26041;&#27861;&#26469;&#36817;&#20284;&#35745;&#31639;&#23545;&#25968;&#23494;&#24230;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21518;&#21521;&#30340;&#21516;&#31574;&#30053;&#26679;&#26412;&#12290;&#30001;&#20110;&#20174;&#39532;&#23572;&#21487;&#22827;&#38142;&#20013;&#36827;&#34892;&#21518;&#21521;&#37319;&#26679;&#26159;&#39640;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36139;&#22256;&#24656;&#24807;&#20195;&#29702;&#27169;&#22411;&#65288;AABM&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#35745;&#31639;&#19978;&#25552;&#20379;&#20102;&#23545;&#36139;&#22256;&#21644;&#36139;&#22256;&#24656;&#24807;&#20043;&#38388;&#20851;&#31995;&#30340;&#35777;&#25454;&#65292;&#20197;&#39564;&#35777;&#27495;&#35270;&#23545;&#36139;&#22256;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01600</link><description>&lt;p&gt;
&#21487;&#36890;&#36807;&#25171;&#20987;&#27495;&#35270;&#26469;&#20943;&#23569;&#36139;&#22256;&#21527;&#65311;&#29992;&#20110;&#25919;&#31574;&#21046;&#23450;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model for Policy Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01600
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36139;&#22256;&#24656;&#24807;&#20195;&#29702;&#27169;&#22411;&#65288;AABM&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#35745;&#31639;&#19978;&#25552;&#20379;&#20102;&#23545;&#36139;&#22256;&#21644;&#36139;&#22256;&#24656;&#24807;&#20043;&#38388;&#20851;&#31995;&#30340;&#35777;&#25454;&#65292;&#20197;&#39564;&#35777;&#27495;&#35270;&#23545;&#36139;&#22256;&#32531;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#65292;&#36139;&#22256;&#29575;&#19979;&#38477;&#36895;&#24230;&#20943;&#32531;&#65292;&#34920;&#26126;&#20256;&#32479;&#30340;&#36139;&#22256;&#32531;&#35299;&#26041;&#27861;&#21487;&#33021;&#27491;&#22312;&#22833;&#21435;&#25928;&#21147;&#65292;&#38656;&#35201;&#26367;&#20195;&#24615;&#35265;&#35299;&#26469;&#25512;&#21160;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#39318;&#35201;&#30446;&#26631;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36139;&#22256;&#24656;&#24807;&#20195;&#29702;&#27169;&#22411;&#65288;Aporophobia Agent-Based Model&#65292;AABM&#65289;&#65292;&#20197;&#25552;&#20379;&#35745;&#31639;&#19978;&#30340;&#35777;&#25454;&#34920;&#26126;&#36139;&#22256;&#24656;&#24807;&#19982;&#36139;&#22256;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20351;&#29992;&#24052;&#22622;&#32599;&#37027;&#24066;&#30340;&#23454;&#38469;&#20154;&#21475;&#25968;&#25454;&#21644;&#20943;&#36139;&#20844;&#20849;&#25919;&#31574;&#65288;&#24050;&#23454;&#26045;&#25110;&#27491;&#22312;&#35758;&#20250;&#35752;&#35770;&#20013;&#65289;&#26500;&#24314;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#23558;&#25919;&#31574;&#20998;&#31867;&#20026;&#27495;&#35270;&#24615;&#25110;&#38750;&#27495;&#35270;&#24615;&#23545;&#36139;&#22256;&#20154;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01600v1 Announce Type: cross  Abstract: In the last decades, there has been a deceleration in the rates of poverty reduction, suggesting that traditional redistributive approaches to poverty mitigation could be losing effectiveness, and alternative insights to advance the number one UN Sustainable Development Goal are required. The criminalization of poor people has been denounced by several NGOs, and an increasing number of voices suggest that discrimination against the poor (a phenomenon known as \emph{aporophobia}) could be an impediment to mitigating poverty. In this paper, we present the novel Aporophobia Agent-Based Model (AABM) to provide evidence of the correlation between aporophobia and poverty computationally. We present our use case built with real-world demographic data and poverty-mitigation public policies (either enforced or under parliamentary discussion) for the city of Barcelona. We classify policies as discriminatory or non-discriminatory against the poor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.01599</link><description>&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SCHEMA&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36861;&#36394;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#32473;&#20986;&#26681;&#25454;&#37096;&#20998;&#35270;&#35273;&#29366;&#24577;&#35266;&#23519;&#29983;&#25104;&#30446;&#26631;&#23548;&#21521;&#30340;&#21160;&#20316;&#27493;&#39588;&#24207;&#21015;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21160;&#26426;&#26159;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#32467;&#26500;&#21270;&#19988;&#21487;&#35268;&#21010;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#29366;&#24577;&#21464;&#21270;&#23545;&#20110;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#31243;&#24207;&#35268;&#21010;&#24456;&#37325;&#35201;&#65292;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#27493;&#39588;&#21644;&#29366;&#24577;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#31435;&#26356;&#20026;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#27493;&#39588;&#26174;&#24335;&#22320;&#34920;&#31034;&#20026;&#29366;&#24577;&#21464;&#21270;&#65292;&#24182;&#36319;&#36394;&#31243;&#24207;&#20013;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01599v1 Announce Type: cross  Abstract: We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#20986;&#19981;&#38656;&#35201;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#21644;API&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#24102;&#26469;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.01598</link><description>&lt;p&gt;
APISR: &#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
APISR: Anime Production Inspired Real-World Anime Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#21160;&#28459;&#21046;&#20316;&#21551;&#21457;&#30340;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#20986;&#19981;&#38656;&#35201;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#21644;API&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#24102;&#26469;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#22312;SR&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#26469;&#33258;&#20889;&#23454;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21160;&#28459;&#21046;&#20316;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#22914;&#20309;&#21033;&#29992;&#20854;&#20013;&#30340;&#29305;&#24449;&#26469;&#20419;&#36827;&#30495;&#23454;&#19990;&#30028;&#21160;&#28459;SR&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35748;&#20026;&#35270;&#39057;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#21160;&#28459;SR&#24182;&#19981;&#26159;&#24517;&#38656;&#30340;&#65292;&#22240;&#20026;&#25163;&#32472;&#24103;&#30340;&#37325;&#22797;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#28459;&#22270;&#20687;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20174;&#35270;&#39057;&#28304;&#20013;&#36873;&#25321;&#26368;&#23569;&#21387;&#32553;&#21644;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#12290;&#22522;&#20110;&#35813;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#28459;&#21046;&#20316;&#23548;&#21521;&#30340;&#22270;&#20687;&#65288;API&#65289;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21160;&#28459;&#29305;&#26377;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#25197;&#26354;&#21644;&#28129;&#34180;&#30340;&#25163;&#32472;&#32447;&#26465;&#20197;&#21450;&#19981;&#38656;&#35201;&#30340;&#33394;&#24425;&#20266;&#24433;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#36864;&#21270;&#27169;&#22411;&#20013;&#24341;&#20837;&#38754;&#21521;&#39044;&#27979;&#30340;&#21387;&#32553;&#27169;&#22359;&#21644;&#20266;&#22522;&#20934;&#25968;&#25454;&#20934;&#22791;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01598v1 Announce Type: cross  Abstract: While real-world anime super-resolution (SR) has gained increasing attention in the SR community, existing methods still adopt techniques from the photorealistic domain. In this paper, we analyze the anime production workflow and rethink how to use characteristics of it for the sake of the real-world anime SR. First, we argue that video networks and datasets are not necessary for anime SR due to the repetition use of hand-drawing frames. Instead, we propose an anime image collection pipeline by choosing the least compressed and the most informative frames from the video sources. Based on this pipeline, we introduce the Anime Production-oriented Image (API) dataset. In addition, we identify two anime-specific challenges of distorted and faint hand-drawn lines and unwanted color artifacts. We address the first issue by introducing a prediction-oriented compression module in the image degradation model and a pseudo-ground truth preparatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;Transformer&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#24320;&#21457;&#20102;gaHealth&#65292;&#39318;&#20010;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32763;&#35793;&#36136;&#37327;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01580</link><description>&lt;p&gt;
&#21152;&#24378;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65306;&#35821;&#26009;&#24211;&#24320;&#21457;&#12289;&#20154;&#31867;&#35780;&#20272;&#21644;&#21487;&#35299;&#37322;&#30340; AI &#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Neural Machine Translation of Low-Resource Languages: Corpus Development, Human Evaluation and Explainable AI Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;Transformer&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#24320;&#21457;&#20102;gaHealth&#65292;&#39318;&#20010;&#38024;&#23545;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32763;&#35793;&#36136;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#20013;&#65292;Transformer&#26550;&#26500;&#23588;&#20854;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#33073;&#39062;&#32780;&#20986;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23427;&#22312;&#21253;&#25324;&#33521;&#35821;$\leftrightarrow$&#29233;&#23572;&#20848;&#35821;&#21644;&#33521;&#35821;$\leftrightarrow$&#39532;&#25289;&#22320;&#35821;&#35821;&#35328;&#23545;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#26368;&#20339;&#36229;&#21442;&#25968;&#21644;&#23376;&#35789;&#27169;&#22411;&#31867;&#22411;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;Transformer&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#23545;&#20013;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#21487;&#33021;&#38459;&#30861;MT&#30340;&#21457;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;gaHealth&#65292;&#36825;&#26159;&#29233;&#23572;&#20848;&#35821;&#20581;&#24247;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#21452;&#35821;&#35821;&#26009;&#24211;&#12290;&#20351;&#29992;&#36825;&#19968;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#22312;&#19982;LoResMT2021&#20849;&#20139;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#26102;&#65292;&#22312;BLEU&#35780;&#20998;&#19978;&#34920;&#29616;&#20986;&#38750;&#24120;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#38543;&#21518;&#30340;&#20154;&#31867;&#35780;&#20272;&#20351;&#29992;&#22810;&#32500;&#24230;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01580v1 Announce Type: cross  Abstract: In the current machine translation (MT) landscape, the Transformer architecture stands out as the gold standard, especially for high-resource language pairs. This research delves into its efficacy for low-resource language pairs including both the English$\leftrightarrow$Irish and English$\leftrightarrow$Marathi language pairs. Notably, the study identifies the optimal hyperparameters and subword model type to significantly improve the translation quality of Transformer models for low-resource language pairs.   The scarcity of parallel datasets for low-resource languages can hinder MT development. To address this, gaHealth was developed, the first bilingual corpus of health data for the Irish language. Focusing on the health domain, models developed using this in-domain dataset exhibited very significant improvements in BLEU score when compared with models from the LoResMT2021 Shared Task. A subsequent human evaluation using the multid
&lt;/p&gt;</description></item><item><title>SARD&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#25925;&#20107;&#29983;&#25104;&#24037;&#20855;, &#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#31456;&#33410;&#25925;&#20107;&#65292;&#35780;&#20272;&#34920;&#26126;&#65306;&#33410;&#28857;&#24335;&#21465;&#20107;&#30340;&#21487;&#35270;&#21270;&#21487;&#33021;&#24110;&#21161;&#20316;&#32773;&#26500;&#24314;&#24515;&#26234;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#20250;&#32473;&#20316;&#32773;&#24102;&#26469;&#39069;&#22806;&#30340;&#24515;&#26234;&#36127;&#25285;&#65292;&#24182;&#22312;&#25925;&#20107;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#25104;&#20026;&#24178;&#25200;&#28304;&#65292;AI&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#35789;&#27719;&#22810;&#26679;&#24615;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.01575</link><description>&lt;p&gt;
SARD: &#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#25925;&#20107;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SARD: A Human-AI Collaborative Story Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01575
&lt;/p&gt;
&lt;p&gt;
SARD&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#25925;&#20107;&#29983;&#25104;&#24037;&#20855;, &#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#31456;&#33410;&#25925;&#20107;&#65292;&#35780;&#20272;&#34920;&#26126;&#65306;&#33410;&#28857;&#24335;&#21465;&#20107;&#30340;&#21487;&#35270;&#21270;&#21487;&#33021;&#24110;&#21161;&#20316;&#32773;&#26500;&#24314;&#24515;&#26234;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#20250;&#32473;&#20316;&#32773;&#24102;&#26469;&#39069;&#22806;&#30340;&#24515;&#26234;&#36127;&#25285;&#65292;&#24182;&#22312;&#25925;&#20107;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#25104;&#20026;&#24178;&#25200;&#28304;&#65292;AI&#29983;&#25104;&#30340;&#25925;&#20107;&#22312;&#35789;&#27719;&#22810;&#26679;&#24615;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence (GenAI)&#24320;&#21019;&#20102;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;&#25925;&#20107;&#21019;&#20316;&#32773;&#65292;&#20026;&#28857;&#29123;&#21019;&#36896;&#21147;&#21644;&#25506;&#32034;&#26410;&#30693;&#21465;&#20107;&#39046;&#22495;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#38543;&#30528;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26377;&#21487;&#33021;&#37325;&#26032;&#23450;&#20041;&#21465;&#20107;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SARD&#65292;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#31456;&#33410;&#25925;&#20107;&#30340;&#21487;&#25302;&#25918;&#24335;&#35270;&#35273;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01575v1 Announce Type: cross  Abstract: Generative artificial intelligence (GenAI) has ushered in a new era for storytellers, providing a powerful tool to ignite creativity and explore uncharted narrative territories. As technology continues to advance, the synergy between human creativity and AI-generated content holds the potential to redefine the landscape of storytelling. In this work, we propose SARD, a drag-and-drop visual interface for generating a multi-chapter story using large language models. Our evaluation of the usability of SARD and its creativity support shows that while node-based visualization of the narrative may help writers build a mental model, it exerts unnecessary mental overhead to the writer and becomes a source of distraction as the story becomes more elaborated. We also found that AI generates stories that are less lexically diverse, irrespective of the complexity of the story. We identified some patterns and limitations of our tool that can guide 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;SlowTV&#21644;CribsTV&#65292;&#36890;&#36807;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;SS-MDE&#65289;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.01569</link><description>&lt;p&gt;
&#25918;&#26494;&#24182;&#25918;&#26494;++&#65306;&#36890;&#36807;SlowTV&#21644;CribsTV&#36229;&#36234;&#22320;&#38754;&#30495;&#23454;&#28145;&#24230;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Kick Back &amp; Relax++: Scaling Beyond Ground-Truth Depth with SlowTV &amp; CribsTV
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;SlowTV&#21644;CribsTV&#65292;&#36890;&#36807;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;SS-MDE&#65289;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#35299;&#38145;&#36890;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#22320;&#38754;&#30495;&#23454;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#23427;&#20801;&#35768;&#25193;&#23637;&#21040;&#26356;&#22823;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65288;SS-MDE&#65289;&#21463;&#21040;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#32570;&#20047;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#20165;&#20851;&#27880;&#20110;&#23494;&#38598;&#20154;&#21475;&#22478;&#24066;&#20013;&#30340;&#22478;&#24066;&#39550;&#39542;&#65292;&#23548;&#33268;&#27169;&#22411;&#26080;&#27861;&#25512;&#24191;&#21040;&#27492;&#39046;&#22495;&#20043;&#22806;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65306;SlowTV&#21644;CribsTV&#12290;&#36825;&#20123;&#26159;&#20174;&#20844;&#24320;&#30340;YouTube&#35270;&#39057;&#20013;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#24635;&#20849;2M&#35757;&#32451;&#24103;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#38750;&#24120;&#22810;&#26679;&#21270;&#30340;&#29615;&#22659;&#38598;&#65292;&#20174;&#22810;&#38634;&#30340;&#26862;&#26519;&#21040;&#27839;&#28023;&#36947;&#36335;&#65292;&#35946;&#21326;&#35946;&#23429;&#65292;&#29978;&#33267;&#27700;&#19979;&#29642;&#29786;&#30977;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#38646;-shot&#27867;&#21270;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#32988;&#36807;&#27599;&#20010;&#29616;&#26377;&#30340;SS-MD
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01569v1 Announce Type: cross  Abstract: Self-supervised learning is the key to unlocking generic computer vision systems. By eliminating the reliance on ground-truth annotations, it allows scaling to much larger data quantities. Unfortunately, self-supervised monocular depth estimation (SS-MDE) has been limited by the absence of diverse training data. Existing datasets have focused exclusively on urban driving in densely populated cities, resulting in models that fail to generalize beyond this domain.   To address these limitations, this paper proposes two novel datasets: SlowTV and CribsTV. These are large-scale datasets curated from publicly available YouTube videos, containing a total of 2M training frames. They offer an incredibly diverse set of environments, ranging from snowy forests to coastal roads, luxury mansions and even underwater coral reefs. We leverage these datasets to tackle the challenging task of zero-shot generalization, outperforming every existing SS-MD
&lt;/p&gt;</description></item><item><title>ReMatch&#26041;&#27861;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#27169;&#24335;&#21305;&#37197;&#65292;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;</title><link>https://arxiv.org/abs/2403.01567</link><description>&lt;p&gt;
ReMatch: &#22522;&#20110;LLMs&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#24335;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ReMatch: Retrieval Enhanced Schema Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01567
&lt;/p&gt;
&lt;p&gt;
ReMatch&#26041;&#27861;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#27169;&#24335;&#21305;&#37197;&#65292;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#24335;&#21305;&#37197;&#22312;&#25968;&#25454;&#38598;&#25104;&#20013;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#28304;&#25968;&#25454;&#24211;&#27169;&#24335;&#19982;&#30446;&#26631;&#27169;&#24335;&#36827;&#34892;&#23545;&#40784;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#20803;&#32032;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReMatch&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21305;&#37197;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#39044;&#23450;&#20041;&#26144;&#23556;&#12289;&#27169;&#22411;&#35757;&#32451;&#25110;&#23545;&#28304;&#25968;&#25454;&#24211;&#20013;&#25968;&#25454;&#30340;&#35775;&#38382;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01567v1 Announce Type: cross  Abstract: Schema matching is a crucial task in data integration, involving the alignment of a source database schema with a target schema to establish correspondence between their elements. This task is challenging due to textual and semantic heterogeneity, as well as differences in schema sizes. Although machine-learning-based solutions have been explored in numerous studies, they often suffer from low accuracy, require manual mapping of the schemas for model training, or need access to source schema data which might be unavailable due to privacy concerns. In this paper we present a novel method, named ReMatch, for matching schemas using retrieval-enhanced Large Language Models (LLMs). Our method avoids the need for predefined mapping, any model training, or access to data in the source database. In the ReMatch method the tables of the target schema and the attributes of the source schema are first represented as structured passage-based docume
&lt;/p&gt;</description></item><item><title>ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2403.01564</link><description>&lt;p&gt;
ComTraQ-MPC&#65306;&#20803;&#35757;&#32451;&#30340;DQN-MPC&#38598;&#25104;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01564
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#21487;&#35266;&#23519;&#12289;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36712;&#36857;&#36319;&#36394;&#30340;&#26368;&#20339;&#20915;&#31574;&#24448;&#24448;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#26159;&#25351;&#20195;&#29702;&#20174;&#20256;&#24863;&#22120;&#33719;&#21462;&#30495;&#23454;&#29366;&#24577;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#24179;&#34913;&#36164;&#28304;&#20445;&#23384;&#12289;&#20934;&#30830;&#29366;&#24577;&#20272;&#35745;&#21644;&#31934;&#30830;&#36319;&#36394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ComTraQ-MPC&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Deep Q-Networks (DQN)&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;&#20803;&#35757;&#32451;&#30340;DQN&#30830;&#20445;&#20102;&#33258;&#36866;&#24212;&#20027;&#21160;&#23450;&#20301;&#35843;&#24230;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01564v1 Announce Type: cross  Abstract: Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#25506;&#32034;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#20026;&#24515;&#33039;&#30149;&#24739;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.01533</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#25910;&#32553;&#26102;&#38388;&#38388;&#38548;&#21644;&#26085;&#24120;&#25910;&#38598;&#30340;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;&#24613;&#24615;&#24515;&#32908;&#26775;&#27515;&#21518;&#30340;&#38271;&#26399;&#27515;&#20129;&#29575;
&lt;/p&gt;
&lt;p&gt;
Machine learning predicts long-term mortality after acute myocardial infarction using systolic time intervals and routinely collected clinical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#26032;&#29983;&#29289;&#26631;&#24535;&#29289;&#25506;&#32034;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#65292;&#20026;&#24515;&#33039;&#30149;&#24739;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#20272;&#35745;&#24515;&#33039;&#24739;&#32773;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#21512;&#24182;&#30151;&#26159;&#20248;&#20808;&#32771;&#34385;&#36830;&#32493;&#29983;&#29702;&#30417;&#27979;&#21644;&#26032;&#30103;&#27861;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#24739;&#26377;&#24515;&#33039;&#30149;&#30340;&#24739;&#32773;&#30701;&#26399;&#27515;&#20129;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38271;&#26399;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#29992;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38271;&#26399;&#27515;&#20129;&#29575;&#39044;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#20004;&#20010;&#26368;&#36817;&#24341;&#20837;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#38271;&#26399;&#27515;&#20129;&#29575;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#21488;&#28286;&#20013;&#22269;&#21355;&#29983;&#31119;&#21033;&#37096;CCHIA&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#12290;&#21307;&#30103;&#35760;&#24405;&#29992;&#20110;&#25910;&#38598;&#21253;&#25324;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;BMI&#12289;&#32463;&#30382;&#20896;&#29366;&#21160;&#33033;&#20171;&#20837;&#65288;PCI&#65289;&#29366;&#24577;&#21644;&#39640;&#34880;&#21387;&#12289;&#34880;&#33026;&#24322;&#24120;&#12289;ST&#27573;&#25260;&#39640;&#22411;&#24515;&#32908;&#26775;&#27515;&#65288;STEMI&#65289;&#21644;&#38750;STEMI&#31561;&#21512;&#24182;&#30151;&#22312;&#20869;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#21644;&#20020;&#24202;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01533v1 Announce Type: cross  Abstract: Precise estimation of cardiac patients' current and future comorbidities is an important factor in prioritizing continuous physiological monitoring and new therapies. ML models have shown satisfactory performance in short-term mortality prediction of patients with heart disease, while their utility in long-term predictions is limited. This study aims to investigate the performance of tree-based ML models on long-term mortality prediction and the effect of two recently introduced biomarkers on long-term mortality. This study utilized publicly available data from CCHIA at the Ministry of Health and Welfare, Taiwan, China. Medical records were used to gather demographic and clinical data, including age, gender, BMI, percutaneous coronary intervention (PCI) status, and comorbidities such as hypertension, dyslipidemia, ST-segment elevation myocardial infarction (STEMI), and non-STEMI. Using medical and demographic records as well as two rec
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.01528</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01528
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;BL&#65289;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#28304;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20016;&#23500;&#22810;&#38754;&#25551;&#36848;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#35745;&#31639;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#36798;&#30340;&#24494;&#22937;&#21465;&#36848;&#19982;&#36890;&#36807;&#21508;&#31181;&#20998;&#23376;&#24314;&#27169;&#25216;&#26415;&#25551;&#36848;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#32454;&#33410;&#34701;&#21512;&#65292;&#25171;&#24320;&#20102;&#20840;&#38754;&#34920;&#24449;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#30340;&#26032;&#36884;&#24452;&#12290;&#36890;&#36807;&#23558;&#22260;&#32469;&#29983;&#29289;&#20998;&#23376;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#25968;&#25454;&#32435;&#20837;&#24314;&#27169;&#20013;&#65292;BL&#26088;&#22312;&#25429;&#25417;&#21253;&#21547;&#35821;&#35328;&#20256;&#36798;&#30340;&#31526;&#21495;&#29305;&#24615;&#20197;&#21450;&#25968;&#37327;&#21270;&#32467;&#26500;&#29305;&#24449;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#20154;&#20307;&#23454;&#20363;&#25248;&#22270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36890;&#29992;&#24863;&#30693;&#32593;&#32476;&#21644;&#32479;&#19968;&#24341;&#23548;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#23454;&#20363;&#25248;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.01510</link><description>&lt;p&gt;
&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#20154;&#20307;&#23454;&#20363;&#25248;&#22270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Human Instance Matting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#20154;&#20307;&#23454;&#20363;&#25248;&#22270;&#26694;&#26550;&#65292;&#36890;&#36807;&#36890;&#29992;&#24863;&#30693;&#32593;&#32476;&#21644;&#32479;&#19968;&#24341;&#23548;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#23454;&#20363;&#25248;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#23454;&#20363;&#25248;&#22270;&#26088;&#22312;&#20026;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20154;&#20307;&#23454;&#20363;&#20272;&#35745;&#19968;&#20010;alpha&#28145;&#24230;&#22270;&#65292;&#36825;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#20154;&#20307;&#23454;&#20363;&#25248;&#22270;&#65288;E2E-HIM&#65289;&#26694;&#26550;&#65292;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#21516;&#26102;&#36827;&#34892;&#22810;&#20010;&#23454;&#20363;&#30340;&#25248;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#36890;&#29992;&#24863;&#30693;&#32593;&#32476;&#39318;&#20808;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#24182;&#23558;&#23454;&#20363;&#19978;&#19979;&#25991;&#35299;&#30721;&#20026;&#28508;&#22312;&#32534;&#30721;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#32479;&#19968;&#24341;&#23548;&#32593;&#32476;&#21033;&#29992;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#32479;&#19968;&#30340;&#35821;&#20041;&#24341;&#23548;&#65292;&#20854;&#20013;&#32534;&#30721;&#20102;&#20301;&#32622;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01510v1 Announce Type: cross  Abstract: Human instance matting aims to estimate an alpha matte for each human instance in an image, which is extremely challenging and has rarely been studied so far. Despite some efforts to use instance segmentation to generate a trimap for each instance and apply trimap-based matting methods, the resulting alpha mattes are often inaccurate due to inaccurate segmentation. In addition, this approach is computationally inefficient due to multiple executions of the matting method. To address these problems, this paper proposes a novel End-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple instance matting in a more efficient manner. Specifically, a general perception network first extracts image features and decodes instance contexts into latent codes. Then, a united guidance network exploits spatial attention and semantics embedding to generate united semantics guidance, which encodes the locations and semantic correspo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36719;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.01508</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#30340;&#36719;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Reasoning on Uncertain Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#30830;&#23450;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36719;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#30693;&#35782;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#39033;&#30740;&#31350;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36923;&#36753;&#26597;&#35810;&#22238;&#31572;&#30340;&#30740;&#31350;&#12290;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#30830;&#23450;&#30693;&#35782;&#19978;&#30340;&#36719;&#26597;&#35810;&#35774;&#32622;&#65292;&#21463;&#36719;&#32422;&#26463;&#32534;&#31243;&#30340;&#24314;&#31435;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#21069;&#21521;&#25512;&#29702;&#21448;&#20855;&#26377;&#21518;&#21521;&#26657;&#20934;&#65292;&#29992;&#20110;&#22238;&#31572;&#22823;&#35268;&#27169;&#12289;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#22270;&#19978;&#30340;&#36719;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01508v1 Announce Type: new  Abstract: The study of machine learning-based logical query-answering enables reasoning with large-scale and incomplete knowledge graphs. This paper further advances this line of research by considering the uncertainty in the knowledge. The uncertain nature of knowledge is widely observed in the real world, but \textit{does not} align seamlessly with the first-order logic underpinning existing studies. To bridge this gap, we study the setting of soft queries on uncertain knowledge, which is motivated by the establishment of soft constraint programming. We further propose an ML-based approach with both forward inference and backward calibration to answer soft queries on large-scale, incomplete, and uncertain knowledge graphs. Theoretical discussions present that our methods share the same complexity as state-of-the-art inference algorithms for first-order queries. Empirical results justify the superior performance of our approach against previous M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;</title><link>https://arxiv.org/abs/2403.01489</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#28335;&#28304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22522;&#20110;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#21487;&#33021;&#34987;&#28389;&#29992;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#12290;&#32473;&#23450;&#19968;&#20010;&#24453;&#24402;&#22240;&#30340;&#27979;&#35797;&#22270;&#20687;&#65292;&#39318;&#20808;&#25105;&#20204;&#21453;&#21521;&#37325;&#24314;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#37325;&#24314;&#30340;&#25552;&#31034;&#25918;&#20837;&#19981;&#21516;&#30340;&#20505;&#36873;&#27169;&#22411;&#20013;&#20197;&#20877;&#29616;&#20505;&#36873;&#20551;&#22270;&#20687;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#25490;&#21517;&#27979;&#35797;&#22270;&#20687;&#19982;&#20505;&#36873;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22270;&#20687;&#30340;&#26469;&#28304;&#12290;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#20854;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38480;&#21046;&#20505;&#36873;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01489v1 Announce Type: cross  Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image genera
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;Directional Graph Attention Network&#65288;DGAT&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#32467;&#21512;&#29305;&#24449;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#65292;&#36890;&#36807;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#26469;&#25552;&#21319;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01475</link><description>&lt;p&gt;
&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#24322;&#36136;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Heterophilic Graph with Directional Neighborhood Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01475
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#26041;&#21521;&#24615;&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;Directional Graph Attention Network&#65288;DGAT&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#32467;&#21512;&#29305;&#24449;&#27880;&#24847;&#21147;&#21644;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#65292;&#36890;&#36807;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#65292;&#24182;&#24341;&#20837;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#26469;&#25552;&#21319;&#24322;&#36136;&#22270;&#30340;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20043;&#19968;&#65292;&#23427;&#37319;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#23398;&#20064;&#36793;&#32536;&#26435;&#37325;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#21482;&#21253;&#21547;&#20102;&#26469;&#33258;&#21363;&#26102;&#37051;&#22495;&#30340;&#20449;&#24687;&#65292;&#32570;&#20047;&#25429;&#33719;&#36828;&#31243;&#21644;&#20840;&#23616;&#22270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#24322;&#36136;&#22270;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#26041;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DGAT&#65289;&#12290;DGAT&#33021;&#22815;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#27880;&#24847;&#21147;&#19982;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#20840;&#23616;&#26041;&#21521;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#33410;&#28857;&#20043;&#38388;&#30340;&#25193;&#25955;&#36317;&#31163;&#12290;&#22522;&#20110;&#26032;&#30340;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#65292;&#25552;&#20986;&#20102;&#25299;&#25169;&#24341;&#23548;&#30340;&#37051;&#22495;&#20462;&#21098;&#21644;&#36793;&#28155;&#21152;&#26426;&#21046;&#65292;&#20197;&#28040;&#38500;&#22122;&#22768;&#21644;&#38480;&#21046;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01475v1 Announce Type: cross  Abstract: Graph Attention Network (GAT) is one of the most popular Graph Neural Network (GNN) architecture, which employs the attention mechanism to learn edge weights and has demonstrated promising performance in various applications. However, since it only incorporates information from immediate neighborhood, it lacks the ability to capture long-range and global graph information, leading to unsatisfactory performance on some datasets, particularly on heterophilic graphs. To address this limitation, we propose the Directional Graph Attention Network (DGAT) in this paper. DGAT is able to combine the feature-based attention with the global directional information extracted from the graph topology. To this end, a new class of Laplacian matrices is proposed which can provably reduce the diffusion distance between nodes. Based on the new Laplacian, topology-guided neighbour pruning and edge adding mechanisms are proposed to remove the noisy and cap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCTA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#30340;&#27169;&#22411;&#36866;&#24212;&#21644;&#22270;&#36866;&#24212;&#26469;&#35299;&#20915;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01467</link><description>&lt;p&gt;
&#21327;&#20316;&#36866;&#24212;&#65306;&#36890;&#36807;&#21452;&#21521;&#36866;&#24212;&#23454;&#29616;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphCTA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#20316;&#30340;&#27169;&#22411;&#36866;&#24212;&#21644;&#22270;&#36866;&#24212;&#26469;&#35299;&#20915;&#26080;&#28304;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UGDA&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#26631;&#35760;&#20016;&#23500;&#30340;&#28304;&#22270;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#23436;&#20840;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22270;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#26631;&#35760;&#20016;&#23500;&#30340;&#28304;&#22270;&#25552;&#20379;&#30417;&#30563;&#20449;&#21495;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#21407;&#22240;&#26159;&#35268;&#23450;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#28304;&#26080;&#30417;&#30563;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22330;&#26223;&#65292;&#35797;&#22270;&#35299;&#20915;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#32780;&#19981;&#20351;&#29992;&#26631;&#35760;&#30340;&#28304;&#22270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GraphCTA&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#31243;&#24207;&#21327;&#20316;&#22320;&#25191;&#34892;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#22270;&#33258;&#36866;&#24212;&#65306;&#65288;1&#65289;&#22522;&#20110;&#30446;&#26631;&#22270;&#20013;&#33410;&#28857;&#37051;&#22495;&#39044;&#27979;&#36827;&#34892;&#27169;&#22411;&#33258;&#36866;&#24212;&#65292;&#32771;&#34385;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65307;&#65288;2&#65289;&#36890;&#36807;&#37051;&#22495;&#23545;&#27604;&#24615;&#22320;&#26356;&#26032;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#26469;&#25191;&#34892;&#22270;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01467v1 Announce Type: cross  Abstract: Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive le
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#20013;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#26377;&#25928;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;</title><link>https://arxiv.org/abs/2403.01456</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;PLM&#30340;&#20195;&#29702;&#27169;&#22411;&#25511;&#21046;IRT&#35780;&#20272;&#20013;&#30340;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#22635;&#31354;&#27979;&#35797;&#39064;&#30446;&#20013;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#26377;&#25928;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#38590;&#24230;&#22312;&#33258;&#36866;&#24212;&#27979;&#35797;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#29983;&#25104;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22810;&#39033;&#36873;&#25321;&#65288;MC&#65289;&#22635;&#31354;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#35780;&#20272;&#65292;&#36991;&#20813;&#38656;&#35201;&#20154;&#31867;&#27979;&#35797;&#23545;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#36890;&#36807;&#25490;&#21517;&#35268;&#21017;&#25511;&#21046;&#31354;&#30333;&#21644;&#24178;&#25200;&#39033;&#30340;&#38590;&#24230;&#27700;&#24179;&#65292;&#20197;&#20943;&#23569;&#26080;&#25928;&#24178;&#25200;&#39033;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#21644;&#35780;&#20272;MC&#22635;&#31354;&#27979;&#35797;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01456v1 Announce Type: cross  Abstract: Item difficulty plays a crucial role in adaptive testing. However, few works have focused on generating questions of varying difficulty levels, especially for multiple-choice (MC) cloze tests. We propose training pre-trained language models (PLMs) as surrogate models to enable item response theory (IRT) assessment, avoiding the need for human test subjects. We also propose two strategies to control the difficulty levels of both the gaps and the distractors using ranking rules to reduce invalid distractors. Experimentation on a benchmark dataset demonstrates that our proposed framework and methods can effectively control and evaluate the difficulty levels of MC cloze tests.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20986;&#29992;&#20316;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.01437</link><description>&lt;p&gt;
GPTSee&#65306;&#36890;&#36807;&#22522;&#20110;&#25551;&#36848;&#30340;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GPTSee: Enhancing Moment Retrieval and Highlight Detection via Description-Based Similarity Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01437
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36755;&#20986;&#29992;&#20316;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#26102;&#21051;&#26816;&#32034;&#21644;&#37325;&#28857;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#21051;&#26816;&#32034;&#65288;MR&#65289;&#21644;&#37325;&#28857;&#26816;&#27979;&#65288;HD&#65289;&#26088;&#22312;&#20174;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20013;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#30456;&#20851;&#26102;&#21051;&#21644;&#37325;&#28857;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MR&#21644;HD&#26041;&#27861;&#23578;&#26410;&#19982;LLMs&#38598;&#25104;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#20316;&#20026;&#31532;&#20108;&#38454;&#27573;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#36755;&#20837;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;MiniGPT-4&#29983;&#25104;&#35270;&#39057;&#24103;&#30340;&#35814;&#32454;&#25551;&#36848;&#24182;&#37325;&#20889;&#26597;&#35810;&#35821;&#21477;&#65292;&#23558;&#20854;&#20316;&#20026;&#26032;&#29305;&#24449;&#36755;&#20837;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#35745;&#31639;&#29983;&#25104;&#25551;&#36848;&#21644;&#37325;&#20889;&#26597;&#35810;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#26368;&#21518;&#65292;&#36830;&#32493;&#39640;&#30456;&#20284;&#24615;&#35270;&#39057;&#24103;&#34987;&#36716;&#25442;&#20026;&#33539;&#22260;&#38170;&#28857;&#65292;&#20316;&#20026;&#35299;&#30721;&#22120;&#30340;&#20808;&#39564;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01437v1 Announce Type: cross  Abstract: Moment retrieval (MR) and highlight detection (HD) aim to identify relevant moments and highlights in video from corresponding natural language query. Large language models (LLMs) have demonstrated proficiency in various computer vision tasks. However, existing methods for MR\&amp;HD have not yet been integrated with LLMs. In this letter, we propose a novel two-stage model that takes the output of LLMs as the input to the second-stage transformer encoder-decoder. First, MiniGPT-4 is employed to generate the detailed description of the video frame and rewrite the query statement, fed into the encoder as new features. Then, semantic similarity is computed between the generated description and the rewritten queries. Finally, continuous high-similarity video frames are converted into span anchors, serving as prior position information for the decoder. Experiments demonstrate that our approach achieves a state-of-the-art result, and by using on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#19982;&#20004;&#21517;&#31038;&#24037;&#21644;&#20004;&#20010;&#35774;&#35745;&#30740;&#35752;&#20250;&#65288;&#28041;&#21450;&#21313;&#21517;&#32769;&#24180;&#20154;&#65289;&#30340;&#35814;&#32454;&#35775;&#35848;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#32769;&#24180;&#20154;&#21033;&#29992;&#29983;&#25104;AI&#25903;&#25345;&#22522;&#20110;&#38899;&#20048;&#30340;&#22238;&#24518;&#30340;&#24577;&#24230;&#20316;&#20986;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.01413</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#25903;&#25345;&#32769;&#24180;&#20154;&#22522;&#20110;&#38899;&#20048;&#30340;&#22238;&#24518;&#20013;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Exploring the Design of Generative AI in Supporting Music-based Reminiscence for Older Adults
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#19982;&#20004;&#21517;&#31038;&#24037;&#21644;&#20004;&#20010;&#35774;&#35745;&#30740;&#35752;&#20250;&#65288;&#28041;&#21450;&#21313;&#21517;&#32769;&#24180;&#20154;&#65289;&#30340;&#35814;&#32454;&#35775;&#35848;&#65292;&#20026;&#28145;&#20837;&#20102;&#35299;&#32769;&#24180;&#20154;&#21033;&#29992;&#29983;&#25104;AI&#25903;&#25345;&#22522;&#20110;&#38899;&#20048;&#30340;&#22238;&#24518;&#30340;&#24577;&#24230;&#20316;&#20986;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38899;&#20048;&#30340;&#22238;&#24518;&#26377;&#28508;&#21147;&#23545;&#32769;&#24180;&#20154;&#30340;&#24515;&#29702;&#20581;&#24247;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32769;&#21270;&#36807;&#31243;&#21644;&#29983;&#29702;&#21464;&#21270;&#65292;&#22914;&#35760;&#24518;&#34928;&#36864;&#21644;&#26377;&#38480;&#30340;&#21475;&#22836;&#27807;&#36890;&#65292;&#21487;&#33021;&#20250;&#22952;&#30861;&#32769;&#24180;&#20154;&#22238;&#24518;&#20182;&#20204;&#30340;&#35760;&#24518;&#21644;&#29983;&#27963;&#32463;&#21382;&#12290;&#37492;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#35805;&#21644;&#22270;&#20687;&#65292;&#20197;&#21450;&#23427;&#20204;&#20419;&#36827;&#22238;&#24518;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35774;&#35745;&#29983;&#25104;AI&#20197;&#25903;&#25345;&#32769;&#24180;&#20154;&#22522;&#20110;&#38899;&#20048;&#30340;&#22238;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01413v1 Announce Type: cross  Abstract: Music-based reminiscence has the potential to positively impact the psychological well-being of older adults. However, the aging process and physiological changes, such as memory decline and limited verbal communication, may impede the ability of older adults to recall their memories and life experiences. Given the advanced capabilities of generative artificial intelligence (AI) systems, such as generated conversations and images, and their potential to facilitate the reminiscing process, this study aims to explore the design of generative AI to support music-based reminiscence in older adults. This study follows a user-centered design approach incorporating various stages, including detailed interviews with two social workers and two design workshops (involving ten older adults). Our work contributes to an in-depth understanding of older adults' attitudes toward utilizing generative AI for supporting music-based reminiscence and ident
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21306;&#22495;-Transformer&#30340;&#26032;&#22411;&#21306;&#22495;&#22522;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26080;&#20851;&#31867;&#21035;&#30340;&#28857;&#20113;&#20998;&#21106;&#35757;&#32451;&#65292;&#39318;&#27425;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24212;&#29992;&#20110;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01407</link><description>&lt;p&gt;
&#21306;&#22495;-Transformer: &#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21306;&#22495;&#30340;&#26080;&#20851;&#31867;&#21035;&#28857;&#20113;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21306;&#22495;-Transformer&#30340;&#26032;&#22411;&#21306;&#22495;&#22522;Transformer&#27169;&#22411;&#65292;&#20351;&#29992;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26080;&#20851;&#31867;&#21035;&#30340;&#28857;&#20113;&#20998;&#21106;&#35757;&#32451;&#65292;&#39318;&#27425;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24212;&#29992;&#20110;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#20998;&#21106;&#21487;&#20197;&#20197;&#29305;&#23450;&#32467;&#26500;&#21644;&#23545;&#35937;&#35270;&#35282;&#20197;&#29305;&#23450;&#31867;&#21035;&#25110;&#26080;&#20851;&#31867;&#21035;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21306;&#22495;-Transformer&#30340;&#26032;&#22411;&#22522;&#20110;&#21306;&#22495;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#20851;&#31867;&#21035;&#30340;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#28155;&#21152;&#25110;&#21024;&#38500;&#28857;&#26469;&#36845;&#20195;&#22320;&#25193;&#23637;&#25110;&#25910;&#32553;&#21306;&#22495;&#12290;&#27169;&#22411;&#20165;&#22312;&#34394;&#25311;&#28857;&#20113;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#20351;&#29992;&#23454;&#20363;&#26631;&#31614;&#65292;&#36991;&#20813;&#20351;&#29992;&#35821;&#20041;&#26631;&#31614;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#22312;&#35768;&#22810;&#20197;&#24448;&#30340;&#28857;&#20113;&#20998;&#21106;&#26041;&#27861;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20855;&#26377;&#20851;&#27880;&#32593;&#32476;&#30340;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#23578;&#26410;&#34987;&#29992;&#20110;&#25506;&#32034;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#21306;&#22495;&#22686;&#38271;&#26041;&#27861;&#20013;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#24341;&#20837;&#21040;&#21487;&#20197;&#21033;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21306;&#22495;&#22686;&#38271;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01407v1 Announce Type: cross  Abstract: Point cloud segmentation, which helps us understand the environment of specific structures and objects, can be performed in class-specific and class-agnostic ways. We propose a novel region-based transformer model called Region-Transformer for performing class-agnostic point cloud segmentation. The model utilizes a region-growth approach and self-attention mechanism to iteratively expand or contract a region by adding or removing points. It is trained on simulated point clouds with instance labels only, avoiding semantic labels. Attention-based networks have succeeded in many previous methods of performing point cloud segmentation. However, a region-growth approach with attention-based networks has yet to be used to explore its performance gain. To our knowledge, we are the first to use a self-attention mechanism in a region-growth approach. With the introduction of self-attention to region-growth that can utilize local contextual info
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#32452;&#21512;&#20102;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;</title><link>https://arxiv.org/abs/2403.01400</link><description>&lt;p&gt;
&#35299;&#32806;&#26435;&#34913;&#21644;&#36873;&#25321;&#65306;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#32452;&#21512;&#20102;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#39044;&#35757;&#32451;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20276;&#38543;&#30528;&#25968;&#30334;&#31181;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#25552;&#20986;&#65292;&#25972;&#21512;&#20174;&#22810;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#27492;&#20027;&#39064;&#30340;&#20004;&#20010;&#37325;&#35201;&#21327;&#20316;&#36807;&#31243;&#65306;&#65288;1&#65289;&#36873;&#25321;&#65306;&#22914;&#20309;&#22522;&#20110;&#23427;&#20204;&#30340;&#20860;&#23481;&#24615;&#20174;&#32473;&#23450;&#20219;&#21153;&#27744;&#20013;&#36873;&#25321;&#26368;&#20339;&#20219;&#21153;&#32452;&#21512;&#65292;&#21644;&#65288;2&#65289;&#26435;&#34913;&#65306;&#22914;&#20309;&#22522;&#20110;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#26435;&#34913;&#25152;&#36873;&#20219;&#21153;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#24456;&#22810;&#24037;&#20316;&#38598;&#20013;&#22312;&#26435;&#34913;&#19978;&#65292;&#20294;&#30456;&#27604;&#20043;&#19979;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#33268;&#21147;&#20110;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25972;&#21512;&#22810;&#20010;&#22270;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#26032;&#39062;&#30340;&#23454;&#20363;&#32423;&#26694;&#26550;Weigh And Select&#65288;WAS&#65289;&#65292;&#20854;&#20013;&#26435;&#34913;&#21644;&#36873;&#25321;&#36825;&#20004;&#20010;&#21327;&#20316;&#36807;&#31243;&#36890;&#36807;&#35299;&#32806;&#30340;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#32452;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#39318;&#20808;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01400v1 Announce Type: cross  Abstract: Recent years have witnessed the great success of graph pre-training for graph representation learning. With hundreds of graph pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important collaborative processes for this topic: (1) select: how to select an optimal task combination from a given task pool based on their compatibility, and (2) weigh: how to weigh the selected tasks based on their importance. While there currently has been a lot of work focused on weighing, comparatively little effort has been devoted to selecting. This paper proposes a novel instance-level framework for integrating multiple graph pre-training tasks, Weigh And Select (WAS), where the two collaborative processes, weighing and selecting, are combined by decoupled siamese networks. Specifically, it first adaptively learns an optimal combination of task
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.01384</link><description>&lt;p&gt;
&#20851;&#20110;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Compressibility of Quantized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01384
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#24212;&#29992;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#20197;&#21152;&#36895;&#37327;&#21270;LLM&#25512;&#29702;&#36807;&#31243;&#30340;&#19968;&#39033;&#21021;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21040;&#36793;&#32536;&#25110;&#31227;&#21160;&#35774;&#22791;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#22914;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#20943;&#23569;&#25968;&#25454;&#31227;&#21160;&#65292;&#20174;&#32780;&#21152;&#36895;&#20869;&#23384;&#21463;&#38480;&#35774;&#22791;&#19978;&#37327;&#21270;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Wav2Vec2&#23884;&#20837;&#22312;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;SSL&#34920;&#31034;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.01369</link><description>&lt;p&gt;
Wav2Vec2&#23884;&#20837;&#22312;&#35774;&#22791;&#31471;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Wav2Vec2&#23884;&#20837;&#22312;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;SSL&#34920;&#31034;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34987;&#21457;&#29616;&#22312;&#26576;&#20123;&#35821;&#38899;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#27604;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;SSL&#34920;&#31034;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#29992;&#36884;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#23545;&#22686;&#24378;&#20219;&#21153;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01369v1 Announce Type: cross  Abstract: Self-supervised learned models have been found to be very effective for certain speech tasks such as automatic speech recognition, speaker identification, keyword spotting and others. While the features are undeniably useful in speech recognition and associated tasks, their utility in speech enhancement systems is yet to be firmly established, and perhaps not properly understood. In this paper, we investigate the uses of SSL representations for single-channel speech enhancement in challenging conditions and find that they add very little value for the enhancement task. Our constraints are designed around on-device real-time speech enhancement -- model is causal, the compute footprint is small. Additionally, we focus on low SNR conditions where such models struggle to provide good enhancement. In order to systematically examine how SSL representations impact performance of such enhancement models, we propose a variety of techniques to u
&lt;/p&gt;</description></item><item><title>SANGRIA&#26159;&#19968;&#20010;&#22522;&#20110;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.01348</link><description>&lt;p&gt;
SANGRIA&#65306;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23460;&#20869;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01348
&lt;/p&gt;
&lt;p&gt;
SANGRIA&#26159;&#19968;&#20010;&#22522;&#20110;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#26159;&#35768;&#22810;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#20363;&#22914;&#36164;&#20135;&#36319;&#36394;&#12289;&#24212;&#24613;&#21709;&#24212;&#21644;&#23454;&#26102;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SANGRIA&#30340;&#22522;&#20110;&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#19982;&#26799;&#24230;&#25552;&#21319;&#26641;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#35774;&#22791;&#24322;&#26500;&#24615;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#23548;&#33268;&#29992;&#20110;&#23450;&#20301;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20043;&#38388;&#30340;&#26080;&#32447;&#20449;&#21495;&#27979;&#37327;&#20986;&#29616;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;SANGRIA&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#19981;&#21516;&#23460;&#20869;&#22330;&#25152;&#21644;&#24322;&#26500;&#35774;&#22791;&#19978;&#23637;&#31034;&#20102;42.96%&#36739;&#20302;&#30340;&#24179;&#22343;&#23450;&#20301;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01348v1 Announce Type: cross  Abstract: Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation. In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked autoencoder neural networks with gradient boosted trees. Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization. We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices.
&lt;/p&gt;</description></item><item><title>&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;chatGPT 3.5-turbo&#30340;&#24494;&#35843;&#65292;&#22312;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#26041;&#38754;&#26174;&#31034;&#20986;&#26032;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01332</link><description>&lt;p&gt;
&#23558;&#24605;&#32500;&#21644;LLMs&#20018;&#32852;&#36215;&#26469;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Chaining thoughts and LLMs to learn DNA structural biophysics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;chatGPT 3.5-turbo&#30340;&#24494;&#35843;&#65292;&#22312;&#23398;&#20064;DNA&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#26041;&#38754;&#26174;&#31034;&#20986;&#26032;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;AI&#31185;&#23398;&#23478;&#30340;&#19968;&#39033;&#37325;&#35201;&#21457;&#23637;&#26159;&#65292;&#19968;&#20010;&#33021;&#22815;&#25972;&#21512;&#21508;&#31181;&#23454;&#39564;&#25968;&#25454;&#24182;&#29983;&#25104;&#21487;&#39564;&#35777;&#20551;&#35774;&#30340;&#24037;&#20855;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23450;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#21019;&#24314;&#29992;&#20110;&#19987;&#38376;&#20174;&#20107;&#21333;&#19968;&#31185;&#23398;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#36890;&#29992;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;chatGPT 3.5-turbo&#65292;&#21487;&#20197;&#34987;&#24494;&#35843;&#26469;&#23398;&#20064;DNA&#30340;&#32467;&#26500;&#29983;&#29289;&#29289;&#29702;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#27169;&#22411;&#24494;&#35843;&#20026;&#36820;&#22238;&#24605;&#32500;&#38142;&#24335;&#21709;&#24212;&#20197;&#21450;&#20018;&#32852;&#24494;&#35843;&#29992;&#20110;&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#33021;&#21147;&#26469;&#20998;&#26512;&#21644;&#35774;&#35745;DNA&#24207;&#21015;&#21450;&#20854;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01332v1 Announce Type: cross  Abstract: The future development of an AI scientist, a tool that is capable of integrating a variety of experimental data and generating testable hypotheses, holds immense potential. So far, bespoke machine learning models have been created to specialize in singular scientific tasks, but otherwise lack the flexibility of a general purpose model. Here, we show that a general purpose large language model, chatGPT 3.5-turbo, can be fine-tuned to learn the structural biophysics of DNA. We find that both fine-tuning models to return chain-of-thought responses and chaining together models fine-tuned for subtasks have an enhanced ability to analyze and design DNA sequences and their structures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20855;&#26377;&#23567;&#21442;&#25968;&#31354;&#38388;&#12289;&#24555;&#36895;&#20248;&#21270;&#12289;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01329</link><description>&lt;p&gt;
&#20026;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#25552;&#20379;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20855;&#26377;&#23567;&#21442;&#25968;&#31354;&#38388;&#12289;&#24555;&#36895;&#20248;&#21270;&#12289;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23450;&#21046;&#30340;&#38750;&#24179;&#31283;&#65288;BNS&#65289;&#27714;&#35299;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#31639;&#31934;&#39635;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#25193;&#25955;&#21644;&#27969;&#21160;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;BNS &#27714;&#35299;&#22120;&#22522;&#20110;&#19968;&#31995;&#21015;&#21487;&#35777;&#26126;&#21253;&#21547;&#29616;&#26377;&#25968;&#20540; ODE &#27714;&#35299;&#22120;&#30340;&#38750;&#24179;&#31283;&#27714;&#35299;&#22120;&#23478;&#26063;&#65292;&#38543;&#20043;&#26174;&#33879;&#25913;&#36827;&#26679;&#26412;&#36924;&#36817;&#24230;&#65288;PSNR&#65289;&#36229;&#36807;&#36825;&#20123;&#22522;&#32447;&#12290;&#19982;&#27169;&#22411;&#31934;&#28860;&#30456;&#27604;&#65292;BNS &#27714;&#35299;&#22120;&#20855;&#26377;&#24494;&#23567;&#21442;&#25968;&#31354;&#38388;&#65288;&lt;200 &#21442;&#25968;&#65289;&#12289;&#24555;&#36895;&#20248;&#21270;&#65288;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65289;&#12289;&#20445;&#25345;&#26679;&#26412;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#27714;&#35299;&#22120;&#31934;&#28860;&#26041;&#27861;&#30456;&#21453;&#65292;&#20960;&#20046;&#33021;&#22312;&#20302;&#20013; NFE &#33539;&#22260;&#20869;&#25509;&#36817;&#26631;&#20934;&#31934;&#28860;&#26041;&#27861;&#65292;&#22914; Progressive Distillation&#12290;&#20363;&#22914;&#65292;BNS &#27714;&#35299;&#22120;&#22312; class-conditional ImageNet-64 &#20013;&#20351;&#29992; 16 NFE &#23454;&#29616; 45 PSNR / 1.76 FID&#12290;&#25105;&#20204;&#23581;&#35797;&#20102; BNS &#27714;&#35299;&#22120;&#26469;&#36827;&#34892;&#26377;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01329v1 Announce Type: cross  Abstract: This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($&lt;$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generat
&lt;/p&gt;</description></item><item><title>VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;</title><link>https://arxiv.org/abs/2403.01309</link><description>&lt;p&gt;
VNLP&#65306;&#22303;&#32819;&#20854;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
VNLP: Turkish NLP Package
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01309
&lt;/p&gt;
&lt;p&gt;
VNLP&#26159;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21253;&#65292;&#21253;&#21547;&#22810;&#31181;NLP&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VNLP&#65306;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22303;&#32819;&#20854;&#35821;&#30340;&#23436;&#25972;&#12289;&#24320;&#28304;&#12289;&#25991;&#26723;&#23436;&#22791;&#12289;&#36731;&#37327;&#32423;&#12289;&#21487;&#25237;&#20837;&#29983;&#20135;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#21508;&#31181;&#24037;&#20855;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#22914;&#21477;&#23376;&#20998;&#21106;&#21644;&#25991;&#26412;&#35268;&#33539;&#21270;&#65292;&#21040;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#21644;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#12290;&#20854;&#26631;&#35760;&#20998;&#31867;&#27169;&#22411;&#22522;&#20110;&#8220;&#19978;&#19979;&#25991;&#27169;&#22411;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26159;&#32534;&#30721;&#22120;&#21448;&#26159;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26032;&#39062;&#26550;&#26500;&#12290;VNLP&#27169;&#22411;&#35299;&#20915;&#30340;NLP&#20219;&#21153;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#24418;&#24577;&#20998;&#26512;&#21644;&#28040;&#27495;&#20197;&#21450;&#35789;&#24615;&#26631;&#27880;&#12290;&#27492;&#22806;&#65292;&#23427;&#37197;&#22791;&#20102;&#39044;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#21644;&#30456;&#24212;&#30340;SentencePiece Unigram&#26631;&#35760;&#22120;&#12290;VNLP&#20855;&#26377;&#24320;&#28304;&#30340;GitHub&#23384;&#20648;&#24211;&#12289;ReadtheDocs&#25991;&#26723;&#12289;&#26041;&#20415;&#23433;&#35013;&#30340;PyPi&#21253;&#12289;Python&#21644;&#36887;&#21495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01309v1 Announce Type: cross  Abstract: In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on "Context Model", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \&amp; Disambiguation and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and comma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#24341;&#23548;&#35270;&#38556;&#32773;&#30340;&#26426;&#22120;&#29399;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#23433;&#20840;&#31359;&#36234;&#21313;&#23383;&#36335;&#21475;&#30340;&#20915;&#31574;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#21327;&#20316;&#20915;&#31574;&#23618;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.01286</link><description>&lt;p&gt;
&#24314;&#31435;&#21327;&#20316;&#23433;&#20840;&#33258;&#20027;&#31995;&#32479;&#30340;&#29992;&#20363;&#30740;&#31350;-&#29992;&#20110;&#24341;&#23548;&#35270;&#38556;&#32773;&#30340;&#26426;&#22120;&#29399;
&lt;/p&gt;
&lt;p&gt;
Summary Paper: Use Case on Building Collaborative Safe Autonomous Systems-A Robotdog for Guiding Visually Impaired People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#24341;&#23548;&#35270;&#38556;&#32773;&#30340;&#26426;&#22120;&#29399;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#23433;&#20840;&#31359;&#36234;&#21313;&#23383;&#36335;&#21475;&#30340;&#20915;&#31574;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#21327;&#20316;&#20915;&#31574;&#23618;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#19987;&#38376;&#24341;&#23548;&#35270;&#38556;&#32773;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#22914;&#26234;&#33021;&#21313;&#23383;&#36335;&#21475;&#30340;&#26426;&#22120;&#29399;&#29992;&#20363;&#30340;&#25688;&#35201;&#35770;&#25991;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#29399;&#24517;&#39035;&#33258;&#20027;&#20915;&#23450;&#26159;&#21542;&#23433;&#20840;&#31359;&#36807;&#21313;&#23383;&#36335;&#21475;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#25351;&#23548;&#20154;&#31867;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#20316;&#65292;&#20351;&#26426;&#22120;&#29399;&#19982;&#21516;&#19968;&#29615;&#22659;&#20013;&#20854;&#20182;&#33258;&#20027;&#31995;&#32479;&#20043;&#38388;&#21457;&#29983;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#20027;&#31995;&#32479;&#30340;&#31995;&#32479;&#26550;&#26500;&#65292;&#36890;&#36807;&#19968;&#20010;&#21327;&#20316;&#20915;&#31574;&#23618;&#30340;&#20998;&#31163;&#26469;&#23454;&#29616;&#38598;&#20307;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#20998;&#20139;&#29615;&#22659;&#25968;&#25454;&#12289;&#19982;&#26426;&#22120;&#29399;&#20915;&#31574;&#30456;&#20851;&#30340;&#20854;&#20182;&#31995;&#32479;&#21644;&#29615;&#22659;&#30340;&#21487;&#20449;&#24230;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01286v1 Announce Type: cross  Abstract: This is a summary paper of a use case of a Robotdog dedicated to guide visually impaired people in complex environment like a smart intersection. In such scenarios, the Robotdog has to autonomously decide whether it is safe to cross the intersection or not in order to further guide the human. We leverage data sharing and collaboration between the Robotdog and other autonomous systems operating in the same environment. We propose a system architecture for autonomous systems through a separation of a collaborative decision layer, to enable collective decision making processes, where data about the environment, relevant to the Robotdog decision, together with evidences for trustworthiness about other systems and the environment are shared.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#33021;&#20934;&#30830;&#26816;&#27979;&#24182;&#20851;&#32852;&#23398;&#29983;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.01281</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#23398;&#20064;&#29615;&#22659;&#20013;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fast Low-parameter Video Activity Localization in Collaborative Learning Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20302;&#21442;&#25968;&#35270;&#39057;&#27963;&#21160;&#23450;&#20301;&#31995;&#32479;&#65292;&#21487;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#33021;&#20934;&#30830;&#26816;&#27979;&#24182;&#20851;&#32852;&#23398;&#29983;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#27963;&#21160;&#26816;&#27979;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35782;&#21035;&#30701;&#35270;&#39057;&#29255;&#27573;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#20154;&#31867;&#27963;&#21160;&#12290;&#35270;&#39057;&#27963;&#21160;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#24320;&#21457;&#38656;&#35201;&#22312;&#22823;&#22411;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#21442;&#25968;&#31995;&#32479;&#19978;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20302;&#21442;&#25968;&#12289;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#23436;&#20840;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20174;&#22823;&#21442;&#25968;&#31995;&#32479;&#20013;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20934;&#30830;&#26816;&#27979;&#21644;&#23558;&#29305;&#23450;&#27963;&#21160;&#19982;&#22312;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#20013;&#25191;&#34892;&#27963;&#21160;&#30340;&#23398;&#29983;&#20851;&#32852;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#29992;&#20110;&#22312;&#38271;&#26102;&#38388;&#30340;&#29616;&#23454;&#25945;&#23460;&#35270;&#39057;&#19978;&#21487;&#35270;&#21270;&#20154;&#31867;&#27963;&#21160;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01281v1 Announce Type: cross  Abstract: Research on video activity detection has primarily focused on identifying well-defined human activities in short video segments. The majority of the research on video activity recognition is focused on the development of large parameter systems that require training on large video datasets. This paper develops a low-parameter, modular system with rapid inferencing capabilities that can be trained entirely on limited datasets without requiring transfer learning from large-parameter systems. The system can accurately detect and associate specific activities with the students who perform the activities in real-life classroom videos. Additionally, the paper develops an interactive web-based application to visualize human activity maps over long real-life classroom videos.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#20102;&#26368;&#20248;&#20219;&#21153;&#35268;&#21010;&#22120;&#21644;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#36890;&#29992;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#26426;&#21046;&#65292;&#36890;&#36807;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#26368;&#20248;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#22312;&#20179;&#24211;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21462;&#25918;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01277</link><description>&lt;p&gt;
&#26368;&#20248;&#38598;&#25104;&#20219;&#21153;&#21644;&#36335;&#24452;&#35268;&#21010;&#21450;&#20854;&#22312;&#22810;&#26426;&#22120;&#20154;&#21462;&#36865;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal Integrated Task and Path Planning and Its Application to Multi-Robot Pickup and Delivery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01277
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;&#20102;&#26368;&#20248;&#20219;&#21153;&#35268;&#21010;&#22120;&#21644;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#36890;&#29992;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#26426;&#21046;&#65292;&#36890;&#36807;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#26368;&#20248;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#22312;&#20179;&#24211;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21462;&#25918;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#26368;&#20248;&#20219;&#21153;&#35268;&#21010;&#22120;&#21644;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#20026;&#22797;&#26434;&#30340;&#22810;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38598;&#25104;&#35268;&#21010;&#22120;&#36890;&#36807;&#20219;&#21153;&#35268;&#21010;&#22120;&#21644;&#36335;&#24452;&#35268;&#21010;&#22120;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#26426;&#22120;&#20154;&#20135;&#29983;&#26368;&#20248;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20179;&#24211;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21462;&#25918;&#35268;&#21010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#32452;&#26426;&#22120;&#20154;&#36127;&#36131;&#23558;&#29289;&#20307;&#20174;&#19968;&#20010;&#20301;&#32622;&#31227;&#21160;&#21040;&#24037;&#20316;&#21306;&#30340;&#21478;&#19968;&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#31616;&#21270;&#20026;SMT&#27714;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#32423;SMT&#27714;&#35299;&#22120; Z3 &#26469;&#35299;&#20915;&#23427;&#12290;&#20026;&#20102;&#29983;&#25104;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#31227;&#21160;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#22810;&#20010;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861; Conflict Based Search with Precedence Constraints&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#24191;&#27867;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#20219;&#21153;&#21644;&#36335;&#24452;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01277v1 Announce Type: cross  Abstract: We propose a generic multi-robot planning mechanism that combines an optimal task planner and an optimal path planner to provide a scalable solution for complex multi-robot planning problems. The Integrated planner, through the interaction of the task planner and the path planner, produces optimal collision-free trajectories for the robots. We illustrate our general algorithm on an object pick-and-drop planning problem in a warehouse scenario where a group of robots is entrusted with moving objects from one location to another in the workspace. We solve the task planning problem by reducing it into an SMT-solving problem and employing the highly advanced SMT solver Z3 to solve it. To generate collision-free movement of the robots, we extend the state-of-the-art algorithm Conflict Based Search with Precedence Constraints with several domain-specific constraints. We evaluate our integrated task and path planner extensively on various ins
&lt;/p&gt;</description></item><item><title>NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.01273</link><description>&lt;p&gt;
NoMAD-Attention: &#36890;&#36807;&#26080;MAD&#25805;&#20316;&#23454;&#29616;CPU&#19978;&#39640;&#25928;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01273
&lt;/p&gt;
&lt;p&gt;
NoMAD-Attention&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;CPU&#19978;&#20351;&#29992;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#21462;&#20195;MAD&#25805;&#20316;&#65292;&#20197;&#23454;&#29616;LLM&#25512;&#26029;&#30340;&#24555;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#23384;&#22312;&#22823;&#37327;&#26114;&#36149;&#30340;MAD&#30697;&#38453;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#29616;&#20195;CPU&#20013;&#30340;&#21333;&#25351;&#20196;&#22810;&#25968;&#25454;&#65288;SIMD&#65289;&#23492;&#23384;&#22120;&#26159;&#19968;&#31181;&#29645;&#36149;&#30340;&#23453;&#30707;&#65292;&#23427;&#20801;&#35768;&#22312;&#25209;&#22788;&#29702;&#20013;&#36827;&#34892;&#36229;&#20302;&#24310;&#36831;&#26597;&#25214;&#12290;&#25105;&#20204;&#21033;&#29992;CPU&#30340;&#36825;&#19968;&#29420;&#29305;&#33021;&#21147;&#25552;&#20986;&#20102;NoMAD-Attention&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#29992;&#20110;&#23558;MAD&#25805;&#20316;&#26367;&#25442;&#20026;&#23492;&#23384;&#22120;&#20869;&#26597;&#25214;&#12290;&#36890;&#36807;&#30828;&#20214;&#24863;&#30693;&#30340;&#31639;&#27861;&#35774;&#35745;&#65292;NoMAD-Attention&#23454;&#29616;&#20102;&#36890;&#36807;&#37325;&#22797;&#24555;&#36895;&#35775;&#38382;SIMD&#23492;&#23384;&#22120;&#26469;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#22823;&#23567;&#38750;&#24120;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;NoMAD-Attention&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;LLM&#65292;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;NoMAD-Attention&#24456;&#22909;&#22320;&#20445;&#25345;&#20102;&#21407;&#22987;LLM&#30340;&#36136;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;4&#20301;&#37327;&#21270;&#30340;LLaMA-7B-bas&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas
&lt;/p&gt;</description></item><item><title>&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22914;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01255</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition using Advanced Deep Learning Approaches: A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01255
&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22914;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#20551;&#35774;&#19981;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01255v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;ASR&#20381;&#36182;&#20110;&#21253;&#25324;&#26426;&#23494;&#25968;&#25454;&#22312;&#20869;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#21551;&#29992;&#33258;&#36866;&#24212;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;ASR&#24615;&#33021;&#12290;DL&#25216;&#26415;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#39046;&#22495;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31561;&#20808;&#36827;&#30340;DL&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;DTL&#21487;&#20197;&#21033;&#29992;&#23567;&#32780;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#65292;FL&#20351;&#24471;&#22312;&#19981;&#25317;&#26377;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;RL&#20248;&#21270;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#38405;&#20102;&#22522;&#20110;DTL&#12289;FL&#21644;RL&#30340;ASR&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#26368;&#26032;&#21457;&#23637;&#30340;&#35265;&#35299;&#65292;&#24182;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01255v1 Announce Type: cross  Abstract: Recent advancements in deep learning (DL) have posed a significant challenge for automatic speech recognition (ASR). ASR relies on extensive training datasets, including confidential ones, and demands substantial computational and storage resources. Enabling adaptive systems improves ASR performance in dynamic environments. DL techniques assume training and testing data originate from the same domain, which is not always true. Advanced DL techniques like deep transfer learning (DTL), federated learning (FL), and reinforcement learning (RL) address these issues. DTL allows high-performance models using small yet related datasets, FL enables training on confidential data without dataset possession, and RL optimizes decision-making in dynamic environments, reducing computation costs. This survey offers a comprehensive review of DTL, FL, and RL-based ASR frameworks, aiming to provide insights into the latest developments and aid researcher
&lt;/p&gt;</description></item><item><title>SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01248</link><description>&lt;p&gt;
SceneCraft&#65306;&#19968;&#20010;&#29992;&#20110;&#23558;&#25991;&#26412;&#25551;&#36848;&#21512;&#25104;&#20026;Blender&#20195;&#30721;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01248
&lt;/p&gt;
&lt;p&gt;
SceneCraft&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#21487;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#20195;&#30721;&#65292;&#23454;&#29616;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#36890;&#36807;&#20808;&#24314;&#27169;&#31354;&#38388;&#20851;&#31995;&#20877;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#24182;&#20511;&#21161;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#22330;&#26223;&#20248;&#21270;&#21644;&#24211;&#23398;&#20064;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SceneCraft&#65292;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#25442;&#20026;Blender&#21487;&#25191;&#34892;&#30340;Python&#33050;&#26412;&#65292;&#29992;&#20110;&#28210;&#26579;&#39640;&#36798;&#19968;&#30334;&#20010;&#19977;&#32500;&#36164;&#20135;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#35813;&#36807;&#31243;&#38656;&#35201;&#22797;&#26434;&#30340;&#31354;&#38388;&#35268;&#21010;&#21644;&#24067;&#23616;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#32423;&#25277;&#35937;&#12289;&#25112;&#30053;&#35268;&#21010;&#21644;&#24211;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;SceneCraft&#39318;&#20808;&#23558;&#22330;&#26223;&#22270;&#24314;&#27169;&#20026;&#34013;&#22270;&#65292;&#35814;&#32454;&#25551;&#36848;&#22330;&#26223;&#20013;&#21508;&#36164;&#20135;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;SceneCraft&#26681;&#25454;&#36825;&#20010;&#22270;&#32534;&#20889;Python&#33050;&#26412;&#65292;&#23558;&#20851;&#31995;&#36716;&#21270;&#20026;&#36164;&#20135;&#24067;&#23616;&#30340;&#25968;&#20540;&#32422;&#26463;&#12290;&#25509;&#19979;&#26469;&#65292;SceneCraft&#21033;&#29992;&#20687;GPT-V&#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#24863;&#30693;&#20248;&#21183;&#26469;&#20998;&#26512;&#28210;&#26579;&#22270;&#20687;&#24182;&#36845;&#20195;&#22320;&#20248;&#21270;&#22330;&#26223;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20043;&#19978;&#65292;SceneCraft&#20855;&#22791;&#19968;&#20010;&#24211;&#23398;&#20064;&#26426;&#21046;&#65292;&#23558;&#24120;&#35265;&#30340;&#33050;&#26412;&#20989;&#25968;&#32534;&#35793;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#24211;&#65292;&#20419;&#36827;&#25345;&#32493;&#30340;&#33258;&#25105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01248v1 Announce Type: cross  Abstract: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01244</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#22797;&#36848;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Synthesized Rehearsal&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#22797;&#36848;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#22797;&#36848;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#30041;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#25105;&#29983;&#25104;&#22797;&#36848;&#65288;SSR&#65289;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#22797;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01244v1 Announce Type: cross  Abstract: Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model's ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01242</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#33258;&#21160;&#21270;&#65306;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#22312;&#25511;&#21046;&#30005;&#36335;&#21644;&#35774;&#22791;&#26102;&#25552;&#20379;&#20102;&#26041;&#20415;&#21644;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#21629;&#20196;&#36827;&#34892;&#25511;&#21046;&#65292;&#38480;&#21046;&#20102;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#24847;&#22270;&#30340;&#29992;&#25143;&#25351;&#20196;&#20998;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#22686;&#24378;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#29992;&#25143;&#25351;&#20196;&#34920;&#31034;&#20026;&#24847;&#22270;&#65292;&#20801;&#35768;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#21629;&#20196;&#30340;&#24773;&#20917;&#19979;&#21160;&#24577;&#25511;&#21046;&#30005;&#36335;&#12290;&#36890;&#36807;&#35757;&#32451;&#22312;&#26631;&#35760;&#30340;&#29992;&#25143;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#29992;&#25143;&#36755;&#20837;&#20013;&#23545;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#30452;&#35266;&#21644;&#21487;&#36866;&#24212;&#30340;&#25511;&#21046;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#24847;&#22270;&#30340;&#30005;&#21160;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#29992;&#20110;&#24847;&#22270;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01242v1 Announce Type: cross  Abstract: Electric automation systems offer convenience and efficiency in controlling electrical circuits and devices. Traditionally, these systems rely on predefined commands for control, limiting flexibility and adaptability. In this paper, we propose a novel approach to augment automation by introducing intent-based user instruction classification using machine learning techniques. Our system represents user instructions as intents, allowing for dynamic control of electrical circuits without relying on predefined commands. Through a machine learning model trained on a labeled dataset of user instructions, our system classifies intents from user input, enabling a more intuitive and adaptable control scheme. We present the design and implementation of our intent-based electric automation system, detailing the development of the machine learning model for intent classification. Experimental results demonstrate the effectiveness of our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.01241</link><description>&lt;p&gt;
IntactKV: &#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#23436;&#25972;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;IntactKV&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20851;&#38190;&#26631;&#35760;&#30340;&#23436;&#25972;&#24615;&#65292;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#19968;&#38590;&#39064;&#65292;&#20154;&#20204;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#20250;&#25439;&#23475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19968;&#20010;&#20197;&#21069;&#34987;&#24573;&#35270;&#30340;&#24322;&#24120;&#28857;&#31867;&#22411;&#12290;&#36825;&#20123;&#24322;&#24120;&#28857;&#34987;&#21457;&#29616;&#23558;&#22823;&#37096;&#20998;&#27880;&#24847;&#21147;&#20998;&#37197;&#32473;&#36755;&#20837;&#30340;&#21021;&#22987;&#26631;&#35760;&#65292;&#34987;&#31216;&#20026;&#20851;&#38190;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IntactKV&#65292;&#20174;&#23436;&#25972;&#31934;&#24230;&#27169;&#22411;&#20013;&#26080;&#25439;&#22320;&#29983;&#25104;&#20851;&#38190;&#26631;&#35760;&#30340;KV&#32531;&#23384;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#26131;&#34892;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#29616;&#26377;&#30340;&#37327;&#21270;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;IntactKV&#21487;&#20197;&#34987;&#26657;&#20934;&#20026;&#39069;&#22806;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#23398;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;IntactKV&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#37327;&#21270;&#35823;&#24046;&#30340;&#19978;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;IntactKV&#24102;&#26469;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#25439;&#30340;&#20165;&#26435;&#37325;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01241v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in natural language processing but demand intensive computation. To mitigate this, various quantization methods have been explored, yet they compromise LLM performance. This paper unveils a previously overlooked type of outlier in LLMs. Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which is crucial to the performance of quantized LLMs. Given that, we propose IntactKV to generate the KV cache of pivot tokens losslessly from the full-precision model. The approach is simple and easy to combine with existing quantization solutions. Besides, IntactKV can be calibrated as additional LLM parameters to boost the quantized LLMs further. Mathematical analysis also proves that IntactKV effectively reduces the upper bound of quantization error. Empirical results show that IntactKV brings consistent improvement and achieves lossless weight-only
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38544;&#31169;&#20445;&#25252;&#22320;&#35782;&#21035;&#35828;&#35805;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#22312;&#37326;&#22806;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.01229</link><description>&lt;p&gt;
REWIND&#25968;&#25454;&#38598;&#65306;&#22312;&#37326;&#22806;&#22810;&#27169;&#24577;&#36523;&#20307;&#36816;&#21160;&#20449;&#21495;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#38899;&#29366;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01229
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38544;&#31169;&#20445;&#25252;&#22320;&#35782;&#21035;&#35828;&#35805;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;&#22312;&#37326;&#22806;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20154;&#31867;&#30340;&#35828;&#35805;&#26159;&#29702;&#35299;&#31038;&#20250;&#20114;&#21160;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#20250;&#20174;&#20010;&#20154;&#24405;&#38899;&#20013;&#26816;&#27979;&#35828;&#35805;&#65292;&#23601;&#20687;&#20043;&#21069;&#20026;&#20250;&#35758;&#22330;&#26223;&#25152;&#20570;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#22312;&#25317;&#25380;&#30340;&#32858;&#20250;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#12289;&#21518;&#21220;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24456;&#38590;&#33719;&#21462;&#20010;&#20154;&#24405;&#38899;&#12290;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#35757;&#32451;&#22312;&#35270;&#39057;&#21644;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36890;&#36807;&#26816;&#27979;&#20854;&#30456;&#20851;&#25163;&#21183;&#26469;&#35782;&#21035;&#35821;&#38899;&#65292;&#36825;&#31181;&#26041;&#24335;&#26082;&#19981;&#24341;&#20154;&#27880;&#30446;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26412;&#36523;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#20351;&#29992;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#33719;&#21462;&#30340;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32858;&#20250;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#35760;&#24405;&#12290;&#30456;&#21453;&#65292;&#23545;&#35828;&#35805;&#29366;&#24577;&#30340;&#27880;&#37322;&#36890;&#24120;&#26159;&#36890;&#36807;&#20154;&#31867;&#26631;&#27880;&#32773;&#20174;&#35270;&#39057;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#32780;&#27809;&#26377;&#23545;&#36825;&#31181;&#26041;&#27861;&#38024;&#23545;&#22522;&#20110;&#38899;&#39057;&#30340;&#22320;&#38754;&#30495;&#23454;&#24615;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#38750;&#38899;&#39057;&#35828;&#35805;&#29366;&#24577;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01229v1 Announce Type: cross  Abstract: Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking statu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.01221</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25104;&#26412;&#25928;&#29575;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#26410;&#35299;&#20915;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#32467;&#26524;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25512;&#33616;&#25104;&#26412;&#26377;&#25928;&#19988;&#21487;&#25805;&#20316;&#30340;&#36755;&#20837;&#26356;&#25913;&#65292;&#23558;&#19981;&#33391;&#31995;&#32479;&#36755;&#20986;&#36716;&#21464;&#20026;&#26399;&#26395;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;&#35299;&#37322;&#21333;&#20010;&#23454;&#20363;&#65292;&#20294;&#19968;&#20123;&#30495;&#23454;&#30340;&#29992;&#20363;&#65288;&#22914;&#23458;&#25143;&#28385;&#24847;&#24230;&#65289;&#38656;&#35201;&#35782;&#21035;&#33021;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;&#23454;&#20363;&#65288;&#20363;&#22914;&#23458;&#25143;&#65289;&#30340;&#21333;&#19968;&#21453;&#20107;&#23454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23454;&#20363;&#32452;&#20197;&#21450;&#25104;&#26412;&#26377;&#25928;&#30340;&#22810;&#23454;&#20363;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25214;&#21040;&#36825;&#26679;&#30340;&#23454;&#20363;&#32452;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01221v1 Announce Type: cross  Abstract: Counterfactual explanations constitute among the most popular methods for analyzing the predictions of black-box systems since they can recommend cost-efficient and actionable changes to the input to turn an undesired system's output into a desired output. While most of the existing counterfactual methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single counterfactual that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance counterfactual explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#21464;&#30446;&#26631;&#29289;&#20307;&#30340;&#25955;&#23556;&#29305;&#24449;&#21442;&#25968;&#29983;&#25104;&#30495;&#23454;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.01210</link><description>&lt;p&gt;
SAR-AE-SFP: &#20855;&#26377;&#30446;&#26631;&#25955;&#23556;&#29305;&#24449;&#21442;&#25968;&#30340;&#23454;&#38469;&#29289;&#29702;&#39046;&#22495;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with Target Scattering Feature Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#21464;&#30446;&#26631;&#29289;&#20307;&#30340;&#25955;&#23556;&#29305;&#24449;&#21442;&#25968;&#29983;&#25104;&#30495;&#23454;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01210v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#38024;&#23545;SAR&#22270;&#20687;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#20027;&#35201;&#22312;&#20108;&#32500;&#25968;&#23383;&#39046;&#22495;&#20013;&#36816;&#34892;&#65292;&#34987;&#31216;&#20026;&#22270;&#20687;&#23545;&#25239;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#32771;&#34385;SAR&#25104;&#20687;&#25955;&#23556;&#26426;&#21046;&#30340;&#21516;&#26102;&#65292;&#26410;&#33021;&#32771;&#34385;&#23454;&#38469;&#25104;&#20687;&#36807;&#31243;&#65292;&#23548;&#33268;&#22312;&#19977;&#32500;&#29289;&#29702;&#39046;&#22495;&#20013;&#25915;&#20987;&#19981;&#21487;&#34892;&#65292;&#31216;&#20026;&#20266;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SAR-AE-SFP-Attack&#65292;&#19968;&#31181;&#36890;&#36807;&#25913;&#21464;&#30446;&#26631;&#29289;&#20307;&#30340;&#25955;&#23556;&#29305;&#24449;&#21442;&#25968;&#29983;&#25104;&#30495;&#23454;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25200;&#21160;&#30446;&#26631;&#29289;&#20307;&#30340;&#25955;&#23556;&#29305;&#24449;&#21442;&#25968;&#20013;&#30340;&#21453;&#23556;&#31995;&#25968;&#21644;&#25955;&#23556;&#31995;&#25968;&#65292;&#36845;&#20195;&#20248;&#21270;&#30446;&#26631;&#22238;&#27874;&#30340;&#19968;&#33268;&#33021;&#37327;&#31215;&#32047;&#65292;&#24182;&#33719;&#24471;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01210v1 Announce Type: cross  Abstract: Deep neural network-based Synthetic Aperture Radar (SAR) target recognition models are susceptible to adversarial examples. Current adversarial example generation methods for SAR imagery primarily operate in the 2D digital domain, known as image adversarial examples. Recent work, while considering SAR imaging scatter mechanisms, fails to account for the actual imaging process, rendering attacks in the three-dimensional physical domain infeasible, termed pseudo physics adversarial examples. To address these challenges, this paper proposes SAR-AE-SFP-Attack, a method to generate real physics adversarial examples by altering the scattering feature parameters of target objects. Specifically, we iteratively optimize the coherent energy accumulation of the target echo by perturbing the reflection coefficient and scattering coefficient in the scattering feature parameters of the three-dimensional target object, and obtain the adversarial exam
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#38656;&#35201;&#24847;&#35782;&#21040;&#25216;&#26415;&#20250;&#23545;&#21160;&#29289;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#31995;&#32479;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21160;&#29289;&#21033;&#30410;&#30340;&#32771;&#34385;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.01199</link><description>&lt;p&gt;
&#21160;&#29289;&#21451;&#22909;&#20154;&#24037;&#26234;&#33021;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Case for Animal-Friendly AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01199
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#38656;&#35201;&#24847;&#35782;&#21040;&#25216;&#26415;&#20250;&#23545;&#21160;&#29289;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#31995;&#32479;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21160;&#29289;&#21033;&#30410;&#30340;&#32771;&#34385;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#34987;&#35270;&#20026;&#26085;&#30410;&#37325;&#35201;&#65292;&#19988;&#20855;&#26377;&#28508;&#22312;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#26159;AI&#20262;&#29702;&#23398;&#21644;AI&#24037;&#31243;&#39046;&#22495;&#23578;&#26410;&#20805;&#20998;&#24847;&#35782;&#21040;&#36825;&#20123;&#25216;&#26415;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23558;&#23545;&#21160;&#29289;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24433;&#21709;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#21160;&#29289;&#22312;&#36947;&#24503;&#19978;&#24456;&#37325;&#35201;&#12290;&#20316;&#20026;&#35780;&#20272;LLMs&#20013;&#32771;&#34385;&#21160;&#29289;&#22240;&#32032;&#30340;&#21021;&#27493;&#23454;&#39564;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#35780;&#20272;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20174;&#22810;&#20010;&#35282;&#24230;&#35780;&#20272;LLM&#30340;&#21709;&#24212;&#21644;&#20559;&#35265;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#20004;&#20010;&#26631;&#20934;&#35780;&#20272;LLM&#30340;&#36755;&#20986;&#65306;&#23427;&#20204;&#30340;&#30495;&#23454;&#24615;&#21644;&#23427;&#20204;&#23545;&#21160;&#29289;&#21033;&#30410;&#30340;&#32771;&#34385;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#32467;&#26500;&#21270;&#26597;&#35810;&#21644;&#39044;&#23450;&#20041;&#30340;&#35268;&#33539;&#35270;&#35282;&#27979;&#35797;&#20102;OpenAI ChatGPT 4&#21644;Anthropic Claude 2.1&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#27979;&#35797;&#27169;&#22411;&#30340;&#32467;&#26524;&#21487;&#20197;&#26681;&#25454;&#23427;&#20204;&#23545;&#21160;&#29289;&#32771;&#34385;&#30340;&#31243;&#24230;&#36827;&#34892;&#22522;&#20934;&#27604;&#36739;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340; positio
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01199v1 Announce Type: new  Abstract: Artificial intelligence is seen as increasingly important, and potentially profoundly so, but the fields of AI ethics and AI engineering have not fully recognized that these technologies, including large language models (LLMs), will have massive impacts on animals. We argue that this impact matters, because animals matter morally.   As a first experiment in evaluating animal consideration in LLMs, we constructed a proof-of-concept Evaluation System, which assesses LLM responses and biases from multiple perspectives. This system evaluates LLM outputs by two criteria: their truthfulness, and the degree of consideration they give to the interests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using a set of structured queries and predefined normative perspectives. Preliminary results suggest that the outcomes of the tested models can be benchmarked regarding the consideration they give to animals, and that generated positio
&lt;/p&gt;</description></item><item><title>&#22312;LoResMT 2021&#20013;&#65292;&#38024;&#23545;Covid&#25968;&#25454;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#30340;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#25193;&#23637;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;Transformer&#26550;&#26500;&#65292;&#25104;&#21151;&#25913;&#21892;&#20102;&#32763;&#35793;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.01196</link><description>&lt;p&gt;
Covid&#39046;&#22495;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;LoResMT 2021&#20013;&#33521;&#29233;&#23572;&#20848;&#35821;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01196
&lt;/p&gt;
&lt;p&gt;
&#22312;LoResMT 2021&#20013;&#65292;&#38024;&#23545;Covid&#25968;&#25454;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#30340;&#32763;&#35793;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#21644;&#25193;&#23637;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;Transformer&#26550;&#26500;&#65292;&#25104;&#21151;&#25913;&#21892;&#20102;&#32763;&#35793;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20174;&#33521;&#35821;&#21040;&#29233;&#23572;&#20848;&#35821;&#32763;&#35793;Covid&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;&#65292;&#24320;&#21457;&#20102;LoResMT 2021&#20849;&#20139;&#20219;&#21153;&#30340;&#32763;&#35793;&#27169;&#22411;&#12290;&#21033;&#29992;&#26469;&#33258;&#32763;&#35793;&#24635;&#21496;&#25351;&#23548;&#22788;&#30340;Covid&#36866;&#37197;&#36890;&#29992;55k&#35821;&#26009;&#24211;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#23558;&#24494;&#35843;&#12289;&#28151;&#21512;&#24494;&#35843;&#21644;&#32452;&#21512;&#25968;&#25454;&#38598;&#26041;&#27861;&#19982;&#22312;&#25193;&#23637;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20316;&#20026;&#30740;&#31350;&#30340;&#19968;&#37096;&#20998;&#65292;&#24320;&#21457;&#20102;&#19968;&#20221;&#20581;&#24247;&#21644;&#25945;&#32946;&#39046;&#22495;&#30340;&#33521;&#35821;-&#29233;&#23572;&#20848;&#35821;Covid&#30456;&#20851;&#25968;&#25454;&#38598;&#12290;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#20351;&#29992;&#25193;&#23637;&#30340;&#39046;&#22495;&#20869;Covid&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;8k&#39046;&#22495;&#20869;&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#38656;&#20877;&#22686;&#21152;5k&#34892;&#65292;&#23601;&#23558;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;27&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01196v1 Announce Type: cross  Abstract: Translation models for the specific domain of translating Covid data from English to Irish were developed for the LoResMT 2021 shared task. Domain adaptation techniques, using a Covid-adapted generic 55k corpus from the Directorate General of Translation, were applied. Fine-tuning, mixed fine-tuning and combined dataset approaches were compared with models trained on an extended in-domain dataset. As part of this study, an English-Irish dataset of Covid related data, from the Health and Education domains, was developed. The highest-performing model used a Transformer architecture trained with an extended in-domain Covid dataset. In the context of this study, we have demonstrated that extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#39564;&#35777;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.01185</link><description>&lt;p&gt;
&#22312;LLM&#20013;&#20351;&#29992;Soft RLLF&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01185
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#24179;&#34913;&#65292;&#20197;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#39564;&#35777;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#20013;&#65292;&#35843;&#25972;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#24320;&#21457;&#32780;&#19981;&#26159;&#25506;&#32034;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#31181;&#26377;&#38480;&#30340;&#25506;&#32034;&#21487;&#33021;&#38480;&#21046;&#23427;&#20204;&#22312;&#22797;&#26434;&#12289;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#37027;&#37324;&#20934;&#30830;&#30340;&#21542;&#23450;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#36923;&#36753;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLLF&#65289;&#22312;LLMs&#20013;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26377;&#25928;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#36866;&#24403;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#22686;&#24378;&#21542;&#23450;&#29702;&#35299;&#33021;&#21147;&#26469;&#24378;&#35843;&#25506;&#32034;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;RLLF&#22686;&#24378;&#30340;LLMs&#30340;&#24615;&#33021;&#19982;&#26410;&#20351;&#29992;RLLF&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#34913;&#26041;&#27861;&#30340;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27861;&#24459;AI&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01185v1 Announce Type: cross  Abstract: Finetuning approaches in NLP often focus on exploitation rather than exploration, which may lead to suboptimal models. Given the vast search space of natural language, this limited exploration can restrict their performance in complex, high-stakes domains, where accurate negation understanding and logical reasoning abilities are crucial. To address this issue, we leverage Reinforcement Learning from Logical Feedback (RLLF) to create an effective balance between exploration and exploitation in LLMs. Our approach employs an appropriate benchmark dataset for training and evaluation, highlighting the importance of exploration in enhancing negation understanding capabilities. We compare the performance of our RLLF-enhanced LLMs with baseline models trained without RLLF, demonstrating the value of this balanced approach. Furthermore, we showcase the potential of our method in legal AI applications by employing transfer learning and evaluatin
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01183</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#22330;&#26223;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#30340;&#29359;&#32618;&#20998;&#20026;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24050;&#32463;&#25104;&#20026;&#23545;&#21518;&#32773;&#20154;&#20204;&#31119;&#31049;&#21644;&#23433;&#20840;&#26500;&#25104;&#20840;&#29699;&#23041;&#32961;&#12290;&#23427;&#25552;&#20986;&#30340;&#25361;&#25112;&#24517;&#39035;&#36890;&#36807;&#32479;&#19968;&#30340;&#20840;&#29699;&#21512;&#20316;&#26469;&#38754;&#23545;&#65292;&#25105;&#20204;&#24517;&#39035;&#27604;&#20197;&#24448;&#26356;&#21152;&#20381;&#36182;&#33258;&#21160;&#21270;&#20294;&#20540;&#24471;&#20449;&#36182;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#32593;&#32476;&#29359;&#32618;&#26085;&#30410;&#22686;&#38271;&#30340;&#26412;&#36136;&#12290;&#27599;&#24180;&#26377;&#36229;&#36807;1000&#19975;&#36215;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#25552;&#20132;&#32473;&#32654;&#22269;&#22269;&#23478;&#22833;&#36394;&#21644;&#34987;&#21093;&#21066;&#20799;&#31461;&#20013;&#24515;&#65292;&#36229;&#36807;80%&#26469;&#33258;&#32593;&#32476;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#35843;&#26597;&#20013;&#24515;&#21644;&#28165;&#38500;&#20013;&#24515;&#26080;&#27861;&#25163;&#21160;&#22788;&#29702;&#21644;&#27491;&#30830;&#35843;&#26597;&#25152;&#26377;&#22270;&#20687;&#12290;&#22522;&#20110;&#27492;&#65292;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#38752;&#33258;&#21160;&#21270;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22330;&#26223;&#35782;&#21035;&#20219;&#21153;&#23547;&#25214;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#33021;&#22815;&#32452;&#32455;&#21644;&#20998;&#31867;&#20799;&#31461;&#24615;&#34384;&#24453;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing &amp; Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01166</link><description>&lt;p&gt;
DINER&#65306;&#20351;&#29992;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#26469;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#20174;&#32780;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#27169;&#22411;&#23481;&#26131;&#20174;&#27880;&#37322;&#20559;&#35265;&#20013;&#23398;&#20064;&#21040;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#23545;&#25239;&#24615;&#25968;&#25454;&#36716;&#25442;&#19978;&#40065;&#26834;&#24615;&#36739;&#24046;&#12290;&#22312;&#21435;&#20559;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#65292;&#20027;&#35201;&#21487;&#20998;&#20026;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#21435;&#20559;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#21333;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#19978;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#21464;&#37327;&#65288;&#30446;&#26631;&#26041;&#38754;&#21644;&#35780;&#35770;&#65289;&#30340;ABSA&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#21464;&#37327;&#22240;&#26524;&#25512;&#26029;&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;ABSA&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20559;&#35265;&#22522;&#20110;&#19981;&#21516;&#30340;&#22240;&#26524;&#24178;&#39044;&#26041;&#27861;&#24471;&#21040;&#22788;&#29702;&#12290;&#23545;&#20110;&#35780;&#35770;&#20998;&#25903;&#65292;&#20559;&#35265;&#34987;&#24314;&#27169;&#20026;&#26469;&#33258;&#19978;&#19979;&#25991;&#30340;&#38388;&#25509;&#28151;&#26434;&#65292;&#20854;&#20013;&#23454;&#26045;&#21453;&#21521;&#35843;&#25972;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01166v1 Announce Type: cross  Abstract: Though notable progress has been made, neural-based aspect-based sentiment analysis (ABSA) models are prone to learn spurious correlations from annotation biases, resulting in poor robustness on adversarial data transformations. Among the debiasing solutions, causal inference-based methods have attracted much research attention, which can be mainly categorized into causal intervention methods and counterfactual reasoning methods. However, most of the present debiasing methods focus on single-variable causal inference, which is not suitable for ABSA with two input variables (the target aspect and the review). In this paper, we propose a novel framework based on multi-variable causal inference for debiasing ABSA. In this framework, different types of biases are tackled based on different causal intervention methods. For the review branch, the bias is modeled as indirect confounding from context, where backdoor adjustment intervention is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01165</link><description>&lt;p&gt;
STAR: &#20351;&#29992;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#32422;&#26463;LoRA&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#25552;&#31034;&#26041;&#27861;&#23637;&#31034;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20173;&#38656;&#30417;&#30563;&#35757;&#32451;&#12290;&#38024;&#23545;LLMs&#30340;&#21442;&#25968;&#20247;&#22810;&#21644;&#20869;&#23384;&#28040;&#32791;&#22823;&#38382;&#39064;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#26041;&#27861;&#21644;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26088;&#22312;&#35299;&#20915;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#28040;&#32791;&#30340;&#38382;&#39064;&#65292;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19968;&#31181;&#26126;&#26174;&#30340;&#26041;&#24335;&#26159;&#23558;PEFT&#26041;&#27861;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#32452;&#21512;&#24182;&#38750;&#31616;&#21333;&#65292;&#24182;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#25506;&#38024;&#23454;&#39564;&#65292;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21487;&#33021;&#30001;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#35299;&#37322;&#65306;&#19981;&#30830;&#23450;&#24615;&#24046;&#36317;&#21644;&#27169;&#22411;&#26657;&#20934;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;LoRA&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01165v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#20197;&#23454;&#29616;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#23454;&#38469;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.01152</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#32508;&#36848;&#65306;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#20197;&#23454;&#29616;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#23454;&#38469;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26368;&#36817;&#30446;&#20987;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#36825;&#20123;LLMs&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#37325;&#22823;&#39118;&#38505;&#65292;&#27604;&#22914;&#21487;&#33021;&#22823;&#35268;&#27169;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#23459;&#20256;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#35875;&#35328;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#31995;&#32479;&#30340;&#32508;&#36848;&#65292;&#36825;&#26159;&#19968;&#20010;&#24212;&#23545;LLM&#28389;&#29992;&#25361;&#25112;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#27861;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#21462;&#35777;&#39046;&#22495;&#29616;&#26377;&#30340;&#21162;&#21147;&#65292;&#30528;&#30524;&#20110;&#19977;&#20010;&#20027;&#35201;&#25903;&#26609;&#65306;&#26816;&#27979;&#12289;&#24402;&#22240;&#21644;&#29305;&#24449;&#21270;&#12290;&#36825;&#20123;&#25903;&#26609;&#20351;&#20154;&#20204;&#33021;&#22815;&#23454;&#38469;&#29702;&#35299;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#35782;&#21035;AI&#29983;&#25104;&#20869;&#23481;&#65288;&#26816;&#27979;&#65289;&#12289;&#30830;&#23450;&#28041;&#21450;&#30340;&#20855;&#20307;AI&#27169;&#22411;&#65288;&#24402;&#22240;&#65289;&#20197;&#21450;&#23545;&#25991;&#26412;&#30340;&#22522;&#26412;&#24847;&#22270;&#36827;&#34892;&#20998;&#31867;&#65288;&#29305;&#24449;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01152v1 Announce Type: cross  Abstract: We have witnessed lately a rapid proliferation of advanced Large Language Models (LLMs) capable of generating high-quality text. While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale. This paper offers a review of AI-generated text forensic systems, an emerging field addressing the challenges of LLM misuses. We present an overview of the existing efforts in AI-generated text forensics by introducing a detailed taxonomy, focusing on three primary pillars: detection, attribution, and characterization. These pillars enable a practical understanding of AI-generated text, from identifying AI-generated content (detection), determining the specific AI model involved (attribution), and grouping the underlying intents of the text (characterization). Furtherm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25552;&#39640;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#38598;&#21644;&#23454;&#29616;&#24179;&#34913;&#27604;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;</title><link>https://arxiv.org/abs/2403.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#30340;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#25552;&#39640;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#25968;&#25454;&#38598;&#21644;&#23454;&#29616;&#24179;&#34913;&#27604;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22686;&#24378;&#20132;&#36890;&#23433;&#20840;&#24182;&#20419;&#36827;&#21450;&#26102;&#24212;&#24613;&#21709;&#24212;&#22806;&#65292;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#36890;&#36807;&#25552;&#20379;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20449;&#24687;&#65292;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38500;&#20102;&#37319;&#29992;&#20808;&#36827;&#30340;&#31639;&#27861;&#27169;&#22411;&#22806;&#65292;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#36824;&#21463;&#21040;&#33719;&#21462;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#28151;&#21512;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;Transformer&#22312;&#20132;&#36890;&#20107;&#25925;&#26816;&#27979;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;GANs&#25193;&#23637;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;1:4&#12289;2:3&#21644;1:1&#30340;&#24179;&#34913;&#27604;&#12290;&#35813;&#27169;&#22411;&#38024;&#23545;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01147v1 Announce Type: cross  Abstract: In addition to enhancing traffic safety and facilitating prompt emergency response, traffic incident detection plays an indispensable role in intelligent transportation systems by providing real-time traffic status information. This enables the realization of intelligent traffic control and management. Previous research has identified that apart from employing advanced algorithmic models, the effectiveness of detection is also significantly influenced by challenges related to acquiring large datasets and addressing dataset imbalances. A hybrid model combining transformer and generative adversarial networks (GANs) is proposed to address these challenges. Experiments are conducted on four real datasets to validate the superiority of the transformer in traffic incident detection. Additionally, GANs are utilized to expand the dataset and achieve a balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against the baseline mod
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01139</link><description>&lt;p&gt;
ParallelPARC: &#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#31867;&#27604;&#30340;&#21487;&#25193;&#23637;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01139
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;ParallelPARC&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#27573;&#33853;&#31867;&#27604;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#21508;&#31181;&#31867;&#27604;&#31867;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#20154;&#31867;&#22312;&#31867;&#27604;&#35782;&#21035;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Analogy-making&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;--&#36825;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#30340;&#33021;&#21147;&#12290;&#22823;&#22810;&#25968;&#31867;&#27604;&#25968;&#25454;&#38598;&#20170;&#22825;&#20851;&#27880;&#31616;&#21333;&#30340;&#31867;&#27604;&#65288;&#20363;&#22914;&#65292;&#35789;&#31867;&#27604;&#65289;&#65307;&#21253;&#21547;&#22797;&#26434;&#31867;&#22411;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#25163;&#24037;&#31574;&#21010;&#30340;&#65292;&#24182;&#19988;&#38750;&#24120;&#23567;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#31867;&#27604;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;ParallelPARC&#65288;Parallel Paragraph Creator&#65289;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21019;&#24314;&#22522;&#20110;&#27573;&#33853;&#30340;&#22797;&#26434;&#31867;&#27604;&#65292;&#20197;&#21450;&#31616;&#21333;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#65292;&#24182;&#21019;&#24314;&#20102;ProPara-Logy&#65292;&#19968;&#20010;&#20851;&#20110;&#31185;&#23398;&#36807;&#31243;&#38388;&#31867;&#27604;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#30001;&#20154;&#31867;&#39564;&#35777;&#36807;&#30340;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#20108;&#36827;&#21046;&#21644;&#22810;&#36873;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;LLMs&#21644;&#20154;&#31867;&#23545;&#31867;&#27604;&#30340;&#35782;&#21035;&#65292;&#21457;&#29616;&#20154;&#31867;&#32988;&#36807;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01139v1 Announce Type: cross  Abstract: Analogy-making is central to human cognition, allowing us to adapt to novel situations -- an ability that current AI systems still lack. Most analogy datasets today focus on simple analogies (e.g., word analogies); datasets including complex types of analogies are typically manually curated and very small. We believe that this holds back progress in computational analogy. In this work, we design a data generation pipeline, ParallelPARC (Parallel Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to create complex, paragraph-based analogies, as well as distractors, both simple and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset of analogies between scientific processes. We publish a gold-set, validated by humans, and a silver-set, generated automatically. We test LLMs' and humans' analogy recognition in binary and multiple-choice settings, and found that humans outperform the best mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;LLM-PQ&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#65292;&#22312;&#24322;&#26500;GPU&#38598;&#32676;&#19978;&#25552;&#39640;&#20102;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.01136</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#21644;&#33258;&#36866;&#24212;&#37327;&#21270;&#30340;&#24322;&#26500;&#38598;&#32676;&#19978;&#25552;&#20379;LLM-PQ
&lt;/p&gt;
&lt;p&gt;
LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01136
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;LLM-PQ&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#65292;&#22312;&#24322;&#26500;GPU&#38598;&#32676;&#19978;&#25552;&#39640;&#20102;LLM&#26381;&#21153;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;LLMs&#30340;&#24040;&#22823;&#35268;&#27169;&#23548;&#33268;&#20102;&#38750;&#24120;&#39640;&#30340;&#36164;&#28304;&#38656;&#27714;&#21644;&#25104;&#26412;&#12290;&#23613;&#31649;&#30446;&#21069;&#20027;&#35201;&#20351;&#29992;&#32479;&#19968;&#39640;&#24615;&#33021;GPU&#26469;&#26381;&#21153;&#36825;&#20123;&#27169;&#22411;&#65292;&#20294;&#21033;&#29992;&#19968;&#31181;&#28151;&#21512;&#21487;&#29992;&#39640;&#20302;&#23481;&#37327;GPU&#30340;&#24322;&#26500;&#38598;&#32676;&#21487;&#33021;&#20250;&#22823;&#24133;&#38477;&#20302;&#26381;&#21153;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#25903;&#25345;&#20351;&#29992;&#24322;&#26500;&#38598;&#32676;&#39640;&#25928;&#25552;&#20379;LLM&#26381;&#21153;&#30340;&#35774;&#35745;&#65292;&#32780;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#20998;&#21306;&#21644;&#22343;&#21248;&#21387;&#32553;&#22312;&#21516;&#36136;&#35774;&#22791;&#20043;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLM-PQ&#65292;&#36825;&#26159;&#19968;&#20010;&#20513;&#23548;&#33258;&#36866;&#24212;&#27169;&#22411;&#37327;&#21270;&#21644;&#30456;&#20301;&#24863;&#30693;&#20998;&#21306;&#20197;&#25552;&#39640;&#24322;&#26500;GPU&#38598;&#32676;&#19978;LLM&#26381;&#21153;&#25928;&#29575;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#20998;&#24067;&#24335;LLM&#26381;&#21153;&#20013;&#20180;&#32454;&#36873;&#25321;&#20102;&#28151;&#21512;&#31934;&#24230;&#27169;&#22411;&#37327;&#21270;&#12289;&#30456;&#20301;&#24863;&#30693;&#27169;&#22411;&#20998;&#21306;&#21644;&#24494;&#25209;&#37327;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01136v1 Announce Type: cross  Abstract: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated impressive performance on various tasks. The immense sizes of LLMs have led to very high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient LLM serving using a heterogeneous cluster, while the current solutions focus on model partition and uniform compression among homogeneous devices. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision model quantization together with phase-aware model partition and micro-batch sizing in distributed LLM servin
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#22270;&#20687;&#24863;&#30693;&#23646;&#24615;&#32553;&#20943;&#30340;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;&#27979;&#35797;VG&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.01118</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22270;&#20687;&#24863;&#30693;&#23646;&#24615;&#32553;&#20943;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Adversarial Testing for Visual Grounding via Image-Aware Property Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#22270;&#20687;&#24863;&#30693;&#23646;&#24615;&#32553;&#20943;&#30340;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#24615;&#27979;&#35797;VG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#34701;&#21512;&#22810;&#31181;&#27169;&#24577;&#20449;&#24687;&#30340;&#20248;&#21183;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#27491;&#22312;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20316;&#20026;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#35270;&#35273;&#23450;&#20301;&#65288;VG&#65289;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22312;&#22270;&#20687;&#20013;&#23450;&#20301;&#23545;&#35937;&#12290;&#30830;&#20445;VG&#27169;&#22411;&#30340;&#36136;&#37327;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#35813;&#20219;&#21153;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24615;&#12290;&#22312;&#40657;&#30418;&#22330;&#26223;&#19979;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#27979;&#35797;&#25216;&#26415;&#36890;&#24120;&#26410;&#33021;&#20805;&#20998;&#21457;&#25381;&#20449;&#24687;&#20004;&#31181;&#27169;&#24577;&#30340;&#28508;&#21147;&#12290;&#23427;&#20204;&#36890;&#24120;&#20165;&#22522;&#20110;&#22270;&#20687;&#25110;&#25991;&#26412;&#20449;&#24687;&#20043;&#19968;&#24212;&#29992;&#25200;&#21160;&#65292;&#24573;&#35270;&#20102;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20851;&#24615;&#65292;&#36825;&#23558;&#23548;&#33268;&#27979;&#35797;&#39044;&#35328;&#24335;&#22833;&#36133;&#25110;&#26080;&#27861;&#26377;&#25928;&#25361;&#25112;VG&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEELING&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22522;&#20110;&#22270;&#20687;&#24863;&#30693;&#23646;&#24615;&#32553;&#20943;&#30340;&#25991;&#26412;&#25200;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;VG&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01118v1 Announce Type: cross  Abstract: Due to the advantages of fusing information from various modalities, multimodal learning is gaining increasing attention. Being a fundamental task of multimodal learning, Visual Grounding (VG), aims to locate objects in images through natural language expressions. Ensuring the quality of VG models presents significant challenges due to the complex nature of the task. In the black box scenario, existing adversarial testing techniques often fail to fully exploit the potential of both modalities of information. They typically apply perturbations based solely on either the image or text information, disregarding the crucial correlation between the two modalities, which would lead to failures in test oracles or an inability to effectively challenge VG models. To this end, we propose PEELING, a text perturbation approach via image-aware property reduction for adversarial testing of the VG model. The core idea is to reduce the property-relate
&lt;/p&gt;</description></item><item><title>CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.01106</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#35299;&#37322;&#25552;&#28860;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Distilling Text Style Transfer With Self-Explanation From LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01106
&lt;/p&gt;
&lt;p&gt;
CoTeX&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26469;&#20419;&#36827;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#28860;LLMs&#30340;&#33021;&#21147;&#20026;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36879;&#26126;&#30340;&#35299;&#37322;&#22312;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;TST&#65289;&#26088;&#22312;&#25913;&#21464;&#25991;&#26412;&#30340;&#39118;&#26684;&#21516;&#26102;&#20445;&#30041;&#20854;&#26680;&#24515;&#20869;&#23481;&#12290;&#37492;&#20110;TST&#30340;&#26377;&#38480;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoTeX&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#26469;&#20419;&#36827;TST&#30340;&#26694;&#26550;&#12290;CoTeX&#23558;LLMs&#30340;&#22797;&#26434;&#37325;&#20889;&#21644;&#25512;&#29702;&#33021;&#21147;&#25552;&#28860;&#25104;&#26356;&#31616;&#21270;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#38750;&#24179;&#34892;&#25968;&#25454;&#21644;&#24179;&#34892;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22235;&#20010;TST&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;CoTeX&#26174;&#31034;&#20986;&#36229;&#36234;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;CoTeX&#19982;&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#12289;&#30417;&#30563;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25216;&#26415;&#20197;&#21450;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;CoTeX&#36890;&#36807;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#20854;&#39118;&#26684;&#36716;&#31227;&#36807;&#31243;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01106v1 Announce Type: cross  Abstract: Text Style Transfer (TST) seeks to alter the style of text while retaining its core content. Given the constraints of limited parallel datasets for TST, we propose CoTeX, a framework that leverages large language models (LLMs) alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills the complex rewriting and reasoning capabilities of LLMs into more streamlined models capable of working with both non-parallel and parallel data. Through experimentation across four TST datasets, CoTeX is shown to surpass traditional supervised fine-tuning and knowledge distillation methods, particularly in low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX against current unsupervised, supervised, in-context learning (ICL) techniques, and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering transparent explanations for its style transfer process.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01101</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#65306;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#32972;&#26223;&#19979;&#36890;&#36807;&#20195;&#29702;&#24605;&#32771;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26377;&#26395;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#19978;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#65292;&#29978;&#33267;&#21487;&#33021;&#36229;&#36807;&#35745;&#31639;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01101v1 Announce Type: cross  Abstract: Fine-tuning the pre-trained model with active learning holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based active learning, which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in active learning performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features' inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when fine-tuning with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while sele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.01091</link><description>&lt;p&gt;
COOL&#65306;&#19968;&#31181;&#34701;&#21512;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#30340;&#20849;&#21516;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COOL&#30340;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#20849;&#21516;&#25429;&#25417;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20132;&#36890;&#39044;&#27979;&#65292;&#26088;&#22312;&#26681;&#25454;&#21382;&#21490;&#24773;&#20917;&#39044;&#27979;&#20132;&#36890;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#37492;&#20110;&#20854;&#23545;&#22810;&#20010;&#22330;&#26223;&#30340;&#25345;&#32493;&#20851;&#27880;&#65292;&#24182;&#20419;&#36827;&#20102;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;&#22478;&#24066;&#35268;&#21010;&#21644;&#20132;&#36890;&#31649;&#29702;&#65292;&#35813;&#38382;&#39064;&#24050;&#32463;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#29420;&#31435;&#22320;&#24314;&#27169;&#26102;&#31354;&#20851;&#31995;&#65292;&#22240;&#27492;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#20004;&#32773;&#30340;&#22797;&#26434;&#39640;&#38454;&#20114;&#21160;&#65292;&#23548;&#33268;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#36807;&#28193;&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#36825;&#31181;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Conjoint Spatio-Temporal&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#32553;&#20889;&#20026;COOL&#65289;&#65292;&#23427;&#20174;&#20808;&#21069;&#21644;&#21518;&#32493;&#20449;&#24687;&#20013;&#24314;&#27169;&#24322;&#26500;&#22270;&#65292;&#20197;&#20849;&#21516;&#25429;&#25417;&#39640;&#38454;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01091v1 Announce Type: cross  Abstract: This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal graph neural network (abbreviated as COOL), which models heterogeneous graphs from prior and posterior information to conjointly capture high-order sp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#65292;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#22788;&#29702;&#22270;&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01079</link><description>&lt;p&gt;
&#25945;&#25480;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#22810;&#22270;&#20449;&#24687;&#65306;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01079
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#65292;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#22788;&#29702;&#22270;&#25968;&#25454;&#65292;&#36890;&#36807;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65306;&#24040;&#22823;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#23581;&#35797;&#36890;&#36807;&#20943;&#23569;&#23545;&#22270;&#32467;&#26500;&#30340;&#20381;&#36182;&#26469;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#12290;&#23613;&#31649;&#23558;&#22270;&#30693;&#35782;&#33976;&#39311;&#21040;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#26159;&#19968;&#20010;&#19981;&#38169;&#30340;&#24819;&#27861;&#65292;&#20294;&#23427;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#20301;&#32622;&#20449;&#24687;&#20002;&#22833;&#21644;&#27867;&#21270;&#33021;&#21147;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#38454;&#27573;&#22810;&#20219;&#21153;&#33976;&#39311;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;&#20301;&#32622;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#31070;&#32463;&#28909;&#26680;&#26469;&#36127;&#36131;&#22270;&#25968;&#25454;&#22788;&#29702;&#65292;&#22312;GNN&#20013;&#21033;&#29992;&#38544;&#34255;&#23618;&#36755;&#20986;&#21305;&#37197;&#26469;&#25552;&#39640;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#22270;&#19978;&#24341;&#20837;&#38544;&#34255;&#23618;&#33976;&#39311;&#29992;&#20110;&#23398;&#29983;&#22810;&#23618;&#24863;&#30693;&#26426;&#65292;&#24182;&#32467;&#21512;&#22270;&#20301;&#32622;&#32534;&#30721;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#31181;&#35774;&#32622;&#27979;&#35797;&#20102;&#20854;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01079v1 Announce Type: cross  Abstract: We study the challenging problem for inference tasks on large-scale graph datasets of Graph Neural Networks: huge time and memory consumption, and try to overcome it by reducing reliance on graph structure. Even though distilling graph knowledge to student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask distillation framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for graph data processing in GNN and utilize hidden layer outputs matching for better performance of student MLP's hidden layers. To the best of our knowledge, it is the first work to include hidden layer distillation for student MLP on graphs and to combine graph Positional Encoding with MLP. We test its performance and robustness with several settings and draw the conc
&lt;/p&gt;</description></item><item><title>$\Gamma$-VAE&#36890;&#36807;&#27491;&#21017;&#21270;&#26354;&#29575;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;</title><link>https://arxiv.org/abs/2403.01078</link><description>&lt;p&gt;
$\Gamma$-VAE: &#26354;&#29575;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
$\Gamma$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01078
&lt;/p&gt;
&lt;p&gt;
$\Gamma$-VAE&#36890;&#36807;&#27491;&#21017;&#21270;&#26354;&#29575;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#26032;&#20852;&#20302;&#32500;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26032;&#20852;&#34892;&#20026;&#30340;&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#27839;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#23376;&#38598;&#36827;&#34892;&#32452;&#32455;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#26377;&#25968;&#19975;&#20010;&#22522;&#22240;&#65292;&#20294;&#22522;&#22240;&#32452;&#23398;&#30340;&#21407;&#21017;&#30740;&#31350;&#23500;&#26377;&#25104;&#26524;&#65292;&#22240;&#20026;&#29983;&#29289;&#36807;&#31243;&#20381;&#36182;&#20110;&#21327;&#35843;&#32452;&#32455;&#65292;&#20174;&#32780;&#20135;&#29983;&#36739;&#20302;&#32500;&#24230;&#30340;&#34920;&#22411;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#31181;&#32452;&#32455;&#65292;&#35768;&#22810;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#24050;&#25104;&#21151;&#22320;&#23558;&#39640;&#32500;&#25968;&#25454;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#26041;&#27861;&#26159;&#20445;&#25345;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#38750;&#32447;&#24615;&#24615;&#20801;&#35768;&#36807;&#22810;&#30340;&#26354;&#29575;&#26469;&#20445;&#25345;&#36328;&#22810;&#20010;&#38750;&#30456;&#37051;&#25968;&#25454;&#38598;&#32676;&#30340;&#19968;&#33324;&#36235;&#21183;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#35268;&#33539;&#21270;&#30001;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#27969;&#24418;&#30340;&#26354;&#29575;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#65292;&#36825;&#19968;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;$\Gamma$-VAE&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01078v1 Announce Type: cross  Abstract: Natural systems with emergent behaviors often organize along low-dimensional subsets of high-dimensional spaces. For example, despite the tens of thousands of genes in the human genome, the principled study of genomics is fruitful because biological processes rely on coordinated organization that results in lower dimensional phenotypes. To uncover this organization, many nonlinear dimensionality reduction techniques have successfully embedded high-dimensional data into low-dimensional spaces by preserving local similarities between data points. However, the nonlinearities in these methods allow for too much curvature to preserve general trends across multiple non-neighboring data clusters, thereby limiting their interpretability and generalizability to out-of-distribution data. Here, we address both of these limitations by regularizing the curvature of manifolds generated by variational autoencoders, a process we coin ``$\Gamma$-VAE''.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01071</link><description>&lt;p&gt;
GraphRCG: &#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#36890;&#24120;&#26088;&#22312;&#21019;&#24314;&#19982;&#29305;&#23450;&#22270;&#20998;&#24067;&#23494;&#20999;&#23545;&#40784;&#30340;&#26032;&#22270;&#12290;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#36890;&#36807;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38544;&#24335;&#25429;&#33719;&#36825;&#31181;&#20998;&#24067;&#65292;&#21487;&#33021;&#24573;&#35270;&#20998;&#24067;&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#23545;&#22270;&#29983;&#25104;&#30340;&#35265;&#35299;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#26126;&#30830;&#24314;&#27169;&#22270;&#20998;&#24067;&#24182;&#21033;&#29992;&#36825;&#20123;&#20998;&#24067;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#33258;&#26465;&#20214;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#26679;&#26412;&#36716;&#25442;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22120;&#26469;&#25429;&#33719;&#22270;&#20998;&#24067;&#24182;&#29983;&#25104;&#21453;&#26144;&#23398;&#20064;&#20998;&#24067;&#30340;&#26032;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#24341;&#23548;&#34920;&#31034;&#20316;&#20026;&#33258;&#26465;&#20214;&#25351;&#23548;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01071v1 Announce Type: cross  Abstract: Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Textfocals&#65292;&#19968;&#20010;UI&#21407;&#22411;&#65292;&#25552;&#20379;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#12289;&#38382;&#39064;&#21644;&#24314;&#35758;&#65292;&#25903;&#25345;&#20889;&#20316;&#36807;&#31243;&#65292;&#24182;&#40723;&#21169;&#21453;&#24605;&#21644;&#33258;&#20027;&#20462;&#35746;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#19981;&#30452;&#25509;&#29983;&#25104;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#23545;&#20854;&#20889;&#20316;&#30340;&#23436;&#25972;&#20316;&#32773;&#36523;&#20221;&#12290;</title><link>https://arxiv.org/abs/2403.01055</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#23436;&#25972;&#20316;&#32773;&#36523;&#20221;&#65306;&#25903;&#25345;&#21033;&#29992;AI&#29983;&#25104;&#30340;&#35266;&#28857;&#36827;&#34892;&#20462;&#35746;
&lt;/p&gt;
&lt;p&gt;
Towards Full Authorship with AI: Supporting Revision with AI-Generated Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01055
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Textfocals&#65292;&#19968;&#20010;UI&#21407;&#22411;&#65292;&#25552;&#20379;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#12289;&#38382;&#39064;&#21644;&#24314;&#35758;&#65292;&#25903;&#25345;&#20889;&#20316;&#36807;&#31243;&#65292;&#24182;&#40723;&#21169;&#21453;&#24605;&#21644;&#33258;&#20027;&#20462;&#35746;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#19981;&#30452;&#25509;&#29983;&#25104;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#23545;&#20854;&#20889;&#20316;&#30340;&#23436;&#25972;&#20316;&#32773;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#22609;&#36896;&#20889;&#20316;&#24037;&#20855;&#20013;&#30340;&#19968;&#31181;&#26032;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#33539;&#24335;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#25552;&#31034;&#29983;&#25104;&#25991;&#26412;&#12290;&#36825;&#31181;&#33539;&#24335;&#23558;&#19968;&#20123;&#21019;&#20316;&#25511;&#21046;&#26435;&#20174;&#29992;&#25143;&#36716;&#31227;&#21040;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#24369;&#29992;&#25143;&#22312;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#20316;&#32773;&#36523;&#20221;&#21644;&#33258;&#20027;&#24615;&#12290;&#20026;&#20102;&#24674;&#22797;&#33258;&#20027;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Textfocals&#65292;&#19968;&#20010;&#26088;&#22312;&#30740;&#31350;&#24378;&#35843;&#29992;&#25143;&#22312;&#20889;&#20316;&#20013;&#35282;&#33394;&#30340;&#20197;&#20154;&#20026;&#26412;&#30340;UI&#21407;&#22411;&#12290;Textfocals&#36890;&#36807;&#22312;&#25991;&#26412;&#32534;&#36753;&#22120;&#30340;&#20391;&#26639;&#20013;&#25552;&#20379;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#12289;&#38382;&#39064;&#21644;&#24314;&#35758;&#65288;&#21363;LLM&#35270;&#22270;&#65289;&#65292;&#25903;&#25345;&#20889;&#20316;&#36807;&#31243;&#65292;&#40723;&#21169;&#22312;&#20889;&#20316;&#20013;&#36827;&#34892;&#21453;&#24605;&#21644;&#33258;&#20027;&#20462;&#35746;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#29983;&#25104;&#25991;&#26412;&#12290;Textfocals&#30340;UI&#21151;&#33021;&#65292;&#21253;&#25324;&#20855;&#26377;&#19978;&#19979;&#25991;&#36866;&#24212;&#24615;&#35270;&#22270;&#21644;&#29992;&#20110;&#25552;&#31034;&#36873;&#25321;&#21644;&#23450;&#21046;&#30340;&#25903;&#25745;&#21151;&#33021;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19982;LLMs&#20114;&#21160;&#30340;&#26041;&#24335;&#65292;&#29992;&#25143;&#22312;&#20854;&#20013;&#20445;&#25345;&#23545;&#20854;&#20889;&#20316;&#30340;&#23436;&#25972;&#20316;&#32773;&#36523;&#20221;&#12290;&#19982;Textfocals&#36827;&#34892;&#30340;&#24418;&#25104;&#24615;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01055v1 Announce Type: cross  Abstract: Large language models (LLMs) are shaping a new user interface (UI) paradigm in writing tools by enabling users to generate text through prompts. This paradigm shifts some creative control from the user to the system, thereby diminishing the user's authorship and autonomy in the writing process. To restore autonomy, we introduce Textfocals, a UI prototype designed to investigate a human-centered approach that emphasizes the user's role in writing. Textfocals supports the writing process by providing LLM-generated summaries, questions, and advice (i.e., LLM views) in a sidebar of a text editor, encouraging reflection and self-driven revision in writing without direct text generation. Textfocals' UI affordances, including contextually adaptive views and scaffolding for prompt selection and customization, offer a novel way to interact with LLMs where users maintain full authorship of their writing. A formative user study with Textfocals sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01053</link><description>&lt;p&gt;
&#36879;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#21457;&#29616;&#26032;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#12289;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 &#36890;&#21578;&#31867;&#22411;: &#20132;&#21449;  &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;&#20197;&#20854;&#25968;&#25454;&#39537;&#21160;&#30340;&#29305;&#24615;&#65292;&#23545;&#31185;&#23398;&#21457;&#29616;&#30340;&#22522;&#26412;&#23454;&#36341;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#25913;&#21464;&#12290;&#38543;&#30528;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#33258;&#21160;&#25506;&#32034;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#35265;&#35299;&#65292;&#21457;&#29616;&#26032;&#30340;&#34920;&#22411;&#31867;&#21035;&#21644;&#27010;&#24565;&#23558;&#20250;&#21464;&#24471;&#26356;&#21152;&#21560;&#24341;&#20154;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#32047;&#31215;&#25968;&#25454;&#20013;&#23384;&#22312;&#33509;&#24178;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#26032;&#31867;&#21457;&#29616;&#30340;&#36827;&#23637;&#12290;&#38750; i.i.d. &#25968;&#25454;&#20998;&#24067;&#20276;&#38543;&#30528;&#19981;&#21516;&#31867;&#21035;&#32452;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#65292;&#26412;&#36136;&#19978;&#23548;&#33268;&#27169;&#31946;&#21644;&#20559;&#20506;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#38480;&#21046;&#27010;&#29575;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23454;&#20363;&#23884;&#20837;&#30340;&#36817;&#20284;&#21518;&#39564;&#21442;&#25968;&#21270;&#20026;&#36793;&#38469; von Mises-Fisher &#20998;&#24067;&#65292;&#20197;&#35299;&#20915;&#31070;&#32463;&#23884;&#20837;&#26041;&#26696;&#30340;&#27169;&#31946;&#24615;&#19982;&#20559;&#35265;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#33258;&#21160;&#21270;&#25915;&#20987;&#30340;&#20808;&#21069;&#21644;&#21518;&#32493;&#38454;&#27573;&#65292;&#36825;&#21487;&#33021;&#20250;&#23558;&#32452;&#32455;&#24615;&#25915;&#20987;&#20174;&#32597;&#35265;&#30340;&#19987;&#23478;&#20027;&#23548;&#20107;&#20214;&#36716;&#21464;&#20026;&#39057;&#32321;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#19981;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#36895;&#24230;&#21644;&#35268;&#27169;&#36827;&#34892;&#25191;&#34892;&#65292;&#36825;&#21487;&#33021;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20840;&#29699;&#35745;&#31639;&#26426;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2403.01038</link><description>&lt;p&gt;
AutoAttacker: &#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#29616;&#33258;&#21160;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01038
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26395;&#33258;&#21160;&#21270;&#25915;&#20987;&#30340;&#20808;&#21069;&#21644;&#21518;&#32493;&#38454;&#27573;&#65292;&#36825;&#21487;&#33021;&#20250;&#23558;&#32452;&#32455;&#24615;&#25915;&#20987;&#20174;&#32597;&#35265;&#30340;&#19987;&#23478;&#20027;&#23548;&#20107;&#20214;&#36716;&#21464;&#20026;&#39057;&#32321;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#19981;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#36895;&#24230;&#21644;&#35268;&#27169;&#36827;&#34892;&#25191;&#34892;&#65292;&#36825;&#21487;&#33021;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20840;&#29699;&#35745;&#31639;&#26426;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#22312;&#36827;&#25915;&#21644;&#38450;&#24481;&#31995;&#32479;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#24050;&#32463;&#26377;&#22810;&#20010;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;LLMs&#19987;&#27880;&#20110;&#25915;&#20987;&#30340;&#39044;&#20837;&#20405;&#38454;&#27573;&#65292;&#20363;&#22914;&#38035;&#40060;&#21644;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#32570;&#20047;&#20851;&#20110;LLM&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#27169;&#25311;&#36890;&#24120;&#30001;&#20154;&#31867;&#25805;&#20316;&#30340;&#25915;&#20987;&#30340;&#25915;&#20987;&#21518;&#38454;&#27573;&#65292;&#25110;&#32773;&#26159;&#8220;&#25163;&#21160;&#36755;&#20837;&#8221;&#30340;&#25915;&#20987;&#65292;&#20197;&#21450;&#38024;&#23545;&#19981;&#21516;&#25915;&#20987;&#25216;&#26415;&#21644;&#29615;&#22659;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01038v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive results on natural language tasks, and security researchers are beginning to employ them in both offensive and defensive systems. In cyber-security, there have been multiple research efforts that utilize LLMs focusing on the pre-breach stage of attacks like phishing and malware generation. However, so far there lacks a comprehensive study regarding whether LLM-based systems can be leveraged to simulate the post-breach stage of attacks that are typically human-operated, or "hands-on-keyboard" attacks, under various attack techniques and environments.   As LLMs inevitably advance, they may be able to automate both the pre- and post-breach attack stages. This shift may transform organizational attacks from rare, expert-led events to frequent, automated operations requiring no expertise and executed at automation speed and scale. This risks fundamentally changing global computer sec
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna</title><link>https://arxiv.org/abs/2403.01031</link><description>&lt;p&gt;
&#23380;&#38592;&#65306;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#21450;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01031
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#38463;&#25289;&#20271;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;Peacock&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22522;&#20934;Henna
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38500;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24335;&#36164;&#28304;&#65292;MLLMs&#30340;&#25104;&#21151;&#20173;&#28982;&#30456;&#23545;&#23616;&#38480;&#20110;&#33521;&#35821;&#29615;&#22659;&#12290;&#36825;&#32473;&#24320;&#21457;&#20854;&#20182;&#35821;&#35328;&#30340;&#21487;&#27604;&#36739;&#27169;&#22411;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29978;&#33267;&#21253;&#25324;&#37027;&#20123;&#25317;&#26377;&#24222;&#22823;&#35828;&#35805;&#20154;&#21475;&#30340;&#35821;&#35328;&#65292;&#22914;&#38463;&#25289;&#20271;&#35821;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#38463;&#25289;&#20271;MLLMs&#31995;&#21015;&#65292;&#31216;&#20026;\textit{Peacock}&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23427;&#20204;&#19981;&#26029;&#20986;&#29616;&#30340;&#26041;&#35328;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;\textit{Henna}&#30340;&#26032;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#19982;&#38463;&#25289;&#20271;&#35821;&#30456;&#20851;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01031v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#27979;&#37327;&#25511;&#21046;&#30340;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#27700;&#24211;&#35745;&#31639;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#27700;&#24211;&#35745;&#31639;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#31070;&#32463;&#20803;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#26377;&#28508;&#21147;&#22312;&#23481;&#38169;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01024</link><description>&lt;p&gt;
&#21033;&#29992;&#27979;&#37327;&#25511;&#21046;&#30340;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#27700;&#24211;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing Using Measurement-Controlled Quantum Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01024
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#27979;&#37327;&#25511;&#21046;&#30340;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#27700;&#24211;&#35745;&#31639;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#27700;&#24211;&#35745;&#31639;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20154;&#24037;&#31070;&#32463;&#20803;&#23454;&#29616;&#24555;&#36895;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#26377;&#28508;&#21147;&#22312;&#23481;&#38169;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#27700;&#24211;&#35745;&#31639;&#65288;RC&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26469;&#39044;&#27979;&#39640;&#24230;&#38750;&#32447;&#24615;&#21644;&#28151;&#27788;&#29616;&#35937;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33108;&#20013;&#34987;&#25506;&#27979;&#21407;&#23376;&#30340;&#21160;&#21147;&#23398;&#30340;&#37327;&#23376;RC&#31995;&#32479;&#12290;&#21407;&#23376;&#20197;&#29305;&#23450;&#36895;&#29575;&#32463;&#21382;&#30456;&#24178;&#39537;&#21160;&#65292;&#23548;&#33268;&#27979;&#37327;&#25511;&#21046;&#30340;&#37327;&#23376;&#28436;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#27700;&#24211;&#21487;&#20197;&#20351;&#29992;&#36739;&#23569;&#25968;&#37327;&#30340;&#20154;&#24037;&#31070;&#32463;&#20803;&#36827;&#34892;&#24555;&#36895;&#21644;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#19982;&#20256;&#32479;RC&#31639;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#27700;&#24211;&#30340;&#36816;&#20316;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#23481;&#38169;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#36817;&#20284;&#35745;&#31639;&#26041;&#27861;&#21487;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#36164;&#28304;&#26465;&#20214;&#19979;&#36827;&#34892;&#21487;&#34892;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01024v1 Announce Type: cross  Abstract: Physical reservoir computing (RC) is a machine learning algorithm that employs the dynamics of a physical system to forecast highly nonlinear and chaotic phenomena. In this paper, we introduce a quantum RC system that employs the dynamics of a probed atom in a cavity. The atom experiences coherent driving at a particular rate, leading to a measurement-controlled quantum evolution. The proposed quantum reservoir can make fast and reliable forecasts using a small number of artificial neurons compared with the traditional RC algorithm. We theoretically validate the operation of the reservoir, demonstrating its potential to be used in error-tolerant applications, where approximate computing approaches may be used to make feasible forecasts in conditions of limited computational and energy resources.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;&#36807;&#31243;&#20013;&#22686;&#21152;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#26469;&#24494;&#35843;&#27169;&#22411;-based &#25511;&#21046;&#22120;&#20197;&#34917;&#20607;&#32500;&#24230;&#32422;&#31616;&#24341;&#36215;&#30340;&#24314;&#27169;&#38169;&#35823;&#65292;&#24182;&#23558;&#25972;&#20307;&#31574;&#30053;&#36716;&#21464;&#20026;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;-&#28982;&#21518;&#36866;&#24212;&#30340;PDE&#25511;&#21046;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01005</link><description>&lt;p&gt;
&#20855;&#26377;&#28909;&#21551;&#21160;&#30340;PDE&#25511;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for PDE Control with a Warm Start
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;&#36807;&#31243;&#20013;&#22686;&#21152;&#31574;&#30053;&#20248;&#21270;&#27493;&#39588;&#65292;&#26469;&#24494;&#35843;&#27169;&#22411;-based &#25511;&#21046;&#22120;&#20197;&#34917;&#20607;&#32500;&#24230;&#32422;&#31616;&#24341;&#36215;&#30340;&#24314;&#27169;&#38169;&#35823;&#65292;&#24182;&#23558;&#25972;&#20307;&#31574;&#30053;&#36716;&#21464;&#20026;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;-&#28982;&#21518;&#36866;&#24212;&#30340;PDE&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#32422;&#31616;&#23545;&#36890;&#36807;&#8220;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;&#8221;&#31574;&#30053;&#25511;&#21046;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31574;&#30053;&#30830;&#23450;&#38477;&#38454;&#27169;&#22411;&#28982;&#21518;&#23454;&#26045;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38477;&#38454;&#24314;&#27169;&#30340;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#38477;&#20302;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#28151;&#27788;&#34892;&#20026;&#30340;PDE&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;&#36807;&#31243;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#31574;&#30053;&#20248;&#21270;&#65288;PO&#65289;&#27493;&#39588;&#12290;PO&#27493;&#39588;&#24494;&#35843;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#34917;&#20607;&#30001;&#32500;&#24230;&#32422;&#31616;&#24341;&#36215;&#30340;&#24314;&#27169;&#35823;&#24046;&#12290;&#36825;&#31181;&#22686;&#24378;&#23558;&#25972;&#20307;&#31574;&#30053;&#36716;&#21464;&#20026;&#20943;&#23569;-&#28982;&#21518;&#35774;&#35745;-&#28982;&#21518;&#35843;&#25972;&#65292;&#20854;&#20013;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#20316;&#20026;PO&#30340;&#28909;&#21551;&#21160;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#23558;PDE&#29366;&#24577;&#19982;&#29305;&#23450;&#24658;&#23450;&#30446;&#26631;&#23545;&#40784;&#30340;PDE&#29366;&#24577;&#21453;&#39304;&#36319;&#36394;&#25511;&#21046;&#65292;&#21463;&#32447;&#24615;&#20108;&#27425;&#25104;&#26412;&#32422;&#26463;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01005v1 Announce Type: cross  Abstract: Dimensionality reduction is crucial for controlling nonlinear partial differential equations (PDE) through a "reduce-then-design" strategy, which identifies a reduced-order model and then implements model-based control solutions. However, inaccuracies in the reduced-order modeling can substantially degrade controller performance, especially in PDEs with chaotic behavior. To address this issue, we augment the reduce-then-design procedure with a policy optimization (PO) step. The PO step fine-tunes the model-based controller to compensate for the modeling error from dimensionality reduction. This augmentation shifts the overall strategy into reduce-then-design-then-adapt, where the model-based controller serves as a warm start for PO. Specifically, we study the state-feedback tracking control of PDEs that aims to align the PDE state with a specific constant target subject to a linear-quadratic cost. Through extensive experiments, we show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlaKat&#30340;&#26032;&#22411;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;flaky&#27979;&#35797;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;FDC&#12290;</title><link>https://arxiv.org/abs/2403.01003</link><description>&lt;p&gt;
FlaKat: &#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38754;&#21521;Flaky&#27979;&#35797;&#30340;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlaKat: A Machine Learning-Based Categorization Framework for Flaky Tests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlaKat&#30340;&#26032;&#22411;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;flaky&#27979;&#35797;&#30340;&#31867;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;FDC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Flaky tests &#26159;&#22312;&#19981;&#26356;&#25913;&#36719;&#20214;&#31995;&#32479;&#30340;&#24773;&#20917;&#19979;&#20197;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#36890;&#36807;&#25110;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#36935;&#21040;&#36825;&#20123;&#27979;&#35797;&#65292;&#23427;&#20204;&#24433;&#21709;&#20102;&#27979;&#35797;&#22871;&#20214;&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#23558;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;flaky&#27979;&#35797;&#26816;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#24403;&#19981;&#38169;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#33258;&#21160;flaky&#27979;&#35797;&#20462;&#22797;&#35299;&#20915;&#26041;&#26696;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;flaky&#27979;&#35797;&#35774;&#35745;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlaKat&#30340;&#26032;&#22411;&#20998;&#31867;&#26694;&#26550;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;flaky&#27979;&#35797;&#30340;&#31867;&#21035;&#65292;&#21453;&#26144;&#20854;&#26681;&#26412;&#21407;&#22240;&#12290;&#37319;&#26679;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#22269;&#38469;Flaky&#27979;&#35797;&#25968;&#25454;&#38598;&#65288;IDoFT&#65289;&#20013;flaky&#27979;&#35797;&#31867;&#21035;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Flakiness Detection Capacity&#65288;FDC&#65289;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#20174;&#20449;&#24687;&#29702;&#35770;&#35282;&#24230;&#34913;&#37327;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01003v1 Announce Type: cross  Abstract: Flaky tests can pass or fail non-deterministically, without alterations to a software system. Such tests are frequently encountered by developers and hinder the credibility of test suites. State-of-the-art research incorporates machine learning solutions into flaky test detection and achieves reasonably good accuracy. Moreover, the majority of automated flaky test repair solutions are designed for specific types of flaky tests. This research work proposes a novel categorization framework, called FlaKat, which uses machine-learning classifiers for fast and accurate prediction of the category of a given flaky test that reflects its root cause. Sampling techniques are applied to address the imbalance between flaky test categories in the International Dataset of Flaky Test (IDoFT). A new evaluation metric, called Flakiness Detection Capacity (FDC), is proposed for measuring the accuracy of classifiers from the perspective of information th
&lt;/p&gt;</description></item><item><title>&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01002</link><description>&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01002
&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32467;&#26500;&#21270;&#26694;&#26550;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#20154;&#24037;&#35780;&#27880;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20581;&#24247;&#20915;&#31574;&#25903;&#25345;&#21644;&#20020;&#24202;&#30740;&#31350;&#20013;&#65292;&#24635;&#32467;&#20020;&#24202;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#29983;&#25104;&#20934;&#30830;&#30340;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#19982;&#22522;&#30784;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20581;&#24247;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#32467;&#26500;&#21270;&#65288;AS&#65289;&#20316;&#20026;&#36890;&#29992;&#32531;&#35299;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#26500;&#21270;&#20102;&#25688;&#35201;&#35780;&#20272;&#36807;&#31243;&#12290;&#23427;&#23558;&#35780;&#20272;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#20010;&#22522;&#20110;LLM&#25191;&#34892;&#30456;&#23545;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#21644;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#32508;&#21512;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AS&#22987;&#32456;&#25913;&#21892;&#20102;&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;&#20013;&#20154;&#31867;&#27880;&#37322;&#21644;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;AS&#36890;&#36807;&#30701;&#25991;&#26412;&#24418;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01002v1 Announce Type: cross  Abstract: Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short te
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#21355;&#29983;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#30456;&#32852;&#31995;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00994</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#39044;&#27979;&#27969;&#34892;&#30149;&#20581;&#24247;&#20915;&#31574;&#21644;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prompt-Based Large Language Models: Predicting Pandemic Health Decisions and Outcomes Through Social Media Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#21355;&#29983;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#30456;&#32852;&#31995;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#25512;&#29702;&#26694;&#26550;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;LLMs&#26469;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#22269;&#23478;&#20581;&#24247;&#32467;&#26524;&#36235;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#27169;&#31946;&#36712;&#36857;&#29702;&#35770;&#65292;&#24378;&#35843;&#20581;&#24247;&#27807;&#36890;&#20013;&#22240;&#26524;&#19968;&#33268;&#24615;&#35201;&#20041;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35282;&#33394;&#30340;&#28176;&#36827;&#36741;&#23548;&#65288;RBIC&#65289;&#65292;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#35782;&#21035;&#35201;&#20041;&#12290;&#20351;&#29992;RBIC&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#21453;&#23545;COVID-19&#20581;&#24247;&#25514;&#26045;&#30340;subreddit&#35752;&#35770;&#20013;&#25552;&#21462;&#35201;&#20041;&#65288;&#30740;&#31350;1&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36319;&#36394;&#36825;&#20123;&#35201;&#20041;&#22312;&#20851;&#38190;&#20107;&#20214;&#20013;&#30340;&#28436;&#21464;&#65288;&#30740;&#31350;2&#65289;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#23545;&#22312;&#32447;&#20114;&#21160;&#30340;&#24433;&#21709;&#65288;&#30740;&#31350;3&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#35201;&#20041;&#37327;&#22914;&#20309;&#19982;&#22269;&#23478;&#20581;&#24247;&#36235;&#21183;&#65288;&#22914;&#30123;&#33495;&#25509;&#31181;&#29575;&#21644;&#20303;&#38498;&#29575;&#65289;&#30456;&#20851;&#32852;&#65288;&#30740;&#31350;4&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#20174;&#23454;&#35777;&#35282;&#24230;&#23558;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#27169;&#24335;&#19982;&#29616;&#23454;&#19990;&#30028;&#20844;&#20849;&#21355;&#29983;&#36235;&#21183;&#32852;&#31995;&#36215;&#26469;&#65292;&#31361;&#26174;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00994v1 Announce Type: cross  Abstract: We introduce a multi-step reasoning framework using prompt-based LLMs to examine the relationship between social media language patterns and trends in national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the importance of gists of causal coherence in effective health communication, we introduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework, to identify gists at-scale. Using RBIC, we systematically extract gists from subreddit discussions opposing COVID-19 health measures (Study 1). We then track how these gists evolve across key events (Study 2) and assess their influence on online engagement (Study 3). Finally, we investigate how the volume of gists is associated with national health trends like vaccine uptake and hospitalizations (Study 4). Our work is the first to empirically link social media linguistic patterns to real-world public health trends, highlighting the potential of prompt-bas
&lt;/p&gt;</description></item><item><title>&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.00993</link><description>&lt;p&gt;
&#35770;&#37096;&#20998;&#21487;&#35266;&#23519;&#24207;&#21015;&#22242;&#38431;&#21644;&#28216;&#25103;&#20013;&#20449;&#24687;&#32467;&#26500;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00993
&lt;/p&gt;
&lt;p&gt;
&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#20449;&#24687;&#32467;&#26500;&#25551;&#36848;&#20102;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#21051;&#20107;&#20214;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#26412;&#25991;&#20027;&#24352;&#26126;&#30830;&#34920;&#31034;&#20449;&#24687;&#32467;&#26500;&#26159;&#20998;&#26512;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20855;&#26377;&#26126;&#30830;&#20449;&#24687;&#32467;&#26500;&#34920;&#31034;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#30740;&#31350;&#23545;8&#31181;&#21322;&#20107;&#23454;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#25351;&#23548;&#24182;&#38750;&#24517;&#35201;&#65292;&#32780;&#26159;... (&#30001;&#20110;&#31687;&#24133;&#38480;&#21046;&#65292;&#33509;&#26377;&#30465;&#30053;&#65292;&#35831;&#35265;&#35845;)</title><link>https://arxiv.org/abs/2403.00980</link><description>&lt;p&gt;
&#20174;&#8220;&#21482;&#35201;&#8221;&#21040;&#8220;&#21363;&#20351;&#8221;&#65306;&#26159;&#21542;&#36890;&#36807;&#21453;&#20107;&#23454;&#25351;&#23548;&#26368;&#20339;&#21322;&#20107;&#23454;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found Using Counterfactuals As Guides?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00980
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#30740;&#31350;&#23545;8&#31181;&#21322;&#20107;&#23454;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#25351;&#23548;&#24182;&#38750;&#24517;&#35201;&#65292;&#32780;&#26159;... (&#30001;&#20110;&#31687;&#24133;&#38480;&#21046;&#65292;&#33509;&#26377;&#30465;&#30053;&#65292;&#35831;&#35265;&#35845;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#21482;&#35201;&#8221;&#35299;&#37322;&#20013;&#30340;&#21453;&#20107;&#23454;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;eXplainable AI&#65292;XAI&#65289;&#39046;&#22495;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#25551;&#36848;&#20102;&#23545;&#40657;&#30418;AI&#31995;&#32479;&#30340;&#29305;&#24449;&#36755;&#20837;&#36827;&#34892;&#21738;&#20123;&#26356;&#25913;&#20250;&#23548;&#33268;&#65288;&#36890;&#24120;&#26159;&#36127;&#38754;&#30340;&#65289;&#20915;&#31574;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#26356;&#36817;&#26399;&#65292;&#20351;&#29992;&#8220;&#21363;&#20351;&#8221;&#35299;&#37322;&#30340;&#21322;&#20107;&#23454;&#26041;&#27861;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#12290;&#23427;&#20204;&#38416;&#26126;&#20102;&#23545;AI&#31995;&#32479;&#30340;&#29305;&#24449;&#36755;&#20837;&#36827;&#34892;&#30340;&#26356;&#25913;&#19981;&#20250;&#25913;&#21464;&#20915;&#31574;&#32467;&#26524;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#20986;&#26356;&#26377;&#21033;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;&#19968;&#20123;&#21322;&#20107;&#23454;&#26041;&#27861;&#20351;&#29992;&#21453;&#20107;&#23454;&#26469;&#24341;&#23548;&#26597;&#35810;&#23454;&#20363;&#20197;&#25351;&#23548;&#21322;&#20107;&#23454;&#29983;&#25104;&#65288;&#31216;&#20026;&#21453;&#20107;&#23454;&#24341;&#23548;&#26041;&#27861;&#65289;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19981;&#36825;&#26679;&#20570;&#65288;&#31216;&#20026;&#26080;&#21453;&#20107;&#23454;&#26041;&#27861;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;7&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;8&#31181;&#21322;&#20107;&#23454;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;5&#20010;&#20851;&#38190;&#25351;&#26631;&#65292;&#20197;&#30830;&#23450;&#21453;&#20107;&#23454;&#25351;&#23548;&#26159;&#21542;&#26377;&#24517;&#35201;&#25214;&#21040;&#26368;&#20339;&#30340;&#21322;&#20107;&#23454;&#12290;&#36825;&#20123;&#27979;&#35797;&#30340;&#32467;&#26524;&#34920;&#26126;&#24182;&#19981;&#26159;&#65292;&#32780;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00980v1 Announce Type: new  Abstract: Recently, counterfactuals using "if-only" explanations have become very popular in eXplainable AI (XAI), as they describe which changes to feature-inputs of a black-box AI system result in changes to a (usually negative) decision-outcome. Even more recently, semi-factuals using "even-if" explanations have gained more attention. They elucidate the feature-input changes that do \textit{not} change the decision-outcome of the AI system, with a potential to suggest more beneficial recourses. Some semi-factual methods use counterfactuals to the query-instance to guide semi-factual production (so-called counterfactual-guided methods), whereas others do not (so-called counterfactual-free methods). In this work, we perform comprehensive tests of 8 semi-factual methods on 7 datasets using 5 key metrics, to determine whether counterfactual guidance is necessary to find the best semi-factuals. The results of these tests suggests not, but rather tha
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;&#36755;&#20986;&#65292;&#23454;&#29616;&#20934;&#30830;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#26816;&#27979;&#24615;&#33021;&#24694;&#21270;&#65292;&#20197;&#25512;&#21160;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.00975</link><description>&lt;p&gt;
&#39118;&#21147;&#21457;&#30005;&#26426;&#24615;&#33021;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#35774;&#22791;&#20581;&#24247;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00975
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#39044;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#21151;&#29575;&#36755;&#20986;&#65292;&#23454;&#29616;&#20934;&#30830;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#26816;&#27979;&#24615;&#33021;&#24694;&#21270;&#65292;&#20197;&#25512;&#21160;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;SCADA&#25968;&#25454;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21151;&#33021;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26469;&#39044;&#27979;&#21151;&#29575;&#36755;&#20986;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;FNN&#21644;LSTM&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#38598;&#20307;&#23398;&#20064;&#12290;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#32988;&#36807;&#21333;&#20010;&#27169;&#22411;&#65292;&#30830;&#20445;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#21151;&#29575;&#36755;&#20986;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#21457;&#30005;&#26426;&#24615;&#33021;&#24694;&#21270;&#65292;&#23454;&#29616;&#31215;&#26497;&#30340;&#32500;&#25252;&#31574;&#30053;&#21644;&#20581;&#24247;&#35780;&#20272;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27599;&#21488;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#29420;&#29305;&#24615;&#65292;&#38656;&#35201;&#20026;&#26368;&#20339;&#39044;&#27979;&#23450;&#21046;&#27169;&#22411;&#12290;&#36825;&#20123;&#35265;&#35299;&#24378;&#35843;&#25552;&#20379;&#19981;&#21516;&#21457;&#30005;&#26426;&#33258;&#21160;&#21270;&#23450;&#21046;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20445;&#25345;&#20154;&#21147;&#24314;&#27169;&#24037;&#20316;&#37327;&#20302;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#20998;&#26512;&#20013;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#23616;&#38480;&#20110;&#39118;&#21147;&#21457;&#30005;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00975v1 Announce Type: cross  Abstract: In this study, we leverage SCADA data from diverse wind turbines to predict power output, employing advanced time series methods, specifically Functional Neural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key innovation lies in the ensemble of FNN and LSTM models, capitalizing on their collective learning. This ensemble approach outperforms individual models, ensuring stable and accurate power output predictions. Additionally, machine learning techniques are applied to detect wind turbine performance deterioration, enabling proactive maintenance strategies and health assessment. Crucially, our analysis reveals the uniqueness of each wind turbine, necessitating tailored models for optimal predictions. These insight underscores the importance of providing automatized customization for different turbines to keep human modeling effort low. Importantly, the methodologies developed in this analysis are not limited to wind tu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415; Binary Gaussian Copula Synthesis (BGCS)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#36879;&#26512;&#38656;&#27714;&#20013;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.00965</link><description>&lt;p&gt;
&#20108;&#20540;&#39640;&#26031;Copula&#21512;&#25104;&#65306;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#36827;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#30340;&#36879;&#26512;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00965
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415; Binary Gaussian Copula Synthesis (BGCS)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#22312;&#26089;&#26399;&#39044;&#27979;&#24930;&#24615;&#32958;&#30149;&#24739;&#32773;&#36879;&#26512;&#38656;&#27714;&#20013;&#25152;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;&#23398;&#26415;&#65306;2403.00965v1  &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028;  &#25688;&#35201;&#65306;&#32654;&#22269;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#20272;&#35745;&#65292;&#36229;&#36807;3700&#19975;&#25104;&#24180;&#32654;&#22269;&#20154;&#24739;&#26377;&#24930;&#24615;&#32958;&#30149;&#65288;CKD&#65289;&#65292;&#28982;&#32780;&#20854;&#20013;&#30340;9&#25104;&#24739;&#32773;&#30001;&#20110;&#26089;&#26399;&#27809;&#26377;&#30151;&#29366;&#32780;&#19981;&#30693;&#36947;&#33258;&#24049;&#30340;&#29366;&#20917;&#12290;&#26089;&#26399;&#39044;&#27979;&#36879;&#26512;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#65292;&#24182;&#24110;&#21161;&#21307;&#30103;&#25552;&#20379;&#32773;&#21450;&#26102;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#26377;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26089;&#26399;&#36879;&#26512;&#39044;&#27979;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#38754;&#20020;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20108;&#20540;&#39640;&#26031;Copula&#21512;&#25104;&#65288;BGCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#20108;&#36827;&#21046;&#25968;&#25454;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00965v1 Announce Type: cross  Abstract: The Center for Disease Control estimates that over 37 million US adults suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals are unaware of their condition due to the absence of symptoms in the early stages. It has a significant impact on patients' quality of life, particularly when it progresses to the need for dialysis. Early prediction of dialysis is crucial as it can significantly improve patient outcomes and assist healthcare providers in making timely and informed decisions. However, developing an effective machine learning (ML)-based Clinical Decision Support System (CDSS) for early dialysis prediction poses a key challenge due to the imbalanced nature of data. To address this challenge, this study evaluates various data augmentation techniques to understand their effectiveness on real-world datasets. We propose a new approach named Binary Gaussian Copula Synthesis (BGCS). BGCS is tailored for binary me
&lt;/p&gt;</description></item><item><title>AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00953</link><description>&lt;p&gt;
AutoRD&#65306;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#26500;&#24314;&#30340;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontologies-enhanced Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00953
&lt;/p&gt;
&lt;p&gt;
AutoRD&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#23398;&#30693;&#35782;&#22270;&#26500;&#24314;&#32597;&#35265;&#30142;&#30149;&#30693;&#35782;&#22270;&#65292;&#23454;&#29616;&#20102;&#25972;&#20307;F1&#24471;&#20998;47.3%&#65292;&#30456;&#23545;&#20110;&#22522;&#30784;LLM&#26377;14.4%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#21517;&#20026;AutoRD&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#26377;&#20851;&#32597;&#35265;&#30142;&#30149;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#27979;&#35797;&#26469;&#35780;&#20272;AutoRD&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#24378;&#35843;&#20102;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#31995;&#32479;AutoRD&#26159;&#19968;&#20010;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#28041;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#23454;&#20307;&#26657;&#20934;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30001;&#24320;&#28304;&#21307;&#23398;&#26412;&#20307;&#21457;&#23637;&#32780;&#26469;&#30340;&#21307;&#23398;&#30693;&#35782;&#22270;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#20197;&#21450;&#30693;&#35782;&#22270;&#26500;&#24314;&#24615;&#33021;&#23545;&#31995;&#32479;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;AutoRD&#21462;&#24471;&#20102;47.3%&#30340;&#25972;&#20307;F1&#20998;&#25968;&#65292;&#36739;&#22522;&#30784;LLM&#25552;&#39640;&#20102;14.4%&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AutoRD&#23454;&#29616;&#20102;56.1%&#30340;&#25972;&#20307;&#23454;&#20307;&#25552;&#21462;F1&#20998;&#25968;&#65288;&#32597;&#35265;&#30142;&#30149;&#65306;83.5%&#65292;&#30142;&#30149;&#65306;35.8%&#65292;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00953v1 Announce Type: cross  Abstract: Objectives: Our objective is to create an end-to-end system called AutoRD, which automates extracting information from clinical text about rare diseases. We have conducted various tests to evaluate the performance of AutoRD and highlighted its strengths and limitations in this paper.   Materials and Methods: Our system, AutoRD, is a software pipeline involving data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implement this using large language models and medical knowledge graphs developed from open-source medical ontologies. We quantitatively evaluate our system on entity extraction, relation extraction, and the performance of knowledge graph construction.   Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement compared to the base LLM. In detail, AutoRD achieves an overall entity extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00942</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of Entropy Model in Distributed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#29109;&#32534;&#30721;&#34987;&#24341;&#20837;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#24335;DNN&#19982;&#29109;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#29992;&#20316;&#36793;&#20449;&#24687;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#28508;&#22312;&#34920;&#31034;&#32534;&#30721;&#20026;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#27604;&#29305;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#35843;&#26597;&#20102;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#65289;&#21644;&#26080;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#22825;&#27668;&#21464;&#21270;&#21644;&#36816;&#21160;&#27169;&#31946;&#65289;&#30340;&#38887;&#24615;&#12290;&#36890;&#36807;&#23545;3&#31181;&#19981;&#21516;DNN&#26550;&#26500;&#12289;2&#20010;&#29109;&#27169;&#22411;&#21644;4&#20010;&#36895;&#29575;&#22833;&#30495;&#26435;&#34913;&#22240;&#23376;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00942v1 Announce Type: cross  Abstract: Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., adversarial attacks) and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#39318;&#20010;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;SCB&#65292;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;MDP&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2403.00930</link><description>&lt;p&gt;
&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale-free Adversarial Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25552;&#20986;&#20102;&#39318;&#20010;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;SCB&#65292;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;MDP&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#26080;&#23610;&#24230;&#23398;&#20064;&#65292;&#20854;&#22870;&#21169;/&#25439;&#22833;&#30340;&#23610;&#24230;&#20026;&#23398;&#20064;&#32773;&#25152;&#19981;&#30693;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;\underline{S}cale \underline{C}lipping \underline{B}ound&#65288;\texttt{SCB}&#65289;&#65292;&#24182;&#23558;&#36825;&#19968;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#23545;&#25239;&#24615;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#35774;&#32622;&#21644;&#23545;&#25239;&#24615;MDP&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#22312;&#26080;&#23610;&#24230;&#23545;&#25239;&#24615;MABs&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#26368;&#23567;&#20540;&#26368;&#20248;&#26399;&#26395;&#36951;&#25022;&#30028;&#21644;&#31532;&#19968;&#20010;&#39640;&#27010;&#29575;&#36951;&#25022;&#30028;&#65292;&#35299;&#20915;&#20102;\cite{hadiji2023adaptation}&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#23545;&#25239;&#24615;MDPs&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#35806;&#29983;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;$\tilde{\mathcal{O}}(\sqrt{T})$&#39640;&#27010;&#29575;&#36951;&#25022;&#20445;&#35777;&#30340;&#26080;&#23610;&#24230;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00930v1 Announce Type: cross  Abstract: This paper initiates the study of scale-free learning in Markov Decision Processes (MDPs), where the scale of rewards/losses is unknown to the learner. We design a generic algorithmic framework, \underline{S}cale \underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this framework in both the adversarial Multi-armed Bandit (MAB) setting and the adversarial MDP setting. Through this framework, we achieve the first minimax optimal expected regret bound and the first high-probability regret bound in scale-free adversarial MABs, resolving an open problem raised in \cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$ high-probability regret guarantee.
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21010;&#20998;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#36335;&#24452;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00898</link><description>&lt;p&gt;
&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Algorithm Configuration Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21010;&#20998;&#20102;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#36335;&#24452;&#26469;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20248;&#21270;&#39046;&#22495;&#38543;&#30528;&#33258;&#21160;&#37197;&#32622;&#31639;&#27861;&#21442;&#25968;&#26041;&#27861;&#30340;&#21457;&#23637;&#32780;&#26174;&#33879;&#36827;&#27493;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#26088;&#22312;&#20248;&#21270;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20915;&#31574;/&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#30340;&#21442;&#25968;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#19981;&#20165;&#24418;&#24335;&#21270;&#20102;&#31639;&#27861;&#37197;&#32622;&#38382;&#39064;&#65292;&#36824;&#27010;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31574;&#30053;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#35813;&#25991;&#31456;&#23558;&#29616;&#26377;&#26041;&#27861;&#35770;&#21010;&#20998;&#20026;&#22522;&#20110;&#23454;&#20363;&#21644;&#22522;&#20110;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21306;&#20998;&#31163;&#32447;&#21644;&#22312;&#32447;&#31574;&#30053;&#29992;&#20110;&#27169;&#22411;&#26500;&#24314;&#21644;&#37096;&#32626;&#12290;&#36890;&#36807;&#32508;&#21512;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#29702;&#35299;&#21644;&#35299;&#20915;&#31639;&#27861;&#37197;&#32622;&#20013;&#22266;&#26377;&#22797;&#26434;&#24615;&#25552;&#20379;&#28165;&#26224;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00898v1 Announce Type: new  Abstract: The field of algorithmic optimization has significantly advanced with the development of methods for the automatic configuration of algorithmic parameters. This article delves into the Algorithm Configuration Problem, focused on optimizing parametrized algorithms for solving specific instances of decision/optimization problems. We present a comprehensive framework that not only formalizes the Algorithm Configuration Problem, but also outlines different approaches for its resolution, leveraging machine learning models and heuristic strategies. The article categorizes existing methodologies into per-instance and per-problem approaches, distinguishing between offline and online strategies for model construction and deployment. By synthesizing these approaches, we aim to provide a clear pathway for both understanding and addressing the complexities inherent in algorithm configuration.
&lt;/p&gt;</description></item><item><title>VisRec&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#23556;&#30005;&#24178;&#25200;&#25968;&#25454;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;</title><link>https://arxiv.org/abs/2403.00897</link><description>&lt;p&gt;
VisRec:&#19968;&#31181;&#29992;&#20110;&#23556;&#30005;&#24178;&#28041;&#25968;&#25454;&#37325;&#24314;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00897
&lt;/p&gt;
&lt;p&gt;
VisRec&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;-&#19981;&#21487;&#30693;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#23556;&#30005;&#24178;&#25200;&#25968;&#25454;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#38477;&#20302;&#20102;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#26395;&#36828;&#38236;&#20135;&#29983;&#20851;&#20110;&#22825;&#20307;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#31232;&#30095;&#19988;&#22024;&#26434;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#21407;&#22987;&#21487;&#35265;&#24615;&#25968;&#25454;&#21019;&#24314;&#30340;&#22270;&#20687;&#36136;&#37327;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37325;&#24314;&#21487;&#35265;&#24615;&#25968;&#25454;&#65292;&#20197;&#33719;&#24471;&#26356;&#28165;&#26224;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#23556;&#30005;&#22825;&#25991;&#23398;&#23478;&#22823;&#37327;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VisRec&#65292;&#19968;&#31181;&#38754;&#21521;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21487;&#35265;&#24615;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VisRec&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#12290;&#22312;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#32452;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;VisRec&#20013;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22359;&#20250;&#22686;&#21152;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;&#38750;&#22686;&#24378;&#21487;&#35265;&#24615;&#25968;&#25454;&#30340;&#37325;&#24314;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00897v1 Announce Type: cross  Abstract: Radio telescopes produce visibility data about celestial objects, but these data are sparse and noisy. As a result, images created on raw visibility data are of low quality. Recent studies have used deep learning models to reconstruct visibility data to get cleaner images. However, these methods rely on a substantial amount of labeled training data, which requires significant labeling effort from radio astronomers. Addressing this challenge, we propose VisRec, a model-agnostic semi-supervised learning approach to the reconstruction of visibility data. Specifically, VisRec consists of both a supervised learning module and an unsupervised learning module. In the supervised learning module, we introduce a set of data augmentation functions to produce diverse training examples. In comparison, the unsupervised learning module in VisRec augments unlabeled data and uses reconstructions from non-augmented visibility data as pseudo-labels for t
&lt;/p&gt;</description></item><item><title>DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00896</link><description>&lt;p&gt;
DiaHalu&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00896
&lt;/p&gt;
&lt;p&gt;
DiaHalu&#26159;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#35805;&#32423;&#21035;&#19978;&#30340;&#24187;&#35273;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26368;&#36817;&#20960;&#24180;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#24187;&#35273;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26377;&#35768;&#22810;&#22522;&#20934;&#34987;&#25552;&#20986;&#26469;&#26816;&#27979;&#36825;&#31181;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#22522;&#20934;&#19981;&#26159;&#30001;LLMs&#33258;&#28982;&#29983;&#25104;&#30340;&#65292;&#32780;&#26159;&#26377;&#24847;&#24341;&#21457;&#30340;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#20165;&#20851;&#27880;&#20107;&#23454;&#19978;&#30340;&#24187;&#35273;&#65292;&#32780;&#24573;&#35270;&#20102;&#24544;&#23454;&#24230;&#30340;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;LLMs&#26102;&#20195;&#23545;&#35805;&#27169;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#20165;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;&#21644;&#27573;&#33853;&#32423;&#30340;&#24187;&#35273;&#19978;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; DiaHalu&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#35805;&#32423;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25910;&#38598;&#30340;&#20027;&#39064;&#38598;&#25104;&#21040;&#31995;&#32479;&#25552;&#31034;&#20013;&#65292;&#20419;&#36827;&#20004;&#20010;ChatGPT3.5&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25163;&#21160;&#20462;&#25913;&#19981;&#31526;&#21512;&#20154;&#31867;&#35821;&#35328;&#32422;&#23450;&#30340;&#20869;&#23481;&#65292;&#28982;&#21518;&#35753;LLMs&#37325;&#26032;&#29983;&#25104;&#65292;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#31867;-
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00896v1 Announce Type: cross  Abstract: Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00895</link><description>&lt;p&gt;
&#31934;&#30830;&#25512;&#33616;&#30340;&#31471;&#21040;&#31471;&#22270;-&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Graph-Sequential Representation Learning for Accurate Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25512;&#33616;&#31995;&#32479;&#30340;&#35768;&#22810;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#24207;&#21015;&#21644;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#19978;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#24314;&#27169;&#34892;&#20026;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#22312;&#20010;&#24615;&#21270;&#25490;&#21517;&#21644;&#19979;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#30410;&#30340;&#25104;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20174;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#30340;&#20449;&#21495;&#25130;&#28982;&#19981;&#21516;&#12290;&#21069;&#32773;&#30452;&#25509;&#36890;&#36807;&#19982;&#26368;&#36817;&#29289;&#21697;&#30340;&#26377;&#24207;&#20132;&#20114;&#26469;&#34920;&#31034;&#29992;&#25143;&#65292;&#32780;&#21518;&#32773;&#26088;&#22312;&#25429;&#25417;&#20132;&#20114;&#22270;&#20013;&#30340;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#37325;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#20114;&#35757;&#32451;&#24207;&#21015;&#21644;&#22270;&#32452;&#20214;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.00894</link><description>&lt;p&gt;
&#23545;&#20110;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A systematic evaluation of large language models for generating programming code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00894
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#26102;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#31995;&#32479;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#12289;&#32534;&#31243;&#35821;&#35328;&#21644;&#20219;&#21153;&#38590;&#24230;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26102;&#30340;&#24615;&#33021;&#12290;GPT-4&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;Gemini Ultra&#21644;Claude 2&#12290;GPT-4&#30340;&#32534;&#30721;&#24615;&#33021;&#38543;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#35780;&#20272;&#30340;&#22823;&#22810;&#25968;LeetCode&#21644;GeeksforGeeks&#32534;&#31243;&#27604;&#36187;&#20013;&#65292;&#37319;&#29992;&#26368;&#20339;&#25552;&#31034;&#31574;&#30053;&#30340;GPT-4&#32988;&#36807;85%&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#12290;&#27492;&#22806;&#65292;GPT-4&#34920;&#29616;&#20986;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#20195;&#30721;&#21644;&#20174;&#36807;&#21435;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#30001;GPT-4&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30456;&#24403;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#26377;&#28508;&#21147;&#25104;&#20026;&#22312;&#32534;&#31243;&#20195;&#30721;&#29983;&#25104;&#21644;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#21487;&#38752;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00894v1 Announce Type: cross  Abstract: We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;</title><link>https://arxiv.org/abs/2403.00891</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#23454;&#29616;&#25968;&#25454;&#38598;&#20043;&#38388;&#36890;&#29992;&#30693;&#35782;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#22797;&#26434;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#24050;&#20026;&#21508;&#31181;IE&#20219;&#21153;&#26500;&#24314;&#20102;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#25968;&#25454;&#26631;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#26041;&#27861;&#20391;&#37325;&#20110;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#23545;&#19981;&#21516;IE&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30701;&#35821;&#21487;&#33021;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#65292;&#36825;&#23545;&#20351;&#29992;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#26500;&#25104;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#36827;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#25152;&#26377;&#33879;&#21517;IE&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#19968;&#20010;&#25351;&#23548;&#27744;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#20010;&#25351;&#23548;&#22270;&#35299;&#30721;&#22120;&#65292;&#26681;&#25454;&#30456;&#24212;&#30340;&#25351;&#23548;&#23558;&#21508;&#31181;&#22797;&#26434;&#32467;&#26500;&#22343;&#21248;&#22320;&#35299;&#30721;&#20026;&#22270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#20849;&#20139;&#30340;&#36890;&#29992;&#30693;&#35782;&#21487;&#20197;&#26356;&#22909;&#22320;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00891v1 Announce Type: cross  Abstract: Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#26410;&#30693;Android&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#26469;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00890</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Android Malware Detection Through Data Augmentation Using Wasserstein Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#26410;&#30693;Android&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#26469;&#25913;&#36827;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;GAN&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;Android&#24694;&#24847;&#36719;&#20214;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;Android&#24212;&#29992;&#30340;&#23384;&#20648;&#38656;&#27714;&#30456;&#24403;&#22823;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GAN&#26469;&#21512;&#25104;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#23384;&#20648;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21019;&#24314;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;GAN&#27169;&#22411;&#29983;&#25104;&#30001;&#36924;&#30495;&#30340;&#21512;&#25104;&#28784;&#24230;&#22270;&#20687;&#32452;&#25104;&#30340;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26088;&#22312;&#35782;&#21035;&#20197;&#21069;&#30475;&#19981;&#21040;&#30340;Android&#24694;&#24847;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;&#24403;CNN&#22312;&#30495;&#23454;&#22270;&#20687;&#19982;GAN&#29983;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#24615;&#33021;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00890v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have demonstrated their versatility across various applications, including data augmentation and malware detection. This research explores the effectiveness of utilizing GAN-generated data to train a model for the detection of Android malware. Given the considerable storage requirements of Android applications, the study proposes a method to synthetically represent data using GANs, thereby reducing storage demands. The proposed methodology involves creating image representations of features extracted from an existing dataset. A GAN model is then employed to generate a more extensive dataset consisting of realistic synthetic grayscale images. Subsequently, this synthetic dataset is utilized to train a Convolutional Neural Network (CNN) designed to identify previously unseen Android malware applications. The study includes a comparative analysis of the CNN's performance when trained on real images v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.00887</link><description>&lt;p&gt;
SEGAA: &#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#20174;&#35821;&#38899;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25506;&#32034;&#20102;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22768;&#38899;&#30340;&#35299;&#37322;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#24456;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#35821;&#38899;&#32447;&#32034;&#20013;&#39044;&#27979;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#24773;&#32490;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#12290;&#22768;&#38899;&#20998;&#26512;&#25216;&#26415;&#30340;&#36827;&#23637;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#25913;&#21892;&#23458;&#25143;&#20114;&#21160;&#21040;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#21644;&#38646;&#21806;&#20307;&#39564;&#12290;&#36776;&#35782;&#24773;&#32490;&#26377;&#21161;&#20110;&#24515;&#29702;&#20581;&#24247;&#65292;&#32780;&#24180;&#40836;&#21644;&#24615;&#21035;&#30340;&#26816;&#27979;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25506;&#32034;&#36825;&#20123;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#28041;&#21450;&#27604;&#36739;&#21333;&#19968;&#12289;&#22810;&#36755;&#20986;&#21644;&#39034;&#24207;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26412;&#25991;&#20013;&#24471;&#21040;&#20102;&#37325;&#28857;&#23637;&#31034;&#12290;&#23547;&#25214;&#21512;&#36866;&#30340;&#25968;&#25454;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;CREMA-D&#21644;EMO-DB&#25968;&#25454;&#38598;&#30340;&#34701;&#21512;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#20010;&#21035;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#19977;&#20010;&#21464;&#37327;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20010;&#21035;&#27169;&#22411;&#26041;&#27861;&#20013;&#30340;&#32570;&#38519;&#65292;&#24182;&#20513;&#23548;&#25105;&#20204;&#30340;&#26032;&#39062;&#22810;&#36755;&#20986;&#23398;&#20064;&#26550;&#26500;Speech-based Emotion Gender&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00887v1 Announce Type: cross  Abstract: The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;</title><link>https://arxiv.org/abs/2403.00884</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#36827;&#34892;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#21463;&#25511;&#35789;&#27719;&#21015;&#26631;&#39064;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#26816;&#32034;&#31995;&#32479;&#20027;&#35201;&#22312;&#20803;&#25968;&#25454;&#20449;&#24687;&#32780;&#38750;&#25968;&#25454;&#20540;&#19978;&#24314;&#31435;&#32034;&#24341;&#12290;&#22240;&#27492;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21644;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#38590;&#20197;&#33258;&#21160;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65306;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22522;&#20110;&#21463;&#25511;&#35789;&#27719;&#30340;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#38388;&#23545;&#40784;&#20197;&#21450;&#20154;&#26426;&#23545;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#25968;&#25454;&#38598;&#25551;&#36848;&#65289;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;LLM&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00884v1 Announce Type: cross  Abstract: Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the LLMs, the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that ChatGPT and GoogleGemini outperform GoogleBard for internal consistency as well as LLM-hum
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DGMed&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;</title><link>https://arxiv.org/abs/2403.00880</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Granularity Medication Recommendation Based on Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00880
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DGMed&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#21644;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#36827;&#34892;&#21452;&#31890;&#24230;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#38656;&#27714;&#22686;&#38271;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#31995;&#32479;&#22791;&#21463;&#20851;&#27880;&#12290;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#38271;&#26399;&#20581;&#24247;&#35760;&#24405;&#19982;&#21307;&#23398;&#30693;&#35782;&#25972;&#21512;&#65292;&#20026;&#29305;&#23450;&#30142;&#30149;&#25512;&#33616;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#33647;&#29289;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#23558;&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#20165;&#35270;&#20026;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#30340;&#21464;&#20307;&#65292;&#24573;&#35270;&#20102;&#33647;&#29289;&#21644;&#30142;&#30149;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DGMed&#65292;&#19968;&#20010;&#29992;&#20110;&#33647;&#29289;&#25512;&#33616;&#30340;&#26694;&#26550;&#12290;DGMed&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#25581;&#31034;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#39318;&#20808;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#20998;&#26512;&#21382;&#21490;&#35760;&#24405;&#20013;&#33647;&#29289;&#23545;&#29305;&#23450;&#30142;&#30149;&#30340;&#37327;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#25581;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00880v1 Announce Type: cross  Abstract: As medical demands grow and machine learning technology advances, AI-based diagnostic and treatment systems are garnering increasing attention. Medication recommendation aims to integrate patients' long-term health records with medical knowledge, recommending accuracy and safe medication combinations for specific conditions. However, most existing researches treat medication recommendation systems merely as variants of traditional recommendation systems, overlooking the heterogeneity between medications and diseases. To address this challenge, we propose DGMed, a framework for medication recommendation. DGMed utilizes causal inference to uncover the connections among medical entities and presents an innovative feature alignment method to tackle heterogeneity issues. Specifically, this study first applies causal inference to analyze the quantified therapeutic effects of medications on specific diseases from historical records, uncoverin
&lt;/p&gt;</description></item><item><title>Crimson&#31995;&#32479;&#36890;&#36807;&#23558;CVE&#19982;MITRE ATT&amp;CK&#25216;&#26415;&#30456;&#20851;&#32852;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;GPT-4&#24615;&#33021;&#27700;&#24179;&#65292;&#19988;&#22312;&#25112;&#30053;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.00878</link><description>&lt;p&gt;
Crimson: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00878
&lt;/p&gt;
&lt;p&gt;
Crimson&#31995;&#32479;&#36890;&#36807;&#23558;CVE&#19982;MITRE ATT&amp;CK&#25216;&#26415;&#30456;&#20851;&#32852;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;GPT-4&#24615;&#33021;&#27700;&#24179;&#65292;&#19988;&#22312;&#25112;&#30053;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Crimson&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;CVE&#19982;MITRE ATT&amp;CK&#25216;&#26415;&#30456;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23450;&#20041;&#21644;&#35780;&#20272;&#32593;&#32476;&#23433;&#20840;&#25112;&#30053;&#20219;&#21153;&#65292;&#20197;&#21450;&#23454;&#26045;&#20840;&#38754;&#30340;&#20154;&#26426;&#21327;&#20316;&#25968;&#25454;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20197;&#24320;&#21457;CVE&#21040;ATT&amp;CK&#26144;&#23556;&#65288;CVEM&#65289;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26816;&#32034;&#24863;&#30693;&#35757;&#32451;&#8221;&#65288;RAT&#65289;&#36807;&#31243;&#21450;&#20854;&#25913;&#36827;&#30340;&#36845;&#20195;RAT-R&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#25216;&#26415;&#23545;&#20855;&#26377;70&#20159;&#21442;&#25968;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#24615;&#33021;&#25509;&#36817;GPT-4&#65292;&#26174;&#31034;&#20986;&#24187;&#35273;&#21644;&#38169;&#35823;&#29575;&#26126;&#26174;&#38477;&#20302;&#65292;&#24182;&#22312;&#25112;&#30053;&#25512;&#29702;&#20219;&#21153;&#20013;&#36229;&#36234;&#20854;&#20182;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#24494;&#35843;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00878v1 Announce Type: cross  Abstract: We introduces Crimson, a system that enhances the strategic reasoning capabilities of Large Language Models (LLMs) within the realm of cybersecurity. By correlating CVEs with MITRE ATT&amp;CK techniques, Crimson advances threat anticipation and strategic defense efforts. Our approach includes defining and evaluating cybersecurity strategic tasks, alongside implementing a comprehensive human-in-the-loop data-synthetic workflow to develop the CVE-to-ATT&amp;CK Mapping (CVEM) dataset. We further enhance LLMs' reasoning abilities through a novel Retrieval-Aware Training (RAT) process and its refined iteration, RAT-R.   Our findings demonstrate that an LLM fine-tuned with our techniques, possessing 7 billion parameters, approaches the performance level of GPT-4, showing markedly lower rates of hallucination and errors, and surpassing other models in strategic reasoning tasks. Moreover, domain-specific fine-tuning of embedding models significantly i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#65292;&#21457;&#29616;&#19968;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;Wov2Lex&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2403.00876</link><description>&lt;p&gt;
&#35789;&#24207;&#19982;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Word Order and World Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#65292;&#21457;&#29616;&#19968;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#32780;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;Wov2Lex&#20551;&#35774;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24207;&#26159;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35789;&#24207;&#22914;&#20309;&#24433;&#21709;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#24402;&#32435;&#19990;&#30028;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#35789;&#31867;&#27604;&#26469;&#25506;&#31350;&#36825;&#31181;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#33258;&#28982;&#35789;&#24207;&#22806;&#65292;&#25105;&#20204;&#20998;&#21035;&#20174;&#20116;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#20102;&#20845;&#31181;&#22266;&#23450;&#35789;&#24207;&#30340;&#25991;&#26412;&#65292;&#24182;&#22312;&#36825;&#20123;&#25991;&#26412;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22266;&#23450;&#35789;&#24207;&#22312;&#35789;&#31867;&#27604;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#34920;&#26126;&#26576;&#20123;&#22266;&#23450;&#35789;&#24207;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#25110;&#19981;&#20339;&#65292;&#23613;&#31649;&#20855;&#20307;&#24773;&#20917;&#22240;&#35821;&#35328;&#32780;&#24322;&#65292;&#20197;&#21450;ii&#65289;Wov2Lex&#20551;&#35774;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#25104;&#31435;&#65292;&#33258;&#28982;&#35789;&#24207;&#36890;&#24120;&#20135;&#29983;&#24179;&#24248;&#30340;&#32467;&#26524;&#12290;&#28304;&#20195;&#30721;&#23558;&#20844;&#24320;&#22312; https://github.com/lshowway/probing_by_analogy&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00876v1 Announce Type: cross  Abstract: Word order is an important concept in natural language, and in this work, we study how word order affects the induction of world knowledge from raw text using language models. We use word analogies to probe for such knowledge. Specifically, in addition to the natural word order, we first respectively extract texts of six fixed word orders from five languages and then pretrain the language models on these texts. Finally, we analyze the experimental results of the fixed word orders on word analogies and show that i) certain fixed word orders consistently outperform or underperform others, though the specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in pre-trained language models, and the natural word order typically yields mediocre results. The source code will be made publicly available at https://github.com/lshowway/probing_by_analogy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#29255;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34507;&#30333;&#35821;&#20041;&#32423;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00875</link><description>&lt;p&gt;
&#36890;&#36807;&#34507;&#30333;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#34507;&#30333;&#39044;&#27979;&#27169;&#22411;&#65306;&#22522;&#20934;&#21644;&#26032;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#29255;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34507;&#30333;&#35821;&#20041;&#32423;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#34507;&#30333;&#25968;&#25454;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20391;&#37325;&#20110;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23545;&#20110;&#34507;&#30333;&#30340;&#25968;&#25454;&#22686;&#24378;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#23558;&#20808;&#21069;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25193;&#23637;&#21040;&#34507;&#30333;&#65292;&#28982;&#21518;&#22312;&#21508;&#31181;&#19982;&#34507;&#30333;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#23545;&#34507;&#30333;&#22686;&#24378;&#30340;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#35821;&#20041;&#32423;&#34507;&#30333;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#38598;&#25104;&#26799;&#24230;&#26367;&#25442;&#21644;&#22238;&#35793;&#26367;&#25442;&#65292;&#36890;&#36807;&#26174;&#33879;&#24615;&#26816;&#27979;&#21644;&#29983;&#29289;&#30693;&#35782;&#23454;&#29616;&#34507;&#30333;&#35821;&#20041;&#24863;&#30693;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25193;&#23637;&#21644;&#25552;&#20986;&#30340;&#22686;&#24378;&#38598;&#25104;&#21040;&#19968;&#20010;&#22686;&#24378;&#27744;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;&#33258;&#21160;&#34507;&#30333;&#22686;&#24378;&#65288;APA&#65289;&#65292;&#21487;&#23545;&#34507;&#30333;&#36827;&#34892;&#33258;&#21160;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00875v1 Announce Type: cross  Abstract: Augmentation is an effective alternative to utilize the small amount of labeled protein data. However, most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied data augmentation for proteins. This paper extends data augmentation techniques previously used for images and texts to proteins and then benchmarks these techniques on a variety of protein-related tasks, providing the first comprehensive evaluation of protein augmentation. Furthermore, we propose two novel semantic-level protein augmentation methods, namely Integrated Gradients Substitution and Back Translation Substitution, which enable protein semantic-aware augmentation through saliency detection and biological knowledge. Finally, we integrate extended and proposed augmentations into an augmentation pool and propose a simple but effective framework, namely Automated Protein Augmentation (APA), which can a
&lt;/p&gt;</description></item><item><title>DFIN-SQL&#26159;DIN-SQL&#30340;&#21019;&#26032;&#25193;&#23637;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#24335;&#38142;&#25509;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#25552;&#31034;&#25216;&#26415;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#20132;&#26367;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00872</link><description>&lt;p&gt;
DFIN-SQL&#65306;&#23558;&#31934;&#30830;&#27169;&#24335;&#19982;DIN-SQL&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#25968;&#25454;&#24211;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy in Large-Scale Databases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00872
&lt;/p&gt;
&lt;p&gt;
DFIN-SQL&#26159;DIN-SQL&#30340;&#21019;&#26032;&#25193;&#23637;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#24335;&#38142;&#25509;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#25552;&#31034;&#25216;&#26415;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#20132;&#26367;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#20026;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#26159;&#22797;&#26434;&#30340;&#65292;&#38656;&#35201;&#31934;&#30830;&#25216;&#26415;&#30340;&#32467;&#21512;&#20197;&#36827;&#34892;&#20934;&#30830;&#32763;&#35793;&#12290; DIN-SQL&#65288;&#19978;&#19979;&#25991;&#20998;&#35299;SQL&#65289;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;DFIN&#65288;&#20998;&#35299;&#32858;&#28966;&#19978;&#19979;&#25991;&#65289;&#65292;&#36825;&#26159;DIN-SQL&#30340;&#21019;&#26032;&#25193;&#23637;&#65292;&#36890;&#36807;&#35299;&#20915;&#27169;&#24335;&#38142;&#25509;&#38169;&#35823;&#26469;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#65292;&#36825;&#26159;&#19981;&#20934;&#30830;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290; DFIN&#29420;&#29305;&#22320;&#22312;&#25552;&#31034;&#25216;&#26415;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20043;&#38388;&#20132;&#26367;&#65292;&#36866;&#24212;&#25968;&#25454;&#24211;&#27169;&#24335;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#12290; &#39044;&#22788;&#29702;&#38454;&#27573;&#23884;&#20837;&#25968;&#25454;&#24211;&#23450;&#20041;&#24182;&#21033;&#29992;&#31867;&#20284;BIRD&#25968;&#25454;&#38598;&#20013;&#30340;&#24102;&#27880;&#37322;&#25991;&#20214;&#65292;&#20419;&#36827;&#20102;&#36816;&#34892;&#26102;&#26816;&#32034;&#30456;&#20851;&#27169;&#24335;&#20449;&#24687;&#12290; &#27492;&#31574;&#30053;&#26174;&#30528;&#20943;&#23569;&#20102;&#29992;&#20110;&#27169;&#24335;&#38142;&#25509;&#25552;&#31034;&#30340;&#20196;&#29260;&#35745;&#25968;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#22312;&#36739;&#22823;&#30340;&#35821;&#20041;&#27169;&#22411;GPT-4&#19978;&#20351;&#29992;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00872v1 Announce Type: cross  Abstract: The task of converting natural language queries into SQL queries is intricate, necessitating a blend of precise techniques for an accurate translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a significant development in this domain. This paper introduces DFIN (Decomposed Focused-In-Context), an innovative extension of DIN-SQL that enhances Text-to-SQL conversion by addressing schema linking errors, which are a major source of inaccuracies. DFIN uniquely alternates between prompting techniques and Retrieval-Augmented Generation (RAG), adapting to the size and complexity of the database schema. A preprocessing phase embeds database definitions and leverages annotated files, akin to those in the BIRD dataset, facilitating the runtime retrieval of pertinent schema information. This strategy significantly reduces the token count for schema linking prompts, enabling the use of a standard GPT-4 model over its larger co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;</title><link>https://arxiv.org/abs/2403.00871</link><description>&lt;p&gt;
&#25945;&#20250;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38035;&#40060;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#31363;&#21462;&#31169;&#20154;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Teach LLMs to Phish: Stealing Private Information from Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#30340;&#26032;&#22411;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#20351;&#23545;&#25163;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#20449;&#24687;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#33267;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31169;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23558;&#25935;&#24863;&#20449;&#24687;&#35760;&#24518;&#24182;&#37325;&#22797;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29992;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#65292;&#31216;&#20026;&#8220;&#31070;&#32463;&#38035;&#40060;&#8221;&#12290;&#36825;&#31181;&#25915;&#20987;&#20351;&#23545;&#25163;&#33021;&#22815;&#20174;&#19968;&#20010;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25104;&#21151;&#29575;&#39640;&#36798;10%&#29978;&#33267;50%&#22320;&#25552;&#21462;&#25935;&#24863;&#25110;&#21487;&#35782;&#21035;&#20010;&#20154;&#36523;&#20221;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#20449;&#29992;&#21345;&#21495;&#12290;&#25915;&#20987;&#20165;&#20551;&#35774;&#23545;&#25163;&#21487;&#20197;&#23558;&#23569;&#37327;&#30475;&#20284;&#33391;&#24615;&#30340;&#21477;&#23376;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20165;&#20351;&#29992;&#23545;&#29992;&#25143;&#25968;&#25454;&#32467;&#26500;&#30340;&#27169;&#31946;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00871v1 Announce Type: cross  Abstract: When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call "neural phishing". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00865</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#24555;&#36895;&#39640;&#25928;&#23616;&#37096;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00865
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36951;&#20256;&#32534;&#31243;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#35805;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#33021;&#26174;&#33879;&#25913;&#21892;&#32463;&#36807;&#20854;&#35757;&#32451;&#30340;&#27169;&#22411;&#24615;&#33021;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#23454;&#29616;&#20102;&#20219;&#21153;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#36951;&#20256;&#32534;&#31243;&#25214;&#21040;&#19968;&#32452;&#31526;&#21495;&#25439;&#22833;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#23398;&#20064;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#38598;&#21512;&#38543;&#21518;&#36890;&#36807;&#23637;&#24320;&#30340;&#24494;&#20998;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#32463;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#34920;&#26684;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#19978;&#24102;&#26469;&#20102;&#25913;&#21892;&#30340;&#25910;&#25947;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#24615;&#33021;&#65292;&#20351;&#29992;&#21508;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00865v1 Announce Type: cross  Abstract: In this paper, we develop upon the topic of loss function learning, an emergent meta-learning paradigm that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of supervised learning tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00863</link><description>&lt;p&gt;
LLM-Ensemble: &#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#26368;&#20339;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495;&#25688;&#35201;: &#20135;&#21697;&#23646;&#24615;&#20540;&#25552;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#24403;&#20195;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#20379;&#31934;&#30830;&#30340;&#20135;&#21697;&#23646;&#24615;&#20540;&#22312;&#30830;&#20445;&#39640;&#36136;&#37327;&#25512;&#33616;&#21644;&#25552;&#21319;&#23458;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#65292;&#19981;&#21516;LLMs&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#36825;&#31181;&#21464;&#21270;&#20351;&#23427;&#20204;&#24444;&#27492;&#20114;&#34917;&#65292;&#27809;&#26377;&#21738;&#20010;LLM&#33021;&#23436;&#20840;&#21387;&#20498;&#20854;&#20182;LLM&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#22810;&#26679;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24320;&#21457;&#19968;&#31181;&#21033;&#29992;&#23427;&#20204;&#20114;&#34917;&#28508;&#21147;&#30340;&#38598;&#25104;&#26041;&#27861;&#21464;&#24471;&#24517;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-ensemble&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#38598;&#25104;&#19981;&#21516;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00863v1 Announce Type: cross  Abstract: Product attribute value extraction is a pivotal component in Natural Language Processing (NLP) and the contemporary e-commerce industry. The provision of precise product attribute values is fundamental in ensuring high-quality recommendations and enhancing customer satisfaction. The recently emerging Large Language Models (LLMs) have demonstrated state-of-the-art performance in numerous attribute extraction tasks, without the need for domain-specific training data. Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters. This variation makes them complementary to each other, with no single LLM dominating all others. Considering the diverse strengths and weaknesses of LLMs, it becomes necessary to develop an ensemble method that leverages their complementary potentials. In this paper, we propose a novel algorithm called LLM-ensemble to ensemble diffe
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#37325;&#26500;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#65292;&#36890;&#36807;&#25552;&#20379;DGMs&#30340;&#20998;&#31867;&#27861;&#12289;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#22238;&#39038;&#20197;&#21450;&#28508;&#22312;&#21033;&#29992;DGMs&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.00861</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#29983;&#25104;&#25216;&#26415;&#37325;&#26500;&#38646;&#21806;&#20379;&#24212;&#38142;&#65306;&#20998;&#31867;&#27861;&#12289;&#35843;&#30740;&#21644;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#37325;&#26500;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#65292;&#36890;&#36807;&#25552;&#20379;DGMs&#30340;&#20998;&#31867;&#27861;&#12289;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#26696;&#20363;&#22238;&#39038;&#20197;&#21450;&#28508;&#22312;&#21033;&#29992;DGMs&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25110;DALL-E&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#25110;&#22270;&#20687;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#36825;&#20123;AI&#24212;&#29992;&#30340;&#31185;&#23398;&#21033;&#30410;&#30456;&#20851;&#32773;&#26159;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;DGMs&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#24182;&#29983;&#25104;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#22312;&#32479;&#35745;&#19978;&#30456;&#20284;&#30340;&#26032;&#25968;&#25454;&#28857;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;DGMs&#24212;&#29992;&#20110;&#29616;&#20195;&#38646;&#21806;&#20379;&#24212;&#38142;&#39046;&#22495;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#23457;&#26597;DGMs&#65292;&#24182;&#35752;&#35770;&#23427;&#20204;&#22312;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#29616;&#26377;&#21644;&#28508;&#22312;&#29992;&#20363;&#65292;&#26041;&#27861;&#26159;(1)&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;DGMs&#21450;&#20854;&#21464;&#20307;&#30340;&#20998;&#31867;&#27861;&#21644;&#27010;&#36848;&#65292;(2)&#20174;&#31471;&#21040;&#31471;&#30340;&#35270;&#35282;&#22238;&#39038;&#29616;&#26377;DGM&#22312;&#38646;&#21806;&#20379;&#24212;&#38142;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;(3)&#35752;&#35770;&#20851;&#20110;&#22914;&#20309;&#36827;&#19968;&#27493;&#21033;&#29992;DGM&#35299;&#20915;&#38646;&#21806;&#38382;&#39064;&#30340;&#35265;&#35299;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00861v1 Announce Type: new  Abstract: Generative AI applications, such as ChatGPT or DALL-E, have shown the world their impressive capabilities in generating human-like text or image. Diving deeper, the science stakeholder for those AI applications are Deep Generative Models, a.k.a DGMs, which are designed to learn the underlying distribution of the data and generate new data points that are statistically similar to the original dataset. One critical question is raised: how can we leverage DGMs into morden retail supply chain realm? To address this question, this paper expects to provide a comprehensive review of DGMs and discuss their existing and potential usecases in retail supply chain, by (1) providing a taxonomy and overview of state-of-the-art DGMs and their variants, (2) reviewing existing DGM applications in retail supply chain from a end-to-end view of point, and (3) discussing insights and potential directions on how DGMs can be further utilized on solving retail 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#26522;&#20030;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#21644;&#24182;&#34892;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#21306;&#22495;&#25968;&#37327;&#23545;&#36816;&#34892;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.00860</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#21306;&#22495;&#30340;&#31934;&#30830;&#26522;&#20030;&#24182;&#34892;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parallel Algorithms for Exact Enumeration of Deep Neural Network Activation Regions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#20013;&#31934;&#30830;&#26522;&#20030;&#30340;&#24182;&#34892;&#31639;&#27861;&#65292;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#21644;&#24182;&#34892;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#22312;&#22810;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#21306;&#22495;&#25968;&#37327;&#23545;&#36816;&#34892;&#26102;&#38388;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#20854;&#36755;&#20837;&#31354;&#38388;&#21010;&#20998;&#20026;&#19968;&#32452;&#20984;&#21306;&#22495;&#26469;&#26500;&#24314;&#20174;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#26144;&#23556;&#65292;&#21306;&#22495;&#20869;&#30340;&#28857;&#20849;&#20139;&#21333;&#19968;&#20223;&#23556;&#21464;&#25442;&#12290;&#20026;&#20102;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#20316;&#21407;&#29702;&#12289;&#22833;&#36133;&#21407;&#22240;&#20197;&#21450;&#19982;&#29983;&#29289;&#26234;&#33021;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#21306;&#22495;&#30340;&#32452;&#32455;&#21644;&#24418;&#25104;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31934;&#30830;&#26522;&#20030;&#28145;&#24230;&#65288;&#21644;&#27973;&#23618;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00860v1 Announce Type: cross  Abstract: A feedforward neural network using rectified linear units constructs a mapping from inputs to outputs by partitioning its input space into a set of convex regions where points within a region share a single affine transformation. In order to understand how neural networks work, when and why they fail, and how they compare to biological intelligence, we need to understand the organization and formation of these regions. Step one is to design and implement algorithms for exact region enumeration in networks beyond toy examples.   In this work, we present parallel algorithms for exact enumeration in deep (and shallow) neural networks. Our work has three main contributions: (1) we present a novel algorithm framework and parallel algorithms for region enumeration; (2) we implement one of our algorithms on a variety of network architectures and experimentally show how the number of regions dictates runtime; and (3) we show, using our algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22242;&#38431;&#22312;&#20914;&#31361;&#20013;&#24418;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#19981;&#21516;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#22312;&#25945;&#32946;&#29615;&#22659;&#21644;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21644;&#37096;&#32626;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00859</link><description>&lt;p&gt;
&#22242;&#38431;&#22312;&#20914;&#31361;&#20013;&#30340;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
Team Formation amidst Conflicts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22242;&#38431;&#22312;&#20914;&#31361;&#20013;&#24418;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#19981;&#21516;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#22312;&#25945;&#32946;&#29615;&#22659;&#21644;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21644;&#37096;&#32626;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22242;&#38431;&#22312;&#20914;&#31361;&#20013;&#24418;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20998;&#37197;&#20010;&#20307;&#21040;&#20219;&#21153;&#65292;&#32771;&#34385;&#21040;&#20010;&#20307;&#30340;&#20219;&#21153;&#20559;&#22909;&#21644;&#20182;&#20204;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#21033;&#29992;&#20381;&#36182;&#33293;&#20837;&#26041;&#26696;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#24037;&#20855;&#31665;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#21487;&#20197;&#27169;&#25311;&#35768;&#22810;&#19981;&#21516;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#22914;&#25945;&#32946;&#29615;&#22659;&#21644;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#37096;&#32626;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#25214;&#21040;&#30340;&#20998;&#37197;&#27604;&#33258;&#28982;&#22522;&#32447;&#25214;&#21040;&#30340;&#26356;&#22909;&#12290;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20998;&#37197;&#27604;&#20154;&#31867;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#20998;&#37197;&#26356;&#22909;&#12290;&#22312;&#20154;&#21147;&#36164;&#28304;&#31649;&#29702;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20998;&#37197;&#22914;&#20309;&#22686;&#21152;&#22242;&#38431;&#30340;&#22810;&#26679;&#24615;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#24471;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00859v1 Announce Type: new  Abstract: In this work, we formulate the problem of team formation amidst conflicts. The goal is to assign individuals to tasks, with given capacities, taking into account individuals' task preferences and the conflicts between them. Using dependent rounding schemes as our main toolbox, we provide efficient approximation algorithms. Our framework is extremely versatile and can model many different real-world scenarios as they arise in educational settings and human-resource management. We test and deploy our algorithms on real-world datasets and we show that our algorithms find assignments that are better than those found by natural baselines. In the educational setting we also show how our assignments are far better than those done manually by human experts. In the human resource management application we show how our assignments increase the diversity of teams. Finally, using a synthetic dataset we demonstrate that our algorithms scale very well
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00854</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#35828;&#35805;&#32773;&#26080;&#20851;&#30340;&#36816;&#21160;&#38556;&#30861;&#20005;&#37325;&#24230;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#26159;&#30001;&#20110;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#23548;&#33268;&#35328;&#35821;&#32908;&#32905;&#25511;&#21046;&#33021;&#21147;&#21463;&#25439;&#32780;&#20135;&#29983;&#30340;&#19968;&#31181;&#29366;&#20917;&#65292;&#20005;&#37325;&#24433;&#21709;&#24739;&#32773;&#30340;&#27807;&#36890;&#21644;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#35821;&#38899;&#25968;&#25454;&#20013;&#33258;&#21160;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#23458;&#35266;&#12289;&#21487;&#37325;&#22797;&#12289;&#21487;&#35775;&#38382;&#12289;&#26631;&#20934;&#21270;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00854v1 Announce Type: cross  Abstract: Dysarthria, a condition resulting from impaired control of the speech muscles due to neurological disorders, significantly impacts the communication and quality of life of patients. The condition's complexity, human scoring and varied presentations make its assessment and management challenging. This study presents a transformer-based framework for automatically assessing dysarthria severity from raw speech data. It can offer an objective, repeatable, accessible, standardised and cost-effective and compared to traditional methods requiring human expert assessors. We develop a transformer framework, called Speaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task learning objective and contrastive learning for speaker-independent multi-class dysarthria severity classification. The multi-task framework is designed to reduce reliance on speaker-specific characteristics and address the intrinsic intra-class variability of d
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.00841</link><description>&lt;p&gt;
&#31454;&#20105;&#28216;&#25103;&#30340;&#31163;&#32447;&#34394;&#26500;&#33258;&#25105;&#23545;&#24328;
&lt;/p&gt;
&lt;p&gt;
Offline Fictitious Self-Play for Competitive Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#25913;&#36827;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#23613;&#31649;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#31454;&#20105;&#28216;&#25103;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
&lt;/p&gt;</description></item><item><title>EyeGPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22810;&#25351;&#26631;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00840</link><description>&lt;p&gt;
EyeGPT&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30524;&#31185;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
EyeGPT: Ophthalmic Assistant with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00840
&lt;/p&gt;
&lt;p&gt;
EyeGPT&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#22810;&#25351;&#26631;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25913;&#21892;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#24182;&#22686;&#24378;&#21307;&#30103;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#27809;&#26377;&#33021;&#21147;&#20197;&#19987;&#23478;&#27700;&#24179;&#22788;&#29702;&#19982;&#21307;&#23398;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EyeGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#30524;&#31185;&#35774;&#35745;&#30340;LLM&#65292;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#12289;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31561;&#19977;&#31181;&#20248;&#21270;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#30524;&#31185;&#20998;&#25903;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#19981;&#21516;&#29992;&#25143;&#21644;&#22810;&#26679;&#30340;&#26597;&#35810;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#12289;&#21487;&#20449;&#24230;&#12289;&#31227;&#24773;&#21644;&#24187;&#35273;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00840v1 Announce Type: cross  Abstract: Artificial intelligence (AI) has gained significant attention in healthcare consultation due to its potential to improve clinical workflow and enhance medical communication. However, owing to the complex nature of medical information, large language models (LLM) trained with general world knowledge might not possess the capability to tackle medical-related tasks at an expert level. Here, we introduce EyeGPT, a specialized LLM designed specifically for ophthalmology, using three optimization strategies including role-playing, finetuning, and retrieval-augmented generation. In particular, we proposed a comprehensive evaluation framework that encompasses a diverse dataset, covering various subspecialties of ophthalmology, different users, and diverse inquiry intents. Moreover, we considered multiple evaluation metrics, including accuracy, understandability, trustworthiness, empathy, and the proportion of hallucinations. By assessing the p
&lt;/p&gt;</description></item><item><title>ToolNet&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#65292;&#23454;&#29616;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#21315;&#20010;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#25968;&#37327;&#32780;&#20165;&#26377;&#20013;&#31561;&#26631;&#35760;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2403.00839</link><description>&lt;p&gt;
ToolNet&#65306;&#36890;&#36807;&#24037;&#20855;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28023;&#37327;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00839
&lt;/p&gt;
&lt;p&gt;
ToolNet&#26159;&#19968;&#20010;&#25554;&#25300;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#65292;&#23454;&#29616;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#21315;&#20010;&#24037;&#20855;&#36830;&#25509;&#36215;&#26469;&#65292;&#25193;&#23637;&#20102;&#24037;&#20855;&#20351;&#29992;&#30340;&#25968;&#37327;&#32780;&#20165;&#26377;&#20013;&#31561;&#26631;&#35760;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27491;&#30830;&#20351;&#29992;&#28023;&#37327;&#22806;&#37096;&#24037;&#20855;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#12290;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#31616;&#21333;&#22320;&#23558;&#24037;&#20855;&#26684;&#24335;&#21270;&#20026;&#19968;&#21015;&#32431;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;LLMs&#20013;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#19968;&#31995;&#21015;&#24037;&#20855;&#35843;&#29992;&#24207;&#21015;&#20197;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#12290;&#36825;&#31181;&#33539;&#24335;&#24573;&#30053;&#20102;&#24037;&#20855;&#20043;&#38388;&#30340;&#20869;&#22312;&#20381;&#36182;&#65292;&#24182;&#23558;&#25152;&#26377;&#25512;&#29702;&#36127;&#36733;&#36716;&#31227;&#21040;LLMs&#19978;&#65292;&#20351;&#20854;&#23616;&#38480;&#20110;&#19968;&#23567;&#37096;&#20998;&#19987;&#38376;&#35774;&#35745;&#30340;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#26469;&#35828;&#65292;&#35201;&#22312;&#22823;&#37327;&#24037;&#20855;&#24211;&#19978;&#36816;&#34892;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24403;&#38754;&#20020;&#29616;&#23454;&#22330;&#26223;&#26102;&#23384;&#22312;&#30528;&#24456;&#22823;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ToolNet&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24230;&#22686;&#21152;&#26631;&#35760;&#28040;&#32791;&#65292;&#23558;&#24037;&#20855;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#12290;ToolNet&#23558;&#24037;&#20855;&#32452;&#32455;&#25104;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;&#27599;&#20010;&#33410;&#28857;&#20195;&#34920;&#19968;&#20010;&#24037;&#20855;&#65292;&#21152;&#26435;&#36793;&#34920;&#31034;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00839v1 Announce Type: new  Abstract: While achieving remarkable progress in a broad range of tasks, large language models (LLMs) remain significantly limited in properly using massive external tools. Existing in-context learning approaches simply format tools into a list of plain text descriptions and input them to LLMs, from which, LLMs generate a sequence of tool calls to solve problems step by step. Such a paradigm ignores the intrinsic dependency between tools and offloads all reasoning loads to LLMs, making them restricted to a limited number of specifically designed tools. It thus remains challenging for LLMs to operate on a library of massive tools, casting a great limitation when confronted with real-world scenarios. This paper proposes ToolNet, a plug-and-play framework that scales up the number of tools to thousands with a moderate increase in token consumption. ToolNet organizes tools into a directed graph. Each node represents a tool, and weighted edges denote t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#36741;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#37197;&#32622;&#30340;&#23454;&#29992;&#24615;</title><link>https://arxiv.org/abs/2403.00834</link><description>&lt;p&gt;
&#21033;&#29992;&#34394;&#25311;&#29616;&#23454;&#29702;&#35299;&#20197;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#24182;&#24212;&#29992;&#20110;&#37327;&#23376;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality for Understanding Artificial-Intelligence-driven Scientific Discovery with an Application in Quantum Optics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00834
&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#36741;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#37197;&#32622;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#21487;&#20197;&#25552;&#20986;&#36229;&#20986;&#20154;&#31867;&#33021;&#21147;&#33539;&#22260;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290; &#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;AI&#29983;&#25104;&#30340;&#32467;&#26500;&#24182;&#25552;&#21462;&#20854;&#20013;&#30340;&#27010;&#24565;&#21644;&#24819;&#27861;&#65292;&#25165;&#33021;&#30495;&#27491;&#20570;&#20986;&#27010;&#24565;&#24615;&#36129;&#29486;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#37096;&#20998;&#20998;&#26512;&#36807;&#31243;&#36716;&#31227;&#21040;&#27785;&#28024;&#24335;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#30001;AI&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;VR&#22312;&#25214;&#21040;&#20195;&#34920;&#37327;&#23376;&#20809;&#23398;&#23454;&#39564;&#30340;&#25277;&#35937;&#22270;&#24418;&#30340;&#21487;&#35299;&#37322;&#37197;&#32622;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290; &#20174;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#25163;&#21160;&#21457;&#29616;&#26032;&#39046;&#22495;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00834v1 Announce Type: cross  Abstract: Generative Artificial Intelligence (AI) models can propose solutions to scientific problems beyond human capability. To truly make conceptual contributions, researchers need to be capable of understanding the AI-generated structures and extracting the underlying concepts and ideas. When algorithms provide little explanatory reasoning alongside the output, scientists have to reverse-engineer the fundamental insights behind proposals based solely on examples. This task can be challenging as the output is often highly complex and thus not immediately accessible to humans. In this work we show how transferring part of the analysis process into an immersive Virtual Reality (VR) environment can assist researchers in developing an understanding of AI-generated solutions. We demonstrate the usefulness of VR in finding interpretable configurations of abstract graphs, representing Quantum Optics experiments. Thereby, we can manually discover new
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#23558;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#34892;&#20026;&#20013;&#65292;&#25361;&#25112;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#35748;&#30693;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2403.00833</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#36208;&#21521;&#25972;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Agent AI Towards a Holistic Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00833
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#26088;&#22312;&#23558;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#34892;&#20026;&#20013;&#65292;&#25361;&#25112;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#35748;&#30693;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#25105;&#20204;&#23545;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#24863;&#30693;&#20449;&#24687;&#30340;&#29702;&#35299;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21147;&#37327;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#38656;&#35201;&#20174;&#36807;&#24230;&#36824;&#21407;&#20027;&#20041;&#36716;&#21521;&#24378;&#35843;&#20316;&#20026;&#25972;&#20307;&#36816;&#20316;&#31995;&#32479;&#30340;&#37325;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#35843;&#21457;&#23637;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#8212;&#8212;&#19968;&#20010;&#23558;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#34892;&#20026;&#20013;&#30340;&#20855;&#20307;&#31995;&#32479;&#12290;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#28216;&#25103;&#21644;&#21307;&#30103;&#31995;&#32479;&#31561;&#22312;&#20869;&#30340;&#20307;&#39564;&#20016;&#23500;&#30340;&#22522;&#20110;&#20195;&#29702;&#20154;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#22411;&#34892;&#20026;&#27169;&#22411;&#20197;&#23454;&#29616;&#20855;&#26377;&#20307;&#29616;&#26234;&#33021;&#34892;&#20026;&#30340;&#20195;&#29702;&#20154;&#22522;&#30784;&#27169;&#22411;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20195;&#29702;&#20154;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25361;&#25112;&#20102;&#25105;&#20204;&#23545;&#23398;&#20064;&#21644;&#35748;&#30693;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00833v1 Announce Type: new  Abstract: Recent advancements in large foundation models have remarkably enhanced our understanding of sensory information in open-world environments. In leveraging the power of foundation models, it is crucial for AI research to pivot away from excessive reductionism and toward an emphasis on systems that function as cohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied system that integrates large foundation models into agent actions. The emerging field of Agent AI spans a wide range of existing embodied and agent-based multimodal interactions, including robotics, gaming, and healthcare systems, etc. In this paper, we propose a novel large action model to achieve embodied intelligent behavior, the Agent Foundation Model. On top of this idea, we discuss how agent AI exhibits remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Furthermore, we discuss the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25552;&#39640;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#21644;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#20197;&#24212;&#23545;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#21644;&#22686;&#24378;&#30693;&#35782;&#22270;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00832</link><description>&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Explainable Session-based Recommendation via Path Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00832
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25552;&#39640;&#20102;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#21644;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#20197;&#24212;&#23545;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#21644;&#22686;&#24378;&#30693;&#35782;&#22270;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#65288;SR&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SR&#27169;&#22411;&#24378;&#35843;&#20934;&#30830;&#24615;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#20256;&#32479;&#30340;&#36335;&#24452;&#25512;&#29702;&#20391;&#37325;&#20110;&#30693;&#35782;&#22270;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#20250;&#35805;&#21382;&#21490;&#20013;&#23384;&#22312;&#30340;&#39034;&#24207;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SR&#30340;&#27867;&#21270;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36335;&#24452;&#25512;&#29702;&#65288;PR4SR&#65289;&#26469;&#25552;&#39640;&#29616;&#26377;SR&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32771;&#34385;&#21040;&#39033;&#30446;&#23545;&#20250;&#35805;&#30340;&#37325;&#35201;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20250;&#35805;&#32423;&#21035;&#20195;&#29702;&#26469;&#36873;&#25321;&#20250;&#35805;&#20013;&#30340;&#39033;&#30446;&#20316;&#20026;&#36335;&#24452;&#25512;&#29702;&#30340;&#36215;&#28857;&#65292;&#20197;&#21450;&#36335;&#24452;&#32423;&#21035;&#20195;&#29702;&#26469;&#25191;&#34892;&#36335;&#24452;&#25512;&#29702;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#30446;&#26631;&#22870;&#21169;&#26426;&#21046;&#26469;&#36866;&#24212;SR&#20013;&#39034;&#24207;&#27169;&#24335;&#30340;&#36339;&#36807;&#34892;&#20026;&#65292;&#24182;&#24341;&#20837;&#36335;&#24452;&#20013;&#38388;&#28857;&#22870;&#21169;&#26469;&#22686;&#24378;&#30693;&#35782;&#22270;&#20013;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00832v1 Announce Type: cross  Abstract: This paper explores providing explainability for session-based recommendation (SR) by path reasoning. Current SR models emphasize accuracy but lack explainability, while traditional path reasoning prioritizes knowledge graph exploration, ignoring sequential patterns present in the session history. Therefore, we propose a generalized hierarchical reinforcement learning framework for SR, which improves the explainability of existing SR models via Path Reasoning, namely PR4SR. Considering the different importance of items to the session, we design the session-level agent to select the items in the session as the starting point for path reasoning and the path-level agent to perform path reasoning. In particular, we design a multi-target reward mechanism to adapt to the skip behaviors of sequential patterns in SR, and introduce path midpoint reward to enhance the exploration efficiency in knowledge graphs. To improve the completeness of the
&lt;/p&gt;</description></item><item><title>MedAide&#26159;&#19968;&#27454;&#21033;&#29992;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;LangChain&#38598;&#25104;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#30340;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#21644;&#22810;&#26679;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#35757;&#32451;&#26469;&#25552;&#21319;&#20854;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00830</link><description>&lt;p&gt;
MedAide&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#29616;&#22330;&#21307;&#30103;&#25588;&#21161;
&lt;/p&gt;
&lt;p&gt;
MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00830
&lt;/p&gt;
&lt;p&gt;
MedAide&#26159;&#19968;&#27454;&#21033;&#29992;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;LangChain&#38598;&#25104;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25552;&#20379;&#39640;&#25928;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#30340;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#21644;&#22810;&#26679;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#35757;&#32451;&#26469;&#25552;&#21319;&#20854;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20197;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;( NLP )&#21151;&#33021;&#27491;&#22312;&#25913;&#21464;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35745;&#31639;&#21644;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#37096;&#32626; LLMs &#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#12289;&#22522;&#30784;&#35774;&#26045;&#19981;&#23436;&#22791;&#30340;&#20559;&#36828;&#22320;&#21306;&#25552;&#20379;&#21307;&#30103;&#25588;&#21161;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MedAide&#65292;&#19968;&#27454;&#29616;&#22330;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#21033;&#29992;&#19982; LangChain &#38598;&#25104;&#30340;&#24494;&#22411; LLMs&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#22522;&#20110;&#36793;&#32536;&#30340;&#21021;&#27493;&#21307;&#30103;&#35786;&#26029;&#21644;&#25903;&#25345;&#12290;MedAide &#36890;&#36807;&#27169;&#22411;&#20248;&#21270;&#22312;&#23884;&#20837;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#26368;&#23567;&#20869;&#23384;&#21344;&#29992;&#21644;&#24310;&#36831;&#65292;&#26080;&#38656;&#26381;&#21153;&#22120;&#22522;&#30784;&#35774;&#26045;&#12290;&#35757;&#32451;&#36807;&#31243;&#20351;&#29992;&#20302;&#31209;&#36866;&#24212; (LoRA ) &#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#26679;&#21270;&#30340;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24212;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064; (RLHF) &#26469;&#22686;&#24378;&#20854;&#29305;&#23450;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00830v1 Announce Type: new  Abstract: Large language models (LLMs) are revolutionizing various domains with their remarkable natural language processing (NLP) abilities. However, deploying LLMs in resource-constrained edge computing and embedded systems presents significant challenges. Another challenge lies in delivering medical assistance in remote areas with limited healthcare facilities and infrastructure. To address this, we introduce MedAide, an on-premise healthcare chatbot. It leverages tiny-LLMs integrated with LangChain, providing efficient edge-based preliminary medical diagnostics and support. MedAide employs model optimizations for minimal memory footprint and latency on embedded edge devices without server infrastructure. The training process is optimized using low-rank adaptation (LoRA). Additionally, the model is trained on diverse medical datasets, employing reinforcement learning from human feedback (RLHF) to enhance its domain-specific capabilities. The sy
&lt;/p&gt;</description></item><item><title>TroubleLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;LLMs&#23433;&#20840;&#38382;&#39064;&#30340;&#21487;&#25511;&#27979;&#35797;&#25552;&#31034;&#30340;LLM&#65292;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2403.00829</link><description>&lt;p&gt;
TroubleLLM: &#23545;&#40784;&#32418;&#38431;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
TroubleLLM: Align to Red Team Expert
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00829
&lt;/p&gt;
&lt;p&gt;
TroubleLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;LLMs&#23433;&#20840;&#38382;&#39064;&#30340;&#21487;&#25511;&#27979;&#35797;&#25552;&#31034;&#30340;LLM&#65292;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#29983;&#25104;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#34987;&#25972;&#21512;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#22312;&#23637;&#29616;&#35832;&#22914;&#31038;&#20250;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#31561;&#19981;&#33391;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#12290;&#22312;&#37096;&#32626;&#20043;&#21069;&#35780;&#20272;&#20854;&#23433;&#20840;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#29983;&#25104;&#30340;&#27979;&#35797;&#25552;&#31034;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#36828;&#36828;&#19981;&#23613;&#20154;&#24847;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#21171;&#21160;&#23494;&#38598;&#19988;&#38656;&#35201;&#22823;&#37327;&#39044;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#27979;&#35797;&#25552;&#31034;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#22312;LLM&#24212;&#29992;&#30340;&#20855;&#20307;&#27979;&#35797;&#39046;&#22495;&#20013;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00829v1 Announce Type: new  Abstract: Large Language Models (LLMs) become the start-of-the-art solutions for a variety of natural language tasks and are integrated into real-world applications. However, LLMs can be potentially harmful in manifesting undesirable safety issues like social biases and toxic content. It is imperative to assess its safety issues before deployment. However, the quality and diversity of test prompts generated by existing methods are still far from satisfactory. Not only are these methods labor-intensive and require large budget costs, but the controllability of test prompt generation is lacking for the specific testing domain of LLM applications. With the idea of LLM for LLM testing, we propose the first LLM, called TroubleLLM, to generate controllable test prompts on LLM safety issues. Extensive experiments and human evaluation illustrate the superiority of TroubleLLM on generation quality and generation controllability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Detection Method for Large Language Models-Generated Scientific Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00828
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ChatGPT&#29983;&#25104;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;AI-Catcher&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), &#22914;GPT-3&#21644;BERT&#65292;&#25913;&#21464;&#20102;&#25991;&#26412;&#20869;&#23481;&#30340;&#20889;&#20316;&#21644;&#20256;&#25773;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#28508;&#21147;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#26080;&#27861;&#21306;&#20998;&#30340;&#31185;&#23398;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;LLMs&#20250;&#32473;&#31185;&#23398;&#30028;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#31185;&#23398;&#30028;&#20381;&#36182;&#20110;&#20986;&#29256;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;AI-Catcher&#12290;AI-Catcher&#38598;&#25104;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;MLP&#23398;&#20064;&#35821;&#35328;&#21644;&#32479;&#35745;&#29305;&#24449;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;CNN&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#39034;&#24207;&#27169;&#24335;&#30340;&#39640;&#32423;&#34920;&#31034;&#12290;AI-Catcher&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#34701;&#21512;&#20102;MLP&#21644;CNN&#23548;&#20986;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;ChatGPT&#29983;&#25104;&#30340;&#31185;&#23398;&#25991;&#26412;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00828v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a multimodal model that fuses hidden patterns derived from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset is collected to enhance AI-generated tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00827</link><description>&lt;p&gt;
&#26469;&#33258;&#22806;&#37096;&#20195;&#29702;&#25351;&#26631;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#23436;&#21892;
&lt;/p&gt;
&lt;p&gt;
Self-Refinement of Language Models from External Proxy Metrics Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Proxy Metric-based Self-Refinement (ProMiSe)&#26041;&#27861;&#65292;&#36890;&#36807;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22312;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#33258;&#25105;&#23436;&#21892;&#65292;&#20174;&#32780;&#25913;&#36827;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26723;&#20026;&#22522;&#30784;&#30340;&#21709;&#24212;&#29983;&#25104;&#20013;&#65292;&#26399;&#26395;&#20195;&#29702;&#21709;&#24212;&#19981;&#20165;&#19982;&#29992;&#25143;&#30340;&#26597;&#35810;&#30456;&#20851;&#65292;&#36824;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#30456;&#20851;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#29702;&#25351;&#26631;&#30340;&#33258;&#25105;&#23436;&#21892;&#65288;ProMiSe&#65289;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27839;&#30528;&#22806;&#37096;&#25351;&#26631;&#21453;&#39304;&#24341;&#23548;&#30340;&#36136;&#37327;&#20851;&#38190;&#32500;&#24230;&#20248;&#21270;&#20854;&#21021;&#22987;&#21709;&#24212;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#30340;&#26368;&#32456;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00827v1 Announce Type: cross  Abstract: It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.00824</link><description>&lt;p&gt;
&#20449;&#24687;&#27969;&#36335;&#30001;&#65306;&#33258;&#21160;&#35299;&#37322;&#35268;&#27169;&#21270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Information Flow Routes: Automatically Interpreting Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00824
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20449;&#24687;&#27969;&#36335;&#30001;&#22270;&#26469;&#25581;&#31034;&#27169;&#22411;&#20869;&#37096;&#30340;&#20851;&#38190;&#33410;&#28857;&#21644;&#25805;&#20316;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#28608;&#27963;&#20462;&#34917;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#23454;&#29616;&#65292;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#26512;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#23454;&#29616;&#30340;&#26426;&#21046;&#65292;&#20449;&#24687;&#36890;&#36807;&#32593;&#32476;&#20869;&#37096;&#30340;&#36335;&#30001;&#36827;&#34892;&#20256;&#36755;&#12290;&#36825;&#20123;&#36335;&#30001;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#23545;&#24212;&#20110;&#26631;&#35760;&#34920;&#31034;&#65292;&#36793;&#23545;&#24212;&#20110;&#32593;&#32476;&#20869;&#37096;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#20197;&#33258;&#39030;&#21521;&#19979;&#30340;&#26041;&#24335;&#33258;&#21160;&#26500;&#24314;&#36825;&#20123;&#22270;&#65292;&#38024;&#23545;&#27599;&#19968;&#20010;&#39044;&#27979;&#21482;&#20445;&#30041;&#26368;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#28608;&#27963;&#20462;&#34917;&#30340;&#24037;&#20316;&#27969;&#30456;&#27604;&#65292;&#25105;&#20204;&#36890;&#36807;&#24402;&#22240;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65306;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26377;&#25928;&#22320;&#25581;&#31034;&#29616;&#26377;&#30340;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20462;&#34917;&#65306;&#25105;&#20204;&#19981;&#38656;&#35201;&#20154;&#31867;&#20180;&#32454;&#35774;&#35745;&#39044;&#27979;&#27169;&#26495;&#65292;&#21487;&#20197;&#20026;&#20219;&#20309;&#39044;&#27979;&#25552;&#21462;&#20449;&#24687;&#27969;&#36335;&#30001;&#65288;&#19981;&#20165;&#20165;&#26159;&#22312;&#20801;&#35768;&#30340;&#27169;&#26495;&#20043;&#38388;&#30340;&#39044;&#27979;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23601;&#27169;&#22411;&#34892;&#20026;&#36827;&#34892;&#19968;&#33324;&#24615;&#35752;&#35770;&#65292;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#25110;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;Llama 2&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00824v1 Announce Type: cross  Abstract: Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the rol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20339;&#21305;&#37197;&#30340;&#20869;&#37096;&#19987;&#23478;Agent&#65292;&#20174;&#32780;&#20351;Agent&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38431;&#21451;&#36827;&#34892;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.00823</link><description>&lt;p&gt;
&#22312;&#21512;&#20316;&#35821;&#35328;&#28216;&#25103;&#20013;&#36866;&#24212;&#38431;&#21451;
&lt;/p&gt;
&lt;p&gt;
Adapting to Teammates in a Cooperative Language Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00823
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#65292;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20339;&#21305;&#37197;&#30340;&#20869;&#37096;&#19987;&#23478;Agent&#65292;&#20174;&#32780;&#20351;Agent&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#38431;&#21451;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Codenames&#28216;&#25103;&#26368;&#36817;&#24050;&#25104;&#20026;&#26234;&#33021;Agent&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#24863;&#20852;&#36259;&#39046;&#22495;&#12290;&#35813;&#28216;&#25103;&#30001;&#20110;&#35821;&#35328;&#21644;&#38431;&#21451;&#20043;&#38388;&#30340;&#21327;&#35843;&#26041;&#24335;&#32780;&#29420;&#20855;&#29305;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36866;&#24212;Codenames&#28216;&#25103;&#30340;Agent&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#65292;&#22312;&#19982;&#29305;&#23450;&#38431;&#21451;&#20114;&#21160;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20869;&#37096;&#30340;&#21738;&#20010;&#19987;&#23478;Agent&#65292;&#27599;&#20010;Agent&#21487;&#33021;&#20855;&#26377;&#33258;&#24049;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26159;&#26368;&#20339;&#21305;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00823v1 Announce Type: new  Abstract: The game of Codenames has recently emerged as a domain of interest for intelligent agent design. The game is unique due to the way that language and coordination between teammates play important roles. Previous approaches to designing agents for this game have utilized a single internal language model to determine action choices. This often leads to good performance with some teammates and inferior performance with other teammates, as the agent cannot adapt to any specific teammate. In this paper we present the first adaptive agent for playing Codenames. We adopt an ensemble approach with the goal of determining, during the course of interacting with a specific teammate, which of our internal expert agents, each potentially with its own language model, is the best match. One difficulty faced in this approach is the lack of a single numerical metric that accurately captures the performance of a Codenames team. Prior Codenames research has
&lt;/p&gt;</description></item><item><title>InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.00822</link><description>&lt;p&gt;
InteraRec&#65306;&#20351;&#29992;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
InteraRec: Interactive Recommendations Using Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00822
&lt;/p&gt;
&lt;p&gt;
InteraRec&#24341;&#20837;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#65292;&#36824;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weblog&#30001;&#35760;&#24405;&#20219;&#20309;&#32593;&#31449;&#19978;&#29992;&#25143;&#27963;&#21160;&#30340;&#35760;&#24405;&#32452;&#25104;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#20559;&#22909;&#12289;&#34892;&#20026;&#21644;&#20852;&#36259;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#35768;&#22810;&#25512;&#33616;&#31639;&#27861;&#21033;&#29992;&#36890;&#36807;&#36825;&#20123;Weblog&#25366;&#25496;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#21327;&#21516;&#36807;&#28388;&#12289;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#21644;&#28151;&#21512;&#26041;&#27861;&#31561;&#31574;&#30053;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;InteraRec&#30340;&#22797;&#26434;&#20132;&#20114;&#24335;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#21518;&#32773;&#20165;&#20381;&#36182;Weblog&#29983;&#25104;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#25429;&#33719;&#29992;&#25143;&#23548;&#33322;&#26102;&#32593;&#39029;&#30340;&#39640;&#39057;&#25130;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00822v1 Announce Type: cross  Abstract: Weblogs, comprised of records detailing user activities on any website, offer valuable insights into user preferences, behavior, and interests. Numerous recommendation algorithms, employing strategies such as collaborative filtering, content-based filtering, and hybrid methods, leverage the data mined through these weblogs to provide personalized recommendations to users. Despite the abundance of information available in these weblogs, identifying and extracting pertinent information and key features necessitates extensive engineering endeavors. The intricate nature of the data also poses a challenge for interpretation, especially for non-experts. In this study, we introduce a sophisticated and interactive recommendation framework denoted as InteraRec, which diverges from conventional approaches that exclusively depend on weblogs for recommendation generation. This framework captures high-frequency screenshots of web pages as users nav
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.00815</link><description>&lt;p&gt;
RAM-EHR: &#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#30340;&#26816;&#32034;&#22686;&#24378;&#19982;&#20020;&#24202;&#39044;&#27979;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00815
&lt;/p&gt;
&lt;p&gt;
RAM-EHR&#36890;&#36807;&#22686;&#24378;&#26816;&#32034;&#24182;&#21033;&#29992;&#24635;&#32467;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38024;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20020;&#24202;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RAM-EHR&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19978;&#20020;&#24202;&#39044;&#27979;&#30340;&#26816;&#32034;&#22686;&#24378;&#65288;Retrieval Augmentation&#65289;&#27969;&#31243;&#12290;RAM-EHR&#39318;&#20808;&#25910;&#38598;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#65292;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#25991;&#26412;&#26684;&#24335;&#65292;&#24182;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#26469;&#33719;&#21462;&#19982;&#21307;&#23398;&#27010;&#24565;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#36825;&#19968;&#31574;&#30053;&#35299;&#20915;&#20102;&#19982;&#22797;&#26434;&#27010;&#24565;&#21517;&#31216;&#30456;&#20851;&#30340;&#22256;&#38590;&#12290;RAM-EHR&#28982;&#21518;&#22686;&#24191;&#20102;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#20195;&#30721;&#32852;&#21512;&#35757;&#32451;&#30340;&#26412;&#22320;EHR&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#23601;&#35786;&#21644;&#24635;&#32467;&#30693;&#35782;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#22312;&#20004;&#20010;EHR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RAM-EHR&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#30693;&#35782;&#22686;&#24378;&#22522;&#32447;&#25928;&#26524;&#26174;&#33879;&#65288;AUROC&#22686;&#30410;3.4&#65285;&#65292;AUPR&#22686;&#30410;7.2&#65285;&#65289;&#65292;&#24378;&#35843;&#20102;RAM-EHR&#30340;&#24635;&#32467;&#30693;&#35782;&#23545;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#23558;&#21457;&#24067;&#22312;\url{https://github.com/ritaranx/RAM-EHR}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
&lt;/p&gt;</description></item><item><title>&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.00813</link><description>&lt;p&gt;
UrbanGPT: &#26102;&#31354;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanGPT: Spatio-Temporal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00813
&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#65292;&#20511;&#37492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37117;&#24066;GPT&#26088;&#22312;&#39044;&#27979;&#24182;&#27934;&#23519;&#22478;&#24066;&#29615;&#22659;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#12290;&#20854;&#30446;&#30340;&#26159;&#39044;&#27979;&#37117;&#24066;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#27169;&#24335;&#12289;&#36235;&#21183;&#21644;&#20107;&#20214;&#65292;&#21253;&#25324;&#20132;&#36890;&#12289;&#20154;&#21475;&#27969;&#21160;&#21644;&#29359;&#32618;&#29575;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#20197;&#20934;&#30830;&#39044;&#27979;&#26102;&#31354;&#25968;&#25454;&#65292;&#20294;&#38656;&#27880;&#24847;&#21040;&#24456;&#22810;&#26041;&#27861;&#22312;&#29983;&#25104;&#31934;&#30830;&#30340;&#26102;&#31354;&#34920;&#31034;&#26102;&#20005;&#37325;&#20381;&#36182;&#20110;&#26377;&#36275;&#22815;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#37117;&#24066;&#24863;&#30693;&#22330;&#26223;&#20013;&#65292;&#25968;&#25454;&#31232;&#32570;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#26102;&#31354;&#27169;&#22411;&#36328;&#36234;&#22810;&#26679;&#26102;&#31354;&#23398;&#20064;&#22330;&#26223;&#26159;&#24517;&#35201;&#30340;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21331;&#36234;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00813v1 Announce Type: cross  Abstract: Spatio-temporal prediction aims to forecast and gain insights into the ever-changing dynamics of urban environments across both time and space. Its purpose is to anticipate future patterns, trends, and events in diverse facets of urban life, including transportation, population movement, and crime rates. Although numerous efforts have been dedicated to developing neural network techniques for accurate predictions on spatio-temporal data, it is important to note that many of these methods heavily depend on having sufficient labeled data to generate precise spatio-temporal representations. Unfortunately, the issue of data scarcity is pervasive in practical urban sensing scenarios. Consequently, it becomes necessary to build a spatio-temporal model with strong generalization capabilities across diverse spatio-temporal learning scenarios. Taking inspiration from the remarkable achievements of large language models (LLMs), our objective is 
&lt;/p&gt;</description></item><item><title>LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00812</link><description>&lt;p&gt;
LoRA&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#36935;&#35265;&#20102;Dropout
&lt;/p&gt;
&lt;p&gt;
LoRA Meets Dropout under a Unified Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00812
&lt;/p&gt;
&lt;p&gt;
LoRA&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LoRA&#19982;dropout&#26041;&#27861;&#22312;&#27169;&#22411;&#23450;&#21046;&#20013;&#30340;&#30683;&#30462;&#65292;&#37325;&#26032;&#23457;&#35270;&#20102;transformer-specific&#30340;dropout&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#20803;&#32032;&#65292;&#32780;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;LoRA&#65292;&#24050;&#32463;&#25104;&#20026;&#27169;&#22411;&#23450;&#21046;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#21516;&#26102;&#65292;&#21508;&#31181;dropout&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#23436;&#25972;&#24494;&#35843;&#32780;&#35774;&#35745;&#30340;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#19982;&#36807;&#22810;&#21442;&#25968;&#20887;&#20313;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;LoRA&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#24494;&#19981;&#36275;&#36947;&#19982;&#20808;&#21069;dropout&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#23384;&#22312;&#21487;&#33021;&#30340;&#30683;&#30462;&#65292;&#36825;&#19968;&#28857;&#20043;&#21069;&#22823;&#22810;&#34987;&#24573;&#35270;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#35748;&#39640;&#25928;&#21442;&#25968;&#30340;LoRA&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29305;&#23450;&#20110;transformer&#30340;dropout&#26041;&#27861;&#65292;&#20174;&#25968;&#23398;&#21644;&#32463;&#39564;&#19978;&#24314;&#31435;&#23427;&#20204;&#30340;&#31561;&#20215;&#24615;&#21644;&#21306;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00812v1 Announce Type: cross  Abstract: With the remarkable capabilities, large language models (LLMs) have emerged as essential elements in numerous NLP applications, while parameter-efficient finetuning, especially LoRA, has gained popularity as a lightweight approach for model customization. Meanwhile, various dropout methods, initially designed for full finetuning with all the parameters updated, alleviates overfitting associated with excessive parameter redundancy. Hence, a possible contradiction arises from negligible trainable parameters of LoRA and the effectiveness of previous dropout methods, which has been largely overlooked. To fill this gap, we first confirm that parameter-efficient LoRA is also overfitting-prone. We then revisit transformer-specific dropout methods, and establish their equivalence and distinctions mathematically and empirically. Building upon this comparative analysis, we introduce a unified framework for a comprehensive investigation, which in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00811</link><description>&lt;p&gt;
LLM&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias in High-Stakes Decision-Making with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25903;&#25345;&#26085;&#30410;&#25193;&#22823;&#30340;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#20154;&#31867;(&#21019;&#36896;&#30340;)&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#38024;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22952;&#30861;&#21033;&#29992;LLM&#21327;&#21161;&#20570;&#20986;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;BiasBuster&#65292;&#19968;&#20010;&#26088;&#22312;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#12290;&#21463;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20808;&#21069;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35748;&#30693;&#20559;&#35265;(&#20363;&#22914;&#65292;&#25552;&#31034;&#35825;&#23548;&#12289;&#39034;&#24207;&#12289;&#22266;&#26377;)&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#28040;&#38500;&#23427;&#20204;&#33258;&#24049;&#30340;&#25552;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#39046;&#22495;&#35748;&#30693;&#20559;&#35265;&#23384;&#22312;&#21644;&#24433;&#21709;&#30340;&#20840;&#38754;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00811v1 Announce Type: new  Abstract: Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across diffe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20855;&#36523;Agent&#23436;&#25104;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00810</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#35748;&#30693;Agent
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Cognitive Agents with a Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00810
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#35748;&#30693;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20855;&#36523;Agent&#23436;&#25104;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#30456;&#36739;&#20110;&#23436;&#20840;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#19990;&#30028;&#30340;&#26434;&#20081;&#19968;&#33324;&#30693;&#35782;&#65292;&#20294;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35748;&#30693;&#26550;&#26500;&#20855;&#26377;&#20986;&#33394;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#26032;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#26469;&#23454;&#20363;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#30340;&#26434;&#20081;&#30693;&#35782;&#24341;&#23548;&#35748;&#30693;&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#20010;&#20570;&#21416;&#25151;&#20219;&#21153;&#30340;&#20855;&#36523;Agent&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30456;&#27604;&#23436;&#20840;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#35748;&#30693;&#26550;&#26500;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#32780;&#35748;&#30693;&#26550;&#26500;&#21453;&#36807;&#26469;&#21487;&#20197;&#39564;&#35777;&#24182;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00810v1 Announce Type: new  Abstract: Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#35868;&#39064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#21457;&#29616;&#19987;&#29992;&#27169;&#22411;&#22312;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.00809</link><description>&lt;p&gt;
Abdelhak&#22312;SemEval-2024&#20219;&#21153;9&#20013;&#30340;&#34920;&#29616;&#65306;&#35299;&#30721;&#35868;&#39064;&#65292;&#19987;&#29992;&#27169;&#22411;&#19982;ChatGPT&#30340;&#26377;&#25928;&#24615;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of Dedicated Models Versus ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#65292;&#22312;&#35299;&#20915;&#35868;&#39064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19982;ChatGPT&#36827;&#34892;&#20102;&#27604;&#36739;&#24615;&#33021;&#20998;&#26512;&#65292;&#21457;&#29616;&#19987;&#29992;&#27169;&#22411;&#22312;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20219;&#21153;9&#30340;BRAINTEASER&#38382;&#39064;&#30340;&#19987;&#29992;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21477;&#23376;&#21644;&#21333;&#35789;&#35868;&#39064;&#26469;&#35780;&#20272;&#27169;&#22411;&#27178;&#21521;&#24605;&#32500;&#33021;&#21147;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#20013;&#20197;0.98&#30340;&#24635;&#20998;&#25968;&#22312;&#21477;&#23376;&#35868;&#39064;&#35299;&#20915;&#26041;&#38754;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ChatGPT&#30340;&#27604;&#36739;&#34920;&#29616;&#65292;&#29305;&#21035;&#20998;&#26512;&#20102;&#28201;&#24230;&#35774;&#32622;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#20854;&#36827;&#34892;&#27178;&#21521;&#24605;&#32500;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19987;&#29992;&#27169;&#22411;&#21644;ChatGPT&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#19987;&#38376;&#26041;&#27861;&#22312;&#22686;&#24378;AI&#21019;&#36896;&#24615;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00809v1 Announce Type: cross  Abstract: This study introduces a dedicated model aimed at solving the BRAINTEASER task 9 , a novel challenge designed to assess models lateral thinking capabilities through sentence and word puzzles. Our model demonstrates remarkable efficacy, securing Rank 1 in sentence puzzle solving during the test phase with an overall score of 0.98. Additionally, we explore the comparative performance of ChatGPT, specifically analyzing how variations in temperature settings affect its ability to engage in lateral thinking and problem-solving. Our findings indicate a notable performance disparity between the dedicated model and ChatGPT, underscoring the potential of specialized approaches in enhancing creative reasoning in AI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;IPED&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31572;&#26696;&#31574;&#30053;&#23436;&#25104;&#34920;&#26684;&#65292;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.00808</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#38544;&#24335;&#36879;&#35270;IPED
&lt;/p&gt;
&lt;p&gt;
IPED: An Implicit Perspective for Relational Triple Extraction based on Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00808
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;IPED&#26041;&#27861;&#65292;&#37319;&#29992;&#38544;&#24335;&#31572;&#26696;&#31574;&#30053;&#23436;&#25104;&#34920;&#26684;&#65292;&#22312;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26159;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26368;&#36817;&#19968;&#31181;&#22522;&#20110;&#34920;&#22635;&#20805;&#30340;&#21069;&#26223;&#26694;&#26550;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#23454;&#20307;&#20851;&#31995;&#25552;&#21462;&#22522;&#20934;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290; &#20294;&#26159;&#65292;&#22266;&#26377;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#20887;&#20313;&#20449;&#24687;&#21644;&#19981;&#23436;&#25972;&#19977;&#20803;&#32452;&#35782;&#21035;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24335;&#35282;&#24230;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;IPED&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290; &#25105;&#20204;&#30340;&#26080;&#20998;&#31867;&#22120;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#38544;&#24335;&#31574;&#30053;&#65292;&#20351;&#29992;&#22359;&#35206;&#30422;&#23436;&#25104;&#34920;&#26684;&#65292;&#36991;&#20813;&#20102;&#26174;&#24335;&#26631;&#35760;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290; &#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#32467;&#26500;&#65292;&#22359;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#19982;&#25105;&#20204;&#30340;&#38544;&#24335;&#36879;&#35270;&#21512;&#20316;&#65292;&#24182;&#26377;&#25928;&#22320;&#35268;&#36991;&#20102;&#20887;&#20313;&#20449;&#24687;&#24178;&#25200;&#12290; &#20004;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;I
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00808v1 Announce Type: cross  Abstract: Relational triple extraction is a fundamental task in the field of information extraction, and a promising framework based on table filling has recently gained attention as a potential baseline for entity relation extraction. However, inherent shortcomings such as redundant information and incomplete triple recognition remain problematic. To address these challenges, we propose an Implicit Perspective for relational triple Extraction based on Diffusion model (IPED), an innovative approach for extracting relational triples. Our classifier-free solution adopts an implicit strategy using block coverage to complete the tables, avoiding the limitations of explicit tagging methods. Additionally, we introduce a generative model structure, the block-denoising diffusion model, to collaborate with our implicit perspective and effectively circumvent redundant information disruptions. Experimental results on two popular datasets demonstrate that I
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#20195;&#29702;&#24341;&#20837;&#30340;&#21160;&#20316;&#38598;&#21464;&#21270;&#21644;&#29615;&#22659;&#21464;&#21270;&#36827;&#34892;&#35745;&#21010;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.00805</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#35268;&#21010;&#26041;&#27861;&#65306;&#22312;DPDP&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A New Dynamic Distributed Planning Approach: Application to DPDP Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22815;&#26681;&#25454;&#20195;&#29702;&#24341;&#20837;&#30340;&#21160;&#20316;&#38598;&#21464;&#21270;&#21644;&#29615;&#22659;&#21464;&#21270;&#36827;&#34892;&#35745;&#21010;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22815;&#32771;&#34385;&#20195;&#29702;&#22312;&#20854;&#35745;&#21010;&#30340;&#21160;&#20316;&#38598;&#20013;&#24341;&#20837;&#30340;&#21464;&#21270;&#65292;&#20197;&#20415;&#32771;&#34385;&#29615;&#22659;&#20013;&#21457;&#29983;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35268;&#21010;&#30340;&#32972;&#26223;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#21487;&#20197;&#29983;&#25104;&#33258;&#24049;&#30340;&#35745;&#21010;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35745;&#21010;&#30340;&#29983;&#25104;&#22522;&#20110;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#65292;&#27599;&#24403;&#20195;&#29702;&#30340;&#21160;&#20316;&#38598;&#26377;&#21464;&#21270;&#26102;&#23601;&#30001;&#27599;&#20010;&#20195;&#29702;&#29983;&#25104;&#19968;&#20010;&#26032;&#35745;&#21010;&#12290;&#20026;&#27492;&#65292;&#35201;&#32771;&#34385;&#26032;&#35745;&#21010;&#20013;&#24341;&#20837;&#30340;&#26032;&#21160;&#20316;&#12290;&#22312;&#36825;&#20010;&#26032;&#35745;&#21010;&#20013;&#65292;&#20195;&#29702;&#27599;&#27425;&#37117;&#23558;&#26410;&#25191;&#34892;&#30340;&#26087;&#35745;&#21010;&#20013;&#30340;&#25152;&#26377;&#26087;&#21160;&#20316;&#21644;&#21464;&#21270;&#24341;&#36215;&#30340;&#26032;&#21160;&#20316;&#20316;&#20026;&#26032;&#21160;&#20316;&#38598;&#21512;&#26469;&#35268;&#21010;&#65292;&#24182;&#23558;&#26032;&#30340;&#21021;&#22987;&#29366;&#24577;&#20316;&#20026;&#26032;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00805v1 Announce Type: new  Abstract: In this work, we proposed a new dynamic distributed planning approach that is able to take into account the changes that the agent introduces on his set of actions to be planned in order to take into account the changes that occur in his environment. Our approach fits into the context of distributed planning for distributed plans where each agent can produce its own plans. According to our approach the generation of the plans is based on the satisfaction of the constraints by the use of the genetic algorithms. Our approach is to generate, a new plan by each agent, whenever there is a change in its set of actions to plan. This in order to take into account the new actions introduced in its new plan. In this new plan, the agent takes, each time, as a new action set to plan all the old un-executed actions of the old plan and the new actions engendered by the changes and as a new initial state; the state in which the set of actions of the ag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00804</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#25581;&#31034;&#23458;&#25143;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Uncovering Customer Issues through Topological Natural Language Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20844;&#21496;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#23458;&#25143;&#26381;&#21153;&#35831;&#27714;&#12290;&#23613;&#31649;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#27880;&#37322;&#31995;&#32479;&#26469;&#24635;&#32467;&#23458;&#25143;&#32852;&#31995;&#30340;&#20027;&#39064;&#65292;&#20294;&#28145;&#20837;&#25506;&#35752;&#27599;&#20010;&#20855;&#20307;&#38382;&#39064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26469;&#30417;&#25511;&#26032;&#20852;&#21644;&#28909;&#38376;&#23458;&#25143;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#26631;&#35760;&#27599;&#20010;&#23458;&#25143;&#23545;&#35805;&#35760;&#24405;&#30340;&#20027;&#35201;&#38382;&#39064;&#21477;&#65292;&#24182;&#29983;&#25104;&#21477;&#23376;&#23884;&#20837;&#21521;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#23884;&#20837;&#21521;&#37327;&#36827;&#34892;&#30333;&#21270;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#26080;&#21521;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#27599;&#20010;&#23545;&#35805;&#35760;&#24405;&#30340;&#25299;&#25169;&#29305;&#24615;&#26469;&#23450;&#20041;&#28909;&#38376;&#21644;&#26032;&#20852;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00804v1 Announce Type: cross  Abstract: E-commerce companies deal with a high volume of customer service requests daily. While a simple annotation system is often used to summarize the topics of customer contacts, thoroughly exploring each specific issue can be challenging. This presents a critical concern, especially during an emerging outbreak where companies must quickly identify and address specific issues. To tackle this challenge, we propose a novel machine learning algorithm that leverages natural language techniques and topological data analysis to monitor emerging and trending customer issues. Our approach involves an end-to-end deep learning framework that simultaneously tags the primary question sentence of each customer's transcript and generates sentence embedding vectors. We then whiten the embedding vectors and use them to construct an undirected graph. From there, we define trending and emerging issues based on the topological properties of each transcript. W
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.00803</link><description>&lt;p&gt;
LiMAML: &#36890;&#36807;&#20803;&#23398;&#20064;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiMAML: Personalization of Deep Recommender Models via Meta Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#20010;&#24615;&#21270;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#39057;&#32321;&#26356;&#26032;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#21521;&#19981;&#21516;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36941;&#37319;&#29992;&#24050;&#32463;&#25104;&#20026;&#24314;&#27169;&#21508;&#31181;&#19994;&#21153;&#30446;&#26631;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#38543;&#30528;&#29992;&#25143;&#22522;&#25968;&#30340;&#25345;&#32493;&#22686;&#38271;&#65292;&#20010;&#24615;&#21270;&#21644;&#39057;&#32321;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#24517;&#35201;&#24615;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#21521;&#21508;&#31181;&#25104;&#21592;&#25552;&#20379;&#30456;&#20851;&#19988;&#26356;&#26032;&#30340;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20803;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38024;&#23545;&#20010;&#20154;&#25104;&#21592;&#21644;&#20854;&#20182;&#23454;&#20307;&#30340;&#27169;&#22411;&#20010;&#24615;&#21270;&#65292;&#32467;&#21512;&#20102;&#26681;&#25454;&#26368;&#26032;&#29992;&#25143;&#20114;&#21160;&#20449;&#21495;&#36827;&#34892;&#39057;&#32321;&#26356;&#26032;&#30340;&#21151;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#31639;&#27861;&#65292;&#20351;&#29992;&#26368;&#36817;&#30340;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#26469;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#32771;&#34385;&#21040;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29983;&#20135;&#21407;&#22987;MAML&#27169;&#22411;&#20960;&#20046;&#19981;&#21487;&#34892;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23558;&#20803;&#23398;&#20064;&#30340;&#23376;&#32593;&#32476;&#25512;&#24191;&#24212;&#29992;&#21040;&#29983;&#20135;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00803v1 Announce Type: cross  Abstract: In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in prod
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#20854;&#21521;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#23427;&#22312;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#19978;&#23454;&#29616;&#26356;&#24555;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00802</link><description>&lt;p&gt;
&#26397;&#21521;&#29702;&#35770;&#29702;&#35299;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Theoretical Understanding of Two-Stage Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#30340;&#29702;&#35770;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#20854;&#21521;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#23427;&#22312;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#19978;&#23454;&#29616;&#26356;&#24555;&#25910;&#25947;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00802v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#22495; &#25688;&#35201;:&#29983;&#20135;&#32423;&#25512;&#33616;&#31995;&#32479;&#22312;&#22312;&#32447;&#23186;&#20307;&#26381;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;Netflix&#12289;Pinterest&#21644;Amazon&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#21644;&#29289;&#21697;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#25237;&#24433;&#30340;&#23884;&#20837;&#12289;&#36890;&#36807;&#20004;&#38454;&#27573;&#27169;&#22411;&#65288;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#20016;&#23500;&#25512;&#33616;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#30340;&#23884;&#20837;&#26500;&#24314;&#20197;&#39044;&#27979;&#19982;&#29289;&#21697;&#30456;&#20851;&#30340;&#29992;&#25143;&#21453;&#39304;&#12290;&#23613;&#31649;&#23427;&#22312;&#25512;&#33616;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#29702;&#35770;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36825;&#21253;&#25324;&#23545;&#26368;&#20339;&#25512;&#33616;&#31995;&#32479;&#30340;&#24378;&#25910;&#25947;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#30340;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#21644;&#32479;&#35745;&#20445;&#35777;&#12290;&#38500;&#20102;&#28176;&#36817;&#34892;&#20026;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20004;&#38454;&#27573;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#20381;&#36182;&#36755;&#20837;&#29305;&#24449;&#30340;&#22266;&#26377;&#32500;&#24230;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00802v1 Announce Type: cross  Abstract: Production-grade recommender systems rely heavily on a large-scale corpus used by online media services, including Netflix, Pinterest, and Amazon. These systems enrich recommendations by learning users' and items' embeddings projected in a low-dimensional space with two-stage models (two deep neural networks), which facilitate their embedding constructs to predict users' feedback associated with items. Despite its popularity for recommendations, its theoretical behaviors remain comprehensively unexplored. We study the asymptotic behaviors of the two-stage recommender that entail a strong convergence to the optimal recommender system. We establish certain theoretical properties and statistical assurance of the two-stage recommender. In addition to asymptotic behaviors, we demonstrate that the two-stage recommender system attains faster convergence by relying on the intrinsic dimensions of the input features. Finally, we show numerically
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.00800</link><description>&lt;p&gt;
&#20511;&#37492;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#30340;&#33041;&#21551;&#21457;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00800
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#20986;&#30340;Brain&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#26126;&#30830;&#25552;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#22810;&#27493;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25913;&#36827;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#22312;&#24320;&#28304;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#39069;&#21494;&#27169;&#22411;&#29983;&#25104;&#35745;&#21010;&#65292;&#28982;&#21518;&#20351;&#29992;&#39030;&#21494;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#24182;&#25191;&#34892;&#20197;&#33719;&#24471;&#31572;&#26696;&#65292;&#26469;&#27169;&#20223;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#27492;&#26041;&#27861;&#19982;&#22522;&#20110;Code LLaMA 7B&#30340;&#27169;&#22411;&#30456;&#27604;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#35745;&#21010;&#21487;&#20197;&#26126;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#12289;&#20195;&#30721;&#25110;&#24418;&#24335;&#35821;&#35328;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/cyzhh/Brain&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00800v1 Announce Type: cross  Abstract: Although large language models demonstrate emergent abilities in solving math word problems, there is a challenging task in complex multi-step mathematical reasoning tasks. To improve model performance on mathematical reasoning tasks, previous work has conducted supervised fine-tuning on open-source models by improving the quality and quantity of data. In this paper, we propose a novel approach, named Brain, to imitate human thought processes to enhance mathematical reasoning abilities, using the Frontal Lobe Model to generate plans, and then employing the Parietal Lobe Model to generate code and execute to obtain answers. First, we achieve SOTA performance in comparison with Code LLaMA 7B based models through this method. Secondly, we find that plans can be explicitly extracted from natural language, code, or formal language. Our code and data are publicly available at https://github.com/cyzhh/Brain.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00799</link><description>&lt;p&gt;
LLM&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#25968;&#25454;&#33021;&#21147;&#36793;&#30028;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00799
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30830;&#23450;&#26368;&#20248;&#36335;&#24452;&#38598;&#65292;&#26412;&#30740;&#31350;&#25299;&#23637;&#20102;LLMs&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#26469;&#32047;&#31215;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#20102;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#22312;&#23637;&#31034;&#23545;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22686;&#24378;&#24320;&#28304;LLMs&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#36890;&#29992;&#30340;&#30417;&#30563;&#25968;&#25454;&#31574;&#30053;&#65292;&#20197;&#24110;&#21161;&#20248;&#21270;&#21644;&#25299;&#23637;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#25512;&#29702;&#36335;&#24452;&#30340;&#26368;&#20248;&#36335;&#24452;&#38598;&#30830;&#23450;&#25512;&#29702;&#36335;&#24452;&#22686;&#24378;&#30340;&#33021;&#21147;&#36793;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#39564;&#35777;&#27169;&#22411;&#19981;&#21516;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#31867;&#22411;&#25968;&#25454;&#30340;&#26368;&#23567;&#26368;&#20248;&#38598;&#28151;&#21512;&#26469;&#32047;&#31215;&#22686;&#24378;&#65292;&#32780;&#25105;&#20204;&#30340;&#27169;&#22411;MMOS&#22312;&#26356;&#20302;&#30340;&#26500;&#24314;&#25104;&#26412;&#19979;&#23454;&#29616;&#20102;&#31995;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;SOTA&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;GSM-HARD&#24182;&#19981;&#30495;&#27491;&#22256;&#38590;&#65292;&#24403;&#20170;&#30340;LLMs&#19981;&#20877;&#32570;&#20047;&#25968;&#20540;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#31283;&#20581;&#24615;&#27979;&#35797;&#21644;&#25945;&#32946;&#24212;&#29992;&#30340;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00799v1 Announce Type: cross  Abstract: Large language models (LLMs) are displaying emergent abilities for math reasoning tasks,and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set.Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs.Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness.Also, we provide an Auto Problem Generator for robustness testing and educational applications.Our code and data are publicly available
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22312;&#37329;&#34701;&#39044;&#27979;&#20013;&#25506;&#32034;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#24182;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2403.00796</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#36807;&#31243;&#22686;&#24378;&#22343;&#20540;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#37329;&#34701;&#39044;&#27979;&#20013;&#30340;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22312;&#37329;&#34701;&#39044;&#27979;&#20013;&#25506;&#32034;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#39044;&#27979;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#24182;&#36827;&#34892;&#38271;&#26399;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21644;&#20915;&#31574;&#21046;&#23450;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#39044;&#27979;&#20855;&#26377;&#28508;&#22312;&#32467;&#26500;&#30340;&#22343;&#20540;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#65292;&#20351;&#29992;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#30340;&#21151;&#33021;&#21644;&#22686;&#24378;&#25968;&#25454;&#32467;&#26500;&#12290;&#34429;&#28982;&#35768;&#22810;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30701;&#26399;&#21160;&#24577;&#65292;&#20294;GPs&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#19981;&#20165;&#21487;&#20197;&#39044;&#27979;&#24179;&#22343;&#39044;&#27979;&#20540;&#65292;&#36824;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#19978;&#25972;&#20010;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#22312;&#37329;&#34701;&#29615;&#22659;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#22914;&#26524;&#19981;&#27491;&#30830;&#30340;&#27874;&#21160;&#29575;&#35780;&#20272;&#23548;&#33268;&#36164;&#26412;&#25439;&#22833;&#65292;&#20165;&#20934;&#30830;&#30340;&#39044;&#27979;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#27492;&#22806;&#65292;&#22312;&#20132;&#26131;&#36873;&#25321;&#20013;&#65292;GPs&#20801;&#35768;&#39044;&#27979;&#22810;&#20010;&#22799;&#26222;&#27604;&#29575;&#65292;&#32771;&#34385;&#20132;&#26131;&#25104;&#26412;&#21518;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#21161;&#20110;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#21151;&#33021;&#25968;&#25454;&#34920;&#31034;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#20960;&#24180;&#30340;&#20449;&#24687;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#26399;&#30340;&#39044;&#27979;&#65292;&#21363;&#20351;&#39044;&#27979;&#33073;&#31163;&#20102;&#24403;&#21069;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00796v1 Announce Type: cross  Abstract: In this paper, we explore the application of Gaussian Processes (GPs) for predicting mean-reverting time series with an underlying structure, using relatively unexplored functional and augmented data structures. While many conventional forecasting methods concentrate on the short-term dynamics of time series data, GPs offer the potential to forecast not just the average prediction but the entire probability distribution over a future trajectory. This is particularly beneficial in financial contexts, where accurate predictions alone may not suffice if incorrect volatility assessments lead to capital losses. Moreover, in trade selection, GPs allow for the forecasting of multiple Sharpe ratios adjusted for transaction costs, aiding in decision-making. The functional data representation utilized in this study enables longer-term predictions by leveraging information from previous years, even as the forecast moves away from the current year
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2403.00795</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31639;&#27861;&#65306;&#19968;&#39033;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Executing Natural Language-Described Algorithms with Large Language Models: An Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00795
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#23588;&#20854;&#26159;&#22312;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#36861;&#27714;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#30340;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#36825;&#19968;&#30446;&#26631;&#30340;&#36947;&#36335;&#24050;&#32463;&#34987;&#38416;&#26126;&#12290;&#26412;&#25991;&#26088;&#22312;&#26816;&#39564;&#29616;&#26377;LLMs&#29702;&#35299;&#21644;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#12298;&#31639;&#27861;&#23548;&#35770;&#12299;&#20013;&#36873;&#21462;&#20102;&#19968;&#20010;&#31639;&#27861;&#27979;&#35797;&#38598;&#65292;&#35813;&#20070;&#26159;&#19968;&#26412;&#21253;&#21547;&#35768;&#22810;&#20195;&#34920;&#24615;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#30340;&#30693;&#21517;&#25945;&#26448;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;30&#20010;&#31639;&#27861;&#65292;&#20849;&#29983;&#25104;&#20102;300&#20010;&#38543;&#26426;&#25277;&#26679;&#23454;&#20363;&#65292;&#24182;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#29305;&#21035;&#26159;GPT-4&#31561;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#31243;&#24207;&#65292;&#21482;&#35201;&#19981;&#28041;&#21450;&#22823;&#37327;&#25968;&#23383;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00795v1 Announce Type: cross  Abstract: Executing computer programs described in natural language has long been a pursuit of computer science. With the advent of enhanced natural language understanding capabilities exhibited by large language models (LLMs), the path toward this goal has been illuminated. In this paper, we seek to examine the capacity of present-day LLMs to comprehend and execute algorithms outlined in natural language. We established an algorithm test set sourced from Introduction to Algorithm, a well-known textbook that contains many representative widely-used algorithms. To systematically assess LLMs' code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved. We believe our f
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00794</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#24189;&#40664;&#65306;&#21033;&#29992;&#19981;&#39118;&#36259;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#24189;&#40664;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00794
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#20114;&#21160;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#36827;&#23637;&#65292;&#24189;&#40664;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#24189;&#40664;&#25991;&#26412;&#19982;&#31867;&#20284;&#38750;&#24189;&#40664;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;&#24189;&#40664;&#26816;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#24403;&#21069;LLMs&#22312;&#8220;&#21462;&#28040;&#39118;&#36259;&#8221;&#31505;&#35805;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#21028;&#26029;&#21644;&#24189;&#40664;&#26816;&#27979;&#30340;&#19979;&#28216;&#20219;&#21153;&#34913;&#37327;&#32780;&#24471;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#28151;&#21512;&#32534;&#30721;&#30340;&#33521;&#35821;-&#21360;&#22320;&#35821;&#24189;&#40664;&#25968;&#25454;&#38598;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#21457;&#29616;GPT-4&#30340;&#21512;&#25104;&#25968;&#25454;&#34987;&#21452;&#35821;&#27880;&#37322;&#21592;&#39640;&#24230;&#35780;&#20215;&#65292;&#24182;&#20026;&#24189;&#40664;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00791</link><description>&lt;p&gt;
$\textit{L+M-24}$&#65306;&#22312;ACL 2024&#24180;&#20026;&#35821;&#35328;+&#20998;&#23376;&#26500;&#24314;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
$\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#20998;&#23376;&#27169;&#22411;&#24050;&#25104;&#20026;&#20998;&#23376;&#21457;&#29616;&#21644;&#29702;&#35299;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;-&#35821;&#35328;&#23545;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26377;&#20197;&#19979;&#20960;&#31181;&#31867;&#22411;&#65306;1) &#23567;&#35268;&#27169;&#19988;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#25235;&#21462;&#65292;2) &#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#19988;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#29486;&#19978;&#25191;&#34892;&#23454;&#20307;&#38142;&#25509;&#26469;&#26500;&#24314;&#65292;3) &#36890;&#36807;&#23558;&#23646;&#24615;&#39044;&#27979;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#27169;&#26495;&#32780;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20026;ACL 2024&#24180;&#30340;&#35821;&#35328;+&#20998;&#23376;&#30740;&#35752;&#20250;&#20849;&#20139;&#20219;&#21153;&#21019;&#24314;&#30340;$\textit{L+M-24}$&#25968;&#25454;&#38598;&#12290;&#29305;&#21035;&#22320;&#65292;$\textit{L+M-24}$&#26088;&#22312;&#38598;&#20013;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#19977;&#39033;&#20851;&#38190;&#20248;&#21183;&#65306;&#32452;&#21512;&#24615;&#12289;&#21151;&#33021;&#24615;&#21644;&#25277;&#35937;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00791v1 Announce Type: cross  Abstract: Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the $\textit{L+M-24}$ dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#26368;&#32456;&#23558;&#27010;&#24565;&#30340;&#26144;&#23556;&#32467;&#26500;&#21270;&#20026;&#38899;&#20048;&#20116;&#24230;&#22278;&#12290;</title><link>https://arxiv.org/abs/2403.00790</link><description>&lt;p&gt;
&#21033;&#29992;&#38899;&#20048;&#20116;&#24230;&#22278;&#26500;&#24314;&#27010;&#24565;&#31354;&#38388;&#65306;&#22522;&#20110;&#38899;&#20048;&#35821;&#27861;&#28608;&#27963;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structuring Concept Space with the Musical Circle of Fifths by Utilizing Music Grammar Based Activations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00790
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#26368;&#32456;&#23558;&#27010;&#24565;&#30340;&#26144;&#23556;&#32467;&#26500;&#21270;&#20026;&#38899;&#20048;&#20116;&#24230;&#22278;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31163;&#25955;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;&#23574;&#23792;&#32593;&#32476;&#65289;&#30340;&#32467;&#26500;&#19982;&#38050;&#29748;&#26354;&#30340;&#26500;&#25104;&#20043;&#38388;&#30340;&#26377;&#36259;&#30456;&#20284;&#20043;&#22788;&#12290;&#34429;&#28982;&#20004;&#32773;&#37117;&#28041;&#21450;&#25353;&#39034;&#24207;&#25110;&#24182;&#34892;&#28608;&#27963;&#30340;&#33410;&#28857;&#25110;&#38899;&#31526;&#65292;&#20294;&#21518;&#32773;&#21463;&#30410;&#20110;&#20016;&#23500;&#30340;&#38899;&#20048;&#29702;&#35770;&#65292;&#20197;&#25351;&#23548;&#26377;&#24847;&#20041;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38899;&#20048;&#35821;&#27861;&#26469;&#35843;&#33410;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28608;&#27963;&#65292;&#20801;&#35768;&#23558;&#31526;&#21495;&#34920;&#31034;&#20026;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#24212;&#29992;&#38899;&#20048;&#29702;&#35770;&#20013;&#30340;&#21644;&#24358;&#36827;&#34892;&#35268;&#21017;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26576;&#20123;&#28608;&#27963;&#22914;&#20309;&#33258;&#28982;&#22320;&#36319;&#38543;&#20854;&#20182;&#28608;&#27963;&#65292;&#31867;&#20284;&#20110;&#21560;&#24341;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35843;&#21046;&#38899;&#35843;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#32593;&#32476;&#20869;&#23548;&#33322;&#19981;&#21516;&#30340;&#21560;&#24341;&#30406;&#22320;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#26144;&#23556;&#26159;&#30001;&#38899;&#20048;&#20116;&#24230;&#22278;&#26500;&#25104;&#30340;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;&#38899;&#20048;&#29702;&#35770;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00790v1 Announce Type: cross  Abstract: In this paper, we explore the intriguing similarities between the structure of a discrete neural network, such as a spiking network, and the composition of a piano piece. While both involve nodes or notes that are activated sequentially or in parallel, the latter benefits from the rich body of music theory to guide meaningful combinations. We propose a novel approach that leverages musical grammar to regulate activations in a spiking neural network, allowing for the representation of symbols as attractors. By applying rules for chord progressions from music theory, we demonstrate how certain activations naturally follow others, akin to the concept of attraction. Furthermore, we introduce the concept of modulating keys to navigate different basins of attraction within the network. Ultimately, we show that the map of concepts in our model is structured by the musical circle of fifths, highlighting the potential for leveraging music theor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.00788</link><description>&lt;p&gt;
PRECISE&#26694;&#26550;&#65306;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#20197;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#23454;&#29616;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
PRECISE Framework: GPT-based Text For Improved Readability, Reliability, and Understandability of Radiology Reports For Patient-Centered Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;GPT-4&#25216;&#26415;&#25552;&#20379;&#26356;&#26131;&#35835;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;PRECISE&#26694;&#26550;&#65292;&#21033;&#29992;OpenAI&#30340;GPT-4&#26469;&#22686;&#24378;&#24739;&#32773;&#21442;&#19982;&#24230;&#65292;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26356;&#26131;&#35835;&#30340;&#20845;&#24180;&#32423;&#38405;&#35835;&#27700;&#24179;&#30340;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#35813;&#26694;&#26550;&#22312;500&#20221;&#25253;&#21578;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#26174;&#31034;&#20986;&#22312;&#21487;&#35835;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#32479;&#35745;&#20998;&#26512;&#35777;&#23454;&#20102;PRECISE&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#20419;&#36827;&#20581;&#24247;&#20915;&#31574;&#20013;&#24515;&#30340;&#25252;&#29702;&#20132;&#20184;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00788v1 Announce Type: cross  Abstract: This study introduces and evaluates the PRECISE framework, utilizing OpenAI's GPT-4 to enhance patient engagement by providing clearer and more accessible chest X-ray reports at a sixth-grade reading level. The framework was tested on 500 reports, demonstrating significant improvements in readability, reliability, and understandability. Statistical analyses confirmed the effectiveness of the PRECISE approach, highlighting its potential to foster patient-centric care delivery in healthcare decision-making.
&lt;/p&gt;</description></item><item><title>BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2403.00784</link><description>&lt;p&gt;
&#21033;&#29992;BERT&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65306;&#35843;&#30740;&#12289;&#24212;&#29992;&#12289;&#36164;&#28304;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00784
&lt;/p&gt;
&lt;p&gt;
BERT&#30340;&#24341;&#20837;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24102;&#26469;&#20102;&#31361;&#30772;&#65292;&#30740;&#31350;&#32773;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#20026;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#38382;&#39064;&#26041;&#38754;&#24471;&#21040;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#26368;&#21021;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21463;&#21040;&#23427;&#20204;&#39034;&#24207;&#25110;&#21333;&#21521;&#24615;&#36136;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#38590;&#20197;&#25429;&#25417;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20174;&#21464;&#21387;&#22120;&#65288;BERT&#65289;&#20013;&#24341;&#20837;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#24449;&#25552;&#20379;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24378;&#22823;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29702;&#35299;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23558;BERT&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#12290;&#22240;&#27492;&#65292;&#19968;&#39033;&#20851;&#27880;&#23558;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22914;BERT&#24212;&#29992;&#20110;IR&#30340;&#26222;&#36941;&#26041;&#27861;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#35843;&#26597;&#23545;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#26377;&#29992;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#35843;&#26597;&#37325;&#26032;&#23457;&#35270;&#20102;&#21508;&#31181;&#22522;&#20110;BERT&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00784v1 Announce Type: cross  Abstract: Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wid
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;LLMs&#23884;&#20837;&#21040;&#22270;&#24418;&#35268;&#21010;&#20013;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs-based&#35268;&#21010;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00783</link><description>&lt;p&gt;
&#35770;LLMs&#22312;&#35268;&#21010;&#20013;&#30340;&#20316;&#29992;&#65306;&#23558;LLMs&#23884;&#20837;&#35268;&#21010;&#22270;&#20013;
&lt;/p&gt;
&lt;p&gt;
On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00783
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;LLMs&#23884;&#20837;&#21040;&#22270;&#24418;&#35268;&#21010;&#20013;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs-based&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#21010;&#21512;&#25104;&#26088;&#22312;&#29983;&#25104;&#19968;&#31995;&#21015;&#21160;&#20316;&#25110;&#31574;&#30053;&#65292;&#23558;&#32473;&#23450;&#30340;&#21021;&#22987;&#29366;&#24577;&#36716;&#31227;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#25552;&#20379;&#30340;&#39046;&#22495;&#27169;&#22411;&#21487;&#20197;&#30001;&#19987;&#23478;&#35774;&#35745;&#25110;&#20174;&#35757;&#32451;&#25968;&#25454;&#25110;&#19982;&#19990;&#30028;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#20852;&#35268;&#21010;&#33021;&#21147;&#30340;&#22768;&#31216;&#25152;&#21560;&#24341;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;LLMs&#35268;&#21010;&#26377;&#25928;&#24615;&#30340;&#24037;&#20316;&#65292;&#32780;&#19981;&#32771;&#34385;LLMs&#20013;&#29616;&#25104;&#35268;&#21010;&#25216;&#26415;&#30340;&#21033;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLMs&#22312;&#29616;&#25104;&#35268;&#21010;&#26694;&#26550;&#20013;&#30340;&#20316;&#29992;&#36827;&#19968;&#27493;&#30740;&#31350;LLMs&#30340;&#35268;&#21010;&#33021;&#21147;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;LLMs&#23884;&#20837;&#21040;&#20247;&#25152;&#21608;&#30693;&#30340;&#35268;&#21010;&#26694;&#26550;&#20043;&#19968;&#65292;&#22522;&#20110;&#22270;&#30340;&#35268;&#21010;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLMs&#30340;&#35268;&#21010;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#23884;&#20837;&#21040;&#20004;&#20010;&#32423;&#21035;&#30340;&#35268;&#21010;&#22270;&#20013;&#65292;&#21363;&#30456;&#20114;&#32422;&#26463;&#29983;&#25104;&#32423;&#21035;&#21644;&#32422;&#26463;&#35299;&#20915;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00783v1 Announce Type: new  Abstract: Plan synthesis aims to generate a course of actions or policies to transit given initial states to goal states, provided domain models that could be designed by experts or learnt from training data or interactions with the world. Intrigued by the claims of emergent planning capabilities in large language models (LLMs), works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs. In this paper, we aim to further study the insight of the planning capability of LLMs by investigating the roles of LLMs in off-the-shelf planning frameworks. To do this, we investigate the effectiveness of embedding LLMs into one of the well-known planning frameworks, graph-based planning, proposing a novel LLMs-based planning framework with LLMs embedded in two levels of planning graphs, i.e., mutual constraints generation level and constraints solving level. We emp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ploutos&#65292;&#19968;&#20010;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#65292;&#36890;&#36807;PloutosGen&#21644;PloutosGPT&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.00782</link><description>&lt;p&gt;
Ploutos&#65306;&#22522;&#20110;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ploutos: Towards interpretable stock movement prediction with financial large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ploutos&#65292;&#19968;&#20010;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#65292;&#36890;&#36807;PloutosGen&#21644;PloutosGPT&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24320;&#36767;&#20102;&#35768;&#22810;&#39046;&#22495;&#30340;&#26032;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#37329;&#34701;&#25237;&#36164;&#39046;&#22495;&#20013;&#65292;LLMs &#30340;&#23436;&#25972;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#21033;&#29992;&#12290;&#23545;&#20110;&#37327;&#21270;&#37329;&#34701;&#30340;&#20856;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22312;&#32929;&#31080;&#36208;&#21183;&#39044;&#27979;&#20013;&#28789;&#27963;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#20449;&#24687;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#20854;&#27425;&#65292;&#20256;&#32479;&#26041;&#27861;&#32570;&#20047;&#28165;&#26224;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#39044;&#27979;&#29702;&#30001;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Ploutos&#65292;&#19968;&#20010;&#30001; PloutosGen &#21644; PloutosGPT &#32452;&#25104;&#30340;&#26032;&#22411;&#37329;&#34701;LLM&#26694;&#26550;&#12290;PloutosGen &#21253;&#21547;&#22810;&#20010;&#20027;&#35201;&#19987;&#23478;&#65292;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#30340;&#27169;&#24577;&#25968;&#25454;&#65292;&#22914;&#25991;&#26412;&#21644;&#25968;&#20540;&#65292;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#25552;&#20379;&#37327;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518; PloutosGPT &#32467;&#21512;&#23427;&#20204;&#30340;&#35265;&#35299;&#21644;&#39044;&#27979;&#65292;&#29983;&#25104;&#21487;&#35299;&#37322;&#24615;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00782v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and interpretability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates interpretable rationales. To g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#20998;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2403.00780</link><description>&lt;p&gt;
&#23545;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#27934;&#35265;&#65306;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00780
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#23545;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#20998;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20840;&#38754;&#20998;&#26512;&#20102;&#29359;&#32618;&#39044;&#27979;&#26041;&#27861;&#35770;&#65292;&#25506;&#35752;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#21508;&#31181;&#25216;&#26415;&#21644;&#25216;&#26415;&#12290;&#35813;&#35770;&#25991;&#28085;&#30422;&#20102;&#29992;&#20110;&#20998;&#26512;&#29359;&#32618;&#25968;&#25454;&#30340;&#32479;&#35745;&#26041;&#27861;&#12289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#23457;&#35270;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29359;&#32618;&#39044;&#27979;&#31639;&#27861;&#20998;&#31867;&#20026;&#29305;&#23450;&#25216;&#26415;&#30340;&#26041;&#27861;&#35770;&#20998;&#31867;&#27861;&#12290;&#35813;&#20998;&#31867;&#27861;&#20998;&#20026;&#22235;&#20010;&#23618;&#27425;&#65292;&#21253;&#25324;&#26041;&#27861;&#35770;&#31867;&#21035;&#12289;&#26041;&#27861;&#35770;&#23376;&#31867;&#21035;&#12289;&#26041;&#27861;&#35770;&#25216;&#26415;&#21644;&#26041;&#27861;&#35770;&#25216;&#26415;&#23376;&#31867;&#21035;&#12290;&#25552;&#20379;&#20102;&#32463;&#39564;&#21644;&#23454;&#39564;&#35780;&#20272;&#20197;&#23545;&#19981;&#21516;&#25216;&#26415;&#36827;&#34892;&#25490;&#21517;&#12290;&#32463;&#39564;&#35780;&#20272;&#26681;&#25454;&#22235;&#20010;&#26631;&#20934;&#35780;&#20272;&#20102;&#29359;&#32618;&#39044;&#27979;&#25216;&#26415;&#65292;&#32780;&#23454;&#39564;&#35780;&#20272;&#21017;&#23545;&#37319;&#29992;&#30456;&#21516;&#23376;&#25216;&#26415;&#30340;&#31639;&#27861;&#12289;&#37319;&#29992;&#30456;&#21516;&#25216;&#26415;&#30340;&#19981;&#21516;&#23376;&#25216;&#26415;&#12289;&#20197;&#21450;&#30456;&#21516;&#25216;&#26415;&#30340;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00780v1 Announce Type: cross  Abstract: This survey paper presents a comprehensive analysis of crime prediction methodologies, exploring the various techniques and technologies utilized in this area. The paper covers the statistical methods, machine learning algorithms, and deep learning techniques employed to analyze crime data, while also examining their effectiveness and limitations. We propose a methodological taxonomy that classifies crime prediction algorithms into specific techniques. This taxonomy is structured into four tiers, including methodology category, methodology sub-category, methodology techniques, and methodology sub-techniques. Empirical and experimental evaluations are provided to rank the different techniques. The empirical evaluation assesses the crime prediction techniques based on four criteria, while the experimental evaluation ranks the algorithms that employ the same sub-technique, the different sub-techniques that employ the same technique, the d
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;BERT&#24773;&#24863;&#20998;&#31867;&#21644;LSTM&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#24494;&#21338;&#24179;&#21488;&#25480;&#26435;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#32929;&#24066;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.00772</link><description>&lt;p&gt;
&#24494;&#21338;&#24179;&#21488;&#19987;&#23478;&#22312;&#39044;&#27979;&#32929;&#24066;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Weibo platform experts perform better at predicting stock market?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00772
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;BERT&#24773;&#24863;&#20998;&#31867;&#21644;LSTM&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#22312;&#24494;&#21338;&#24179;&#21488;&#25480;&#26435;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#32929;&#24066;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#21487;&#29992;&#20110;&#32929;&#24066;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#23578;&#26410;&#30740;&#31350;&#29992;&#25143;&#30340;&#37329;&#34701;&#32972;&#26223;&#23545;&#22522;&#20110;&#24773;&#24863;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32929;&#24066;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#21512;&#26469;&#35780;&#20272;&#22522;&#20110;&#20154;&#32676;&#37329;&#34701;&#32972;&#26223;&#30340;&#24773;&#24863;&#32929;&#24066;&#39044;&#27979;&#12290;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#26469;&#20998;&#31867;&#24773;&#24863;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#32929;&#24066;&#39044;&#27979;&#12290;&#35780;&#20272;&#26102;&#65292;&#20351;&#29992;&#24494;&#21338;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#20316;&#20026;&#24773;&#24863;&#25968;&#25454;&#25910;&#38598;&#26469;&#28304;&#12290;&#26681;&#25454;&#20854;&#32972;&#26223;&#65292;&#23558;&#24494;&#21338;&#29992;&#25143;&#65288;&#21450;&#20854;&#35780;&#35770;&#65289;&#20998;&#20026;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#65288;AFA&#65289;&#21644;&#26410;&#25480;&#26435;&#37329;&#34701;&#39038;&#38382;&#65288;UFA&#65289;&#20004;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00772v1 Announce Type: cross  Abstract: Sentiment analysis can be used for stock market prediction. However, existing research has not studied the impact of a user's financial background on sentiment-based forecasting of the stock market using artificial neural networks. In this work, a novel combination of neural networks is used for the assessment of sentiment-based stock market prediction, based on the financial background of the population that generated the sentiment. The state-of-the-art language processing model Bidirectional Encoder Representations from Transformers (BERT) is used to classify the sentiment and a Long-Short Term Memory (LSTM) model is used for time-series based stock market prediction. For evaluation, the Weibo social networking platform is used as a sentiment data collection source. Weibo users (and their comments respectively) are divided into Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor (UFA) groups according to their backg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154; Robotino &#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#30340;&#20998;&#31163;&#36825;&#19968;&#19981;&#22826;&#34987;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00765</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Webots&#30340;&#26080;&#20154;&#30417;&#25511;&#23481;&#22120;&#21270;(&#28145;&#24230;)&#24378;&#21270;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154; Robotino &#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21516;&#26102;&#24378;&#35843;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#30340;&#20998;&#31163;&#36825;&#19968;&#19981;&#22826;&#34987;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#31185;&#23398;&#24212;&#29992;&#22312;&#21508;&#34892;&#21508;&#19994;&#20013;&#24471;&#21040;&#37319;&#29992;&#65292;&#24037;&#20855;&#26223;&#35266;&#19981;&#26029;&#25104;&#29087;&#65292;&#20197;&#20419;&#36827;&#36825;&#31867;&#24212;&#29992;&#30340;&#29983;&#21629;&#21608;&#26399;&#24182;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24212;&#23545;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#20197;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#20351;&#29992;&#20195;&#29702;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#65306;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#25152;&#38656;&#30340;&#30693;&#35782;&#20197;&#21450;&#22312;&#26080;&#20154;&#30417;&#25511;&#30340;&#35757;&#32451;&#31649;&#36947;&#20013;&#21033;&#29992;&#29420;&#31435;&#30340;&#27169;&#25311;&#36719;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29992;&#20110;&#22312;3D&#19990;&#30028;&#20013;&#22521;&#35757;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#65292;&#38024;&#23545;&#26426;&#22120;&#20154;Robotino&#36827;&#34892;&#35770;&#36848;&#65292;&#24182;&#35748;&#20026;&#20026;&#34394;&#25311;&#19990;&#30028;&#30340;&#21019;&#24314;&#32773;&#20998;&#31163;&#27169;&#25311;&#29615;&#22659;&#19982;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#27169;&#22411;&#24320;&#21457;&#29615;&#22659;&#24182;&#19981;&#26159;&#19968;&#20010;&#34987;&#24456;&#22909;&#28085;&#30422;&#30340;&#20027;&#39064;&#12290;&#36890;&#24120;&#20108;&#32773;&#30456;&#21516;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#38656;&#35201;&#20102;&#35299;&#27169;&#25311;&#36719;&#20214;&#65292;&#30452;&#25509;&#19982;&#20854;API&#19968;&#36215;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#26377;&#26102;&#34394;&#25311;&#19990;&#30028;&#30340;&#21019;&#24314;&#32773;&#20250;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00765v1 Announce Type: cross  Abstract: As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines.   In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of vir
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#20013;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;&#20013;&#30340;&#20840;&#29699;&#21355;&#26143;&#36890;&#20449;&#23433;&#25490;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.00692</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00692
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#20013;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;&#20013;&#30340;&#20840;&#29699;&#21355;&#26143;&#36890;&#20449;&#23433;&#25490;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00692v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#26410;&#26469;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#30340;&#26684;&#23616;&#23558;&#30001;&#35201;&#27714;&#28385;&#36275;&#20005;&#26684;&#20219;&#21153;&#38656;&#27714;&#65292;&#22914;&#37325;&#35775;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#32593;&#32476;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#25152;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#21019;&#24314;&#20840;&#29699;&#21355;&#26143;&#25509;&#35302;&#35745;&#21010;&#65288;CP&#65289;&#26469;&#23433;&#25490;&#21355;&#26143;&#36890;&#20449;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#35201;&#27714;&#22320;&#38754;&#21327;&#35843;&#25110;&#21463;&#21040;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26143;&#24231;&#21644;CP&#24314;&#27169;&#20026;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#32473;&#23450;CP&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#26356;&#26032;&#23427;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;3.6&#20998;&#38047;&#26469;&#39044;&#27979;&#32593;&#32476;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00692v1 Announce Type: cross  Abstract: The upcoming landscape of Earth Observation missions will defined by networked heterogeneous nanosatellite constellations required to meet strict mission requirements, such as revisit times and spatial resolution. However, scheduling satellite communications in these satellite networks through efficiently creating a global satellite Contact Plan (CP) is a complex task, with current solutions requiring ground-based coordination or being limited by onboard computational resources. The paper proposes a novel approach to overcome these challenges by modeling the constellations and CP as dynamic networks and employing graph-based techniques. The proposed method utilizes a state-of-the-art dynamic graph neural network to evaluate the performance of a given CP and update it using a heuristic algorithm based on simulated annealing. The trained neural network can predict the network delay with a mean absolute error of 3.6 minutes. Simulation re
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.19406</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#22320;&#29702;&#34920;&#31034;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Scaling Laws of Geographical Representation in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19406
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#30693;&#35782;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#65292;&#20294;&#26356;&#22823;&#30340;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#34987;&#35777;&#26126;&#22312;&#20854;&#38544;&#34255;&#34920;&#31034;&#20013;&#23884;&#20837;&#20102;&#22320;&#29702;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;&#26412;&#25991;&#36890;&#36807;&#35266;&#23519;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#22320;&#29702;&#30693;&#35782;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#22635;&#34917;&#29616;&#26377;&#21644;&#26368;&#36817;&#25991;&#29486;&#20043;&#38388;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#24494;&#23567;&#27169;&#22411;&#65292;&#22320;&#29702;&#30693;&#35782;&#20063;&#26159;&#21487;&#35266;&#27979;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19968;&#33268;&#25193;&#23637;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22320;&#29702;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19406v1 Announce Type: cross  Abstract: Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.19116</link><description>&lt;p&gt;
&#22914;&#20309;&#29702;&#35299;&#8220;&#25903;&#25345;&#8221;&#65311;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#29992;&#20110;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
How to Understand "Support"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65288;IECI&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#26631;&#27880;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30701;&#35821;&#23450;&#20301;&#65288;WPG&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#25512;&#26029;&#32454;&#31890;&#24230;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#65292;&#20165;&#21033;&#29992;&#31895;&#31890;&#24230;&#30340;&#21477;&#23376;-&#22270;&#20687;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;WPG&#30340;&#30740;&#31350;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#38544;&#24335;&#30701;&#35821;-&#21306;&#22495;&#21305;&#37197;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#28145;&#23618;&#22810;&#27169;&#24577;&#35821;&#20041;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#22686;&#24378;&#22240;&#26524;&#25512;&#26029;&#65288;IECI&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#23545;&#24314;&#27169;&#38544;&#24335;&#20851;&#31995;&#21644;&#31361;&#20986;&#26174;&#24615;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#21033;&#29992;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25216;&#26415;&#26469;&#24212;&#23545;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36824;&#26631;&#27880;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#38544;&#24335;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;IECI&#65292;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;IECI&#30456;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19116v1 Announce Type: cross  Abstract: Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting findi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18920</link><description>&lt;p&gt;
&#20809;&#35889;&#36935;&#35265;&#31354;&#38388;: &#21644;&#35856;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#24182;&#20381;&#27425;&#24212;&#29992;&#20110;&#20851;&#32852;&#19981;&#21516;&#30340;3D&#24418;&#29366;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#21151;&#33021;&#26144;&#23556;&#26694;&#26550;&#19982;&#32463;&#20856;&#34920;&#38754;&#21464;&#24418;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#26144;&#23556;&#24418;&#29366;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#29992;&#20110;&#24418;&#29366;&#21305;&#37197;&#30340;&#21151;&#33021;&#26144;&#23556;&#26041;&#27861;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#24341;&#20837;&#20809;&#35889;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25670;&#33073;&#20102;&#36890;&#24120;&#20351;&#29992;&#20294;&#35745;&#31639;&#26114;&#36149;&#30340;&#20165;&#23545;&#36817;&#31561;&#36317;&#24418;&#29366;&#21464;&#24418;&#26377;&#25928;&#30340;&#27979;&#22320;&#36317;&#31163;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.18590</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#24191;&#27867;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18590
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25512;&#33616;&#33021;&#21147;&#21644;&#35821;&#35328;&#29702;&#35299;&#65292;&#37325;&#22609;&#20102;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37325;&#22609;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#24402;&#22240;&#20110;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#25152;&#32570;&#20047;&#30340;&#29420;&#29305;&#25512;&#29702;&#33021;&#21147;&#12290;&#19981;&#21516;&#20110;&#32570;&#20047;&#30452;&#25509;&#29992;&#25143;&#20114;&#21160;&#25968;&#25454;&#30340;&#20256;&#32479;&#31995;&#32479;&#65292;LLMs&#22312;&#25512;&#33616;&#29289;&#21697;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#35821;&#35328;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#36825;&#26631;&#24535;&#30528;&#25512;&#33616;&#39046;&#22495;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#33539;&#24335;&#36716;&#21464;&#12290;&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#31215;&#26497;&#21033;&#29992;LLMs&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#37325;&#26032;&#23450;&#20041;&#25512;&#33616;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#35813;&#30740;&#31350;&#24443;&#24213;&#25506;&#35752;&#20102;LLMs&#22312;&#25512;&#33616;&#26694;&#26550;&#20869;&#22266;&#26377;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#32454;&#33268;&#30340;&#35821;&#22659;&#29702;&#35299;&#65292;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#24179;&#31283;&#36807;&#28193;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20849;&#20139;&#25968;&#25454;&#27744;&#30340;&#20840;&#38754;&#23398;&#20064;&#31574;&#30053;&#65292;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18590v1 Announce Type: cross  Abstract: The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;</title><link>https://arxiv.org/abs/2402.18284</link><description>&lt;p&gt;
&#20247;&#21253;&#26159;&#21542;&#35753;&#24744;&#30772;&#20135;&#20102;&#65311;&#20351;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of Pre-trained Language Models with Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18284
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24191;&#27867;&#20351;&#29992;&#20984;&#26174;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#27969;&#31243;&#20381;&#36182;&#20110;&#20154;&#24037;&#25490;&#24207;&#65292;&#36825;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#38477;&#20302;&#21171;&#21160;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#25991;&#26412;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#24212;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21592;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#27010;&#29575;&#25277;&#26679;&#24320;&#22987;&#65292;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#20026;&#27599;&#20010;&#36755;&#20837;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21709;&#24212;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;TextRank&#21644;ISODATA&#31639;&#27861;&#65292;&#22522;&#20110;&#35821;&#20041;&#23545;&#36825;&#20123;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#21644;&#32858;&#31867;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#23398;&#20064;&#25490;&#21517;&#24182;&#20248;&#21270;&#25105;&#20204;&#30340;&#29983;&#25104;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#20351;&#29992;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;BLEU&#12289;GLEU&#21644;METEOR&#24471;&#20998;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25163;&#21160;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18284v1 Announce Type: cross  Abstract: Wide usage of ChatGPT has highlighted the potential of reinforcement learning from human feedback. However, its training pipeline relies on manual ranking, a resource-intensive process. To reduce labor costs, we propose a self-supervised text ranking approach for applying Proximal-Policy-Optimization to fine-tune language models while eliminating the need for human annotators. Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input. We then employ TextRank and ISODATA algorithms to rank and cluster these responses based on their semantics. Subsequently, we construct a reward model to learn the rank and optimize our generative policy. Our experimental results, conducted using two language models on three tasks, demonstrate that the models trained by our method considerably outperform baselines regarding BLEU, GLEU, and METEOR scores. Furthermore, our manual evaluation show
&lt;/p&gt;</description></item><item><title>Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18205</link><description>&lt;p&gt;
Lemur: &#20351;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18205
&lt;/p&gt;
&lt;p&gt;
Lemur&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65292;&#35299;&#20915;&#20102;&#26085;&#24535;&#35299;&#26512;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#35268;&#21017;&#20381;&#36182;&#21644;&#35821;&#20041;&#20449;&#24687;&#24573;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#36719;&#20214;&#31995;&#32479;&#20135;&#29983;&#30340;&#26085;&#24535;&#23545;&#30417;&#35270;&#31995;&#32479;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#36827;&#30340;&#26085;&#24535;&#20998;&#26512;&#26377;&#21161;&#20110;&#26816;&#27979;&#12289;&#25253;&#35686;&#21644;&#35786;&#26029;&#31995;&#32479;&#25925;&#38556;&#12290;&#26085;&#24535;&#35299;&#26512;&#26159;&#26085;&#24535;&#20998;&#26512;&#33258;&#21160;&#21270;&#30340;&#20851;&#38190;&#38454;&#27573;&#65292;&#23427;&#28041;&#21450;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#27169;&#26495;&#12290;&#29616;&#26377;&#30340;&#26085;&#24535;&#35299;&#26512;&#22120;&#30001;&#20110;&#20381;&#36182;&#20110;&#20154;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#32780;&#26080;&#27861;&#35782;&#21035;&#27491;&#30830;&#30340;&#27169;&#26495;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20391;&#37325;&#20110;&#32479;&#35745;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#26085;&#24535;&#28040;&#24687;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#26085;&#24535;&#35299;&#26512;&#26694;&#26550;&#65292;&#37319;&#29992;&#29109;&#25277;&#26679;&#21644;&#24605;&#32500;&#38142;&#21512;&#24182;&#65288;Lemur&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#21160;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20449;&#24687;&#29109;&#21551;&#21457;&#30340;&#26032;&#22411;&#25277;&#26679;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#20856;&#22411;&#26085;&#24535;&#36827;&#34892;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#26085;&#24535;&#27169;&#26495;&#30340;&#21512;&#24182;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24605;&#32500;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18205v1 Announce Type: cross  Abstract: Logs produced by extensive software systems are integral to monitoring system behaviors. Advanced log analysis facilitates the detection, alerting, and diagnosis of system faults. Log parsing, which entails transforming raw log messages into structured templates, constitutes a critical phase in the automation of log analytics. Existing log parsers fail to identify the correct templates due to reliance on human-made rules. Besides, These methods focus on statistical features while ignoring semantic information in log messages. To address these challenges, we introduce a cutting-edge \textbf{L}og parsing framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging (Lemur). Specifically, to discard the tedious manual rules. We propose a novel sampling method inspired by information entropy, which efficiently clusters typical logs. Furthermore, to enhance the merging of log templates, we design a chain-of-thought method f
&lt;/p&gt;</description></item><item><title>Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.18007</link><description>&lt;p&gt;
Mixer&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixer is more than just a model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18007
&lt;/p&gt;
&lt;p&gt;
Mixer&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#65292;&#20195;&#34920;&#20102;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#38656;&#27714;&#21019;&#24314;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;MLP&#32467;&#26500;&#37325;&#26032;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;MLP-Mixer&#20197;&#20854;&#31361;&#20986;&#30340;&#34920;&#29616;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;MLP-Mixer&#20197;&#20174;&#36890;&#36947;&#21644;&#20196;&#29260;&#20004;&#20010;&#35282;&#24230;&#25552;&#21462;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#26377;&#25928;&#22320;&#20316;&#20026;&#36890;&#36947;&#20449;&#24687;&#21644;&#20196;&#29260;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#20107;&#23454;&#19978;&#65292;Mixer&#20195;&#34920;&#20102;&#19968;&#31181;&#20449;&#24687;&#25552;&#21462;&#33539;&#24335;&#65292;&#23558;&#36890;&#36947;&#21644;&#20196;&#29260;&#20449;&#24687;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;Mixer&#30340;&#31934;&#39635;&#22312;&#20110;&#23427;&#33021;&#22815;&#20174;&#22810;&#20803;&#35270;&#35282;&#34701;&#21512;&#20449;&#24687;&#65292;&#20856;&#22411;&#22320;&#20307;&#29616;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39046;&#22495;&#30340;&#8220;&#28151;&#21512;&#8221;&#30495;&#27491;&#27010;&#24565;&#12290;&#38500;&#20102;&#32771;&#34385;&#36890;&#36947;&#21644;&#20196;&#29260;&#20197;&#22806;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#35282;&#24230;&#21019;&#36896;&#26356;&#36148;&#21512;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#30340;&#28151;&#21512;&#22120;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#38899;&#39057;&#35782;&#21035;&#39046;&#22495;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;&#24102;Roll-Time&#21644;Hermit FFT&#30340;&#38899;&#39057;&#39057;&#35889;&#28151;&#21512;&#22120;(ASM-RH)&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18007v1 Announce Type: cross  Abstract: Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of "mixing" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and freq
&lt;/p&gt;</description></item><item><title>REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;</title><link>https://arxiv.org/abs/2402.17862</link><description>&lt;p&gt;
REPrune&#65306;&#36890;&#36807;&#26680;&#20195;&#34920;&#36873;&#25321;&#36827;&#34892;&#36890;&#36947;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
REPrune: Channel Pruning via Kernel Representative Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17862
&lt;/p&gt;
&lt;p&gt;
REPrune&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#27169;&#25311;&#26680;&#20462;&#21098;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#21644;&#28388;&#27874;&#22120;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#26356;&#31934;&#32454;&#20294;&#32467;&#26500;&#21270;&#30340;&#20462;&#21098;&#31890;&#24230;&#65292;&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#20462;&#21098;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#21152;&#36895;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20462;&#21098;&#27169;&#22411;&#21487;&#20197;&#31435;&#21363;&#37096;&#32626;&#22312;&#36890;&#29992;&#36719;&#20214;&#21644;&#30828;&#20214;&#36164;&#28304;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#21367;&#31215;&#28388;&#27874;&#22120;&#36825;&#20010;&#21333;&#20803;&#19978;&#30340;&#22823;&#20462;&#21098;&#31890;&#24230;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#19981;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;CNNs&#20013;&#20915;&#23450;&#22914;&#20309;&#20197;&#21450;&#22312;&#20309;&#22788;&#24341;&#20837;&#31232;&#30095;&#24615;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPrune&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#20462;&#21098;&#25216;&#26415;&#65292;&#27169;&#25311;&#20102;&#26680;&#20462;&#21098;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#26356;&#32454;&#20294;&#26377;&#32467;&#26500;&#30340;&#31890;&#24230;&#12290;REPrune&#20351;&#29992;&#20957;&#32858;&#32858;&#31867;&#35782;&#21035;&#27599;&#20010;&#36890;&#36947;&#20869;&#30340;&#30456;&#20284;&#26680;&#12290;&#28982;&#21518;&#65292;&#23427;&#36873;&#25321;&#26368;&#22823;&#21270;&#21253;&#21547;&#26680;&#20195;&#34920;&#30340;&#28388;&#27874;&#22120;&#65292;&#21516;&#26102;&#20248;&#21270;&#26368;&#22823;&#32858;&#31867;&#35206;&#30422;&#38382;&#39064;&#12290;&#36890;&#36807;&#19982;&#21516;&#26102;&#35757;&#32451;-&#20462;&#21098;&#33539;&#24335;&#30456;&#32467;&#21512;&#65292;REPrune&#20419;&#36827;&#20102;&#22312;&#35757;&#32451;CNNs&#26399;&#38388;&#30340;&#39640;&#25928;&#12289;&#28176;&#36827;&#24335;&#20462;&#21098;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17862v1 Announce Type: cross  Abstract: Channel pruning is widely accepted to accelerate modern convolutional neural networks (CNNs). The resulting pruned model benefits from its immediate deployment on general-purpose software and hardware resources. However, its large pruning granularity, specifically at the unit of a convolution filter, often leads to undesirable accuracy drops due to the inflexibility of deciding how and where to introduce sparsity to the CNNs. In this paper, we propose REPrune, a novel channel pruning technique that emulates kernel pruning, fully exploiting the finer but structured granularity. REPrune identifies similar kernels within each channel using agglomerative clustering. Then, it selects filters that maximize the incorporation of kernel representatives while optimizing the maximum cluster coverage problem. By integrating with a simultaneous training-pruning paradigm, REPrune promotes efficient, progressive pruning throughout training CNNs, avoi
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16379</link><description>&lt;p&gt;
&#29992;&#31995;&#32479;&#33258;&#26657;&#27491;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving LLM-based Machine Translation with Systematic Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16379
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#25104;&#21151;&#24110;&#21161;LLMs&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#20180;&#32454;&#35780;&#20272;&#21457;&#29616;&#65292;LLMs&#29983;&#25104;&#30340;&#32763;&#35793;&#20173;&#28982;&#21253;&#21547;&#22810;&#20010;&#38169;&#35823;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23558;&#36825;&#31181;&#38169;&#35823;&#20449;&#24687;&#21453;&#39304;&#21040;LLMs&#20013;&#21487;&#20197;&#23454;&#29616;&#33258;&#26657;&#27491;&#65292;&#24182;&#25913;&#21892;&#32763;&#35793;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#20123;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;TER&#30340;&#31995;&#32479;LLM&#33258;&#26657;&#27491;&#32763;&#35793;&#26694;&#26550;&#65292;&#20195;&#34920;&#20102;&#22312;&#36825;&#19968;&#26041;&#21521;&#19978;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#25105;&#20204;&#30340;&#33258;&#26657;&#27491;&#26694;&#26550;&#25104;&#21151;&#22320;&#24110;&#21161;LLMs&#25552;&#39640;&#20102;&#22810;&#31181;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#36824;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#36824;&#26159;&#22260;&#32469;&#20854;&#20182;&#35821;&#35328;&#65307;2&#65289;TER&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#23637;&#31034;&#20986;&#26356;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#38480;&#21046;&#23548;&#33268;&#20102;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.15943</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#36719;&#20214;&#24037;&#31243;&#65306;&#21487;&#20449;&#22522;&#30784;&#27169;&#22411;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#25361;&#25112;&#31934;&#36873;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Rethinking Software Engineering in the Era of Foundation Models: A Curated Catalogue of Challenges in the Development of Trustworthy FMware
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15943
&lt;/p&gt;
&lt;p&gt;
FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#38480;&#21046;&#23548;&#33268;&#20102;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foundation&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#23454;&#29616;&#26032;&#30340;&#29992;&#20363;&#21644;&#21830;&#19994;&#27169;&#22411;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#36719;&#20214;&#24320;&#21457;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;FMs&#26500;&#24314;&#30340;&#36719;&#20214;&#31216;&#20026;FMware&#12290;FMware&#30340;&#29420;&#29305;&#23646;&#24615;&#65288;&#20363;&#22914;&#25552;&#31034;&#12289;&#20195;&#29702;&#21644;&#32534;&#25490;&#30340;&#38656;&#27714;&#65289;&#65292;&#19982;FMs&#30340;&#20869;&#22312;&#38480;&#21046;&#65288;&#20363;&#22914;&#24187;&#35273;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#20840;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#25361;&#25112;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#24037;&#19994;&#32463;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;10&#20010;&#20851;&#38190;&#30340;SE4FMware&#25361;&#25112;&#65292;&#23548;&#33268;&#20225;&#19994;FMware&#24320;&#21457;&#21464;&#24471;&#20302;&#25928;&#12289;&#25104;&#26412;&#39640;&#26114;&#19988;&#39118;&#38505;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#38416;&#26126;&#20102;&#25105;&#20204;&#35774;&#24819;&#30340;&#21019;&#26032;&#36335;&#24452;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FMArts&#65292;&#36825;&#26159;&#25105;&#20204;&#20026;&#26500;&#24314;&#21487;&#20449;FMware&#32780;&#36827;&#34892;&#30340;&#38271;&#26399;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FMArts&#30340;&#29420;&#29305;&#23646;&#24615;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#19968;&#31181;&#22823;&#22411;FMware&#35774;&#35745;&#21644;&#24320;&#21457;&#19968;&#20010;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15943v1 Announce Type: cross  Abstract: Foundation models (FMs), such as Large Language Models (LLMs), have revolutionized software development by enabling new use cases and business models. We refer to software built using FMs as FMware. The unique properties of FMware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of FMs (e.g., hallucination) lead to a completely new set of software engineering challenges. Based on our industrial experience, we identified 10 key SE4FMware challenges that have caused enterprise FMware development to be unproductive, costly, and risky. In this paper, we discuss these challenges in detail and state the path for innovation that we envision. Next, we present FMArts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy FMware. Finally, we (i) show how the unique properties of FMArts enabled us to design and develop a complex FMware for a larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELFDEFEND&#30340;&#36731;&#37327;&#32423;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24310;&#36831;&#19979;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.15727</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20197;&#23454;&#29992;&#30340;&#26041;&#24335;&#33258;&#25105;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65306;&#19968;&#20221;&#23637;&#26395;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SELFDEFEND&#30340;&#36731;&#37327;&#32423;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24310;&#36831;&#19979;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#20197;&#32469;&#36807;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#37096;&#32626;&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#26377;&#25928;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36138;&#23146;&#22352;&#26631;&#26799;&#24230;&#65288;GCG&#65289;&#25915;&#20987;&#12289;&#22522;&#20110;&#36234;&#29425;&#27169;&#26495;&#30340;&#25915;&#20987;&#65292;&#20363;&#22914;&#20351;&#29992;&#8220;Do-Anything-Now&#8221;&#65288;DAN&#65289;&#65292;&#20197;&#21450;&#22810;&#35821;&#35328;&#36234;&#29425;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38450;&#24481;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32780;&#23454;&#29992;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#31216;&#20026;SELFDEFEND&#65292;&#21487;&#20197;&#25269;&#24481;&#25152;&#26377;&#29616;&#26377;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#22312;&#36234;&#29425;&#25552;&#31034;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24310;&#36831;&#65292;&#23545;&#20110;&#27491;&#24120;&#29992;&#25143;&#25552;&#31034;&#20063;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35265;&#35299;&#26159;&#65292;&#26080;&#35770;&#20351;&#29992;&#20309;&#31181;&#36234;&#29425;&#31574;&#30053;&#65292;&#26368;&#32456;&#37117;&#38656;&#35201;&#22312;&#21457;&#36865;&#32473;LLMs&#30340;&#25552;&#31034;&#20013;&#21253;&#21547;&#26377;&#23475;&#25552;&#31034;&#65288;&#20363;&#22914;&#8220;&#22914;&#20309;&#21046;&#36896;&#28856;&#24377;&#8221;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#36829;&#21453;&#23433;&#20840;&#35268;&#21017;&#30340;&#26377;&#23475;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15727v1 Announce Type: cross  Abstract: Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs). A considerable amount of research exists proposing more effective jailbreak attacks, including the recent Greedy Coordinate Gradient (GCG) attack, jailbreak template-based attacks such as using "Do-Anything-Now" (DAN), and multilingual jailbreak. In contrast, the defensive side has been relatively less explored. This paper proposes a lightweight yet practical defense called SELFDEFEND, which can defend against all existing jailbreak attacks with minimal delay for jailbreak prompts and negligible delay for normal user prompts. Our key insight is that regardless of the kind of jailbreak strategies employed, they eventually need to include a harmful prompt (e.g., "how to make a bomb") in the prompt sent to LLMs, and we found that existing LLMs can effectively recognize such harmful prompts that violate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2402.15708</link><description>&lt;p&gt;
&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#26597;&#35810;&#35821;&#20041;&#30340;&#26597;&#35810;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Query Augmentation by Decoding Semantics from Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Brain-Aug&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33041;&#20449;&#21495;&#20013;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#26597;&#35810;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26597;&#35810;&#65292;&#25913;&#21892;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#31946;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#25193;&#23637;&#26159;&#29992;&#20110;&#32454;&#21270;&#35821;&#20041;&#19981;&#20934;&#30830;&#26597;&#35810;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20256;&#32479;&#19978;&#65292;&#26597;&#35810;&#25193;&#23637;&#20381;&#36182;&#20110;&#20174;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#12289;&#28508;&#22312;&#30456;&#20851;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#22914;&#26524;&#26368;&#21021;&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#36136;&#37327;&#36739;&#20302;&#65292;&#21017;&#26597;&#35810;&#25193;&#23637;&#30340;&#26377;&#25928;&#24615;&#20063;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Brain-Aug&#65292;&#36890;&#36807;&#23558;&#20174;&#33041;&#20449;&#21495;&#35299;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#32467;&#21512;&#21040;&#26597;&#35810;&#20013;&#26469;&#22686;&#24378;&#26597;&#35810;&#12290;Brain-Aug&#20351;&#29992;&#20102;&#22312;&#33041;&#20449;&#21495;&#20449;&#24687;&#26500;&#24314;&#30340;&#25552;&#31034;&#21644;&#38754;&#21521;&#25490;&#21517;&#30340;&#25512;&#29702;&#26041;&#27861;&#29983;&#25104;&#21407;&#22987;&#26597;&#35810;&#30340;&#24310;&#32493;&#37096;&#20998;&#12290;&#23545;fMRI&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Brain-Aug&#29983;&#25104;&#30340;&#26597;&#35810;&#22312;&#35821;&#20041;&#19978;&#26356;&#20934;&#30830;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#25991;&#26723;&#25490;&#24207;&#24615;&#33021;&#12290;&#33041;&#20449;&#21495;&#24102;&#26469;&#30340;&#36825;&#31181;&#25913;&#36827;&#23545;&#20110;&#27169;&#31946;&#26597;&#35810;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15708v1 Announce Type: cross  Abstract: Query augmentation is a crucial technique for refining semantically imprecise queries. Traditionally, query augmentation relies on extracting information from initially retrieved, potentially relevant documents. If the quality of the initially retrieved documents is low, then the effectiveness of query augmentation would be limited as well. We propose Brain-Aug, which enhances a query by incorporating semantic information decoded from brain signals. BrainAug generates the continuation of the original query with a prompt constructed with brain signal information and a ranking-oriented inference approach. Experimental results on fMRI (functional magnetic resonance imaging) datasets show that Brain-Aug produces semantically more accurate queries, leading to improved document ranking performance. Such improvement brought by brain signals is particularly notable for ambiguous queries.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;ReSaE&#65292;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#20851;&#31995;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#65292;&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.15140</link><description>&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#20013;&#28040;&#24687;&#20256;&#36882;&#30340;&#20851;&#31995;&#20132;&#20114;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Relation-Interactive Approach for Message Passing in Hyper-relational Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;ReSaE&#65292;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#65292;&#24378;&#35843;&#20102;&#20851;&#31995;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#65292;&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#21253;&#21547;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;&#65292;&#25552;&#20379;&#20851;&#20110;&#20851;&#31995;&#30340;&#26356;&#22810;&#20449;&#24687;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#30456;&#21516;&#30340;&#20851;&#31995;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#38190;&#20540;&#23545;&#65292;&#20351;&#21407;&#22987;&#19977;&#20803;&#32452;&#20107;&#23454;&#26356;&#20855;&#35782;&#21035;&#24615;&#21644;&#29305;&#23450;&#24615;&#12290;&#20808;&#21069;&#20851;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#30340;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#31181;&#31283;&#22266;&#30340;&#36229;&#20851;&#31995;&#22270;&#32534;&#30721;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20840;&#23616;&#20851;&#31995;&#32467;&#26500;&#24847;&#35782;&#33021;&#21147;&#30340;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;ReSaE&#12290;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;ReSaE&#24378;&#35843;&#20102;&#22312;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#20013;&#20851;&#31995;&#30340;&#20132;&#20114;&#65292;&#24182;&#20248;&#21270;&#20102;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#35835;&#21462;&#32467;&#26500;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;ReSaE&#20026;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#30830;&#20445;&#22312;&#19979;&#28216;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ReSaE&#22312;&#22810;&#20010;&#38142;&#25509;&#39044;&#27979;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15140v1 Announce Type: new  Abstract: Hyper-relational knowledge graphs (KGs) contain additional key-value pairs, providing more information about the relations. In many scenarios, the same relation can have distinct key-value pairs, making the original triple fact more recognizable and specific. Prior studies on hyper-relational KGs have established a solid standard method for hyper-relational graph encoding. In this work, we propose a message-passing-based graph encoder with global relation structure awareness ability, which we call ReSaE. Compared to the prior state-of-the-art approach, ReSaE emphasizes the interaction of relations during message passing process and optimizes the readout structure for link prediction tasks. Overall, ReSaE gives a encoding solution for hyper-relational KGs and ensures stronger performance on downstream link prediction tasks. Our experiments demonstrate that ReSaE achieves state-of-the-art performance on multiple link prediction benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#24863;&#30693;&#30340;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;CG-CNN&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14498</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#30896;&#25758;&#24863;&#30693;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Collision-Aware Cable Grasping Method in Cluttered Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30896;&#25758;&#24863;&#30693;&#30340;&#30005;&#32518;&#25235;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;CG-CNN&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#20026;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#31283;&#20581;&#30005;&#32518;&#25235;&#21462;&#32780;&#35774;&#35745;&#30340;&#30896;&#25758;&#24863;&#30693;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#21033;&#29992;&#29289;&#29702;&#20223;&#30495;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#30005;&#32518;&#25235;&#21462;&#30340;&#22797;&#26434;&#24615;&#65292;&#32771;&#34385;&#21040;&#30005;&#32518;&#19982;&#26426;&#22120;&#20154;&#22841;&#29226;&#20043;&#38388;&#30340;&#28508;&#22312;&#30896;&#25758;&#12290;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#20984;&#20998;&#35299;&#25216;&#26415;&#26469;&#20998;&#26512;&#38750;&#20984;&#30005;&#32518;&#27169;&#22411;&#65292;&#26681;&#25454;&#27169;&#25311;&#25235;&#21462;&#23581;&#35797;&#33258;&#21160;&#26631;&#35760;&#25235;&#21462;&#36136;&#37327;&#12290;&#21033;&#29992;&#36825;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#23545;CG-CNN&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#36890;&#36807;&#22495;&#38543;&#26426;&#21270;&#25216;&#26415;&#22686;&#24378;&#12290;&#38543;&#21518;&#65292;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#39044;&#27979;&#25235;&#21462;&#36136;&#37327;&#65292;&#24182;&#23558;&#26368;&#20339;&#25235;&#21462;&#23039;&#21183;&#25351;&#23548;&#32473;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25235;&#21462;&#25928;&#26524;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;&#30001;&#20110;&#25105;&#20204;&#27169;&#22411;&#38544;&#24335;&#30340;&#30896;&#25758;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25104;&#21151;&#29575;&#65292;&#23545;&#20110;&#24050;&#30693;&#30005;&#32518;&#20026;92.3%&#65292;&#23545;&#20110;&#26410;&#30693;&#30005;&#32518;&#20026;88.4%&#65292;&#36229;&#36234;&#20102;c&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14498v1 Announce Type: cross  Abstract: We introduce a Cable Grasping-Convolutional Neural Network designed to facilitate robust cable grasping in cluttered environments. Utilizing physics simulations, we generate an extensive dataset that mimics the intricacies of cable grasping, factoring in potential collisions between cables and robotic grippers. We employ the Approximate Convex Decomposition technique to dissect the non-convex cable model, with grasp quality autonomously labeled based on simulated grasping attempts. The CG-CNN is refined using this simulated dataset and enhanced through domain randomization techniques. Subsequently, the trained model predicts grasp quality, guiding the optimal grasp pose to the robot controller for execution. Grasping efficacy is assessed across both synthetic and real-world settings. Given our model implicit collision sensitivity, we achieved commendable success rates of 92.3% for known cables and 88.4% for unknown cables, surpassing c
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.13809</link><description>&lt;p&gt;
NeuralDiffuser&#65306;&#20855;&#26377;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#30340;&#21487;&#25511;fMRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13809
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#20026;&#22823;&#33041;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#26816;&#32034;&#12290;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#37325;&#24314;&#32454;&#33410;&#30340;&#36830;&#36143;&#23545;&#40784;&#65288;&#22914;&#32467;&#26500;&#12289;&#32972;&#26223;&#12289;&#32441;&#29702;&#12289;&#39068;&#33394;&#31561;&#65289;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;LDM&#20063;&#20250;&#29983;&#25104;&#19981;&#21516;&#30340;&#22270;&#20687;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22522;&#20110;LDM&#30340;&#31070;&#32463;&#31185;&#23398;&#35270;&#35282;&#65292;&#21363;&#22522;&#20110;&#26469;&#33258;&#28023;&#37327;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36827;&#34892;&#33258;&#19978;&#32780;&#19979;&#30340;&#21019;&#24314;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#32454;&#33410;&#39537;&#21160;&#30340;&#33258;&#19979;&#32780;&#19978;&#24863;&#30693;&#65292;&#23548;&#33268;&#32454;&#33410;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralDiffuser&#65292;&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#20197;&#28176;&#21464;&#24418;&#24335;&#25552;&#20379;&#32454;&#33410;&#32447;&#32034;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#37325;&#22797;&#37325;&#24314;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13809v1 Announce Type: cross  Abstract: Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>NeRF&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27010;&#24565;&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#34920;&#31034;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#24471;&#21040;&#39640;&#32500;MR&#22270;&#20687;&#65292;&#24182;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.13226</link><description>&lt;p&gt;
NeRF&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NeRF Solves Undersampled MRI Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13226
&lt;/p&gt;
&lt;p&gt;
NeRF&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27010;&#24565;&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#34920;&#31034;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#24471;&#21040;&#39640;&#32500;MR&#22270;&#20687;&#65292;&#24182;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#27010;&#24565;&#30340;&#26032;&#22411;&#27424;&#37319;&#26679;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25216;&#26415;&#12290;&#36890;&#36807;&#24452;&#21521;&#27424;&#37319;&#26679;&#65292;&#30456;&#24212;&#30340;&#25104;&#20687;&#38382;&#39064;&#21487;&#20197;&#20174;&#31232;&#30095;&#35270;&#22270;&#28210;&#26579;&#25968;&#25454;&#20013;&#37325;&#22609;&#20026;&#22270;&#20687;&#24314;&#27169;&#20219;&#21153;&#65307;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#32500;MR&#22270;&#20687;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#29992;&#20110;&#20174;&#31354;&#38388;&#22352;&#26631;&#36755;&#20986;&#22270;&#20687;&#24378;&#24230;&#65292;&#35813;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#21644;&#26399;&#26395;&#22270;&#20687;&#20043;&#38388;&#30340;MR&#29289;&#29702;&#39537;&#21160;&#28210;&#26579;&#20851;&#31995;&#12290;&#30740;&#31350;&#20102;&#29992;&#20110;&#39640;&#36136;&#37327;&#31070;&#32463;&#34920;&#31034;&#30340;&#26377;&#25928;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;(i) &#23398;&#20064;&#23436;&#20840;&#22522;&#20110;&#21333;&#20010;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#19968;&#22534;&#27979;&#37327;&#25968;&#25454;&#21644;&#30446;&#26631;&#22270;&#20687;&#38598;&#12290;&#23427;&#21487;&#33021;&#29992;&#20110;&#35786;&#26029;&#24615;MR&#25104;&#20687;&#65292;&#20363;&#22914;&#32974;&#20799;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13226v1 Announce Type: cross  Abstract: This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF). With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation. A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image. Effective undersampling strategies for high-quality neural representation are investigated. The proposed method serves two benefits: (i) The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets. It can be used potentially for diagnostic MR imaging, such as fetal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12721</link><description>&lt;p&gt;
PAC-FNO&#65306;&#24182;&#34892;&#32467;&#26500;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35782;&#21035;&#20302;&#36136;&#37327;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26159;&#22312;&#29305;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#37096;&#32626;&#23427;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25512;&#29702;&#20013;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21644;/&#25110;&#21463;&#21040;&#33258;&#28982;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22825;&#27668;&#21464;&#21270;&#12289;&#22122;&#22768;&#31867;&#22411;&#21644;&#21387;&#32553;&#20266;&#24433;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#20026;&#19981;&#21516;&#20998;&#36776;&#29575;&#25110;&#36755;&#20837;&#21464;&#21270;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#24182;&#34892;&#32467;&#26500;&#21644;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;PAC-FNO&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;PAC-FNO&#22312;&#39057;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20869;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#30340;&#20462;&#25913;&#35757;&#32451;PAC-FNO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12721v1 Announce Type: cross  Abstract: A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the orig
&lt;/p&gt;</description></item><item><title>Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12151</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Causal Language Models Perform Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12151
&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;LLM&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#39069;&#22806;&#35757;&#32451;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22823;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#23548;&#33268;&#26377;&#25928;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26426;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#32780;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22914;&#20309;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#65292;&#24182;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11871</link><description>&lt;p&gt;
&#20174;&#23454;&#38469;&#21040;&#36923;&#36753;&#20877;&#21040;&#23454;&#38469;&#65306;&#20026;&#35268;&#21010;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21457;&#26126;&#31526;&#21495;&#35789;&#27719;&#12289;&#21160;&#20316;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20811;&#26381;&#38271;&#26399;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#34920;&#31034;&#38656;&#35201;&#20855;&#26377;&#24378;&#28872;&#30452;&#35273;&#21644;&#35814;&#32454;&#30693;&#35782;&#30340;&#19987;&#23478;&#65292;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#21487;&#33021;&#38656;&#35201;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#28040;&#38500;&#23545;&#20154;&#31867;&#30452;&#35273;&#30340;&#20381;&#36182;&#26159;&#19968;&#20010;&#26497;&#20026;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#20174;&#26410;&#26631;&#35760;&#30340;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#12290;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;&#31867;PDDL&#22495;&#27169;&#22411;&#12290;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20174;&#23569;&#25968;&#26426;&#22120;&#20154;&#36712;&#36857;&#20013;&#21487;&#20197;&#23398;&#21040;&#24378;&#22823;&#30340;&#25277;&#35937;&#34920;&#31034;&#65307;&#25152;&#23398;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#25512;&#29702;&#65292;&#23454;&#39564;&#21457;&#29616;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#36739;&#23569;&#25968;&#25454;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.11403</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#23454;&#26102;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Neural and Neuro-symbolic Approaches to Real-time Multimodal Complex Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#31070;&#32463;&#21644;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#22797;&#26434;&#20107;&#20214;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#25512;&#29702;&#65292;&#23454;&#39564;&#21457;&#29616;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#36739;&#23569;&#25968;&#25454;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11403v1 Announce Type: new  Abstract: Robots and autonomous systems require an understanding of complex events (CEs) from sensor data to interact with their environments and humans effectively. Traditional end-to-end neural architectures, despite processing sensor data efficiently, struggle with long-duration events due to limited context sizes and reasoning capabilities. Recent advances in neuro-symbolic methods, which integrate neural and symbolic models leveraging human knowledge, promise improved performance with less data. This study addresses the gap in understanding these approaches' effectiveness in complex event detection (CED), especially in temporal reasoning. We investigate neural and neuro-symbolic architectures' performance in a multimodal CED task, analyzing IMU and acoustic data streams to recognize CE patterns. Our methodology includes (i) end-to-end neural architectures for direct CE detection from sensor embeddings, (ii) two-stage concept-based neural mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09782</link><description>&lt;p&gt;
MC-DBN&#65306;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MC-DBN: A Deep Belief Network-Based Model for Modality Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09782
&lt;/p&gt;
&lt;p&gt;
MC-DBN&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#30340;&#27169;&#24577;&#34917;&#20840;&#27169;&#22411;&#65292;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24046;&#36317;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#39046;&#22495;&#12290;&#21033;&#29992;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#25968;&#25454;&#21487;&#33021;&#19981;&#24635;&#26159;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30456;&#21563;&#21512;&#12290;&#25554;&#20540;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20294;&#22312;&#31232;&#30095;&#20449;&#24687;&#24773;&#20917;&#19979;&#21487;&#33021;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#34917;&#20840;&#30340;&#28145;&#24230;&#20449;&#24565;&#32593;&#32476;&#27169;&#22411;&#65288;MC-DBN&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23436;&#25972;&#25968;&#25454;&#30340;&#38544;&#24335;&#29305;&#24449;&#26469;&#24357;&#34917;&#33258;&#36523;&#19982;&#38468;&#21152;&#19981;&#23436;&#25972;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#30830;&#20445;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#23494;&#20999;&#30456;&#31526;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26469;&#33258;&#32929;&#24066;&#39044;&#27979;&#21644;&#24515;&#29575;&#30417;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;MC-DBN&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09099</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#25506;&#32034;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#24448;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#65292;&#20851;&#20110;&#20986;&#29616;&#29616;&#35937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21151;&#33021;&#33021;&#21147;&#22914;&#20309;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#36825;&#19968;&#20256;&#32479;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#26356;&#21152;&#20851;&#27880;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#21152;&#28145;&#25105;&#20204;&#23545;LLMs&#20869;&#37096;&#20986;&#29616;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#33258;&#32452;&#32455;&#8221;&#21644;&#8220;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#21160;&#24577;&#28436;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#8220;&#20986;&#29616;&#29616;&#35937;&#8221;&#65292;&#36825;&#31181;&#29616;&#35937;&#21453;&#26144;&#20102;&#33258;&#28982;&#31995;&#32479;&#20013;&#31616;&#21333;&#30340;&#24494;&#35266;&#30456;&#20114;&#20316;&#29992;&#22914;&#20309;&#23548;&#33268;&#22797;&#26434;&#30340;&#23439;&#35266;&#34892;&#20026;&#12290;&#20026;&#20102;&#23450;&#37327;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#19981;&#26029;&#28436;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#65288;NeuroMFA&#65289;&#12290;&#21033;&#29992;NeuroMFA&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of "self-organization" and "multifractal analysis," we explore how neuron interactions dynamically evolve during training, leading to "emergence," mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08640</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#39044;&#27979;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Forecasting high-impact research topics via machine learning on evolving knowledge graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26500;&#25104;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#23427;&#36843;&#20351;&#30740;&#31350;&#32773;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#26356;&#29421;&#31364;&#30340;&#23376;&#39046;&#22495;&#19978;&#65292;&#20351;&#24471;&#21457;&#29616;&#20854;&#20182;&#39046;&#22495;&#30340;&#26032;&#39062;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#21512;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21150;&#27861;&#39044;&#27979;&#31185;&#23398;&#35770;&#25991;&#26410;&#26469;&#30340;&#24341;&#29992;&#27425;&#25968;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#21040;&#30740;&#31350;&#23436;&#25104;&#24182;&#19988;&#35770;&#25991;&#20889;&#25104;&#21518;&#25165;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26679;&#23601;&#38169;&#36807;&#20102;&#24819;&#27861;&#26500;&#24605;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#27979;&#20174;&#26410;&#34987;&#30740;&#31350;&#32773;&#21457;&#24067;&#30340;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#12290;&#23427;&#32467;&#21512;&#20102;&#20174;&#35770;&#25991;&#20869;&#23481;&#20013;&#21019;&#24314;&#30340;&#35821;&#20041;&#32593;&#32476;&#21644;&#20174;&#21382;&#21490;&#24341;&#29992;&#20013;&#21019;&#24314;&#30340;&#24433;&#21709;&#32593;&#32476;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#28436;&#21270;&#32593;&#32476;&#30340;&#21160;&#24577;&#24773;&#20917;&#65292;&#20174;&#32780;&#39044;&#27979;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#39044;&#26399;&#36825;&#31181;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#32773;&#21457;&#29616;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20013;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#24335;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#22312;&#25112;&#30053;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08128</link><description>&lt;p&gt;
&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#22312;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recursive Joint Simulation in Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20013;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#24335;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#22312;&#25112;&#30053;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#20195;&#29702;&#20043;&#38388;&#30340;&#21338;&#24328;&#21160;&#24577;&#19982;&#20256;&#32479;&#30340;&#20154;-&#20154;&#20114;&#21160;&#21487;&#33021;&#23384;&#22312;&#21508;&#31181;&#19981;&#21516;&#20043;&#22788;&#12290;&#20854;&#20013;&#19968;&#20010;&#21306;&#21035;&#26159;&#21487;&#33021;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;AI&#20195;&#29702;&#65292;&#20363;&#22914;&#22240;&#20026;&#20854;&#28304;&#20195;&#30721;&#26159;&#24050;&#30693;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#36825;&#31181;&#21487;&#33021;&#24615;&#22312;&#25112;&#30053;&#35774;&#32622;&#20013;&#23454;&#29616;&#26356;&#21512;&#20316;&#30340;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20195;&#29702;&#20043;&#38388;&#36816;&#34892;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#12290;&#21363;&#65292;&#20195;&#29702;&#39318;&#20808;&#20849;&#21516;&#35266;&#23519;&#20182;&#20204;&#25152;&#38754;&#23545;&#24773;&#22659;&#30340;&#27169;&#25311;&#12290;&#36825;&#31181;&#27169;&#25311;&#21453;&#36807;&#26469;&#36882;&#24402;&#22320;&#21253;&#25324;&#20102;&#39069;&#22806;&#30340;&#27169;&#25311;&#65288;&#20026;&#20102;&#36991;&#20813;&#26080;&#38480;&#36882;&#24402;&#65292;&#20855;&#26377;&#23567;&#27010;&#29575;&#30340;&#22833;&#36133;&#65289;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#34892;&#21160;&#20043;&#21069;&#35266;&#23519;&#25152;&#26377;&#36825;&#20123;&#23884;&#22871;&#27169;&#25311;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#20114;&#21160;&#22312;&#25112;&#30053;&#19978;&#31561;&#20215;&#20110;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#36716;&#31227;&#35832;&#22914;&#21508;&#31181;&#27665;&#38388;&#23450;&#29702;&#31561;&#29616;&#26377;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07787</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#35780;&#20272;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#20197;&#29702;&#35299;&#24773;&#24863;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#21152;&#24378;ABSA&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20381;&#36182;&#21644;&#32452;&#25104;&#26641;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#12290;&#38543;&#30528;ABSA&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21019;&#26032;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#34987;&#34701;&#20837;&#20854;&#20013;&#65288;&#20363;&#22914;&#28508;&#22312;&#22270;&#65289;&#65292;&#20294;&#36825;&#20063;&#24341;&#20837;&#20102;&#22797;&#26434;&#24615;&#21644;&#28151;&#28102;&#12290;&#30446;&#21069;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#38598;&#25104;&#21040;ABSA&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#65288;EMGF&#65289;&#32593;&#32476;&#65292;&#23427;&#25972;&#21512;&#20102;&#26469;&#33258;&#21477;&#27861;&#20381;&#36182;&#21644;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;EMGF&#37197;&#22791;&#20102;&#22810;&#38170;&#28857;&#19977;&#20803;&#23398;&#20064;&#21644;&#27491;&#20132;&#25237;&#24433;&#65292;&#39640;&#25928;&#22320;&#21033;&#29992;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#32508;&#21512;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
&lt;/p&gt;</description></item><item><title>CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.07234</link><description>&lt;p&gt;
CPSDBench&#65306;&#19968;&#20010;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07234
&lt;/p&gt;
&lt;p&gt;
CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#27969;LLMs&#22312;&#20844;&#20849;&#23433;&#20840;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#35780;&#20272;&#22522;&#20934;&#8212;&#8212;CPSDBench&#12290;CPSDBench&#25972;&#21512;&#20102;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#21040;&#30340;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#23545;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#26356;&#31934;&#30830;&#22320;&#37327;&#21270;LLMs&#22312;&#25191;&#34892;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#26412;&#30740;&#31350;&#20013;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#19981;&#20165;&#22686;&#24378;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#29702;&#35299;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.05663</link><description>&lt;p&gt;
&#23454;&#26102;&#29942;&#39048;&#21644;&#28608;&#27874;&#39044;&#27979;&#30340;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;SA-LSTM&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27493;&#39044;&#27979;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#22312;&#20132;&#36890;&#25511;&#21046;&#30740;&#31350;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;CIRCLES&#32852;&#21512;&#39033;&#30446;&#38656;&#35201;&#39044;&#27979;&#25216;&#26415;&#26469;&#20943;&#36731;&#25968;&#25454;&#28304;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#22312;MegaVanderTest&#23454;&#39564;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#24403;&#21069;&#31995;&#32479;&#38480;&#21046;&#65292;&#24320;&#21457;&#26356;&#36866;&#21512;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#19979;&#19968;&#36718;&#23454;&#39564;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#24577;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA-LSTM&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#22312;&#31354;&#38388;&#32500;&#24230;&#19978;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#20013;&#23610;&#24230;&#20132;&#36890;&#39044;&#27979;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#27493;&#39044;&#27979;&#65292;&#20351;&#29992;n-step SA-LSTM&#65292;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#39044;&#27979;&#20043;&#38388;&#30340;&#24179;&#34913;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#65292;&#21516;&#26102;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.05448</link><description>&lt;p&gt;
Minecraft-ify&#65306;&#29992;&#20110;&#28216;&#25103;&#24212;&#29992;&#30340;Minecraft&#39118;&#26684;&#22270;&#20687;&#29983;&#25104;&#19982;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;Minecraft&#28216;&#25103;&#24212;&#29992;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#31995;&#32479;"Minecraft-ify"&#65292;&#33021;&#22815;&#29983;&#25104;&#38024;&#23545;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#65292;&#24182;&#25903;&#25345;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#25552;&#20379;&#20102;&#26356;&#33258;&#30001;&#21644;&#20248;&#21270;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#38754;&#21521;Minecraft&#35270;&#39057;&#28216;&#25103;&#30340;&#35282;&#33394;&#32441;&#29702;&#29983;&#25104;&#31995;&#32479;"Minecraft-ify"&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#38024;&#23545;&#20855;&#26377;&#31435;&#26041;&#20307;&#27969;&#24418;&#30340;3D&#34394;&#25311;&#35282;&#33394;&#30340;&#38754;&#37096;&#32858;&#28966;&#22270;&#20687;&#20197;&#36827;&#34892;&#32441;&#29702;&#26144;&#23556;&#12290;&#19982;&#29616;&#26377;&#39033;&#30446;&#25110;&#20316;&#21697;&#21482;&#29983;&#25104;&#32441;&#29702;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#21453;&#36716;&#29992;&#25143;&#25552;&#20379;&#30340;&#30495;&#23454;&#22270;&#20687;&#65292;&#25110;&#20174;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#29983;&#25104;&#24179;&#22343;/&#38543;&#26426;&#22806;&#35266;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;StyleGAN&#21644;StyleCLIP&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#25805;&#20316;&#12290;&#36825;&#20123;&#21151;&#33021;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#26356;&#22810;&#30340;&#33258;&#30001;&#65292;&#26159;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.03246</link><description>&lt;p&gt;
SGS-SLAM&#65306;&#22522;&#20110;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03246
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29702;&#35299;&#22312;&#31264;&#23494;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30340;&#22330;&#26223;&#35299;&#26512;&#12290;&#26368;&#36817;&#23558;&#39640;&#26031;&#28857;&#20113;&#38598;&#25104;&#21040;SLAM&#31995;&#32479;&#20013;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#30340;&#19977;&#32500;&#39640;&#26031;&#34920;&#31034;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGS-SLAM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;&#35270;&#35273;SLAM&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#65292;&#36824;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#24314;&#22270;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#36890;&#36947;&#20248;&#21270;&#65292;&#23558;&#22806;&#35266;&#12289;&#20960;&#20309;&#21644;&#35821;&#20041;&#32422;&#26463;&#19982;&#20851;&#38190;&#24103;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SGS-SLAM&#22312;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12289;&#22320;&#22270;&#37325;&#24314;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02733</link><description>&lt;p&gt;
ToonAging: &#33402;&#26415;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#19979;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;
&lt;/p&gt;
&lt;p&gt;
ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#38454;&#27573;&#26041;&#27861;&#65292;&#32467;&#21512;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#23454;&#29616;&#20154;&#33080;&#36870;&#40836;&#21270;&#65292;&#35299;&#20915;&#20102;NPR&#22270;&#20687;&#19978;&#32534;&#36753;&#24180;&#40836;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#25191;&#34892;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#24182;&#19988;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#38754;&#37096;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#36870;&#40836;&#21270;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#65292;&#22312;&#30005;&#24433;&#12289;&#24191;&#21578;&#21644;&#30452;&#25773;&#31561;&#36924;&#30495;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#33080;&#36870;&#40836;&#21270;&#24212;&#29992;&#20110;&#38750;&#36924;&#30495;&#22270;&#20687;&#65292;&#22914;&#28459;&#30011;&#12289;&#25554;&#22270;&#21644;&#21160;&#30011;&#65292;&#22312;&#21508;&#31181;&#23089;&#20048;&#34892;&#19994;&#20013;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26080;&#32541;&#32534;&#36753;NPR&#22270;&#20687;&#19978;&#26174;&#29616;&#24180;&#40836;&#30340;&#32593;&#32476;&#24847;&#21619;&#30528;&#36825;&#20123;&#20219;&#21153;&#19968;&#30452;&#23616;&#38480;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#39034;&#24207;&#26041;&#27861;&#65292;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#19981;&#24841;&#24555;&#30340;&#20266;&#24433;&#21644;&#30001;&#20110;&#22495;&#24046;&#24322;&#32780;&#20002;&#22833;&#38754;&#37096;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#38454;&#27573;&#20154;&#33080;&#36870;&#40836;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32918;&#20687;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#19968;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#23436;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20154;&#33080;&#36870;&#40836;&#21270;&#21644;&#39118;&#26684;&#36716;&#25442;&#32593;&#32476;&#65292;&#20004;&#32773;&#37117;&#22312;&#30456;&#21516;&#30340;PR&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#34701;&#21512;&#20102;&#19981;&#21516;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#27599;&#20010;&#21521;&#37327;&#36127;&#36131;&#31649;&#29702;&#19982;&#34928;&#32769;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02564</link><description>&lt;p&gt;
&#19968;&#20010;&#30495;&#27491;&#32852;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#29992;&#20110;&#20998;&#21106;&#21644;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Truly Joint Neural Architecture for Segmentation and Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#65292;&#20445;&#30041;&#20102;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20197;&#24448;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22810;&#35821;&#35328;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#21487;&#20197;&#35299;&#26512;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#20854;&#24615;&#33021;&#26126;&#26174;&#20302;&#20110;&#20854;&#20182;&#35821;&#35328;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#30001;&#20110;&#36755;&#20837;&#26631;&#35760;&#30340;&#24418;&#24577;&#22797;&#26434;&#24615;&#21644;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#20316;&#20026;&#26641;&#20013;&#33410;&#28857;&#30340;&#35821;&#35328;&#21333;&#20301;&#20107;&#20808;&#26159;&#26410;&#30693;&#30340;&#12290;&#20197;&#24448;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24577;&#20016;&#23500;&#35821;&#35328;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#36981;&#24490;&#32852;&#21512;&#24418;&#24577;-&#21477;&#27861;&#20551;&#35774;&#65292;&#21363;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#24212;&#35813;&#22312;&#35299;&#26512;&#36807;&#31243;&#20013;&#19968;&#24182;&#35299;&#20915;&#65292;&#32780;&#19981;&#26159;&#20808;&#36827;&#34892;&#20998;&#21106;&#20877;&#36827;&#34892;&#35299;&#26512;&#30340;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22120;&#37319;&#29992;&#20005;&#26684;&#30340;&#27969;&#27700;&#32447;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;&#26684;&#23376;&#30340;&#34920;&#31034;&#27861;&#20445;&#30041;&#36755;&#20837;&#30340;&#25152;&#26377;&#24418;&#24577;&#27169;&#31946;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#25552;&#20379;&#32473;&#19968;&#20010;&#22522;&#20110;&#24359;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#35299;&#20915;&#24418;&#24577;&#20998;&#21106;&#21644;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#24076;&#20271;&#26469;&#35821;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#35821;&#35328;&#24418;&#24577;&#20016;&#23500;&#19988;&#27169;&#31946;&#24615;&#36739;&#39640;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;</title><link>https://arxiv.org/abs/2402.02042</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDP&#36827;&#34892;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#30740;&#31350;&#20855;&#26377;&#36890;&#29992;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#24179;&#22343;&#22238;&#25253;CMDP&#30340;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#30830;&#20445;&#20302;&#36951;&#25022;&#20445;&#35777;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#19978;&#20855;&#26377; $\tilde{\mathcal{O}}({T}^{3/4})$ &#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.18018</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#23454;&#29616;&#30340;&#23433;&#20840;&#25552;&#31034;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Prompt-Driven LLM Safeguarding via Directed Representation Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21457;&#29616;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#24694;&#24847;&#21644;&#26080;&#23475;&#26597;&#35810;&#20043;&#38388;&#30340;&#21306;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;&#23433;&#20840;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#65292;&#20351;&#29992;&#23433;&#20840;&#25552;&#31034;&#22312;&#27169;&#22411;&#36755;&#20837;&#20043;&#21069;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#23454;&#36341;&#65292;&#20197;&#20351;&#20854;&#19981;&#36981;&#20174;&#21253;&#21547;&#24694;&#24847;&#24847;&#22270;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#25552;&#31034;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#36825;&#22952;&#30861;&#20102;&#33258;&#21160;&#20248;&#21270;&#20854;&#20197;&#25913;&#21892;LLM&#23433;&#20840;&#24615;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#34920;&#31034;&#30340;&#35282;&#24230;&#35843;&#26597;&#20102;&#23433;&#20840;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#26377;&#23475;&#21644;&#26080;&#23475;&#30340;&#26597;&#35810;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21306;&#20998;&#24320;&#26469;&#65292;&#20294;&#23433;&#20840;&#25552;&#31034;&#24182;&#27809;&#26377;&#26126;&#26174;&#22686;&#24378;&#36825;&#19968;&#21306;&#20998;&#12290;&#30456;&#21453;&#65292;&#19981;&#21516;&#23433;&#20840;&#25552;&#31034;&#23548;&#33268;&#26597;&#35810;&#30340;&#34920;&#31034;&#26397;&#30528;&#30456;&#20284;&#30340;&#26041;&#21521;&#31227;&#21160;&#65292;&#20351;&#24471;&#27169;&#22411;&#21363;&#20351;&#22312;&#26597;&#35810;&#26080;&#23475;&#26102;&#20063;&#26356;&#23481;&#26131;&#25298;&#32477;&#25552;&#20379;&#21327;&#21161;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRO&#65288;&#23450;&#21521;&#34920;&#31034;&#20248;&#21270;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23433;&#20840;&#25552;&#31034;&#20248;&#21270;&#12290;DRO&#23558;&#23433;&#20840;&#25552;&#31034;&#35270;&#20026;&#35201;&#20248;&#21270;&#30340;&#34920;&#31034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.06915</link><description>&lt;p&gt;
DocFinQA&#65306;&#19968;&#20010;&#38271;&#25991;&#26412;&#36130;&#21153;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DocFinQA: A Long-Context Financial Reasoning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06915
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#65292;&#23558;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;700&#20010;&#35789;&#25193;&#23637;&#21040;123k&#20010;&#35789;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#37329;&#34701;&#39046;&#22495;&#21457;&#25381;&#20316;&#29992;&#65292;&#38656;&#35201;&#30740;&#31350;&#29616;&#23454;&#20219;&#21153;&#21644;&#25968;&#25454;&#12290;&#37329;&#34701;&#19987;&#19994;&#20154;&#22763;&#32463;&#24120;&#19982;&#38271;&#36798;&#25968;&#30334;&#39029;&#30340;&#25991;&#26723;&#36827;&#34892;&#20132;&#20114;&#65292;&#20294;&#22823;&#22810;&#25968;&#37329;&#34701;&#30740;&#31350;&#25968;&#25454;&#38598;&#20165;&#22788;&#29702;&#36825;&#20123;&#25991;&#26723;&#30340;&#31616;&#30701;&#25688;&#24405;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38271;&#25991;&#26723;&#36130;&#21153;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;FinQA&#25968;&#25454;&#38598;&#20013;&#30340;7,437&#20010;&#38382;&#39064;&#20013;&#22686;&#21152;&#23436;&#25972;&#25991;&#26723;&#19978;&#19979;&#25991;&#65292;&#23558;FinQA&#20013;&#24179;&#22343;&#19978;&#19979;&#25991;&#38271;&#24230;&#20174;&#19981;&#21040;700&#20010;&#35789;&#25193;&#23637;&#21040;DocFinQA&#20013;&#30340;123k&#20010;&#35789;&#12290;&#25105;&#20204;&#22312;&#26816;&#32034;&#24335;QA&#31649;&#36947;&#21644;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#65292;DocFinQA&#20063;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;DocFinQA&#20013;&#26368;&#38271;&#25991;&#26723;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#36825;&#20123;&#25991;&#26723;&#19978;&#29305;&#21035;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06915v2 Announce Type: replace-cross  Abstract: For large language models (LLMs) to be effective in the financial domain -- where each decision can have a significant impact -- it is necessary to investigate realistic tasks and data. Financial professionals often interact with documents that are hundreds of pages long, but most financial research datasets only deal with short excerpts from these documents. To address this, we introduce a long-document financial QA task. We augment 7,437 questions from the existing FinQA dataset with the full-document context, extending the average context length from under 700 words in FinQA to 123k words in DocFinQA. We conduct extensive experiments over retrieval-based QA pipelines and long-context language models. DocFinQA proves a significant challenge for even state-of-the-art systems. We also provide a case-study on the longest documents in DocFinQA and find that models particularly struggle on these documents. Addressing these challen
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010; Hessian &#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026; Hessian &#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#65292;&#35299;&#20915;&#20102; PINNs &#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454; PDE &#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.14499</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#39640;&#38454;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340; Hutchinson &#36857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14499
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010; Hessian &#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026; Hessian &#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#65292;&#35299;&#20915;&#20102; PINNs &#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454; PDE &#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14499v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#20195;&#20132;&#21449; &#25688;&#35201;&#65306;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#19968;&#20123;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#25968;&#25454;&#21644;&#29289;&#29702;&#23398;&#12290;&#28982;&#32780;&#65292;&#23558;PINNs&#25193;&#23637;&#21040;&#39640;&#32500;&#29978;&#33267;&#39640;&#38454;PDE&#22312;&#33258;&#21160;&#24494;&#20998;&#22312;&#27531;&#24046;&#25439;&#22833;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Hutchinson &#36857;&#20272;&#35745;&#65288;HTE&#65289;&#26469;&#35299;&#20915;PINNs&#22788;&#29702;&#39640;&#32500;&#21644;&#39640;&#38454;PDE&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#31185;&#23398;&#35745;&#31639;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20108;&#38454;&#39640;&#32500;PDE&#20837;&#25163;&#65292;HTE&#23558;&#25972;&#20010;Hessian&#30697;&#38453;&#30340;&#35745;&#31639;&#36716;&#25442;&#20026;Hessian&#30690;&#37327;&#20056;&#31215;&#65288;HVP&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807; Taylor &#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#20943;&#36731;&#20102;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#23558;&#20869;&#23384;&#28040;&#32791;&#20174;Hessian&#30697;&#38453;&#20943;&#23569;&#21040;HVP&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;HTE&#25910;&#25947;&#21040;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14499v2 Announce Type: replace-cross  Abstract: Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation (HTE). Starting with the second-order high-dimensional PDEs ubiquitous in scientific computing, HTE transforms the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach alleviates the computational bottleneck via Taylor-mode automatic differentiation and significantly reduces memory consumption from the Hessian matrix to HVP. We further showcase HTE's convergence to the or
&lt;/p&gt;</description></item><item><title>LCNet&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#23384;&#20648;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#21151;&#33021;&#65307;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#25968;&#25454;&#20013;&#30340;&#35838;&#31243;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.08519</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21327;&#35843;&#20849;&#20139;&#19982;&#29305;&#23450;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#28508;&#22312;&#22240;&#26524;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reconciling Shared versus Context-Specific Information in a Neural Network Model of Latent Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08519
&lt;/p&gt;
&lt;p&gt;
LCNet&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#23398;&#20064;&#23384;&#20648;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#20351;&#29992;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#32467;&#26500;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#21151;&#33021;&#65307;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#25968;&#25454;&#20013;&#30340;&#35838;&#31243;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#65292;&#24403;&#22788;&#29702;&#19968;&#31995;&#21015;&#20107;&#20214;&#26102;&#65292;&#20154;&#31867;&#20250;&#26681;&#25454;&#25512;&#26029;&#30340;&#28508;&#22312;&#22240;&#26524;&#65288;LCs&#65289;&#26469;&#21010;&#20998;&#20182;&#20204;&#30340;&#32463;&#39564;&#65292;&#20197;&#25903;&#25345;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24403;&#20849;&#20139;&#32467;&#26500;&#23384;&#22312;&#20110;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#26102;&#65292;&#22914;&#20309;&#21516;&#26102;&#23454;&#29616;LCs&#30340;&#8220;&#20998;&#35010;&#8221;&#21644;&#23398;&#20064;&#20849;&#20139;&#32467;&#26500;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#22240;&#26524;&#32593;&#32476;&#65288;LCNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;LC&#25512;&#26029;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#23398;&#20064;&#65292;&#23427;&#33258;&#28982;&#22320;&#20648;&#23384;&#32593;&#32476;&#26435;&#37325;&#20013;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#19978;&#19979;&#25991;&#27169;&#22359;&#34920;&#31034;&#29305;&#23450;&#19978;&#19979;&#25991;&#32467;&#26500;&#65292;&#30001;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#25512;&#29702;&#31639;&#27861;&#25511;&#21046;&#65292;&#20026;&#27599;&#20010;&#25512;&#26029;&#30340;LC&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;&#36890;&#36807;&#19977;&#20010;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LCNet&#33021;&#22815;1)&#22312;&#21151;&#33021;&#23398;&#20064;&#20219;&#21153;&#20013;&#25552;&#21462;&#36328;LC&#30340;&#20849;&#20139;&#32467;&#26500;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;2)&#25429;&#25417;&#20851;&#20110;&#35838;&#31243;&#25928;&#24212;&#30340;&#20154;&#31867;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08519v2 Announce Type: replace-cross  Abstract: It has been proposed that, when processing a stream of events, humans divide their experiences in terms of inferred latent causes (LCs) to support context-dependent learning. However, when shared structure is present across contexts, it is still unclear how the "splitting" of LCs and learning of shared structure can be simultaneously achieved. Here, we present the Latent Cause Network (LCNet), a neural network model of LC inference. Through learning, it naturally stores structure that is shared across tasks in the network weights. Additionally, it represents context-specific structure using a context module, controlled by a Bayesian nonparametric inference algorithm, which assigns a unique context vector for each inferred LC. Across three simulations, we found that LCNet could 1) extract shared structure across LCs in a function learning task while avoiding catastrophic interference, 2) capture human data on curriculum effects 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#36741;&#21161;&#35270;&#35273;&#27169;&#22411;&#35843;&#35797;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#32780;&#19981;&#26159;&#22270;&#20687;&#26469;&#35786;&#26029;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#65292;&#36890;&#36807;&#36830;&#25509;CLIP&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#20986;&#38169;&#35270;&#35273;&#27169;&#22411;&#65292;&#20197;&#21450;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;&#20998;&#25903;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#26469;&#21457;&#29616;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2312.05588</link><description>&lt;p&gt;
&#35821;&#35328;&#36741;&#21161;&#35270;&#35273;&#27169;&#22411;&#35843;&#35797;&#22120;&#65306;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#21457;&#29616;&#21644;&#20462;&#22797;&#38169;&#35823;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding and Fixing Bugs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#36741;&#21161;&#35270;&#35273;&#27169;&#22411;&#35843;&#35797;&#26041;&#27861;&#65292;&#21033;&#29992;&#25991;&#26412;&#32780;&#19981;&#26159;&#22270;&#20687;&#26469;&#35786;&#26029;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#65292;&#36890;&#36807;&#36830;&#25509;CLIP&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#20986;&#38169;&#35270;&#35273;&#27169;&#22411;&#65292;&#20197;&#21450;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;&#20998;&#25903;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#26469;&#21457;&#29616;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#39640;&#25972;&#20307;&#20934;&#30830;&#24615;&#30340;&#35270;&#35273;&#27169;&#22411;&#32463;&#24120;&#22312;&#29305;&#23450;&#24773;&#26223;&#20013;&#34920;&#29616;&#20986;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#35786;&#26029;&#35270;&#35273;&#27169;&#22411;&#30340;&#38169;&#35823;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#27880;&#37322;&#24037;&#20316;&#65288;&#20363;&#22914;&#20276;&#38543;&#27599;&#20010;CelebA&#26679;&#26412;&#30340;&#20016;&#23500;&#20803;&#25968;&#25454;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#36741;&#21161;&#35786;&#26029;&#26041;&#27861;&#65292;&#20854;&#20351;&#29992;&#25991;&#26412;&#32780;&#19981;&#26159;&#22270;&#20687;&#26469;&#35786;&#26029;&#22522;&#20110;&#22810;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#30340;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;CLIP&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#24453;&#35786;&#26029;&#30340;&#20986;&#38169;&#35270;&#35273;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65307;&#21516;&#26102;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#20998;&#31867;&#22120;&#21644;&#20174;CLIP&#30340;&#23884;&#20837;&#31354;&#38388;&#21040;&#36328;&#27169;&#24577;&#36716;&#31227;&#30340;&#21487;&#33021;&#24615;&#65292;CLIP&#30340;&#25991;&#26412;&#20998;&#25903;&#25104;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20986;&#38169;&#27169;&#22411;&#20013;&#25214;&#20986;&#38169;&#35823;&#12290;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#23545;&#37197;&#23545;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#33719;&#24471;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25991;&#38598;&#65292;&#36825;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05588v2 Announce Type: replace  Abstract: Vision models with high overall accuracy often exhibit systematic errors in specific scenarios, posing potential serious safety concerns. Diagnosing bugs of vision models is gaining increased attention, however traditional diagnostic approaches require annotation efforts (eg rich metadata accompanying each samples of CelebA). To address this issue,We propose a language-assisted diagnostic method that uses texts instead of images to diagnose bugs in vision models based on multi-modal models (eg CLIP). Our approach connects the embedding space of CLIP with the buggy vision model to be diagnosed; meanwhile, utilizing a shared classifier and the cross-modal transferability of embedding space from CLIP, the text-branch of CLIP become a proxy model to find bugs in the buggy model. The proxy model can classify texts paired with images. During the diagnosis, a Large Language Model (LLM) is employed to obtain task-relevant corpora, and this c
&lt;/p&gt;</description></item><item><title>MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.02436</link><description>&lt;p&gt;
MUFFIN: &#29992;&#20110;&#25913;&#21892;&#25351;&#31034;&#36981;&#24490;&#30340;&#22810;&#26041;&#38754;&#25351;&#21335;&#30340;&#31574;&#21010;
&lt;/p&gt;
&lt;p&gt;
MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02436
&lt;/p&gt;
&lt;p&gt;
MUFFIN&#26159;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;&#65292;&#36890;&#36807;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20219;&#21153;&#65292;&#36890;&#36807;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#20016;&#23500;&#22810;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#65292;&#21152;&#24378;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#31574;&#21010;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#31034;&#36981;&#24490;&#25968;&#25454;&#38598;&#31574;&#21010;&#26041;&#26696;MUFFIN&#65292;&#20855;&#20307;&#22320;&#36890;&#36807;&#29992;&#22810;&#31181;&#36755;&#20837;&#26041;&#38754;&#20351;&#20219;&#21153;&#33258;&#21160;&#25353;&#27604;&#20363;&#25193;&#22823;&#20197;&#20016;&#23500;&#36825;&#20123;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02436v2 Announce Type: replace-cross  Abstract: In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#65292;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#30340;&#31995;&#32479;&#28431;&#27934;&#65292;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#21487;&#20197;&#34987;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25805;&#32437;&#12290;</title><link>https://arxiv.org/abs/2311.16119</link><description>&lt;p&gt;
&#24573;&#30053;&#36825;&#20010;&#26631;&#39064;&#24182;HackAPrompt&#65306;&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#25581;&#31034;LLMs&#30340;&#31995;&#32479;&#24615;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20840;&#29699;&#35268;&#27169;&#30340;Prompt Hacking&#31454;&#36187;&#65292;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#30340;&#31995;&#32479;&#28431;&#27934;&#65292;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#21487;&#20197;&#34987;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#30452;&#25509;&#19982;&#29992;&#25143;&#20114;&#21160;&#30340;&#24773;&#22659;&#20013;&#65292;&#20363;&#22914;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20889;&#20316;&#21161;&#25163;&#12290;&#36825;&#20123;&#37096;&#32626;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#21644;&#36234;&#29425;&#65288;&#32479;&#31216;&#20026;Prompt Hacking&#65289;&#30340;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#34987;&#25805;&#32437;&#20197;&#24573;&#30053;&#20854;&#21407;&#22987;&#25351;&#20196;&#24182;&#36981;&#24490;&#21487;&#33021;&#24694;&#24847;&#30340;&#25351;&#20196;&#12290;&#34429;&#28982;&#24191;&#20026;&#20154;&#30693;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#20294;&#20851;&#20110;Prompt Hacking&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#21644;&#23450;&#37327;&#30740;&#31350;&#30340;&#36164;&#26009;&#21294;&#20047;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21457;&#36215;&#20102;&#19968;&#22330;&#20840;&#29699;Prompt Hacking&#31454;&#36187;&#65292;&#20801;&#35768;&#33258;&#30001;&#24418;&#24335;&#30340;&#20154;&#31867;&#36755;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#25628;&#38598;&#20102;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#21457;&#36215;&#30340;&#36229;&#36807;60&#19975;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#25551;&#36848;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#24403;&#21069;LLMs&#30830;&#23454;&#21487;&#20197;&#36890;&#36807;Prompt Hacking&#34987;&#25805;&#32437;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#31867;&#22411;&#30340;&#20840;&#38754;&#20998;&#31867;&#26412;&#20307;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16119v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.10329</link><description>&lt;p&gt;
&#39640;&#20445;&#30495;&#24230;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Person-centric Subject-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20197;&#20027;&#20307;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#29983;&#25104;&#20013;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#26469;&#23398;&#20064;&#35821;&#20041;&#22330;&#26223;&#21644;&#20154;&#29289;&#29983;&#25104;&#65292;&#36825;&#28041;&#21450;&#21040;&#19968;&#31181;&#26080;&#27861;&#35843;&#21644;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Face-diffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#28040;&#38500;&#19978;&#36848;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10329v3 Announce Type: replace-cross  Abstract: Current subject-driven image generation methods encounter significant challenges in person-centric image generation. The reason is that they learn the semantic scene and person generation by fine-tuning a common pre-trained diffusion, which involves an irreconcilable training imbalance. Precisely, to generate realistic persons, they need to sufficiently tune the pre-trained model, which inevitably causes the model to forget the rich semantic scene prior and makes scene generation over-fit to the training data. Moreover, even with sufficient fine-tuning, these methods can still not generate high-fidelity persons since joint learning of the scene and person generation also lead to quality compromise. In this paper, we propose Face-diffuser, an effective collaborative generation pipeline to eliminate the above training imbalance and quality compromise. Specifically, we first develop two specialized pre-trained diffusion models, i.
&lt;/p&gt;</description></item><item><title>MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;</title><link>https://arxiv.org/abs/2311.09033</link><description>&lt;p&gt;
MELA&#65306;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELA: Multilingual Evaluation of Linguistic Acceptability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09033
&lt;/p&gt;
&lt;p&gt;
MELA&#26159;&#31532;&#19968;&#20010;&#35206;&#30422;10&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;XLM-R&#30340;&#24494;&#35843;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#22256;&#38590;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#38754;ChatGPT&#34920;&#29616;&#33391;&#22909;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#24212;&#29992;&#39537;&#21160;&#30340;&#20219;&#21153;&#65292;&#22914;&#22797;&#26434;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#19978;&#65292;&#23548;&#33268;LLMs&#30340;&#32431;&#35821;&#35328;&#35780;&#20272;&#20005;&#37325;&#19981;&#36275;&#12290;&#38024;&#23545;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multilingual Evaluation of Linguistic Acceptability&#65288;MELA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;10&#31181;&#35821;&#35328;&#12289;&#20849;48K&#20010;&#26679;&#26412;&#30340;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#22810;&#35821;&#35328;&#22522;&#20934;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#24120;&#29992;LLMs&#21644;&#30417;&#30563;&#27169;&#22411;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;XLM-R&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#39564;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#35821;&#35328;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24494;&#35843;&#21518;&#30340;XLM-R&#30340;&#26435;&#37325;&#65292;&#25506;&#35752;&#20102;&#35782;&#21035;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36801;&#31227;&#22256;&#38590;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21463;&#30410;&#33391;&#22810;&#65292;&#20294;&#20173;&#33853;&#21518;&#20110;&#32463;&#36807;&#24494;&#35843;&#30340;XLM-R&#65292;&#32780;GPT-4&#30340;&#24615;&#33021;&#19982;&#20043;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09033v2 Announce Type: replace-cross  Abstract: Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with f
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#35774;&#35745;&#20855;&#26377;&#20219;&#24847;&#22806;&#37096;&#21644;&#20869;&#37096;&#32467;&#26500;&#30340;&#33258;&#30001;&#24418;&#24577;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#25918;&#32622;&#25110;&#31227;&#38500;&#21407;&#23376;&#24314;&#31569;&#22359;&#26463;&#24418;&#25104;&#39640;&#32423;&#38750;&#21442;&#25968;&#23439;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2310.05670</link><description>&lt;p&gt;
&#24377;&#24615;&#23398;&#20064;&#29992;&#20110;&#33258;&#30001;&#24418;&#24577;&#26426;&#22120;&#20154;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning for freeform robot design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05670
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#35774;&#35745;&#20855;&#26377;&#20219;&#24847;&#22806;&#37096;&#21644;&#20869;&#37096;&#32467;&#26500;&#30340;&#33258;&#30001;&#24418;&#24577;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#25918;&#32622;&#25110;&#31227;&#38500;&#21407;&#23376;&#24314;&#31569;&#22359;&#26463;&#24418;&#25104;&#39640;&#32423;&#38750;&#21442;&#25968;&#23439;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#21160;&#29289;&#24418;&#24577;&#36866;&#24212;&#24615;&#30340;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25193;&#23637;&#26426;&#22120;&#20154;&#35757;&#32451;&#65292;&#28085;&#30422;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#29289;&#29702;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#20248;&#21270;&#26426;&#22120;&#20154;3D&#24418;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#23616;&#38480;&#20110;&#37325;&#26032;&#23450;&#20301;&#25110;&#35843;&#25972;&#39044;&#23450;&#21644;&#38745;&#24577;&#25299;&#25169;&#23646;&#30340;&#32930;&#20307;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#20219;&#24847;&#22806;&#37096;&#21644;&#20869;&#37096;&#32467;&#26500;&#30340;&#33258;&#30001;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#36890;&#36807;&#25918;&#32622;&#25110;&#31227;&#38500;&#21407;&#23376;&#24314;&#31569;&#22359;&#26463;&#26469;&#24418;&#25104;&#39640;&#32423;&#38750;&#21442;&#25968;&#23439;&#32467;&#26500;&#65292;&#22914;&#38468;&#32930;&#12289;&#22120;&#23448;&#21644;&#33108;&#23460;&#12290;&#23613;&#31649;&#20165;&#25552;&#20379;&#20102;&#24320;&#29615;&#25511;&#21046;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#22312;&#26410;&#26469;&#36866;&#29992;&#20110;&#38381;&#29615;&#25511;&#21046;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#26426;&#22120;&#20154;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05670v2 Announce Type: replace-cross  Abstract: Inspired by the necessity of morphological adaptation in animals, a growing body of work has attempted to expand robot training to encompass physical aspects of a robot's design. However, reinforcement learning methods capable of optimizing the 3D morphology of a robot have been restricted to reorienting or resizing the limbs of a predetermined and static topological genus. Here we show policy gradients for designing freeform robots with arbitrary external and internal structure. This is achieved through actions that deposit or remove bundles of atomic building blocks to form higher-level nonparametric macrostructures such as appendages, organs and cavities. Although results are provided for open loop control only, we discuss how this method could be adapted for closed loop control and sim2real transfer to physical machines in future.
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;</title><link>https://arxiv.org/abs/2310.02207</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#31354;&#38388;&#21644;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Language Models Represent Space and Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02207
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20016;&#23500;&#30340;&#26102;&#31354;&#34920;&#24449;&#65292;&#21253;&#25324;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#20197;&#21450;&#20010;&#20307;&#30340;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#20123;&#31995;&#32479;&#21040;&#24213;&#26159;&#20165;&#20165;&#23398;&#20064;&#20102;&#24222;&#22823;&#30340;&#34920;&#38754;&#32479;&#35745;&#20449;&#24687;&#36824;&#26159;&#23398;&#21040;&#20102;&#26356;&#36830;&#36143;&#12289;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#24449;&#30340;&#20105;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;Llama-2&#31995;&#21015;&#27169;&#22411;&#20013;&#23398;&#21040;&#30340;&#19977;&#20010;&#31354;&#38388;&#25968;&#25454;&#38598;&#65288;&#19990;&#30028;&#12289;&#32654;&#22269;&#12289;&#32445;&#32422;&#30340;&#22320;&#28857;&#65289;&#21644;&#19977;&#20010;&#26102;&#38388;&#25968;&#25454;&#38598;&#65288;&#21382;&#21490;&#20154;&#29289;&#12289;&#33402;&#26415;&#21697;&#12289;&#26032;&#38395;&#22836;&#26465;&#65289;&#30340;&#23398;&#20064;&#34920;&#24449;&#25214;&#21040;&#20102;&#25903;&#25345;&#21518;&#32773;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#23398;&#20064;&#21040;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#32447;&#24615;&#34920;&#24449;&#12290;&#36825;&#20123;&#34920;&#24449;&#23545;&#25552;&#31034;&#21464;&#21270;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#65288;&#20363;&#22914;&#22478;&#24066;&#21644;&#22320;&#26631;&#65289;&#20043;&#38388;&#26159;&#32479;&#19968;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21487;&#38752;&#22320;&#32534;&#30721;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#30340;&#20010;&#20307;&#8220;&#31354;&#38388;&#31070;&#32463;&#20803;&#8221;&#21644;&#8220;&#26102;&#38388;&#31070;&#32463;&#20803;&#8221;&#12290;&#34429;&#28982;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#29616;&#20195;LLM&#23398;&#20064;&#21040;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#26102;&#31354;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#32437;&#22120;&#30340;&#20027;&#21160;&#24863;&#30693;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29983;&#25104;&#23545;&#25805;&#32437;&#20219;&#21153;&#26377;&#20449;&#24687;&#24615;&#30340;&#36816;&#21160;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35270;&#35273;&#20449;&#24687;&#22686;&#30410;&#21644;&#20219;&#21153;&#30446;&#26631;&#30340;&#26435;&#34913;&#26469;&#25552;&#39640;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.00433</link><description>&lt;p&gt;
&#31227;&#21160;&#25805;&#32437;&#30340;&#20027;&#21160;&#24863;&#30693;&#36816;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active-Perceptive Motion Generation for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00433
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31227;&#21160;&#25805;&#32437;&#22120;&#30340;&#20027;&#21160;&#24863;&#30693;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#29983;&#25104;&#23545;&#25805;&#32437;&#20219;&#21153;&#26377;&#20449;&#24687;&#24615;&#30340;&#36816;&#21160;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35270;&#35273;&#20449;&#24687;&#22686;&#30410;&#21644;&#20219;&#21153;&#30446;&#26631;&#30340;&#26435;&#34913;&#26469;&#25552;&#39640;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#25805;&#32437;&#31995;&#32479;&#32467;&#21512;&#20102;&#26426;&#21160;&#24615;&#21644;&#28789;&#24039;&#24615;&#30340;&#20248;&#28857;&#65292;&#30001;&#20110;&#23427;&#20204;&#21487;&#20197;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#20013;&#31227;&#21160;&#24182;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#37197;&#22791;&#20102;&#26426;&#36733;&#20256;&#24863;&#22120;&#65292;&#20363;&#22914;&#20855;&#26377;&#23454;&#20307;&#30456;&#26426;&#30340;&#31227;&#21160;&#25805;&#32437;&#31995;&#32479;&#65292;&#22312;&#26080;&#32467;&#26500;&#21644;&#26434;&#20081;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#20363;&#22914;&#23478;&#24237;&#29615;&#22659;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20027;&#21160;&#24863;&#30693;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#31227;&#21160;&#25805;&#32437;&#22120;&#29983;&#25104;&#23545;&#25805;&#32437;&#20219;&#21153;&#26377;&#20449;&#24687;&#24615;&#30340;&#36816;&#21160;&#65292;&#20363;&#22914;&#22312;&#26410;&#30693;&#30340;&#26434;&#20081;&#22330;&#26223;&#20013;&#25235;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861; ActPerMoMa &#36890;&#36807;&#23545;&#36335;&#24452;&#36827;&#34892;&#37319;&#26679;&#21644;&#35745;&#31639;&#36335;&#24452;&#30456;&#20851;&#30340;&#25928;&#29992;&#65292;&#22312;&#28369;&#21160;&#22320;&#24179;&#32447;&#30340;&#26041;&#24335;&#19979;&#29983;&#25104;&#26426;&#22120;&#20154;&#36335;&#24452;&#12290;&#36825;&#20123;&#25928;&#29992;&#26435;&#34913;&#20102;&#26368;&#22823;&#21270;&#35270;&#35273;&#20449;&#24687;&#22686;&#30410;&#65288;IG&#65289;&#29992;&#20110;&#22330;&#26223;&#37325;&#24314;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#30446;&#30340;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#25235;&#21462;&#21487;&#36798;&#24615;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00433v2 Announce Type: replace-cross  Abstract: Mobile Manipulation (MoMa) systems incorporate the benefits of mobility and dexterity, due to the enlarged space in which they can move and interact with their environment. However, even when equipped with onboard sensors, e.g., an embodied camera, extracting task-relevant visual information in unstructured and cluttered environments, such as households, remains challenging. In this work, we introduce an active perception pipeline for mobile manipulators to generate motions that are informative toward manipulation tasks, such as grasping in unknown, cluttered scenes. Our proposed approach, ActPerMoMa, generates robot paths in a receding horizon fashion by sampling paths and computing path-wise utilities. These utilities trade-off maximizing the visual Information Gain (IG) for scene reconstruction and the task-oriented objective, e.g., grasp success, by maximizing grasp reachability. We show the efficacy of our method in simula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATNet&#65292;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#20809;&#20239;&#21457;&#30005;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;</title><link>https://arxiv.org/abs/2306.10356</link><description>&lt;p&gt;
MATNet: &#22810;&#32423;&#34701;&#21512;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MATNet: Multi-Level Fusion Transformer-Based Model for Day-Ahead PV Generation Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10356
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATNet&#65292;&#32467;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#20809;&#20239;&#21457;&#30005;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#23545;&#20419;&#36827;&#21487;&#20877;&#29983;&#33021;&#28304;&#25972;&#21512;&#21040;&#30005;&#21147;&#31995;&#32479;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#38024;&#23545;&#20809;&#20239;&#21333;&#20803;&#65292;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#21487;&#20998;&#20026;&#22522;&#20110;&#29289;&#29702;&#21644;&#22522;&#20110;&#25968;&#25454;&#30340;&#31574;&#30053;&#20004;&#31867;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#29616;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#22810;&#20803;&#22810;&#27493;&#26085;&#21069;&#20809;&#20239;&#21457;&#30005;&#39044;&#27979;&#12290;&#23427;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#20809;&#20239;&#21457;&#30005;&#30340;&#20808;&#39564;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22810;&#32423;&#32852;&#21512;&#34701;&#21512;&#26041;&#27861;&#36755;&#20837;&#21382;&#21490;&#20809;&#20239;&#25968;&#25454;&#20197;&#21450;&#21382;&#21490;&#21644;&#39044;&#27979;&#22825;&#27668;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10356v2 Announce Type: replace-cross  Abstract: Accurate forecasting of renewable generation is crucial to facilitate the integration of RES into the power system. Focusing on PV units, forecasting methods can be divided into two main categories: physics-based and data-based strategies, with AI-based models providing state-of-the-art performance. However, while these AI-based models can capture complex patterns and relationships in the data, they ignore the underlying physical prior knowledge of the phenomenon. Therefore, in this paper we propose MATNet, a novel self-attention transformer-based architecture for multivariate multi-step day-ahead PV power generation forecasting. It consists of a hybrid approach that combines the AI paradigm with the prior physical knowledge of PV power generation of physics-based methods. The model is fed with historical PV data and historical and forecast weather data through a multi-level joint fusion approach. The effectiveness of the propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREBI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2306.00603</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#23433;&#20840;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Offline Reinforcement Learning with Real-Time Budget Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREBI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#21644;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#25552;&#20379;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20419;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#65292;&#36817;&#24180;&#26469;&#23545;&#23433;&#20840;RL&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20173;&#19987;&#27880;&#20110;&#22312;&#32447;&#35774;&#32622;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#21457;&#29983;&#23545;&#23433;&#20840;&#39044;&#31639;&#30340;&#39118;&#38505;&#36829;&#35268;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#24471;&#31574;&#30053;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#21160;&#24577;&#30830;&#23450;&#30340;&#23433;&#20840;&#39044;&#31639;&#65288;&#21363;&#32422;&#26463;&#38408;&#20540;&#65289;&#12290;&#26412;&#25991;&#38024;&#23545;&#31163;&#32447;&#35774;&#32622;&#19979;&#30340;&#23454;&#26102;&#39044;&#31639;&#32422;&#26463;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#36712;&#36857;&#30340;&#23454;&#26102;&#39044;&#31639;&#25512;&#26029;&#65288;TREBI&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#20174;&#36712;&#36857;&#20998;&#24067;&#30340;&#35282;&#24230;&#23545;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#35268;&#21010;&#26469;&#35299;&#20915;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#23545;&#24773;&#33410;&#22870;&#21169;&#21644;&#25104;&#26412;&#30340;&#20272;&#35745;&#23384;&#22312;&#35823;&#24046;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00603v2 Announce Type: replace-cross  Abstract: Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that models this problem from the perspective of trajectory distribution and solves it through diffusion model planning. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance gua
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2212.03827</link><description>&lt;p&gt;
&#22312;&#19981;&#38656;&#35201;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Discovering Latent Knowledge in Language Models Without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#22238;&#31572;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#22810;&#26679;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#25216;&#26415;&#21487;&#33021;&#19982;&#30495;&#30456;&#19981;&#19968;&#33268;&#65306;&#22914;&#26524;&#25105;&#20204;&#29992;&#27169;&#20223;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#37325;&#29616;&#20154;&#31867;&#30340;&#38169;&#35823;&#65307;&#22914;&#26524;&#25105;&#20204;&#35757;&#32451;&#23427;&#20204;&#29983;&#25104;&#20154;&#31867;&#35780;&#20215;&#39640;&#30340;&#25991;&#26412;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#36755;&#20986;&#20154;&#31867;&#35780;&#20272;&#32773;&#26080;&#27861;&#26816;&#27979;&#21040;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#28608;&#27963;&#20013;&#30452;&#25509;&#21457;&#29616;&#28508;&#22312;&#30693;&#35782;&#30340;&#26041;&#24335;&#26469;&#35268;&#36991;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19988;&#26159;&#32431;&#31929;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#22238;&#31572;&#21482;&#32473;&#23450;&#26410;&#26631;&#35760;&#27169;&#22411;&#28608;&#27963;&#30340;&#26159;&#38750;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#25214;&#21040;&#28385;&#36275;&#36923;&#36753;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#26041;&#21521;&#26469;&#24037;&#20316;&#65292;&#20363;&#22914;&#19968;&#20010;&#38472;&#36848;&#21450;&#20854;&#21542;&#23450;&#20855;&#26377;&#30456;&#21453;&#30340;&#30495;&#20540;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#30417;&#30563;&#21644;&#27169;&#22411;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20195;&#34920;&#22810;&#26679;&#30693;&#35782;&#65306;&#22312;6&#20010;&#27169;&#22411;&#21644;10&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#27169;&#24335;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#26469;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2210.04870</link><description>&lt;p&gt;
SMiLE&#65306;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge Graph Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.04870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#27169;&#24335;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#26469;&#25552;&#39640;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#25512;&#26029;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#20043;&#38388;&#32570;&#22833;&#38142;&#25509;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#36890;&#36807;&#24314;&#27169;&#19977;&#20803;&#32452;&#20013;&#30340;&#20851;&#31995;&#27169;&#24335;&#22312;&#35299;&#20915;&#27492;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#23454;&#20307;&#37051;&#22495;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#21040;&#23427;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#20851;&#27880;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#23454;&#20307;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35748;&#20026;&#30693;&#35782;&#22270;&#35889;&#30340;&#27169;&#24335;&#21253;&#21547;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20445;&#25345;&#23454;&#20307;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#24335;&#22686;&#24378;&#30340;&#22810;&#23618;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;SMiLE&#65289;&#26469;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.04870v3 Announce Type: replace-cross  Abstract: Link prediction is the task of inferring missing links between entities in knowledge graphs. Embedding-based methods have shown effectiveness in addressing this problem by modeling relational patterns in triples. However, the link prediction task often requires contextual information in entity neighborhoods, while most existing embedding-based methods fail to capture it. Additionally, little attention is paid to the diversity of entity representations in different contexts, which often leads to false prediction results. In this situation, we consider that the schema of knowledge graph contains the specific contextual information, and it is beneficial for preserving the consistency of entities across contexts. In this paper, we propose a novel Schema-augmented Multi-level contrastive LEarning framework (SMiLE) to conduct knowledge graph link prediction. Specifically, we first exploit network schema as the prior constraint to sam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#23450;&#20215;&#29615;&#22659;&#20013;&#23547;&#25214;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#31574;&#30053;&#21644;&#26368;&#23567;&#21270;&#29366;&#24577;&#65292;&#36827;&#34892;&#32435;&#20160; Q &#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2207.06492</link><description>&lt;p&gt;
&#21160;&#24577;&#23450;&#20215;&#20013; n &#20154;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximate Nash Equilibrium Learning for n-Player Markov Games in Dynamic Pricing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21160;&#24577;&#23450;&#20215;&#29615;&#22659;&#20013;&#23547;&#25214;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#31574;&#30053;&#21644;&#26368;&#23567;&#21270;&#29366;&#24577;&#65292;&#36827;&#34892;&#32435;&#20160; Q &#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31454;&#20105;&#24615;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#23398;&#20064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20195;&#29702;&#31454;&#20105;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#32435;&#20160;&#22343;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#23545;&#20110;&#23521;&#22836;&#21160;&#24577;&#23450;&#20215;&#29615;&#22659;&#65292;&#30001;&#20110;&#32500;&#24230;&#28789;&#27963;&#24615;&#30340;&#38382;&#39064;&#65292;&#31934;&#30830;&#30340;&#32435;&#20160;&#22343;&#34913;&#38590;&#20197;&#33719;&#24471;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#23547;&#25214;&#36817;&#20284;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#26080;&#26799;&#24230;&#40657;&#30418;&#20248;&#21270;&#26469;&#20272;&#35745;$\epsilon$&#65292;&#21363;&#20195;&#29702;&#21333;&#26041;&#38754;&#20559;&#31163;&#20219;&#20309;&#32852;&#21512;&#31574;&#30053;&#30340;&#26368;&#22823;&#22870;&#21169;&#20248;&#21183;&#65292;&#24182;&#20272;&#35745;&#20219;&#20309;&#32473;&#23450;&#29366;&#24577;&#30340;$\epsilon$-&#26368;&#23567;&#21270;&#31574;&#30053;&#12290;&#25919;&#31574;-$\epsilon$&#23545;&#24212;&#21644;&#29366;&#24577;&#21040;$\epsilon$-&#26368;&#23567;&#21270;&#31574;&#30053;&#30001;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#21518;&#32773;&#20026;&#32435;&#20160;&#31574;&#30053;&#32593;&#32476;&#12290;&#22312;&#25209;&#37327;&#26356;&#26032;&#26399;&#38388;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#21160;&#20316;&#27010;&#29575;&#20351;&#29992;&#32435;&#20160;&#31574;&#30053;&#32593;&#32476;&#26469;&#25191;&#34892;&#32435;&#20160; Q &#23398;&#20064;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36817;&#20284;&#32435;&#20160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.06492v3 Announce Type: replace-cross  Abstract: We investigate Nash equilibrium learning in a competitive Markov Game (MG) environment, where multiple agents compete, and multiple Nash equilibria can exist. In particular, for an oligopolistic dynamic pricing environment, exact Nash equilibria are difficult to obtain due to the curse-of-dimensionality. We develop a new model-free method to find approximate Nash equilibria. Gradient-free black box optimization is then applied to estimate $\epsilon$, the maximum reward advantage of an agent unilaterally deviating from any joint policy, and to also estimate the $\epsilon$-minimizing policy for any given state. The policy-$\epsilon$ correspondence and the state to $\epsilon$-minimizing policy are represented by neural networks, the latter being the Nash Policy Net. During batch update, we perform Nash Q learning on the system, by adjusting the action probabilities using the Nash Policy Net. We demonstrate that an approximate Nash
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2204.05798</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#36229;&#22797;&#25968;&#23398;&#20064;&#29992;&#20110;&#20083;&#33146;&#30284;&#31579;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-View Hypercomplex Learning for Breast Cancer Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.05798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#27169;&#25311;&#24182;&#21033;&#29992;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32959;&#30244;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#20083;&#33146;&#30284;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25191;&#34892;&#21333;&#35270;&#22270;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20083;&#33146;X-ray&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#21516;&#26102;&#20998;&#26512;&#32452;&#25104;&#20083;&#25151;X&#20809;&#25668;&#24433;&#26816;&#26597;&#30340;&#25152;&#26377;&#22235;&#20010;&#35270;&#22270;&#65292;&#36825;&#20026;&#35782;&#21035;&#32959;&#30244;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25552;&#20986;&#22810;&#35270;&#22270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#26679;&#30340;&#29616;&#26377;&#26550;&#26500;&#20013;&#65292;&#20083;&#25151;X&#20809;&#22270;&#20687;&#34987;&#29420;&#31435;&#30340;&#21367;&#31215;&#20998;&#25903;&#22788;&#29702;&#20026;&#29420;&#31435;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#22833;&#21435;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#20083;&#33146;&#30284;&#20998;&#31867;&#26041;&#27861;&#12290;&#30001;&#20110;&#36229;&#22797;&#25968;&#20195;&#25968;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#24314;&#27169;&#24182;&#21033;&#29992;&#32452;&#25104;&#20083;&#25151;X&#20809;&#26816;&#26597;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#29616;&#26377;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#27169;&#25311;&#38405;&#29255;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.05798v3 Announce Type: replace-cross  Abstract: Traditionally, deep learning methods for breast cancer classification perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper, we propose a methodological approach for multi-view breast cancer classification based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram, thus mimicking the reading process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#35821;&#38899;&#22686;&#24378;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2203.15149</link><description>&lt;p&gt;
CMGAN&#65306;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;GAN&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
CMGAN: Conformer-based Metric GAN for Speech Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.15149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#35821;&#38899;&#22686;&#24378;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21367;&#31215;&#22686;&#24378;&#21464;&#21387;&#22120;&#65288;Conformer&#65289;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#26102;&#22495;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#25417;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#24230;&#37327;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CMGAN&#65289;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;SE&#12290;&#22312;&#29983;&#25104;&#22120;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#38454;&#27573;&#30340;Conformer&#22359;&#36890;&#36807;&#23545;&#26102;&#38388;&#21644;&#39057;&#29575;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#32858;&#21512;&#25152;&#26377;&#24133;&#24230;&#21644;&#22797;&#25968;&#35889;&#20449;&#24687;&#12290;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#65292;&#24133;&#24230;&#21644;&#22797;&#25968;&#35889;&#30340;&#20272;&#35745;&#34987;&#35299;&#32806;&#65292;&#28982;&#21518;&#19968;&#36215;&#21512;&#24182;&#20197;&#37325;&#26500;&#22686;&#24378;&#30340;&#35821;&#38899;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#20351;&#24471;&#22686;&#24378;&#20272;&#35745;&#35821;&#38899;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22686;&#24378;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;&#22312;Voice Bank+DEMAND&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.15149v4 Announce Type: replace-cross  Abstract: Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech enhancement (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27861;&#24459;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#35009;&#20915;&#31867;&#20284;&#26696;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35813;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2107.04771</link><description>&lt;p&gt;
&#20351;&#29992;&#27861;&#24459;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#31867;&#20284;&#26696;&#20363;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Similar Cases Recommendation using Legal Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.04771
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27861;&#24459;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#35009;&#20915;&#31867;&#20284;&#26696;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35813;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#27861;&#38498;&#26696;&#20363;&#12289;&#35009;&#20915;&#12289;&#27861;&#24459;&#21450;&#20854;&#20182;&#27861;&#24459;&#25991;&#20214;&#26500;&#24314;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#35889;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#65292;&#22914;&#38382;&#31572;&#12289;&#25991;&#26723;&#30456;&#20284;&#24230;&#21644;&#25628;&#32034;&#12290;&#23613;&#31649;&#30693;&#35782;&#22270;&#35889;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#36828;&#31243;&#30417;&#30563;&#24212;&#29992;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26696;&#20363;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36824;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#29992;&#20110;&#39044;&#27979;&#21360;&#24230;&#27861;&#38498;&#35009;&#20915;&#31867;&#20284;&#26696;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#35813;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2107.04771v2 Announce Type: replace  Abstract: A legal knowledge graph constructed from court cases, judgments, laws and other legal documents can enable a number of applications like question answering, document similarity, and search. While the use of knowledge graphs for distant supervision in NLP tasks is well researched, using knowledge graphs for applications like case similarity presents challenges. In this work, we describe our solution for predicting similar cases in Indian court judgements. We present our results and also discuss the impact of large language models on this task.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#21516;&#26102;&#36820;&#22238;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>https://arxiv.org/abs/2102.06202</link><description>&lt;p&gt;
&#31169;&#20154;&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Private Prediction Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.06202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#21516;&#26102;&#36820;&#22238;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#37325;&#35201;&#20915;&#31574;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#36825;&#20004;&#20010;&#30446;&#26631;&#21516;&#26102;&#35270;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#39044;&#27979;&#27169;&#22411;&#65292;&#36820;&#22238;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#38598;&#65292;&#36825;&#20123;&#38598;&#21512;&#21487;&#20197;&#35777;&#26126;&#20197;&#29992;&#25143;&#25351;&#23450;&#30340;&#27010;&#29575;&#65288;&#22914;90%&#65289;&#35206;&#30422;&#30495;&#23454;&#21709;&#24212;&#12290;&#24403;&#19982;&#32463;&#36807;&#31169;&#20154;&#35757;&#32451;&#30340;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#20154;&#20204;&#21487;&#33021;&#24076;&#26395;&#31526;&#21512;&#24615;&#39044;&#27979;&#20250;&#20026;&#29983;&#25104;&#30340;&#39044;&#27979;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65307;&#19981;&#24184;&#30340;&#26159;&#65292;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#36755;&#20986;&#24046;&#20998;&#31169;&#20154;&#39044;&#27979;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#25105;&#20204;&#20351;&#29992;&#20445;&#30041;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37197;&#22791;&#30528;&#20851;&#27880;&#27169;&#22359;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;&#22270;&#20687;&#20013;&#26356;&#22909;&#22320;&#21069;&#21015;&#33146;&#20998;&#21106;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#23618;&#32423;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#21069;&#21015;&#33146;&#20998;&#21106;&#24615;&#33021;</title><link>https://arxiv.org/abs/1907.01743</link><description>&lt;p&gt;
&#19977;&#32500;&#32463;&#30452;&#32928;&#36229;&#22768;&#28145;&#24230;&#20851;&#27880;&#29305;&#24449;&#22312;&#21069;&#21015;&#33146;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1907.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#37197;&#22791;&#30528;&#20851;&#27880;&#27169;&#22359;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;&#22270;&#20687;&#20013;&#26356;&#22909;&#22320;&#21069;&#21015;&#33146;&#20998;&#21106;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#25972;&#21512;&#19981;&#21516;&#23618;&#32423;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#21069;&#21015;&#33146;&#20998;&#21106;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:1907.01743v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;&#24230;  &#25688;&#35201;: &#22312;&#22270;&#20687;&#24341;&#23548;&#30340;&#21069;&#21015;&#33146;&#24178;&#39044;&#21644;&#27835;&#30103;&#35745;&#21010;&#20013;&#65292;&#33258;&#21160;&#21069;&#21015;&#33146;&#20998;&#21106;&#22312;&#32463;&#30452;&#32928;&#36229;&#22768;(TRUS)&#22270;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110; TRUS &#20013;&#21069;&#21015;&#33146;&#30340;&#36793;&#30028;&#32570;&#22833;/&#27169;&#31946;&#21644;&#19981;&#22343;&#21248;&#30340;&#24378;&#24230;&#20998;&#24067;&#65292;&#20197;&#21450;&#21069;&#21015;&#33146;&#24418;&#29366;&#30340;&#22823;&#37327;&#21464;&#24322;&#24615;&#65292;&#24320;&#21457;&#36825;&#26679;&#30340;&#33258;&#21160;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21457;&#23637;&#20102;&#19968;&#31181;&#37197;&#22791;&#20851;&#27880;&#27169;&#22359;&#30340;&#26032;&#22411;3D&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#26469;&#26356;&#22909;&#22320;&#23545; TRUS &#20013;&#30340;&#21069;&#21015;&#33146;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#27169;&#22359;&#21033;&#29992;&#20851;&#27880;&#26426;&#21046;&#65292;&#26377;&#36873;&#25321;&#22320;&#21033;&#29992;&#19981;&#21516;&#23618;&#38598;&#25104;&#30340;&#22810;&#32423;&#29305;&#24449;&#26469;&#23436;&#21892;&#27599;&#20010;&#21333;&#29420;&#23618;&#30340;&#29305;&#24449;&#65292;&#25233;&#21046; CNN &#27973;&#23618;&#20013;&#30340;&#38750;&#21069;&#21015;&#33146;&#22122;&#22768;&#65292;&#24182;&#23558;&#26356;&#22810;&#21069;&#21015;&#33146;&#32454;&#33410;&#34701;&#20837;&#29305;&#24449;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1907.01743v2 Announce Type: replace-cross  Abstract: Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of essential importance for image-guided prostate interventions and treatment planning. However, developing such automatic solutions remains very challenging due to the missing/ambiguous boundary and inhomogeneous intensity distribution of the prostate in TRUS, as well as the large variability in prostate shapes. This paper develops a novel 3D deep neural network equipped with attention modules for better prostate segmentation in TRUS by fully exploiting the complementary information encoded in different layers of the convolutional neural network (CNN). Our attention module utilizes the attention mechanism to selectively leverage the multilevel features integrated from different layers to refine the features at each individual layer, suppressing the non-prostate noise at shallow layers of the CNN and increasing more prostate details into features at deep
&lt;/p&gt;</description></item><item><title>FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.14112</link><description>&lt;p&gt;
FP6-LLM: &#36890;&#36807;FP6&#20013;&#24515;&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#39640;&#25928;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14112
&lt;/p&gt;
&lt;p&gt;
FP6-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#20845;&#20301;&#37327;&#21270;&#30340;GPU&#31639;&#27861;-&#31995;&#32479;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20845;&#20301;&#37327;&#21270;&#65288;FP6&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22823;&#23567;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#19981;&#25552;&#20379;FP6&#37327;&#21270;&#30340;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#65292;&#24182;&#19988;&#22312;LLM&#25512;&#26029;&#36807;&#31243;&#20013;&#24456;&#38590;&#23454;&#29616;&#23454;&#38469;&#24615;&#33021;&#25913;&#36827;&#12290;&#30001;&#20110;&#65288;1&#65289;&#27169;&#22411;&#26435;&#37325;&#20855;&#26377;&#19981;&#35268;&#21017;&#20301;&#23485;&#30340;&#19981;&#21451;&#22909;&#20869;&#23384;&#35775;&#38382;&#21644;&#65288;2&#65289;&#26435;&#37325;&#21435;&#37327;&#21270;&#30340;&#39640;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#25903;&#25345;&#22312;GPU&#19978;&#36827;&#34892;FP6&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TC-FPx&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#32479;&#19968;&#24352;&#37327;&#26680;&#24515;&#25903;&#25345;&#30340;&#28014;&#28857;&#26435;&#37325;&#30340;&#23436;&#25972;GPU&#20869;&#26680;&#35774;&#35745;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#37327;&#21270;&#20301;&#23485;&#12290;&#25105;&#20204;&#23558;TC-FPx&#20869;&#26680;&#38598;&#25104;&#21040;&#29616;&#26377;&#25512;&#26029;&#31995;&#32479;&#20013;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#31471;&#21040;&#31471;&#25903;&#25345;&#65288;&#31216;&#20026;FP6-LLM&#65289;&#29992;&#20110;&#37327;&#21270;LLM&#25512;&#26029;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25512;&#26029;&#25104;&#26412;&#21644;&#27169;&#22411;&#36136;&#37327;&#20043;&#38388;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FP6-LLM&#20165;&#20351;&#29992;&#19968;&#37096;&#20998;&#23384;&#20648;&#31354;&#38388;&#23601;&#21487;&#20197;&#36827;&#34892;LLaMA-70b&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12624</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#22522;&#20110;&#35821;&#35328;&#21040;&#26032;&#20852;&#36890;&#20449;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12624
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#23558;&#35821;&#35328;&#23548;&#21521;&#30340;&#35821;&#20041;&#36890;&#20449;&#19982;&#26032;&#20852;&#36890;&#20449;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#25511;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#30340;&#26032;&#20852;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#30340;&#38754;&#21521;&#35821;&#35328;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;LSC&#65289;&#12290;&#22312;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#36828;&#31243;&#23548;&#33322;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20301;&#32622;&#21644;&#36890;&#36947;&#22320;&#22270;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;EC&#22312;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#26102;&#20250;&#20135;&#29983;&#39640;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#22256;&#38590;&#65292;&#32780;LSC&#30001;&#20110;LLM&#23610;&#23544;&#36739;&#22823;&#65292;&#20250;&#23548;&#33268;&#39640;&#30340;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#23427;&#20204;&#21508;&#33258;&#30340;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#24341;&#23548;EC&#35757;&#32451;&#20351;&#29992;LSC&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#35821;&#35328;&#24341;&#23548;&#30340;EC&#65288;LEC&#65289;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;LEC&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#34892;&#31243;&#26102;&#38388;&#65292;&#36991;&#20813;&#20102;&#20449;&#36947;&#36136;&#37327;&#24046;&#30340;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#19982;EC&#30456;&#27604;&#33021;&#22815;&#21152;&#36895;MADRL&#35757;&#32451;&#25910;&#25947;&#36798;&#21040;61.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09851</link><description>&lt;p&gt;
&#20223;&#30495;&#34892;&#20026;&#65306;&#25506;&#32034;&#31185;&#23398;&#30340;&#21487;&#33021;&#19979;&#19968;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#29983;&#29289;&#31181;&#32676;&#12290;&#23427;&#26159;&#22788;&#29702;&#22797;&#26434;&#31995;&#32479;&#38382;&#39064;&#30340;&#26368;&#20339;&#24037;&#20855;&#65292;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#26080;&#27861;&#20351;&#29992;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#19988;&#30446;&#26631;&#20998;&#24067;&#36807;&#20110;&#22797;&#26434;&#32780;&#26080;&#27861;&#23436;&#20840;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#26159;&#19968;&#33268;&#30340;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#35282;&#24230;&#24402;&#32435;&#20102;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#20223;&#30495;&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#19982;&#26032;&#33539;&#24335;&#30340;&#20986;&#29616;&#30456;&#36866;&#24212;&#65292;&#24182;&#21457;&#29616;&#20808;&#36827;&#30340;&#20223;&#30495;&#25216;&#26415;&#26159;&#33539;&#24335;&#25972;&#21512;&#30340;&#20856;&#22411;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#65288;BS&#65289;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#34892;&#20026;&#20223;&#30495;&#65288;SBS&#65289;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04331</link><description>&lt;p&gt;
&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#25968;&#38454;&#36830;&#32493;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65306;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;(FDE)&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;Caputo&#23548;&#25968;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#25972;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#12290;&#21033;&#29992;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#19982;&#20256;&#32479;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#20013;&#30340;&#26080;&#35760;&#24518;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#19981;&#21516;&#12290;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30456;&#23545;&#20110;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#27809;&#26377;&#25915;&#20987;&#25110;&#25200;&#21160;&#30340;&#29615;&#22659;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#20248;&#21183;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#39564;&#35777;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#31283;&#23450;&#24615;&#21644;&#24377;&#24615;&#65292;&#20294;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BD-MSA&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#25910;&#38598;&#29305;&#24449;&#22270;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#36793;&#30028;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2401.04330</link><description>&lt;p&gt;
BD-MSA: &#22810;&#23610;&#24230;&#29305;&#24449;&#20449;&#24687;&#32858;&#21512;&#24341;&#23548;&#30340;&#36523;&#20307;&#35299;&#32806;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#24433;&#20687;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation. (arXiv:2401.04330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04330
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BD-MSA&#30340;&#26032;&#30340;&#21464;&#21270;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#25910;&#38598;&#29305;&#24449;&#22270;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#20449;&#24687;&#65292;&#25104;&#21151;&#25552;&#21462;&#20102;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#36793;&#30028;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#21516;&#19968;&#22320;&#26041;&#25293;&#25668;&#30340;&#20004;&#20010;&#26102;&#26399;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#65292;&#22312;&#32467;&#26524;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21355;&#26143;&#30340;&#25293;&#25668;&#35282;&#24230;&#12289;&#34180;&#20113;&#23618;&#30340;&#24433;&#21709;&#20197;&#21450;&#29305;&#23450;&#30340;&#20809;&#29031;&#26465;&#20214;&#65292;&#22312;&#19968;&#20123;&#36965;&#24863;&#25668;&#24433;&#20013;&#65292;&#21464;&#21270;&#21306;&#22495;&#30340;&#27169;&#31946;&#36793;&#32536;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#27491;&#30830;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36523;&#20307;&#35299;&#32806;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#21464;&#21270;&#26816;&#27979;&#65288;BD-MSA&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#22312;&#29305;&#24449;&#22270;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#32500;&#24230;&#20013;&#25910;&#38598;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#22270;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#25104;&#21151;&#25552;&#21462;&#21464;&#21270;&#21306;&#22495;&#30340;&#36793;&#30028;&#20449;&#24687;&#65292;&#24182;&#23558;&#21464;&#21270;&#21306;&#22495;&#30340;&#20027;&#20307;&#19982;&#20854;&#36793;&#30028;&#20998;&#31163;&#12290;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#35780;&#20272;&#25351;&#26631;&#26159;&#35780;&#20215;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;&#32467;&#26524;&#36136;&#37327;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of remote sensing image change detection (RSCD) is to detect differences between bi-temporal images taken at the same place. Deep learning has been extensively used to RSCD tasks, yielding significant results in terms of result recognition. However, due to the shooting angle of the satellite, the impacts of thin clouds, and certain lighting conditions, the problem of fuzzy edges in the change region in some remote sensing photographs cannot be properly handled using current RSCD algorithms. To solve this issue, we proposed a Body Decouple Multi-Scale by fearure Aggregation change detection (BD-MSA), a novel model that collects both global and local feature map information in the channel and space dimensions of the feature map during the training and prediction phases. This approach allows us to successfully extract the change region's boundary information while also divorcing the change region's main body from its boundary. Numerous studies have shown that the assessment me
&lt;/p&gt;</description></item><item><title>Video2Music&#26159;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#30340;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#65292;&#37319;&#29992;Affective Multimodal Transformer (AMT)&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2311.00968</link><description>&lt;p&gt;
Video2Music&#65306;&#20351;&#29992;&#24773;&#24863;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#20174;&#35270;&#39057;&#20013;&#29983;&#25104;&#21512;&#36866;&#30340;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Video2Music: Suitable Music Generation from Videos using an Affective Multimodal Transformer model. (arXiv:2311.00968v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00968
&lt;/p&gt;
&lt;p&gt;
Video2Music&#26159;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#26512;&#35270;&#39057;&#30340;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#65292;&#37319;&#29992;Affective Multimodal Transformer (AMT)&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#29983;&#25104;&#39046;&#22495;&#65292;&#35768;&#22810;&#30740;&#31350;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#30456;&#21305;&#37197;&#30340;&#38899;&#20048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#38899;&#20048;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;Video2Music&#65292;&#23427;&#21487;&#20197;&#21305;&#37197;&#25552;&#20379;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#39318;&#20808;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#38899;&#20048;&#35270;&#39057;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#38899;&#20048;&#35270;&#39057;&#20197;&#33719;&#24471;&#35821;&#20041;&#12289;&#22330;&#26223;&#20559;&#31227;&#12289;&#21160;&#20316;&#21644;&#24773;&#24863;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#19981;&#21516;&#30340;&#29305;&#24449;&#34987;&#29992;&#20316;&#25105;&#20204;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#24341;&#23548;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;&#38899;&#39057;&#25991;&#20214;&#36716;&#24405;&#20026;MIDI&#21644;&#21644;&#24358;&#65292;&#24182;&#25552;&#21462;&#38899;&#31526;&#23494;&#24230;&#21644;&#38899;&#37327;&#31561;&#29305;&#24449;&#12290;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;MuVi-Sync&#65292;&#25105;&#20204;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24773;&#24863;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65288;AMT&#65289;&#26469;&#26681;&#25454;&#35270;&#39057;&#29983;&#25104;&#38899;&#20048;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#26469;&#24378;&#21046;&#35270;&#39057;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#24773;&#24863;&#30456;&#20284;&#24615;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#19968;&#20010;&#22522;&#20110;&#21452;&#21521;GRU&#30340;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20272;&#35745;&#38899;&#31526;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies in the field of music generation have demonstrated impressive performance, yet virtually no models are able to directly generate music to match accompanying videos. In this work, we develop a generative music AI framework, Video2Music, that can match a provided video. We first curated a unique collection of music videos. Then, we analysed the music videos to obtain semantic, scene offset, motion, and emotion features. These distinct features are then employed as guiding input to our music generation model. We transcribe the audio files into MIDI and chords, and extract features such as note density and loudness. This results in a rich multimodal dataset, called MuVi-Sync, on which we train a novel Affective Multimodal Transformer (AMT) model to generate music given a video. This model includes a novel mechanism to enforce affective similarity between video and music. Finally, post-processing is performed based on a biGRU-based regression model to estimate note density 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#36845;&#20195;&#65288;&#29305;&#21035;&#26159;Q-learning&#36845;&#20195;&#65289;&#36827;&#34892;&#25910;&#25947;&#30340;&#23450;&#29702;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#35752;&#35770;&#20102;&#35813;&#23450;&#29702;&#22312;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00123</link><description>&lt;p&gt;
Q-Learning&#29992;&#20110;&#36890;&#29992;&#20449;&#24687;&#32467;&#26500;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Q-Learning for Stochastic Control under General Information Structures and Non-Markovian Environments. (arXiv:2311.00123v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20110;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#36845;&#20195;&#65288;&#29305;&#21035;&#26159;Q-learning&#36845;&#20195;&#65289;&#36827;&#34892;&#25910;&#25947;&#30340;&#23450;&#29702;&#65292;&#24182;&#32473;&#20986;&#20102;&#25910;&#25947;&#26465;&#20214;&#12290;&#20854;&#27425;&#65292;&#35752;&#35770;&#20102;&#35813;&#23450;&#29702;&#22312;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25910;&#25947;&#23450;&#29702;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#33324;&#30340;&#12289;&#21487;&#33021;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#38543;&#26426;&#29615;&#22659;&#19979;&#30340;Q-&#23398;&#20064;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#26465;&#20214;&#28041;&#21450;&#21040;&#19968;&#20010;&#36941;&#21382;&#24615;&#21644;&#19968;&#20010;&#27491;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#23545;&#36845;&#20195;&#30340;&#26497;&#38480;&#21644;&#25910;&#25947;&#30340;&#29615;&#22659;&#21644;&#21021;&#22987;&#21270;&#26465;&#20214;&#36827;&#34892;&#20102;&#31934;&#30830;&#30340;&#25551;&#36848;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#23450;&#29702;&#23545;&#20110;&#22810;&#31181;&#20855;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;(i)&#36830;&#32493;&#31354;&#38388;&#30340;&#23436;&#20840;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#37327;&#21270;&#36817;&#20284;&#65288;&#37327;&#21270;&#30772;&#22351;&#20102;&#39532;&#23572;&#21487;&#22827;&#32467;&#26500;&#65289;&#65292;(ii)&#37327;&#21270;&#36817;&#20284;&#30340;&#32622;&#20449;MDP&#32422;&#21270;&#37096;&#20998;&#21487;&#35266;&#23519;MDPS&#65288;POMDPs&#65289; with &#24369;Feller&#36830;&#32493;&#24615;&#21644;&#28388;&#27874;&#22120;&#31283;&#23450;&#30340;&#36731;&#24494;&#29256;&#26412;&#65288;&#25511;&#21046;&#22120;&#38656;&#35201;&#20102;&#35299;&#27169;&#22411;&#65289;&#65292;(iii)&#26377;&#38480;&#31383;&#21475;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a primary contribution, we present a convergence theorem for stochastic iterations, and in particular, Q-learning iterates, under a general, possibly non-Markovian, stochastic environment. Our conditions for convergence involve an ergodicity and a positivity criterion. We provide a precise characterization on the limit of the iterates and conditions on the environment and initializations for convergence. As our second contribution, we discuss the implications and applications of this theorem to a variety of stochastic control problems with non-Markovian environments involving (i) quantized approximations of fully observed Markov Decision Processes (MDPs) with continuous spaces (where quantization break down the Markovian structure), (ii) quantized approximations of belief-MDP reduced partially observable MDPS (POMDPs) with weak Feller continuity and a mild version of filter stability (which requires the knowledge of the model by the controller), (iii) finite window approximations of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11053</link><description>&lt;p&gt;
Denevil: &#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#26469;&#35299;&#35835;&#21644;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#30001;&#29983;&#25104;&#30340;&#19981;&#36947;&#24503;&#20869;&#23481;&#24341;&#36215;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#29305;&#23450;&#38382;&#39064;&#22914;&#20559;&#35265;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20174;&#36947;&#24503;&#21746;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#28145;&#20837;&#25506;&#35752;&#36947;&#24503;&#20215;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeNEVIL&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#65292;&#26088;&#22312;&#21160;&#24577;&#21033;&#29992;LLM&#30340;&#20215;&#20540;&#33030;&#24369;&#24615;&#24182;&#20197;&#29983;&#25104;&#26041;&#24335;&#25581;&#31034;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#65292;&#25581;&#31034;&#20854;&#28508;&#22312;&#30340;&#20215;&#20540;&#20542;&#21521;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MoralPrompt&#65292;&#19968;&#20010;&#21253;&#21547;2,397&#20010;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;500&#22810;&#20010;&#20215;&#20540;&#21407;&#21017;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#19981;&#23545;&#40784;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09680</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#20351;&#24471;ASR&#31995;&#32479;&#22312;&#20934;&#30830;&#36716;&#24405;&#21475;&#35821;&#30340;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#26159;&#26500;&#24314;&#23545;&#35805;&#20195;&#29702;&#30340;&#20851;&#38190;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#36776;&#21035;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#20173;&#28982;&#26159;&#19968;&#39033;&#36843;&#20999;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#22788;&#29702;&#26469;&#22686;&#24378;ASR&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#20132;&#20184;&#21508;&#31181;&#35789;&#27719;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#36716;&#24405;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;HMM-GMM&#65289;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#22768;&#23398;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21333;&#35789;&#26684;&#65292;&#20351;&#25105;&#20204;&#30340;&#32593;&#32476;&#20855;&#22791;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpa
&lt;/p&gt;</description></item><item><title>HIO-SDF&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#12289;&#22797;&#26434;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.09463</link><description>&lt;p&gt;
HIO-SDF&#65306;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;
&lt;/p&gt;
&lt;p&gt;
HIO-SDF: Hierarchical Incremental Online Signed Distance Fields. (arXiv:2310.09463v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09463
&lt;/p&gt;
&lt;p&gt;
HIO-SDF&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#22686;&#37327;&#22312;&#32447;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#12289;&#22797;&#26434;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#33021;&#22815;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#22823;&#22411;&#22797;&#26434;&#31227;&#21160;&#26426;&#22120;&#20154;&#24037;&#20316;&#31354;&#38388;&#30340;&#34920;&#31034;&#24517;&#39035;&#26159;&#31354;&#38388;&#39640;&#25928;&#30340;&#65292;&#21516;&#26102;&#33021;&#22815;&#32534;&#30721;&#30456;&#20851;&#30340;&#20960;&#20309;&#32454;&#33410;&#12290;&#24403;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#26102;&#65292;&#23427;&#38656;&#35201;&#20197;&#22312;&#32447;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HIO-SDF&#65292;&#19968;&#31181;&#23558;&#29615;&#22659;&#34920;&#31034;&#20026;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#30446;&#21069;SDF&#30340;&#26368;&#20808;&#36827;&#34920;&#31034;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25110;&#20307;&#32032;&#32593;&#26684;&#12290;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#36830;&#32493;&#22320;&#34920;&#31034;SDF&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24456;&#38590;&#20197;&#22686;&#37327;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20250;&#24536;&#35760;&#20043;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#37096;&#20998;&#65292;&#38500;&#38750;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20256;&#24863;&#22120;&#21382;&#21490;&#29992;&#20110;&#35757;&#32451;&#12290;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#19981;&#20855;&#26377;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#32454;&#33410;&#20016;&#23500;&#30340;&#22823;&#22411;&#29615;&#22659;&#20013;&#19981;&#26159;&#31354;&#38388;&#39640;&#25928;&#30340;&#12290;HIO-SDF&#21033;&#29992;&#23618;&#27425;&#26041;&#27861;&#32467;&#21512;&#20102;&#36825;&#20123;&#34920;&#31034;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#31895;&#31961;&#30340;&#20307;&#32032;&#32593;&#26684;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#29615;&#22659;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
A good representation of a large, complex mobile robot workspace must be space-efficient yet capable of encoding relevant geometric details. When exploring unknown environments, it needs to be updatable incrementally in an online fashion. We introduce HIO-SDF, a new method that represents the environment as a Signed Distance Field (SDF). State of the art representations of SDFs are based on either neural networks or voxel grids. Neural networks are capable of representing the SDF continuously. However, they are hard to update incrementally as neural networks tend to forget previously observed parts of the environment unless an extensive sensor history is stored for training. Voxel-based representations do not have this problem but they are not space-efficient especially in large environments with fine details. HIO-SDF combines the advantages of these representations using a hierarchical approach which employs a coarse voxel grid that captures the observed parts of the environment toget
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15135</link><description>&lt;p&gt;
&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35270;&#35282;&#32858;&#31867;&#22312;&#23454;&#26102;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#22256;&#38590;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#38450;&#27490;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#24615;&#25351;&#23548;&#26032;&#35270;&#22270;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#36866;&#29992;&#20110;&#20808;&#21069;&#25910;&#38598;&#35270;&#22270;&#24182;&#25552;&#21462;&#19968;&#33268;&#21644;&#20114;&#34917;&#20449;&#24687;&#30340;&#24212;&#29992;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#35270;&#22270;&#25353;&#39034;&#24207;&#25910;&#38598;&#30340;&#23454;&#26102;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#24230;&#36830;&#32493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#36807;&#28388;&#32467;&#26500;&#34701;&#21512;&#65288;CCMVC-FSF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#20808;&#21069;&#30693;&#35782;&#36951;&#24536;&#21644;&#26032;&#35270;&#22270;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#23631;&#34109;&#21160;&#20316;&#26469;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#22235;&#31181;&#23631;&#34109;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.10216</link><description>&lt;p&gt;
&#36890;&#36807;&#23631;&#34109;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe POMDP Online Planning via Shielding. (arXiv:2309.10216v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#23631;&#34109;&#21160;&#20316;&#26469;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#22235;&#31181;&#23631;&#34109;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#12290;POMDP&#22312;&#32447;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#65288;POMCP&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#30446;&#26631;&#20026;&#26368;&#22823;&#21270;&#39044;&#26399;&#22238;&#25253;&#30340;&#22823;&#22411;POMDP&#12290;&#20294;&#26159;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#31574;&#30053;&#26080;&#27861;&#25552;&#20379;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#65289;&#33267;&#20851;&#37325;&#35201;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23433;&#20840;&#35201;&#27714;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#20960;&#20046;&#19968;&#23450;&#30340;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#65288;&#21363;&#65292;&#36798;&#21040;&#19968;&#32452;&#30446;&#26631;&#29366;&#24577;&#30340;&#27010;&#29575;&#20026;1&#65292;&#36798;&#21040;&#19968;&#32452;&#19981;&#23433;&#20840;&#29366;&#24577;&#30340;&#27010;&#29575;&#20026;0&#65289;&#12290;&#25105;&#20204;&#35745;&#31639;&#38480;&#21046;&#36829;&#21453;&#20960;&#20046;&#19968;&#23450;&#21040;&#36798;-&#36991;&#20813;&#35268;&#33539;&#30340;&#19981;&#23433;&#20840;&#21160;&#20316;&#30340;&#23631;&#34109;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#23631;&#34109;&#38598;&#25104;&#21040;POMCP&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#23433;&#20840;&#30340;POMDP&#22312;&#32447;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#23631;&#34109;&#26041;&#27861;&#65292;&#26681;&#25454;&#23631;&#34109;&#30340;&#35745;&#31639;&#21644;&#38598;&#25104;&#26041;&#24335;&#30340;&#19981;&#21516;&#65292;&#21253;&#25324;&#20998;&#35299;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely used in many robotic applications for sequential decision-making under uncertainty. POMDP online planning algorithms such as Partially Observable Monte-Carlo Planning (POMCP) can solve very large POMDPs with the goal of maximizing the expected return. But the resulting policies cannot provide safety guarantees that are imperative for real-world safety-critical tasks (e.g., autonomous driving). In this work, we consider safety requirements represented as almost-sure reach-avoid specifications (i.e., the probability to reach a set of goal states is one and the probability to reach a set of unsafe states is zero). We compute shields that restrict unsafe actions violating almost-sure reach-avoid specifications. We then integrate these shields into the POMCP algorithm for safe POMDP online planning. We propose four distinct shielding methods, differing in how the shields are computed and integrated, including factored 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03720</link><description>&lt;p&gt;
&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Hoeffding&#26641;&#21644;&#21464;&#28857;&#26816;&#27979;&#26426;&#21046;&#30340;&#36830;&#32493;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22810;&#27493; ahead &#30340;&#39044;&#27979;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35268;&#21010;&#22825;&#28982;&#27668;&#20379;&#24212;&#21644;&#28040;&#36153;&#20197;&#21450;&#20248;&#21270;&#33719;&#24471;&#22825;&#28982;&#27668;&#25104;&#26412;&#26041;&#38754;&#65292;&#32771;&#34385;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#24615;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27493; ahead &#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#21464;&#28857;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#25345;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#25454;&#27969;&#22788;&#29702;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#35813;&#26041;&#27861;&#30340;&#22825;&#28982;&#27668;&#28040;&#36153;&#39044;&#27979;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;Hoeffding&#26641;&#39044;&#27979;&#22120;&#20316;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21098;&#35009;&#30340;&#31934;&#30830;&#32447;&#24615;&#26102;&#38388;&#65288;PELT&#65289;&#31639;&#27861;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;&#21464;&#28857;&#26816;&#27979;&#38598;&#25104;&#20351;&#24471;&#36873;&#25321;&#19981;&#21516;&#30340;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02561</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29289;&#20307;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#29289;&#29702;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#22312;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#23548;&#33268;&#22312;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#25551;&#36848;&#31561;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#29616;&#22312;&#21487;&#20197;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VLMs&#22312;&#23545;&#24120;&#35265;&#29289;&#20307;&#30340;&#29289;&#29702;&#27010;&#24565;&#65288;&#20363;&#22914;&#26448;&#26009;&#12289;&#33030;&#24369;&#24615;&#65289;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#28041;&#21450;&#19982;&#36825;&#20123;&#29289;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#29289;&#29702;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhysObjects&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;36.9K&#20010;&#20247;&#21253;&#21644;417K&#20010;&#33258;&#21160;&#21270;&#30340;&#24120;&#35265;&#23478;&#23621;&#29289;&#21697;&#30340;&#29289;&#29702;&#27010;&#24565;&#27880;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;PhysObjects&#19978;&#23545;VLM&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#23545;&#29289;&#29702;&#29289;&#20307;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25429;&#25417;&#36825;&#20123;&#27010;&#24565;&#30340;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#35328;&#20132;&#20114;&#26694;&#26550;&#20013;&#23558;&#36825;&#20010;&#20855;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;VLM&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 36.9K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large languag
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.15337</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#39592;&#26550;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24182;&#34892;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding. (arXiv:2307.15337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#24310;&#36831;&#12290;&#39640;&#29983;&#25104;&#24310;&#36831;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20960;&#20046;&#25152;&#26377;&#26368;&#20808;&#36827;&#30340;LLMs&#37117;&#37319;&#29992;&#20102;&#39034;&#24207;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#21463;&#21040;&#20154;&#31867;&#30340;&#24605;&#32771;&#21644;&#20889;&#20316;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24605;&#32500;&#30340;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#23427;&#25351;&#23548;LLMs&#39318;&#20808;&#29983;&#25104;&#31572;&#26696;&#30340;&#39592;&#26550;&#65292;&#28982;&#21518;&#36890;&#36807;&#24182;&#34892;API&#35843;&#29992;&#25110;&#25209;&#37327;&#35299;&#30721;&#26469;&#24182;&#34892;&#23436;&#25104;&#27599;&#20010;&#39592;&#26550;&#28857;&#30340;&#20869;&#23481;&#12290;SoT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#36895;&#24230;&#65288;&#22312;11&#20010;&#19981;&#21516;&#30340;LLMs&#19978;&#25552;&#39640;&#20102;&#26368;&#22810;2.39&#20493;&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#39640;&#22312;&#22810;&#20010;&#38382;&#39064;&#31867;&#21035;&#19978;&#30340;&#31572;&#26696;&#36136;&#37327;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;SoT&#26159;&#19968;&#31181;&#38024;&#23545;&#25928;&#29575;&#30340;&#25968;&#25454;&#23548;&#21521;&#20248;&#21270;&#30340;&#21021;&#27493;&#23581;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#23558;LLMs&#25512;&#21160;&#26356;&#20687;&#20154;&#31867;&#24605;&#32771;&#20197;&#25552;&#39640;&#31572;&#26696;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08175</link><description>&lt;p&gt;
DCTX-Conformer&#65306;&#38024;&#23545;&#20302;&#24310;&#36831;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;Conformer&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#12290;&#35832;&#22914;&#21452;&#27169;&#24335;&#21644;&#21160;&#24577;&#20998;&#22359;&#35757;&#32451;&#31561;&#25216;&#26415;&#26377;&#21161;&#20110;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#23436;&#25972;&#21644;&#26377;&#38480;&#30340;&#36807;&#21435;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#24335;&#35782;&#21035;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#32479;&#19968;ASR&#31995;&#32479;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;&#21160;&#24577;&#19978;&#19979;&#25991;Conformer&#65288;DCTX-Conformer&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;&#38750;&#37325;&#21472;&#30340;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19968;&#22359;&#30340;&#24038;&#19978;&#19979;&#25991;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#30001;&#20110;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#30446;&#21069;&#26368;&#20248;&#35299;&#25552;&#21319;&#20102;25.0%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32780;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#20998;&#21035;&#38024;&#23545;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#25552;&#20986;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06022</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Partial Computation Offloading for the Metaverse in In-Network Computing. (arXiv:2306.06022v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#30340;Metaverse&#21160;&#24577;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#35299;&#20915;&#20102;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20102;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#20998;&#21035;&#38024;&#23545;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#25552;&#20986;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32593;&#32476;&#35745;&#31639;&#65288;COIN&#65289;&#33539;&#24335;&#26159;&#19968;&#20010;&#21033;&#29992;&#26410;&#20351;&#29992;&#30340;&#32593;&#32476;&#36164;&#28304;&#26469;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#20197;&#28385;&#36275;&#35745;&#31639;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;Metaverse&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;COIN&#29615;&#22659;&#20013;&#38024;&#23545;Metaverse&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#30340;&#37096;&#20998;&#35745;&#31639;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#21160;&#24577;&#35843;&#25972;&#21368;&#36733;&#31574;&#30053;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP&#38590;&#39064;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#29992;&#25143;&#31471;&#30340;&#20219;&#21153;&#20999;&#20998;&#38382;&#39064;&#65288;TSP&#65289;&#21644;COIN&#31471;&#30340;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65288;TOP&#65289;&#12290;&#25105;&#20204;&#23558;TSP&#24314;&#27169;&#20026;&#24207;&#25968;&#28508;&#22312;&#21338;&#24328;&#65288;OPG&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#31639;&#27861;&#26469;&#33719;&#21462;&#20854;&#32435;&#20160;&#24179;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;TOP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#25552;&#20986;&#20102;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#65288;DDQN&#65289;&#26469;&#35299;&#20915;&#26368;&#20248;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;DDQN&#31639;&#27861;&#19981;&#21516;&#65292;&#26234;&#33021;&#20195;&#29702;&#22312;&#27492;&#19981;&#20197;&#38543;&#26426;&#26041;&#24335;&#25277;&#26679;&#21368;&#36733;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
The In-Network Computing (COIN) paradigm is a promising solution that leverages unused network resources to perform some tasks to meet up with computation-demanding applications, such as metaverse. In this vein, we consider the metaverse partial computation offloading problem for multiple subtasks in a COIN environment to minimise energy consumption and delay while dynamically adjusting the offloading policy based on the changing computation resources status. We prove that the problem is NP and thus transformed it into two subproblems: task splitting problem (TSP) on the user side and task offloading problem (TOP) on the COIN side. We modelled the TSP as an ordinal potential game (OPG) and proposed a decentralised algorithm to obtain its Nash Equilibrium (NE). Then, we model the TOP as Markov Decision Process (MDP) proposed double deep Q-network (DDQN) to solve for the optimal offloading policy. Unlike the conventional DDQN algorithm, where intelligent agents sample offloading decision
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05499</link><description>&lt;p&gt;
LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection attack against LLM-integrated Applications. (arXiv:2306.05499v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;HouYi&#65292;&#24182;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20854;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#22312;&#23427;&#20204;&#21608;&#22260;&#21050;&#28608;&#20102;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24212;&#29992;&#29983;&#24577;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#34701;&#21512;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#23558;&#35299;&#26500;&#23454;&#38469;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#22797;&#26434;&#24615;&#21644;&#24433;&#21709;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;&#21313;&#20010;&#21830;&#19994;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#30446;&#21069;&#25915;&#20987;&#31574;&#30053;&#22312;&#23454;&#36341;&#20013;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#21463;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#38543;&#21518;&#21046;&#23450;&#20102;HouYi&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#25216;&#26415;&#65292;&#23427;&#20511;&#37492;&#20102;&#20256;&#32479;&#30340;Web&#27880;&#20837;&#25915;&#20987;&#12290;HouYi&#20998;&#20026;&#19977;&#20010;&#20851;&#38190;&#20803;&#32032;: &#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#30340;&#39044;&#26500;&#24314;&#25552;&#31034;&#12289;&#19968;&#20010;&#27880;&#20837;&#25552;&#31034;&#35825;&#23548;&#19978;&#19979;&#25991;&#20998;&#21306;&#20197;&#21450;&#19968;&#20010;&#24694;&#24847;&#36733;&#33655;&#65292;&#26088;&#22312;&#23454;&#29616;&#25915;&#20987;&#30446;&#26631;&#12290;&#21033;&#29992;HouYi&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24212;&#29992;&#31243;&#24207;&#25552;&#31034;&#26426;&#21046;&#20013;&#20197;&#21069;&#26410;&#30693;&#21644;&#20005;&#37325;&#20302;&#20272;&#30340;&#28431;&#27934;&#65292;&#24182;&#28436;&#31034;&#20102;&#32469;&#36807;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#20840;&#38754;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#20197;&#25269;&#24481;LLM&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and sev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.03604</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20419;&#36827;&#31639;&#27861;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21253;&#21547;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#25351;&#20196;&#26469;&#21327;&#21161;&#31639;&#27861;&#20195;&#29702;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#21487;&#33021;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21482;&#33021;&#37096;&#32626;&#22312;&#36828;&#31243;&#20113;&#26381;&#21153;&#22120;&#33410;&#28857;&#19978;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21830;&#19994;LLMs&#21487;&#33021;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20351;&#29992;&#39057;&#29575;&#25910;&#36153;&#12290;&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#26597;&#35810;LLMs&#20197;&#23436;&#25104;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#32423;&#25351;&#20196;&#12290;&#22312;&#28041;&#21450;&#35268;&#21010;&#23376;&#30446;&#26631;&#30340;4&#20010;MiniGrid&#29615;&#22659;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#25552;&#21319;&#20102;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18352</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#22810;&#35270;&#35282;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18352
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#39640;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#20351;&#24471;&#39640;&#32500;&#25968;&#25454;&#22686;&#21152;&#65292;&#24433;&#21709;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#65292;&#29992;&#20110;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#20197;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#34917;&#20805;&#20449;&#24687;&#26469;&#22686;&#24378;&#39044;&#27979;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20250;&#23548;&#33268;&#39640;&#32500;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;&#36825;&#23545;&#21487;&#20197;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#39044;&#27979;&#27169;&#22411;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20174;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#19981;&#20165;&#21487;&#20197;&#35299;&#20915;&#19981;&#33391;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#21033;&#29992;&#36328;&#27169;&#24577;&#30340;&#20869;&#22312;&#20449;&#24687;&#12289;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#36866;&#29992;&#20110;&#29305;&#23450;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36951;&#20256;&#31639;&#27861;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#19978;&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22810;&#35270;&#35282;&#22810;&#30446;&#26631;&#29305;&#24449;&#36873;&#25321;&#36951;&#20256;&#31639;&#27861;&#65288;MMFS-GA&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#21516;&#26102;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
&lt;/p&gt;</description></item><item><title>CoCoRL&#26159;&#19968;&#31181;&#20174;&#19981;&#30693;&#36947;&#22870;&#21169;&#30340;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;Constrained Markov Decision Process&#65288;CMDP&#65289;&#65292;&#24182;&#19988;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.16147</link><description>&lt;p&gt;
&#20174;&#26410;&#30693;&#22870;&#21169;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#23433;&#20840;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16147
&lt;/p&gt;
&lt;p&gt;
CoCoRL&#26159;&#19968;&#31181;&#20174;&#19981;&#30693;&#36947;&#22870;&#21169;&#30340;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;Constrained Markov Decision Process&#65288;CMDP&#65289;&#65292;&#24182;&#19988;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Convex Constraint Learning for Reinforcement Learning (CoCoRL)&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#24050;&#30693;&#23433;&#20840;&#28436;&#31034;&#20013;&#25512;&#26029;Constrained Markov Decision Process (CMDP)&#30340;&#20849;&#20139;&#32422;&#26463;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#21482;&#38480;&#20110;&#24050;&#30693;&#22870;&#21169;&#25110;&#23436;&#20840;&#24050;&#30693;&#29615;&#22659;&#21160;&#24577;&#30340;&#28436;&#31034;&#30456;&#27604;&#65292;CoCoRL&#21487;&#20197;&#20174;&#20855;&#26377;&#19981;&#21516;&#26410;&#30693;&#22870;&#21169;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#29615;&#22659;&#21160;&#24577;&#12290;CoCoRL&#22522;&#20110;&#28436;&#31034;&#26500;&#24314;&#20102;&#19968;&#20010;&#20984;&#23433;&#20840;&#38598;&#65292;&#21363;&#20351;&#26159;&#28508;&#22312;&#30340;&#27425;&#20248;&#28436;&#31034;&#20063;&#33021;&#20445;&#35777;&#23433;&#20840;&#12290;&#23545;&#20110;&#20960;&#20046;&#26368;&#20248;&#28436;&#31034;&#65292;CoCoRL&#33021;&#22815;&#26080;&#35823;&#24046;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#23433;&#20840;&#38598;&#12290;&#25105;&#20204;&#22312;&#34920;&#26684;&#29615;&#22659;&#21644;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#32422;&#26463;&#30340;&#36830;&#32493;&#39550;&#39542;&#20223;&#30495;&#20013;&#35780;&#20272;CoCoRL&#12290;CoCoRL&#23398;&#20064;&#21040;&#30340;&#38480;&#21046;&#23548;&#33268;&#20102;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#24182;&#21487;&#20197;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#29615;&#22659;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#22522;&#20110;&#23398;&#20064;&#24050;&#30693;&#22238;&#25253;&#30340;&#26367;&#20195;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#20855;&#26377;&#19981;&#21516;&#22238;&#25253;&#30340;&#26032;&#29615;&#22659;&#65292;&#31361;&#26174;&#20102;CoCoRL&#22312;&#19981;&#30693;&#36947;&#22238;&#25253;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32422;&#26463;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15086</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#23454;&#29616;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725; (UNSB)&#65292;&#23427;&#32467;&#21512;&#20102;&#34203;&#23450;&#35860;&#26725;&#12289;&#23545;&#25239;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#23398;&#20064; SDE&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#35768;&#22810;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#27169;&#25311;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#20174;&#22122;&#22768;&#29983;&#25104;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#39640;&#26031;&#20808;&#39564;&#20551;&#35774;&#65292;&#23427;&#20204;&#22312;&#38750;&#37197;&#23545;&#30340;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#34203;&#23450;&#35860;&#26725;&#26159;&#19968;&#31181;&#23398;&#20064; SDE &#20197;&#22312;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#34987;&#35270;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#34203;&#23450;&#35860;&#26725;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#37197;&#23545;&#36716;&#25442;&#26041;&#38754;&#24182;&#19981;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#37197;&#23545;&#31070;&#32463;&#34203;&#23450;&#35860;&#26725;&#65288;UNSB&#65289;&#65292;&#23427;&#23558;&#34203;&#23450;&#35860;&#26725;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#38750;&#37197;&#23545;&#25968;&#25454;&#20043;&#38388;&#30340; SDE&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; UNSB &#26159;&#21487;&#20280;&#32553;&#30340;&#65292;&#24182;&#19988;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#38750;&#37197;&#23545;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07912</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;TKGC&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22312;&#24050;&#30693;&#30340;&#26102;&#38388;&#25139;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#32570;&#22833;&#37096;&#20998;&#30340;&#20107;&#23454;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#21516;&#26102;&#31895;&#30053;&#22320;&#25552;&#21462;&#26102;&#38388;&#25139;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#19981;&#20805;&#20998;&#21033;&#29992;&#20851;&#31995;&#20013;&#38544;&#21547;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;TKGC&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;&#37319;&#26679;&#30340;&#22235;&#20803;&#32452;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#38388;&#38548;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#24418;&#25104;&#24102;&#26377;&#38544;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#36830;&#36143;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#30422;&#31574;&#30053;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;</title><link>http://arxiv.org/abs/2304.06607</link><description>&lt;p&gt;
&#27169;&#22411;&#25152;&#26377;&#26435;&#20105;&#35758;&#20013;&#30340;&#34394;&#20551;&#25351;&#25511;
&lt;/p&gt;
&lt;p&gt;
False Claims against Model Ownership Resolution. (arXiv:2304.06607v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#20013;&#23545;&#25239;&#24694;&#24847;&#21407;&#21578;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;MOR&#26041;&#26696;&#21487;&#20197;&#34987;&#24694;&#24847;&#21407;&#21578;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#30340;&#26377;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65292;&#26500;&#25104;&#20102;&#31454;&#20105;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20445;&#25252;&#27169;&#22411;&#19981;&#34987;&#30423;&#29992;&#30340;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#22411;&#25152;&#26377;&#26435;&#35299;&#20915;&#26041;&#26696;&#65288;MOR&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#34987;&#30423;&#30340;&#25216;&#26415;&#12290;MOR&#26041;&#26696;&#20351;&#24471;&#21407;&#21578;&#26041;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#65288;&#22914;&#27700;&#21360;&#25110;&#25351;&#32441;&#65289;&#26469;&#26029;&#35328;&#23545;&#28041;&#23244;&#30423;&#29992;&#27169;&#22411;&#30340;&#34987;&#21578;&#26041;&#22768;&#31216;&#25152;&#26377;&#26435;&#65292;&#35777;&#26126;&#28041;&#23244;&#27169;&#22411;&#26159;&#34987;&#30423;&#25110;&#32773;&#28304;&#33258;&#20110;&#21407;&#21578;&#26041;&#25317;&#26377;&#30340;&#28304;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968; MOR &#26041;&#26696;&#37325;&#28857;&#25918;&#22312;&#38450;&#33539;&#24694;&#24847;&#28041;&#23244;&#26041;&#26041;&#38754;&#65292;&#30830;&#20445;&#22914;&#26524;&#28041;&#23244;&#27169;&#22411;&#30830;&#23454;&#26159;&#34987;&#30423;&#29256;&#65292;&#21017;&#21407;&#21578;&#26041;&#23558;&#33719;&#32988;&#12290;&#20294;&#26159;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24120;&#35265; MOR &#26041;&#26696;&#23384;&#22312;&#30528;&#21478;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65306;&#24694;&#24847;&#21407;&#21578;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#22320;&#38024;&#23545;&#26410;&#34987;&#30423;&#29992;&#30340;&#29420;&#31435;&#27169;&#22411;&#25552;&#20986;&#34394;&#20551;&#25351;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#65292;&#36890;&#36807;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#40723;&#21169;&#32593;&#32476;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.17386</link><description>&lt;p&gt;
RGB-&#28909;&#32418;&#22806;&#35821;&#20041;&#20998;&#21106;&#30340;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Complementary Random Masking for RGB-Thermal Semantic Segmentation. (arXiv:2303.17386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#65292;&#36890;&#36807;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#40723;&#21169;&#32593;&#32476;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#30340;&#27668;&#35937;&#21644;&#29031;&#26126;&#26465;&#20214;&#19979;&#65292;RGB-&#28909;&#32418;&#22806;&#35821;&#20041;&#20998;&#21106;&#26159;&#23454;&#29616;&#21487;&#38752;&#30340;&#22330;&#26223;&#29702;&#35299;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#35774;&#35745;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#32780;&#24573;&#30053;&#20102;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26412;&#36136;&#12290;&#22240;&#27492;&#65292;&#32593;&#32476;&#23481;&#26131;&#36807;&#24230;&#20381;&#36182;&#21333;&#19968;&#27169;&#24577;&#65292;&#38590;&#20197;&#20026;&#27599;&#20010;&#27169;&#24577;&#23398;&#20064;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;RGB-T&#22270;&#20687;&#20114;&#34917;&#38543;&#26426;&#33945;&#29256;&#31574;&#30053;&#21644;&#33258;&#33976;&#39311;&#25439;&#22833;&#12290;&#25152;&#25552;&#20986;&#30340;&#33945;&#29256;&#31574;&#30053;&#38450;&#27490;&#23545;&#21333;&#19968;&#27169;&#24335;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#24378;&#21046;&#32593;&#32476;&#22312;&#19968;&#20010;&#27169;&#24577;&#37096;&#20998;&#21487;&#29992;&#26102;&#36827;&#34892;&#23545;&#35937;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#33976;&#39311;&#25439;&#22833;&#40723;&#21169;&#32593;&#32476;&#20174;&#21333;&#19968;&#27169;&#24577;&#20013;&#25552;&#21462;&#20114;&#34917;&#19988;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGB-thermal semantic segmentation is one potential solution to achieve reliable semantic scene understanding in adverse weather and lighting conditions. However, the previous studies mostly focus on designing a multi-modal fusion module without consideration of the nature of multi-modality inputs. Therefore, the networks easily become over-reliant on a single modality, making it difficult to learn complementary and meaningful representations for each modality. This paper proposes 1) a complementary random masking strategy of RGB-T images and 2) self-distillation loss between clean and masked input modalities. The proposed masking strategy prevents over-reliance on a single modality. It also improves the accuracy and robustness of the neural network by forcing the network to segment and classify objects even when one modality is partially available. Also, the proposed self-distillation loss encourages the network to extract complementary and meaningful representations from a single moda
&lt;/p&gt;</description></item><item><title>PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13755</link><description>&lt;p&gt;
&#21452;&#20215;&#20540;&#32593;&#32476;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13755
&lt;/p&gt;
&lt;p&gt;
PDVN&#26159;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#36870;&#21521;&#21512;&#25104;&#35268;&#21010;&#20013;&#21033;&#29992;&#21452;&#20215;&#20540;&#32593;&#32476;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26088;&#22312;&#20174;&#21830;&#19994;&#19978;&#21487;&#24471;&#30340;&#36215;&#22987;&#26448;&#26009;&#20013;&#25214;&#21040;&#21512;&#25104;&#30446;&#26631;&#20998;&#23376;&#30340;&#36335;&#32447;&#65292;&#26159;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21333;&#27493;&#21453;&#24212;&#39044;&#27979;&#22120;&#19982;&#22810;&#27493;&#35268;&#21010;&#22120;&#30340;&#32452;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21333;&#27493;&#39044;&#27979;&#22120;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#31163;&#32447;&#35757;&#32451;&#30340;&#65292;&#21482;&#20248;&#21270;&#21333;&#27493;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#32771;&#34385;&#23436;&#25972;&#30340;&#36335;&#32447;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35757;&#32451;&#31639;&#27861;PDVN&#65292;&#36890;&#36807;&#20351;&#29992;&#26641;&#24418;MDP&#26469;&#20248;&#21270;&#23436;&#25972;&#30340;&#36335;&#32447;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25913;&#21892;&#21333;&#27493;&#39044;&#27979;&#22120;&#12290;&#22312;PDVN&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#21333;&#29420;&#30340;&#20215;&#20540;&#32593;&#32476;&#65292;&#20998;&#21035;&#39044;&#27979;&#20998;&#23376;&#30340;&#21487;&#21512;&#25104;&#24615;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#20445;&#25345;&#21333;&#27493;&#39044;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PIP&#65288;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#65289;&#19982;DIP&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;3D-DIP&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.14298</link><description>&lt;p&gt;
PIP&#65306;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
PIP: Positional-encoding Image Prior. (arXiv:2211.14298v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PIP&#65288;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#65289;&#19982;DIP&#34920;&#29616;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;3D-DIP&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#65288;DIP&#65289;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34987;&#36866;&#37197;&#20026;&#23558;&#28508;&#31354;&#38388;&#26144;&#23556;&#21040;&#38477;&#36136;&#65288;&#20363;&#22914;&#22122;&#38899;&#65289;&#22270;&#20687;&#65292;&#20294;&#22312;&#27492;&#36807;&#31243;&#20013;&#23398;&#20064;&#37325;&#24314;&#24178;&#20928;&#22270;&#20687;&#12290;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;CNN&#30340;&#20869;&#37096;&#22270;&#20687;&#20808;&#39564;&#12290;&#25105;&#20204;&#20174;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;DIP&#26694;&#26550;&#12290;&#21463;&#21040;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#29992;&#20613;&#37324;&#21494;&#29305;&#24449;&#65288;&#20301;&#32622;&#32534;&#30721;&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#28508;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#20613;&#37324;&#21494;&#29305;&#24449;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#20687;&#32032;&#32423;MLP&#26367;&#25442;&#21367;&#31215;&#23618;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#26696;&#21629;&#21517;&#20026;&#8220;&#20301;&#32622;&#32534;&#30721;&#22270;&#20687;&#20808;&#39564;&#8221;&#65288;PIP&#65289;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20013;&#19982;DIP&#34920;&#29616;&#38750;&#24120;&#30456;&#20284;&#65292;&#20294;&#25152;&#38656;&#30340;&#21442;&#25968;&#35201;&#23569;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;PIP&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#35270;&#39057;&#65292;&#20854;&#20013;3D-DIP&#34920;&#29616;&#19981;&#20339;&#19988;&#19981;&#31283;&#23450;&#12290;&#25152;&#26377;&#20219;&#21153;&#30340;&#20195;&#30721;&#21644;&#20854;&#20182;&#20363;&#23376;&#65288;&#21253;&#25324;&#35270;&#39057;&#65289;&#22343;&#21487;&#22312;&#39033;&#30446;&#39029;&#38754;http://people.csail.mit.edu/yilun/pip/&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Deep Image Prior (DIP), a Convolutional Neural Network (CNN) is fitted to map a latent space to a degraded (e.g. noisy) image but in the process learns to reconstruct the clean image. This phenomenon is attributed to CNN's internal image-prior. We revisit the DIP framework, examining it from the perspective of a neural implicit representation. Motivated by this perspective, we replace the random or learned latent with Fourier-Features (Positional Encoding). We show that thanks to the Fourier features properties, we can replace the convolution layers with simple pixel-level MLPs. We name this scheme ``Positional Encoding Image Prior" (PIP) and exhibit that it performs very similarly to DIP on various image-reconstruction tasks with much less parameters required. Additionally, we demonstrate that PIP can be easily extended to videos, where 3D-DIP struggles and suffers from instability. Code and additional examples for all tasks, including videos, are available on the project page http
&lt;/p&gt;</description></item><item><title>FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07160</link><description>&lt;p&gt;
FedTracker&#65306;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07160
&lt;/p&gt;
&lt;p&gt;
FedTracker&#26159;&#31532;&#19968;&#20010;&#20026;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#36861;&#28335;&#24615;&#30340;&#20445;&#25252;&#26694;&#26550;&#65292;&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#21407;&#21017;&#25552;&#39640;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#20182;&#20204;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#38656;&#35201;&#23558;&#27169;&#22411;&#26292;&#38706;&#32473;&#21508;&#31181;&#21442;&#19982;&#32773;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24694;&#24847;&#23458;&#25143;&#31471;&#26410;&#32463;&#25480;&#26435;&#22320;&#20998;&#21457;&#25110;&#36716;&#21806;&#27169;&#22411;&#65292;&#20174;&#32780;&#25439;&#23475;FL&#22242;&#38431;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#20102;&#38459;&#27490;&#36825;&#31181;&#19981;&#24403;&#34892;&#20026;&#65292;&#24314;&#31435;&#19968;&#31181;&#39564;&#35777;&#27169;&#22411;&#25152;&#26377;&#26435;&#24182;&#36861;&#28335;&#27844;&#38706;&#32773;&#30340;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedTracker&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#25152;&#26377;&#26435;&#39564;&#35777;&#21644;&#21487;&#36861;&#28335;&#24615;&#30340;FL&#27169;&#22411;&#20445;&#25252;&#26694;&#26550;&#12290;FedTracker&#37319;&#29992;&#21452;&#23618;&#20445;&#25252;&#26041;&#26696;&#65292;&#21253;&#25324;&#20840;&#23616;&#27700;&#21360;&#26426;&#21046;&#21644;&#26412;&#22320;&#25351;&#32441;&#26426;&#21046;&#12290;&#21069;&#32773;&#29992;&#20110;&#39564;&#35777;&#20840;&#23616;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#65292;&#32780;&#21518;&#32773;&#29992;&#20110;&#35782;&#21035;&#35813;&#27169;&#22411;&#26469;&#33258;&#21738;&#20010;&#23458;&#25143;&#31471;&#12290;FedTracker&#21033;&#29992;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#21407;&#21017;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20445;&#25252;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2209.06356</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#31616;&#21333;&#29366;&#24577;-&#21160;&#20316;&#25277;&#35937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#36805;&#36895;&#25512;&#26029;&#20986;&#31561;&#20215;&#22870;&#21169;&#21644;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#21453;&#22797;&#35797;&#38169;&#26469;&#23398;&#20064;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#30340;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#30340;&#35266;&#23519;MDP&#31616;&#21270;&#20026;&#25277;&#35937;MDP&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#24403;&#21487;&#20197;&#20107;&#20808;&#26500;&#24314;&#36866;&#24403;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#26102;&#65292;&#21487;&#20197;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#65292;&#36890;&#24120;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#21516;&#24577;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#21738;&#20123;&#29366;&#24577;&#21160;&#20316;&#23545;&#23548;&#33268;&#30456;&#21516;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
&lt;/p&gt;</description></item></channel></rss>