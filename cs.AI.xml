<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#29616;&#35937;&#21644;&#25439;&#22833;&#36339;&#36291;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;RNN&#20013;&#23384;&#22312;&#30528;&#26576;&#20123;&#20998;&#27495;&#12290;</title><link>http://arxiv.org/abs/2310.17561</link><description>&lt;p&gt;
RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#21644;&#25439;&#22833;&#36339;&#36291;
&lt;/p&gt;
&lt;p&gt;
Bifurcations and loss jumps in RNN training. (arXiv:2310.17561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17561
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;RNN&#35757;&#32451;&#20013;&#30340;&#20998;&#27495;&#29616;&#35937;&#21644;&#25439;&#22833;&#36339;&#36291;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;RNN&#20013;&#23384;&#22312;&#30528;&#26576;&#20123;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26159;&#29992;&#20110;&#24314;&#27169;&#21644;&#39044;&#27979;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#25512;&#26029;&#21160;&#21147;&#31995;&#32479;&#65288;DS&#65289;&#30340;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#12290;DS&#29702;&#35770;&#30340;&#27010;&#24565;&#24050;&#34987;&#29992;&#20110;&#36827;&#19968;&#27493;&#29702;&#35299;&#32463;&#36807;&#35757;&#32451;&#30340;RNN&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20197;&#21450;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#12290;&#20998;&#27495;&#26159;DS&#20013;&#29305;&#21035;&#37325;&#35201;&#30340;&#29616;&#35937;&#65292;&#21253;&#25324;RNN&#65292;&#22312;&#31995;&#32479;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#21442;&#25968;&#21464;&#21270;&#26102;&#65292;&#25351;&#31995;&#32479;&#30340;&#21160;&#21147;&#34892;&#20026;&#30340;&#25299;&#25169;&#65288;&#23450;&#24615;&#65289;&#21464;&#21270;&#12290;&#20102;&#35299;RNN&#30340;&#20998;&#27495;&#32467;&#26500;&#23558;&#26377;&#21161;&#20110;&#25512;&#26029;&#20854;&#35768;&#22810;&#35745;&#31639;&#21644;&#21160;&#21147;&#23646;&#24615;&#65292;&#20363;&#22914;&#23545;&#21442;&#25968;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#25110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#26159;&#65292;&#20998;&#27495;&#21487;&#33021;&#35299;&#37322;RNN&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#31361;&#28982;&#25439;&#22833;&#36339;&#36291;&#65292;&#36825;&#21487;&#33021;&#20005;&#37325;&#38459;&#30861;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#20808;&#25968;&#23398;&#22320;&#35777;&#26126;&#20102;&#38024;&#23545;&#19968;&#31867;&#22522;&#20110;ReLU&#30340;RNN&#65292;&#30830;&#23454;&#23384;&#22312;&#19968;&#20123;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25351;&#20986;&#36825;&#26159;&#24403;&#21069;&#38382;&#39064;&#34920;&#36848;&#26041;&#24335;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;&#35813;&#38382;&#39064;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#12289;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#12289;&#20840;&#38754;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#21644;&#38450;&#27490;&#25915;&#20987;&#32773;&#35775;&#38382;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#26469;&#37096;&#20998;&#32531;&#35299;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.17559</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;&#20219;&#21153;&#26412;&#36523;&#30340;&#24517;&#28982;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Instability of computer vision models is a necessary result of the task itself. (arXiv:2310.17559v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25351;&#20986;&#36825;&#26159;&#24403;&#21069;&#38382;&#39064;&#34920;&#36848;&#26041;&#24335;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;&#35813;&#38382;&#39064;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#12289;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#12289;&#20840;&#38754;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#21644;&#38450;&#27490;&#25915;&#20987;&#32773;&#35775;&#38382;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#26469;&#37096;&#20998;&#32531;&#35299;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#24615;&#32780;&#20135;&#29983;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#35805;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21487;&#33021;&#21361;&#21450;&#20219;&#20309;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#31283;&#23450;&#24615;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#65306;a) &#25968;&#25454;&#30340;&#23545;&#31216;&#24615;&#65288;&#24179;&#31227;&#19981;&#21464;&#24615;&#65289;&#65292;b) &#20998;&#31867;&#20219;&#21153;&#30340;&#33539;&#30068;&#24615;&#36136;&#65292;&#20197;&#21450;c) &#23558;&#22270;&#20687;&#20998;&#31867;&#20026;&#23545;&#35937;&#26412;&#36523;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;&#19981;&#23436;&#20840;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19981;&#31283;&#23450;&#24615;&#26159;&#24403;&#21069;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#34920;&#36848;&#26041;&#24335;&#25152;&#24517;&#28982;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#38382;&#39064;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#65292;&#20294;&#36890;&#36807;&#20998;&#26512;&#21407;&#22240;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19968;&#20123;&#26041;&#24335;&#26469;&#37096;&#20998;&#32531;&#35299;&#35813;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;i) &#25552;&#39640;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#65292;ii) &#20026;&#22270;&#20687;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;iii) &#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#26631;&#27880;&#65292;&#21644;iv) &#38450;&#27490;&#25915;&#20987;&#32773;&#39057;&#32321;&#35775;&#38382;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples resulting from instability of current computer vision models are an extremely important topic due to their potential to compromise any application. In this paper we demonstrate that instability is inevitable due to a) symmetries (translational invariance) of the data, b) the categorical nature of the classification task, and c) the fundamental discrepancy of classifying images as objects themselves. The issue is further exacerbated by non-exhaustive labelling of the training data. Therefore we conclude that instability is a necessary result of how the problem of computer vision is currently formulated. While the problem cannot be eliminated, through the analysis of the causes, we have arrived at ways how it can be partially alleviated. These include i) increasing the resolution of images, ii) providing contextual information for the image, iii) exhaustive labelling of training data, and iv) preventing attackers from frequent access to the computer vision system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24182;&#33021;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20174;&#32780;&#36991;&#20813;&#37325;&#22797;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#20013;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#30340;&#31574;&#30053;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.17555</link><description>&lt;p&gt;
&#20174;&#21475;&#22836;&#32416;&#27491;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interactive Robot Learning from Verbal Correction. (arXiv:2310.17555v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24182;&#33021;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20174;&#32780;&#36991;&#20813;&#37325;&#22797;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#65292;&#35813;&#31995;&#32479;&#22312;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#20013;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#30340;&#31574;&#30053;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25105;&#20204;&#23558;&#26426;&#22120;&#20154;&#35774;&#35745;&#25104;&#33021;&#22312;&#23478;&#24237;&#31561;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#23398;&#20064;&#21644;&#25913;&#36827;&#34892;&#20026;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;OLAF&#30340;&#26032;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#20801;&#35768;&#26085;&#24120;&#29992;&#25143;&#36890;&#36807;&#21475;&#22836;&#32416;&#27491;&#25945;&#25480;&#26426;&#22120;&#20154;&#65292;&#24403;&#26426;&#22120;&#20154;&#29359;&#38169;&#35823;&#26102;&#65292;&#20363;&#22914;&#35828;&#8220;&#20572;&#19979;&#20320;&#27491;&#22312;&#20570;&#30340;&#20107;&#24773;&#12290;&#20320;&#24212;&#35813;&#38752;&#36817;&#26479;&#23376;&#12290;&#8221; OLAF&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23427;&#33021;&#22815;&#26681;&#25454;&#21475;&#22836;&#21453;&#39304;&#26356;&#26032;&#26426;&#22120;&#20154;&#30340;&#35270;&#35273;&#36816;&#21160;&#31070;&#32463;&#31574;&#30053;&#65292;&#20197;&#36991;&#20813;&#23558;&#26469;&#37325;&#22797;&#38169;&#35823;&#12290;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#20165;&#20165;&#36981;&#24490;&#21475;&#22836;&#21629;&#20196;&#25110;&#32416;&#27491;&#65292;&#32780;&#19981;&#20250;&#20174;&#20013;&#23398;&#20064;&#12290;&#22312;&#27169;&#25311;&#21644;&#29289;&#29702;&#30828;&#20214;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#29992;&#25143;&#25945;&#26426;&#22120;&#20154;&#25191;&#34892;&#38271;&#26102;&#38388;&#32447;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#22312;&#31574;&#30053;&#25104;&#21151;&#29575;&#26041;&#38754;&#24179;&#22343;&#25913;&#21892;&#20102;20.0%&#12290;&#35270;&#39057;&#21644;&#26356;&#22810;&#32467;&#26524;&#21487;&#22312;ht&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying "Stop what you're doing. You should move closer to the cup." A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17552</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#19982;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Based Runtime Monitoring with Interactive Imitation Learning. (arXiv:2310.17552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#26041;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#30340;&#25361;&#25112;&#20173;&#28982;&#21046;&#32422;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26080;&#27861;&#26816;&#27979;&#21644;&#35299;&#20915;&#28508;&#22312;&#25925;&#38556;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#31995;&#32479;&#26080;&#27861;&#22312;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#25237;&#20837;&#20351;&#29992;&#12290;&#26368;&#36817;&#22312;&#20132;&#20114;&#24335;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23433;&#20840;&#25805;&#20316;&#24182;&#22312;&#38271;&#26399;&#37096;&#32626;&#20013;&#19981;&#26029;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25345;&#32493;&#30340;&#20154;&#24037;&#30417;&#30563;&#21644;&#39044;&#20808;&#30340;&#21453;&#39304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36171;&#20104;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#25191;&#34892;&#36807;&#31243;&#20013;&#30417;&#25511;&#21644;&#26816;&#27979;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#34892;&#26102;&#30417;&#25511;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#37096;&#32626;&#25968;&#25454;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#24182;&#39044;&#27979;&#25925;&#38556;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#39044;&#35265;&#26410;&#26469;&#30340;&#25925;&#38556;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#25925;&#38556;&#32463;&#39564;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20262;&#29702;&#20542;&#21521;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17551</link><description>&lt;p&gt;
&#35299;&#35835;&#22823;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unpacking the Ethical Value Alignment in Big Models. (arXiv:2310.17551v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#27169;&#22411;&#20013;&#30340;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#38382;&#39064;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20262;&#29702;&#20542;&#21521;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#20154;&#24037;&#26234;&#33021;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25805;&#20316;&#20449;&#24687;&#19982;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#24182;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#20854;&#22266;&#26377;&#30340;&#20262;&#29702;&#20215;&#20540;&#35266;&#21644;&#28508;&#22312;&#20559;&#35265;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#26410;&#39044;&#26009;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#19982;&#22823;&#22411;&#27169;&#22411;&#26377;&#20851;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#65292;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#20934;&#21017;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#25152;&#24341;&#21457;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;&#20174;&#35268;&#33539;&#20262;&#29702;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26368;&#36817;&#30340;&#35268;&#33539;&#20934;&#21017;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#30340;&#24314;&#35758;&#65292;&#24378;&#35843;&#20102;&#23398;&#26415;&#30028;&#22312;&#24314;&#31435;&#32479;&#19968;&#21644;&#26222;&#36941;&#30340;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26694;&#26550;&#26041;&#38754;&#30340;&#21512;&#20316;&#21162;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#35843;&#26597;&#20102;&#24403;&#21069;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20542;&#21521;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20215;&#20540;&#35266;&#23545;&#40784;&#31639;&#27861;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#23454;&#29616;&#20262;&#29702;&#20215;&#20540;&#23545;&#40784;&#26102;&#36935;&#21040;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17550</link><description>&lt;p&gt;
&#20154;&#31867;&#24341;&#23548;&#30340;&#22797;&#26434;&#24230;&#25511;&#21046;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-Guided Complexity-Controlled Abstractions. (arXiv:2310.17550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#26469;&#25552;&#39640;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#20339;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#20063;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20294;&#36825;&#20123;&#34920;&#31034;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22312;&#21508;&#31181;&#25277;&#35937;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#8220;&#40479;&#8221;&#19982;&#8220;&#40635;&#38592;&#8221;&#65289;&#19978;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65288;&#21363;&#27010;&#24565;&#25110;&#21333;&#35789;&#65289;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#20351;&#29992;&#36866;&#24403;&#30340;&#25277;&#35937;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#34920;&#31034;&#20998;&#24067;&#30340;&#29109;&#26469;&#25511;&#21046;&#34920;&#31034;&#30340;&#22797;&#26434;&#24615;&#65288;&#22823;&#33268;&#19978;&#26159;&#20026;&#32534;&#30721;&#36755;&#20837;&#20998;&#37197;&#20102;&#22810;&#23569;&#20301;&#65289;&#12290;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#24102;&#26631;&#31614;&#30340;&#31034;&#20363;&#29992;&#20110;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65288;1&#65289;&#35843;&#25972;&#34920;&#31034;&#20197;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#25903;&#25345;&#26368;&#39640;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#19968;&#20010;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#31163;&#25955;&#34920;&#31034;&#30340;&#21487;&#35270;&#21270;&#26469;&#30830;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24403;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., ``bird'' vs. ``sparrow'') and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27573;&#21644;&#22238;&#28335;&#26469;&#20811;&#26381;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.17537</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#27573;&#21644;&#22238;&#28335;&#27861;&#20811;&#26381;&#22909;&#22855;&#24515;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity. (arXiv:2310.17537v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31070;&#32463;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#27573;&#21644;&#22238;&#28335;&#26469;&#20811;&#26381;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20869;&#22312;&#22870;&#21169;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#31232;&#30095;&#22870;&#21169;&#30340;&#22256;&#38590;&#25506;&#32034;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#21069;&#21521;&#27169;&#22411;&#39044;&#27979;&#35823;&#24046;&#29983;&#25104;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36825;&#20123;&#35823;&#24046;&#38543;&#30528;&#29615;&#22659;&#30340;&#29087;&#24713;&#31243;&#24230;&#32780;&#20943;&#23569;&#65292;&#24182;&#28608;&#21169;&#20195;&#29702;&#25506;&#32034;&#26032;&#30340;&#29366;&#24577;&#12290;&#34429;&#28982;&#22522;&#20110;&#39044;&#27979;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#20195;&#29702;&#35299;&#20915;&#22256;&#38590;&#30340;&#25506;&#32034;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#19988;&#23454;&#38469;&#19978;&#20250;&#22312;&#35775;&#38382;&#30340;&#29366;&#24577;&#19978;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#26684;&#29366;&#19990;&#30028;&#29615;&#22659;&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26465;&#20214;&#21644;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#21644;&#21160;&#29289;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#26041;&#27861;FARCuriosity&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20998;&#27573;&#21644;&#22238;&#28335;&#65306;&#20195;&#29702;&#26681;&#25454;&#24778;&#35766;&#24615;&#23545;&#29615;&#22659;&#36827;&#34892;&#20998;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#20998;&#27573;&#20351;&#29992;&#19981;&#21516;&#30340;&#26412;&#22320;&#22909;&#22855;&#27169;&#22359;&#65288;&#22522;&#20110;&#39044;&#27979;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#65289;&#65292;&#20197;&#20351;&#27169;&#22359;&#19981;&#26159;&#22312;&#25972;&#20010;&#29615;&#22659;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#27599;&#20010;&#20998;&#27573;&#20013;&#65292;&#20195;&#29702;&#21487;&#20197;&#21516;&#26102;&#23384;&#20648;&#24182;&#22238;&#28335;&#20808;&#21069;&#30340;&#22870;&#21169;&#20540;&#20197;&#24212;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning methods exhibit impressive performance on a range of tasks but still struggle on hard exploration tasks in large environments with sparse rewards. To address this, intrinsic rewards can be generated using forward model prediction errors that decrease as the environment becomes known, and incentivize an agent to explore novel states. While prediction-based intrinsic rewards can help agents solve hard exploration tasks, they can suffer from catastrophic forgetting and actually increase at visited states. We first examine the conditions and causes of catastrophic forgetting in grid world environments. We then propose a new method FARCuriosity, inspired by how humans and animals learn. The method depends on fragmentation and recall: an agent fragments an environment based on surprisal, and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment so that modules are not trained on the entire environment. At each fragm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17534</link><description>&lt;p&gt;
SoK&#65306;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#28041;&#21450;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#23545;&#23545;&#25163;&#30340;&#30693;&#35782;&#20551;&#35774;&#19981;&#21516;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#22260;&#32469;&#23041;&#32961;&#27169;&#22411;&#36827;&#34892;&#32452;&#32455;&#30340;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#21270;&#35813;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#23041;&#32961;&#31354;&#38388;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#21453;&#39304;&#31890;&#24230;&#12289;&#20132;&#20114;&#24335;&#26597;&#35810;&#30340;&#35775;&#38382;&#21644;&#25915;&#20987;&#32773;&#21487;&#29992;&#30340;&#36741;&#21161;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#19977;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26032;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;1) &#23613;&#31649;&#26377;&#24191;&#27867;&#25991;&#29486;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#26080;&#27861;&#36890;&#36807;&#20174;&#24050;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#31616;&#21333;&#22320;&#25913;&#36827;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;&#24050;&#30693;&#39046;&#22495;&#20174;&#23436;&#25972;&#32622;&#20449;&#21521;&#37327;&#35775;&#38382;&#30340;&#25216;&#26415;&#36866;&#24212;&#21040;&#35775;&#38382;&#21069;k&#20010;&#32622;&#20449;&#24471;&#20998;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35774;&#32622;&#20013;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#20294;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#23427;&#22312;&#20165;&#33719;&#24471;&#39044;&#27979;&#26631;&#31614;&#30340;&#26356;&#20005;&#26684;&#35774;&#32622;&#20013;&#20173;&#28982;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17526</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#20154;&#31867;&#22312;&#31995;&#32479;&#35780;&#20215;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#65311;&#35780;&#20272;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#22312;&#22810;&#31181;&#35821;&#35328;&#30340;&#21516;&#34892;&#35780;&#23457;&#25991;&#29486;&#21644;&#28784;&#33394;&#25991;&#29486;&#31579;&#36873;&#21644;&#25552;&#21462;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#22312;&#35843;&#25972;&#20102;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#21518;&#65292;&#20854;&#22312;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#35780;&#20215;&#23545;&#20110;&#25351;&#23548;&#23454;&#36341;&#12289;&#30740;&#31350;&#21644;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#20154;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#33021;&#22815;&#21152;&#24555;&#21644;&#33258;&#21160;&#21270;&#31995;&#32479;&#35780;&#20215;&#30340;&#36807;&#31243;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#32463;&#36807;&#20840;&#38754;&#35780;&#20272;&#65292;&#32780;&#19988;&#36824;&#27809;&#26377;&#30740;&#31350;&#27979;&#35797;&#36807;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;LLM&#8212;&#8212;GPT-4&#12290;&#26412;&#39044;&#27880;&#20876;&#30740;&#31350;&#37319;&#29992;&#8220;&#26080;&#20154;&#21442;&#19982;&#8221;&#30340;&#26041;&#27861;&#35780;&#20272;&#20102;GPT-4&#22312;&#26631;&#39064;/&#25688;&#35201;&#31579;&#36873;&#12289;&#20840;&#25991;&#23457;&#26597;&#21644;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#22312;&#19981;&#21516;&#25991;&#29486;&#31867;&#22411;&#21644;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24230;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#24403;&#65292;&#20294;&#32467;&#26524;&#21463;&#21040;&#20598;&#28982;&#19968;&#33268;&#24615;&#21644;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#35843;&#25972;&#20102;&#36825;&#20123;&#22240;&#32032;&#21518;&#65292;&#25968;&#25454;&#25552;&#21462;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#27700;&#24179;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;&#20351;&#29992;&#39640;&#21487;&#38752;&#24615;&#25552;&#31034;&#36827;&#34892;&#31579;&#36873;&#30340;&#30740;&#31350;&#20013;&#65292;&#31579;&#36873;&#20840;&#25991;&#25991;&#29486;&#30340;&#34920;&#29616;&#27700;&#24179;&#22312;&#19981;&#21516;&#38454;&#27573;&#21644;&#35821;&#35328;&#19978;&#22343;&#20026;&#26080;&#21040;&#20013;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.17513</link><description>&lt;p&gt;
&#12298;&#20302;&#31209;&#36866;&#24212;&#30340;&#34920;&#36798;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#26102;&#65292;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#30697;&#38453;&#30340;&#20302;&#31209;&#36866;&#24212;&#24615;&#65292;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;LoRA&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#29702;&#35770;&#35282;&#24230;&#20998;&#26512;LoRA&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#39318;&#27425;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#26524;LoRA-rank&#8805;&#65288;f&#30340;&#23485;&#24230;&#65289;&#215;&#65288;&#30446;&#26631;&#27169;&#22411;&#30340;&#28145;&#24230;/ f&#30340;&#28145;&#24230;&#65289;&#65292;&#21017;LoRA&#21487;&#20197;&#20351;&#20219;&#20309;&#27169;&#22411;f&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#36739;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;f&#12290;&#24403;LoRA-rank&#20302;&#20110;&#38408;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#37327;&#21270;&#20102;&#36924;&#36817;&#35823;&#24046;&#12290;&#23545;&#20110;Transformer&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20219;&#20309;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;rank-&#65288;&#23884;&#20837;&#22823;&#23567;/ 2&#65289;&#30340;LoRA&#36866;&#37197;&#22120;&#36866;&#24212;&#20110;&#30456;&#21516;&#22823;&#23567;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17512</link><description>&lt;p&gt;
CompeteAI:&#29702;&#35299;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#31454;&#20105;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents. (arXiv:2310.17512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#31454;&#20105;&#29615;&#22659;&#24182;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#31454;&#20105;&#21487;&#20197;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#36716;&#21464;&#21644;&#37319;&#21462;&#26032;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#65292;&#22914;&#20010;&#20154;&#21161;&#29702;&#25110;&#20107;&#20214;&#35268;&#21010;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#19982;&#21327;&#20316;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#21478;&#19968;&#20010;&#37325;&#35201;&#26426;&#21046;&#8212;&#8212;&#31454;&#20105;&#65292;&#23427;&#26159;&#31038;&#20250;&#21644;&#32463;&#27982;&#21457;&#23637;&#30340;&#25512;&#21160;&#21147;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLM&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31454;&#20105;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31454;&#20105;&#29615;&#22659;&#65292;&#27169;&#25311;&#20102;&#19968;&#20010;&#30001;&#39184;&#39302;&#26234;&#33021;&#20307;&#21644;&#39038;&#23458;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#34394;&#25311;&#22478;&#38215;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39184;&#39302;&#26234;&#33021;&#20307;&#30456;&#20114;&#31454;&#20105;&#20197;&#21560;&#24341;&#26356;&#22810;&#39038;&#23458;&#65292;&#36825;&#31181;&#31454;&#20105;&#20419;&#20351;&#23427;&#20204;&#36827;&#34892;&#36716;&#21464;&#65292;&#27604;&#22914;&#22521;&#20859;&#26032;&#30340;&#36816;&#33829;&#31574;&#30053;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20174;&#31038;&#20250;&#23398;&#20064;&#21040;&#39532;&#22826;&#25928;&#24212;&#31561;&#22810;&#20010;&#26377;&#36259;&#21457;&#29616;&#65292;&#19982;&#29616;&#26377;&#30340;&#31038;&#20250;&#23398;&#21644;&#32463;&#27982;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#21644;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17492</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#65306;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach. (arXiv:2310.17492v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22120;&#36741;&#21161;&#30340;&#31227;&#21160;&#36793;&#32536;&#35843;&#20248;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#32534;&#25490;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#21644;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#39640;&#25928;&#37096;&#32626;&#21644;&#31934;&#35843;&#22522;&#30784;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#33539;&#24335;&#65292;&#23558;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#19982;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#65292;&#26088;&#22312;&#22686;&#24378;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#21019;&#26032;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#26550;&#26500;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#20998;&#21106;&#20026;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#12290;&#36825;&#31181;&#35774;&#35745;&#19981;&#20165;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#30830;&#20445;&#20102;&#36866;&#24212;&#24615;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#31934;&#35843;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#25311;&#22120;&#36866;&#37197;&#22120;&#32467;&#26500;&#30340;&#38656;&#27714;&#36827;&#34892;&#31934;&#35843;&#12290;&#20026;&#24212;&#23545;&#35813;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31574;&#30053;&#65292;&#25797;&#38271;&#22788;&#29702;&#28151;&#21512;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65292;&#30830;&#20445;&#21160;&#24577;&#21644;&#26368;&#20248;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficient deployment and fine-tuning of foundation models are pivotal in contemporary artificial intelligence. In this study, we present a groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation models, specifically designed to enhance local task performance on user equipment (UE). Central to our approach is the innovative Emulator-Adapter architecture, segmenting the foundation model into two cohesive modules. This design not only conserves computational resources but also ensures adaptability and fine-tuning efficiency for downstream tasks. Additionally, we introduce an advanced resource allocation mechanism that is fine-tuned to the needs of the Emulator-Adapter structure in decentralized settings. To address the challenges presented by this system, we employ a hybrid multi-agent Deep Reinforcement Learning (DRL) strategy, adept at handling mixed discrete-continuous action spaces, ensuring dynamic and optimal resource allocations. Our comprehensive simula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17490</link><description>&lt;p&gt;
&#25552;&#39640;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#23545;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24178;&#25200;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#26080;&#20851;&#25991;&#26723;&#30340;&#24178;&#25200;&#26469;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;LLMs&#21463;&#21040;&#24178;&#25200;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22320;&#25913;&#21892;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20351;&#24471;&#22312;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;(ODQA)&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#38405;&#35835;&#22120;&#30456;&#23545;&#20110;&#26816;&#32034;&#22120;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#19968;&#31181;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#25104;&#26412;&#21644;&#26631;&#27880;&#25968;&#25454;&#38656;&#27714;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30001;&#20110;&#26816;&#32034;&#21040;&#30340;&#26080;&#20851;&#25991;&#26723;&#20197;&#21450;&#20316;&#20026;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#26102;&#29983;&#25104;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#32780;&#21463;&#21040;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21542;&#23450;&#30340;&#25351;&#20196;&#21644;&#20998;&#25968;&#35843;&#25972;&#30340;&#24178;&#25200;&#24863;&#30693;&#30340;&#31572;&#26696;&#36873;&#25321;(DAS)&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25991;&#26723;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22788;&#29702;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24178;&#25200;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38754;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#32780;&#22256;&#38590;&#37325;&#37325;&#30340;&#30417;&#30563;&#24335;&#38405;&#35835;&#22120;&#19981;&#21516;&#65292;&#38646;&#26679;&#26412;&#38405;&#35835;&#22120;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#20998;&#26512;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#26469;&#32771;&#34385;&#20559;&#35265;&#65292;&#24182;&#23545;&#21442;&#25968;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#25506;&#31350;&#20854;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#21644;&#25968;&#25454;&#25311;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17489</link><description>&lt;p&gt;
&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#65306;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bias in Evaluation Processes: An Optimization-Based Model. (arXiv:2310.17489v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#21270;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#20998;&#26512;&#12290;&#27169;&#22411;&#36890;&#36807;&#23558;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#26469;&#32771;&#34385;&#20559;&#35265;&#65292;&#24182;&#23545;&#21442;&#25968;&#36827;&#34892;&#30740;&#31350;&#65292;&#20197;&#25506;&#31350;&#20854;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#21644;&#25968;&#25454;&#25311;&#21512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35832;&#22914;&#25307;&#29983;&#21644;&#25307;&#32856;&#31561;&#35774;&#32622;&#20013;&#65292;&#20851;&#20110;&#20010;&#20154;&#31038;&#20250;&#26174;&#33879;&#23646;&#24615;&#30340;&#20559;&#35265;&#24050;&#34987;&#24191;&#27867;&#35760;&#24405;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#35780;&#20272;&#36807;&#31243;&#35270;&#20026;&#23558;&#20010;&#20154;&#23545;&#20219;&#21153;&#30340;&#30495;&#23454;&#25928;&#29992;&#20998;&#24067;&#36716;&#21270;&#20026;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#65292;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#22312;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#21442;&#25968;&#65292;&#34987;&#35270;&#20026;&#23548;&#33268;&#20559;&#35265;&#30340;&#22240;&#32032;&#65306;&#20449;&#24687;&#32422;&#26463;&#20013;&#30340;&#36164;&#28304;&#20449;&#24687;&#20132;&#25442;&#21442;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#39118;&#38505;&#21388;&#24694;&#21442;&#25968;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#30001;&#25105;&#20204;&#30340;&#27169;&#22411;&#20135;&#29983;&#30340;&#20998;&#24067;&#65292;&#24182;&#30740;&#31350;&#20102;&#21442;&#25968;&#23545;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#36755;&#20986;&#20016;&#23500;&#20102;&#21487;&#29992;&#20110;&#25429;&#25417;&#35266;&#23519;&#35780;&#20272;&#20013;&#32676;&#32452;&#38388;&#21464;&#21270;&#30340;&#20998;&#24067;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#25311;&#21512;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#20171;&#20837;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17462</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#36816;&#21160;&#23450;&#24459;&#20174;2D&#26631;&#27880;&#20013;&#23398;&#20064;&#21333;&#30446;3D&#29289;&#20307;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;3D&#29289;&#20307;&#23450;&#20301;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#29289;&#29702;&#30693;&#35782;&#21644;&#31616;&#21333;&#30340;2D&#26631;&#27880;&#65292;&#20174;&#21333;&#20010;&#26657;&#20934;&#30456;&#26426;&#30340;&#21333;&#20010;&#22270;&#20687;&#20013;&#31934;&#30830;&#23450;&#20301;3D&#29289;&#20307;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;3D&#26631;&#27880;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#38544;&#21547;&#30340;&#31532;&#19977;&#20010;&#32500;&#24230;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#27492;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#20165;&#20026;6&#21400;&#31859;&#12290;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#27861;&#25910;&#38598;3D&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#20316;&#20026;&#23398;&#20064;3D&#29289;&#20307;&#23450;&#20301;&#20272;&#35745;&#30340;&#19968;&#27493;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26469;&#33258;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25968;&#25454;&#38598;LSA64&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;3200&#20010;&#35270;&#39057;&#65292;&#21253;&#25324;64&#20010;&#19981;&#21516;&#30340;&#38463;&#26681;&#24311;&#25163;&#35821;&#25163;&#21183;&#65292;&#20026;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#38463;&#26681;&#24311;&#25163;&#21183;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.17429</link><description>&lt;p&gt;
LSA64&#65306;&#38463;&#26681;&#24311;&#25163;&#35821;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LSA64: An Argentinian Sign Language Dataset. (arXiv:2310.17429v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17429
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26469;&#33258;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25968;&#25454;&#38598;LSA64&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;3200&#20010;&#35270;&#39057;&#65292;&#21253;&#25324;64&#20010;&#19981;&#21516;&#30340;&#38463;&#26681;&#24311;&#25163;&#35821;&#25163;&#21183;&#65292;&#20026;&#26500;&#24314;&#19968;&#20010;&#20840;&#38754;&#30340;&#38463;&#26681;&#24311;&#25163;&#21183;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#26159;&#28085;&#30422;&#20154;&#26426;&#20132;&#20114;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#31283;&#20581;&#30340;&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#32763;&#35793;&#36807;&#31243;&#21644;&#34701;&#21512;&#21548;&#38556;&#20154;&#22763;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#25945;&#25480;&#21548;&#35273;&#20154;&#32676;&#25163;&#35821;&#12290;&#19981;&#21516;&#22269;&#23478;&#29978;&#33267;&#22320;&#21306;&#30340;&#25163;&#35821;&#24046;&#24322;&#26174;&#33879;,&#23427;&#20204;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#19982;&#20070;&#38754;&#35821;&#35328;&#20063;&#19981;&#21516;&#12290;&#23613;&#31649;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#25216;&#26415;&#22823;&#33268;&#30456;&#21516;&#65292;&#20294;&#35201;&#35757;&#32451;&#19968;&#20010;&#26032;&#35821;&#35328;&#30340;&#35782;&#21035;&#31995;&#32479;&#65292;&#23601;&#38656;&#35201;&#26377;&#36825;&#20010;&#35821;&#35328;&#30340;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26469;&#33258;&#38463;&#26681;&#24311;&#25163;&#35821;&#65288;LSA&#65289;&#30340;64&#20010;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;LSA64&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;10&#20010;&#21463;&#35797;&#32773;&#24405;&#21046;&#30340;64&#20010;&#19981;&#21516;LSA&#25163;&#21183;&#30340;3200&#20010;&#35270;&#39057;&#65292;&#26159;&#26500;&#24314;&#38463;&#26681;&#24311;&#25163;&#21183;&#20840;&#38754;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sign language recognition is a research area that encompasses human-computer interaction, computer vision and machine learning. Robust automatic recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language to the hearing population. Sign languages differ significantly in different countries and even regions, and their syntax and semantics are different as well from those of written languages. While the techniques for automatic sign language recognition are mostly the same for different languages, training a recognition system for a new language requires having an entire dataset for that language. This paper presents a dataset of 64 signs from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains 3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first step towards building a comprehensive research-level dataset of Argentinian signs, specific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#39318;&#27425;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ProbSom&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#21644;&#25163;&#21183;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2310.17427</link><description>&lt;p&gt;
&#20351;&#29992;ProbSom&#36827;&#34892;&#38463;&#26681;&#24311;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#39318;&#27425;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ProbSom&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#21644;&#25163;&#21183;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#22312;&#30446;&#21069;&#30340;&#30740;&#31350;&#20013;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#35782;&#21035;&#26159;&#20154;&#26426;&#20132;&#20114;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#36825;&#26082;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#28041;&#21450;&#35270;&#39057;&#22788;&#29702;&#12289;&#22270;&#20687;&#22788;&#29702;&#12289;&#26234;&#33021;&#31995;&#32479;&#21644;&#35821;&#35328;&#23398;&#31561;&#22810;&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;&#20171;&#20837;&#65292;&#21448;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25163;&#35821;&#35782;&#21035;&#65292;&#21487;&#20197;&#36741;&#21161;&#32763;&#35793;&#36807;&#31243;&#21644;&#34701;&#20837;&#21548;&#38556;&#20154;&#22763;&#30340;&#25972;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#38463;&#26681;&#24311;&#25163;&#35821;&#65288;LSA&#65289;&#30340;&#25163;&#21183;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#36804;&#20170;&#20026;&#27490;&#20960;&#20046;&#27809;&#26377;&#35752;&#35770;&#36807;&#30340;&#35805;&#39064;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;ProbSom&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#30340;&#30417;&#30563;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#21518;&#32493;&#25163;&#21183;&#20998;&#31867;&#12290;&#35813;&#25216;&#26415;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sign language recognition is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people.  This paper offers two main contributions: first, the creation of a database of handshapes for the Argentinian Sign Language (LSA), which is a topic that has barely been discussed so far. Secondly, a technique for image processing, descriptor extraction and subsequent handshape classification using a supervised adaptation of self-organizing maps that is called ProbSom. This technique is compared to others in the state of the art, such as Support Vector Machines (SVM), Random Forests, and Neural Networks.  The database that was built conta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#25551;&#36848;&#22120;&#65292;&#21160;&#20316;&#36816;&#21160;&#30340;&#20998;&#24067;&#25551;&#36848;&#22120;&#65288;DAM&#65289;&#65292;&#36890;&#36807;&#35745;&#31639;&#20851;&#33410;&#36816;&#21160;&#26041;&#21521;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#30452;&#26041;&#22270;&#26469;&#34920;&#31034;&#21160;&#20316;&#12290;&#35813;&#25551;&#36848;&#22120;&#22312;&#22810;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.17421</link><description>&lt;p&gt;
&#21160;&#20316;&#36816;&#21160;&#30340;&#20998;&#24067;&#25551;&#36848;&#22120;&#65288;DAM&#65289;&#65306;&#29992;&#20110;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#30340;&#25551;&#36848;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition. (arXiv:2310.17421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#20316;&#25551;&#36848;&#22120;&#65292;&#21160;&#20316;&#36816;&#21160;&#30340;&#20998;&#24067;&#25551;&#36848;&#22120;&#65288;DAM&#65289;&#65292;&#36890;&#36807;&#35745;&#31639;&#20851;&#33410;&#36816;&#21160;&#26041;&#21521;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#30452;&#26041;&#22270;&#26469;&#34920;&#31034;&#21160;&#20316;&#12290;&#35813;&#25551;&#36848;&#22120;&#22312;&#22810;&#20010;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39592;&#39612;&#25968;&#25454;&#20013;&#36827;&#34892;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#35768;&#22810;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#65292;&#30446;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20173;&#26410;&#33021;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#20316;&#25551;&#36848;&#22120;&#65292;&#21363;&#21160;&#20316;&#36816;&#21160;&#30340;&#20998;&#24067;&#25551;&#36848;&#22120;&#65288;DAM&#65289;&#65292;&#23427;&#22522;&#20110;&#20851;&#33410;&#22312;&#24103;&#20043;&#38388;&#36816;&#21160;&#30340;&#26041;&#21521;&#20998;&#24067;&#65292;&#36890;&#36807;&#35745;&#31639;&#22312;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#21487;&#33021;&#36816;&#21160;&#30340;&#26041;&#21521;&#38598;&#21512;&#19978;&#30340;&#24402;&#19968;&#21270;&#30452;&#26041;&#22270;&#26469;&#34920;&#31034;&#12290;&#35813;&#25551;&#36848;&#22120;&#36890;&#36807;&#32858;&#31867;&#33719;&#24471;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#20851;&#33410;&#36816;&#21160;&#26041;&#21521;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#31383;&#21475;&#26041;&#26696;&#37096;&#20998;&#20445;&#30041;&#20854;&#26102;&#38388;&#32467;&#26500;&#12290;&#19982;&#26631;&#20934;&#20998;&#31867;&#22120;&#30456;&#32467;&#21512;&#65292;&#35813;&#25551;&#36848;&#22120;&#22312;&#35768;&#22810;&#33879;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#20960;&#31181;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human action recognition from skeletal data is an important and active area of research in which the state of the art has not yet achieved near-perfect accuracy on many well-known datasets. In this paper, we introduce the Distribution of Action Movements Descriptor, a novel action descriptor based on the distribution of the directions of the motions of the joints between frames, over the set of all possible motions in the dataset. The descriptor is computed as a normalized histogram over a set of representative directions of the joints, which are in turn obtained via clustering. While the descriptor is global in the sense that it represents the overall distribution of movement directions of an action, it is able to partially retain its temporal structure by applying a windowing scheme.  The descriptor, together with a standard classifier, outperforms several state-of-the-art techniques on many well-known datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;IMF&#20013;&#24341;&#23548;&#26410;&#30693;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20043;&#38388;&#21363;&#26102;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24847;&#22270;&#39537;&#21160;&#30340;&#31649;&#29702;&#36827;&#34892;&#23618;&#27425;&#21327;&#35843;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#31995;&#32479;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.17416</link><description>&lt;p&gt;
&#30446;&#26631;&#36275;&#22815;&#65306;&#22312;IMF&#20013;&#24341;&#23548;&#26410;&#30693;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20043;&#38388;&#30340;&#21363;&#26102;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs. (arXiv:2310.17416v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;IMF&#20013;&#24341;&#23548;&#26410;&#30693;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20043;&#38388;&#21363;&#26102;&#21512;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#24847;&#22270;&#39537;&#21160;&#30340;&#31649;&#29702;&#36827;&#34892;&#23618;&#27425;&#21327;&#35843;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#31995;&#32479;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#39537;&#21160;&#30340;&#31649;&#29702;&#22312;&#19979;&#19968;&#20195;&#31227;&#21160;&#32593;&#32476;&#20013;&#23558;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#31649;&#29702;&#36164;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#20542;&#21521;&#20110;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#26399;&#26395;&#12290;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65289;&#22312;&#32593;&#32476;&#20999;&#29255;&#19978;&#23384;&#22312;&#20914;&#31361;&#30340;&#26399;&#26395;&#26102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#37197;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#31995;&#32479;&#24448;&#24448;&#26356;&#21152;&#22797;&#26434;&#65292;&#26080;&#27861;&#20165;&#36890;&#36807;&#21333;&#29420;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#36890;&#24120;&#23384;&#22312;&#30528;&#24847;&#22270;&#23454;&#29616;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#12289;&#33258;&#21033;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#38656;&#35201;&#30001;&#19968;&#20010;&#30417;&#25511;&#25110;&#25511;&#21046;&#26234;&#33021;&#20307;&#36827;&#19968;&#27493;&#21327;&#35843;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#21487;&#33021;&#20197;adhoc&#26041;&#24335;&#36827;&#20837;&#31995;&#32479;&#65292;&#38656;&#35201;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#26234;&#33021;&#20307;&#19968;&#36215;&#21327;&#35843;&#12290;&#22522;&#20110;&#25972;&#20010;&#31995;&#32479;&#30340;&#37325;&#26032;&#35757;&#32451;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#30456;&#20851;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#21327;&#35843;&#39044;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;adhoc&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent-based management will play a critical role in achieving customers' expectations in the next-generation mobile networks. Traditional methods cannot perform efficient resource management since they tend to handle each expectation independently. Existing approaches, e.g., based on multi-agent reinforcement learning (MARL) allocate resources in an efficient fashion when there are conflicting expectations on the network slice. However, in reality, systems are often far more complex to be addressed by a standalone MARL formulation. Often there exists a hierarchical structure of intent fulfilment where multiple pre-trained, self-interested agents may need to be further orchestrated by a supervisor or controller agent. Such agents may arrive in the system adhoc, which then needs to be orchestrated along with other available agents. Retraining the whole system every time is often infeasible given the associated time and cost. Given the challenges, such adhoc coordination of pre-trained s
&lt;/p&gt;</description></item><item><title>PETA&#36890;&#36807;&#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#22312;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17415</link><description>&lt;p&gt;
PETA: &#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#23545;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications. (arXiv:2310.17415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17415
&lt;/p&gt;
&lt;p&gt;
PETA&#36890;&#36807;&#35780;&#20272;&#20122;&#35789;&#20999;&#20998;&#22312;&#34507;&#30333;&#36136;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#25429;&#25417;&#21407;&#22987;&#32467;&#26500;&#20013;&#30340;&#36827;&#21270;&#20449;&#24687;&#65292;&#23545;&#34507;&#30333;&#36136;&#24037;&#31243;&#20855;&#26377;&#37325;&#35201;&#23454;&#29992;&#20215;&#20540;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#34507;&#30333;&#36136;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#25968;&#25454;&#37327;&#36739;&#23567;&#65292;&#32452;&#21512;&#31354;&#38388;&#26377;&#38480;&#12290;&#36873;&#25321;&#21512;&#36866;&#30340;&#35789;&#27719;&#34920;&#22823;&#23567;&#26469;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#22522;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;PETA&#20351;&#29992;&#20102;&#19977;&#31181;&#26631;&#35760;&#21270;&#26041;&#27861;&#65292;&#22312;14&#31181;&#19981;&#21516;&#30340;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;33&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25968;&#21315;&#27425;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#20004;&#20010;&#20998;&#31867;&#22836;&#21644;&#19977;&#20010;&#38543;&#26426;&#31181;&#23376;&#20197;&#20943;&#36731;&#28508;&#22312;&#20559;&#35265;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#35789;&#27719;&#34920;&#22823;&#23567;&#22312;50&#20197;&#19978;&#22823;&#32422;&#33021;&#22815;&#33719;&#24471;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21512;&#25104;&#20174;&#31995;&#32479;&#25191;&#34892;&#20013;&#24471;&#21040;&#24418;&#24335;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#20219;&#21153;&#31616;&#21270;&#20026;&#32447;&#24615;&#23454;&#25968;&#31639;&#26415;&#38382;&#39064;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21512;&#25104;&#20102;&#20855;&#26377;&#26377;&#38480;&#20808;&#34892;&#26597;&#30475;&#30340;&#31616;&#27905;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17410</link><description>&lt;p&gt;
&#22312;&#24230;&#37327;&#26102;&#24207;&#36923;&#36753;&#20013;&#21512;&#25104;&#39640;&#25928;&#30340;&#21487;&#30417;&#25511;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic. (arXiv:2310.17410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21512;&#25104;&#20174;&#31995;&#32479;&#25191;&#34892;&#20013;&#24471;&#21040;&#24418;&#24335;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#20219;&#21153;&#31616;&#21270;&#20026;&#32447;&#24615;&#23454;&#25968;&#31639;&#26415;&#38382;&#39064;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#21512;&#25104;&#20102;&#20855;&#26377;&#26377;&#38480;&#20808;&#34892;&#26597;&#30475;&#30340;&#31616;&#27905;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#34892;&#26102;&#39564;&#35777;&#20013;&#65292;&#25163;&#21160;&#20026;&#30417;&#25511;&#31995;&#32479;&#25191;&#34892;&#35268;&#33539;&#21270;&#19968;&#20010;&#35268;&#33539;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#31995;&#32479;&#25191;&#34892;&#20013;&#33258;&#21160;&#21512;&#25104;&#24418;&#24335;&#21270;&#35268;&#33539;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#35268;&#33539;&#35821;&#35328;&#24230;&#37327;&#26102;&#24207;&#36923;&#36753;&#65288;MTL&#65289;&#65292;&#35813;&#35821;&#35328;&#29305;&#21035;&#36866;&#29992;&#20110;&#20026;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#25351;&#23450;&#26102;&#38388;&#23646;&#24615;&#12290;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;&#21512;&#25104;&#26102;&#24207;&#36923;&#36753;&#20844;&#24335;&#30340;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#20844;&#24335;&#30340;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30417;&#25511;&#30340;&#25928;&#29575;&#32780;&#35328;&#65292;&#38500;&#20102;&#22823;&#23567;&#65292;&#35268;&#33539;&#25152;&#38656;&#30340;&#8220;&#20808;&#34892;&#26597;&#30475;&#8221;&#37327;&#20063;&#21464;&#24471;&#30456;&#20851;&#65292;&#23588;&#20854;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#26469;&#35828;&#12290;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#20855;&#26377;&#26377;&#38480;&#20808;&#34892;&#26597;&#30475;&#30340;&#31616;&#27905;&#20844;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#21512;&#25104;&#20219;&#21153;&#31616;&#21270;&#20026;&#19968;&#31995;&#21015;&#32447;&#24615;&#23454;&#25968;&#31639;&#26415;&#38382;&#39064;&#30340;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In runtime verification, manually formalizing a specification for monitoring system executions is a tedious and error-prone process. To address this issue, we consider the problem of automatically synthesizing formal specifications from system executions. To demonstrate our approach, we consider the popular specification language Metric Temporal Logic (MTL), which is particularly tailored towards specifying temporal properties for cyber-physical systems (CPS). Most of the classical approaches for synthesizing temporal logic formulas aim at minimizing the size of the formula. However, for efficiency in monitoring, along with the size, the amount of "lookahead" required for the specification becomes relevant, especially for safety-critical applications. We formalize this notion and devise a learning algorithm that synthesizes concise formulas having bounded lookahead. To do so, our algorithm reduces the synthesis task to a series of satisfiability problems in Linear Real Arithmetic (LRA)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25935;&#24863;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17404</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21464;&#24615;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25935;&#24863;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#21464;&#24615;&#23545;&#35768;&#22810;&#20219;&#21153;&#37117;&#26159;&#26377;&#29992;&#19988;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#21464;&#24615;&#34920;&#31034;&#23578;&#26410;&#34987;&#26126;&#30830;&#34920;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#19981;&#21464;&#24615;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#20043;&#21069;&#23450;&#20041;&#30340;&#27979;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#19981;&#21464;&#24615;&#26356;&#20026;&#25935;&#24863;&#12290;&#25105;&#20204;&#22312;&#20223;&#23556;&#21464;&#25442;&#39046;&#22495;&#21644;CIFAR10&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#21450;&#20854;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#31283;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;CNN&#27169;&#22411;&#36827;&#34892;&#20102;&#39318;&#27425;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20869;&#37096;&#19981;&#21464;&#24615;&#23545;&#20110;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#38750;&#24120;&#31283;&#23450;&#65292;&#20294;&#23545;&#20110;&#25968;&#25454;&#38598;&#25110;&#21464;&#25442;&#30340;&#25913;&#21464;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#23558;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#27602;&#24615;&#26816;&#27979;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#25581;&#31034;&#20102;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#22312;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.17389</link><description>&lt;p&gt;
ToxicChat: &#25581;&#31034;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#30340;&#27602;&#24615;&#26816;&#27979;&#38544;&#34255;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#19968;&#20010;&#22522;&#20110;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#27602;&#24615;&#26816;&#27979;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#25581;&#31034;&#20102;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#22312;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23545;&#35805;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22914;&#20170;&#32500;&#25345;&#19968;&#20010;&#38750;&#27602;&#24615;&#30340;&#29992;&#25143;-AI&#20114;&#21160;&#29615;&#22659;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27602;&#24615;&#26816;&#27979;&#24037;&#20316;&#22823;&#22810;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23548;&#20986;&#30340;&#22522;&#20934;&#65292;&#26410;&#20805;&#20998;&#25506;&#32034;&#23454;&#38469;&#29992;&#25143;-AI&#20114;&#21160;&#20013;&#22266;&#26377;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ToxicChat&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23454;&#38469;&#29992;&#25143;&#26597;&#35810;&#26500;&#24314;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;&#20102;&#23545;&#24403;&#21069;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#38590;&#20197;&#35782;&#21035;&#30340;&#20016;&#23500;&#32780;&#24494;&#22937;&#30340;&#29616;&#35937;&#65292;&#30456;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#26469;&#35828;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;&#22312;&#29616;&#26377;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24212;&#29992;&#20110;ToxicChat&#30340;&#36825;&#20010;&#29420;&#29305;&#39046;&#22495;&#26102;&#23384;&#22312;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23454;&#38469;&#29992;&#25143;-AI&#23545;&#35805;&#20013;&#27602;&#24615;&#26816;&#27979;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#25361;&#25112;&#12290;&#23558;&#26469;&#65292;ToxicChat&#30340;&#30740;&#31350;&#21487;&#20197;&#25512;&#21160;&#26356;&#22909;&#30340;&#27602;&#24615;&#26816;&#27979;&#26041;&#27861;&#21644;&#24037;&#20855;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, Toxic
&lt;/p&gt;</description></item><item><title>YOLO-BEV&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29420;&#29305;&#30340;&#25668;&#20687;&#22836;&#35774;&#32622;&#29983;&#25104;&#36710;&#36742;&#29615;&#22659;&#30340;2D&#40479;&#30640;&#22270;&#12290;&#23427;&#37319;&#29992;&#20102;YOLO&#30340;&#26816;&#27979;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#23450;&#20041;&#35774;&#35745;&#30340;&#26816;&#27979;&#22836;&#37096;&#23558;&#20840;&#26223;&#25968;&#25454;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#40479;&#30640;&#35270;&#22270;&#22320;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#26102;&#36710;&#36742;&#24863;&#30693;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17379</link><description>&lt;p&gt;
YOLO-BEV&#65306;&#20197;&#19982;2D&#29289;&#20307;&#26816;&#27979;&#30456;&#21516;&#30340;&#26041;&#24335;&#29983;&#25104;&#40479;&#30640;&#22270;
&lt;/p&gt;
&lt;p&gt;
YOLO-BEV: Generating Bird's-Eye View in the Same Way as 2D Object Detection. (arXiv:2310.17379v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17379
&lt;/p&gt;
&lt;p&gt;
YOLO-BEV&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29420;&#29305;&#30340;&#25668;&#20687;&#22836;&#35774;&#32622;&#29983;&#25104;&#36710;&#36742;&#29615;&#22659;&#30340;2D&#40479;&#30640;&#22270;&#12290;&#23427;&#37319;&#29992;&#20102;YOLO&#30340;&#26816;&#27979;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#23450;&#20041;&#35774;&#35745;&#30340;&#26816;&#27979;&#22836;&#37096;&#23558;&#20840;&#26223;&#25968;&#25454;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#40479;&#30640;&#35270;&#22270;&#22320;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#26102;&#36710;&#36742;&#24863;&#30693;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#24863;&#30693;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#24555;&#36895;&#22320;&#35299;&#37322;&#20854;&#21608;&#22260;&#29615;&#22659;&#20197;&#25913;&#21892;&#23433;&#20840;&#24615;&#21644;&#23548;&#33322;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;YOLO-BEV&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29420;&#29305;&#30340;&#21608;&#22260;&#25668;&#20687;&#22836;&#35774;&#32622;&#29983;&#25104;&#36710;&#36742;&#29615;&#22659;&#30340;2D&#40479;&#30640;&#22270;&#12290;&#36890;&#36807;&#22312;45&#24230;&#38388;&#38548;&#22788;&#31574;&#30053;&#24615;&#22320;&#25918;&#32622;&#20843;&#20010;&#25668;&#20687;&#22836;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#25429;&#33719;&#24182;&#25972;&#21512;&#22270;&#20687;&#21040;&#19968;&#20010;&#36830;&#36143;&#30340;3x3&#32593;&#26684;&#26684;&#24335;&#65292;&#20013;&#24515;&#30041;&#31354;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#31354;&#38388;&#34920;&#31034;&#65292;&#20415;&#20110;&#39640;&#25928;&#22788;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;YOLO&#30340;&#26816;&#27979;&#26426;&#21046;&#65292;&#20511;&#21161;&#20854;&#24555;&#36895;&#21709;&#24212;&#21644;&#32039;&#20945;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#27809;&#26377;&#20351;&#29992;&#24120;&#35268;&#30340;YOLO&#26816;&#27979;&#22836;&#37096;&#65292;&#32780;&#26159;&#20351;&#29992;&#33258;&#23450;&#20041;&#35774;&#35745;&#30340;&#26816;&#27979;&#22836;&#37096;&#65292;&#23558;&#20840;&#26223;&#25429;&#33719;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#32479;&#19968;&#30340;&#33258;&#36710;&#40479;&#30640;&#35270;&#22270;&#22320;&#22270;&#12290;&#21021;&#27493;&#32467;&#26524;&#39564;&#35777;&#20102;YOLO-BEV&#22312;&#23454;&#26102;&#36710;&#36742;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle perception systems strive to achieve comprehensive and rapid visual interpretation of their surroundings for improved safety and navigation. We introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding cameras setup to generate a 2D bird's-eye view of the vehicular environment. By strategically positioning eight cameras, each at a 45-degree interval, our system captures and integrates imagery into a coherent 3x3 grid format, leaving the center blank, providing an enriched spatial representation that facilitates efficient processing. In our approach, we employ YOLO's detection mechanism, favoring its inherent advantages of swift response and compact model structure. Instead of leveraging the conventional YOLO detection head, we augment it with a custom-designed detection head, translating the panoramically captured data into a unified bird's-eye view map of ego car. Preliminary results validate the feasibility of YOLO-BEV in real-time vehicular perception ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17378</link><description>&lt;p&gt;
&#22522;&#20110;&#20999;&#21521;&#31354;&#38388;&#20013;&#30340;&#28789;&#25935;&#24230;&#30340;ReLU&#32593;&#32476;&#30340;&#20248;&#21270;&#30456;&#20851;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#65292;&#36890;&#36807;&#38480;&#21046;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#65292;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#19968;&#20123;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#28982;&#32780;&#25991;&#29486;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#35299;&#37322;&#20026;&#20160;&#20040;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#65292;&#21516;&#26102;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#24471;&#21040;&#30340;&#21021;&#22987;&#21442;&#25968;&#21521;&#37327;&#20013;&#21487;&#29992;&#30340;&#32593;&#32476;&#38598;&#21512;&#30340;Rademacher&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21069;&#21521;ReLU&#32593;&#32476;&#27867;&#21270;&#35823;&#24046;&#30340;PAC&#31867;&#22411;&#30028;&#38480;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#32593;&#32476;&#26799;&#24230;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#22312;&#20248;&#21270;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#30340;&#28789;&#25935;&#24230;&#38480;&#21046;&#22312;&#19968;&#20010;&#30028;&#38480;&#20869;&#12290;&#25152;&#24471;&#21040;&#30340;&#30028;&#38480;&#19981;&#26174;&#24335;&#22320;&#20381;&#36182;&#32593;&#32476;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#26144;&#23556;&#20026;&#39046;&#22495;&#29305;&#23450;&#20195;&#30721;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#25429;&#25417;&#21040;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17372</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Dialogue-based generation of self-driving simulation scenarios using Large Language Models. (arXiv:2310.17372v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#26144;&#23556;&#20026;&#39046;&#22495;&#29305;&#23450;&#20195;&#30721;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#29983;&#25104;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22330;&#26223;&#65292;&#24182;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#25152;&#33021;&#25429;&#25417;&#21040;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#26159;&#24320;&#21457;&#21644;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25511;&#21046;&#22120;&#30340;&#26080;&#20215;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#20223;&#30495;&#26694;&#26550;&#22522;&#20110;&#39640;&#24230;&#19987;&#19994;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#22240;&#27492;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23558;&#26497;&#22823;&#22320;&#22686;&#21152;&#21487;&#29992;&#24615;&#12290;&#20294;&#26159;&#65292;&#33521;&#25991;&#31616;&#27905;&#29992;&#35821;&#21644;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#19968;&#23450;&#30340;&#38544;&#21547;&#20551;&#35774;&#24046;&#36317;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#25903;&#25345;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#20462;&#27491;&#25110;&#20462;&#25913;&#26469;&#36319;&#36827;&#20043;&#21069;&#30340;&#25351;&#20196;&#65292;&#20197;&#23545;&#36804;&#20170;&#20026;&#27490;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#29983;&#25104;&#30340;&#20223;&#30495;&#20316;&#20986;&#21453;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#29992;&#25143;&#30340;&#33521;&#25991;&#35805;&#35821;&#22312;&#36825;&#31181;&#20114;&#21160;&#20013;&#26144;&#23556;&#21040;&#29305;&#23450;&#39046;&#22495;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#25105;&#20204;&#25506;&#32034;&#20102;LLMs&#33021;&#21542;&#25429;&#25417;&#21040;&#35745;&#31639;&#21457;&#35328;&#32773;&#22312;&#35805;&#35821;&#20013;&#30340;&#39044;&#26399;&#20449;&#24687;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;WebDiffusion&#24037;&#20855;&#26469;&#27169;&#25311;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#19975;&#32500;&#32593;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17370</link><description>&lt;p&gt;
&#25506;&#32034;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Generative AI for the World Wide Web. (arXiv:2310.17370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;WebDiffusion&#24037;&#20855;&#26469;&#27169;&#25311;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#19975;&#32500;&#32593;&#65292;&#24182;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#29992;&#25143;&#25552;&#31034;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#21508;&#31181;&#23186;&#20307;&#20869;&#23481;&#12290;&#22312;2022&#24180;&#21040;2023&#24180;&#26399;&#38388;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;AI&#30005;&#24433;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#20247;&#22810;&#24212;&#29992;&#39046;&#22495;&#36805;&#36895;&#22686;&#38271;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#22312;&#19975;&#32500;&#32593;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#22270;&#20687;&#29983;&#25104;&#12290;&#32593;&#32476;&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#21033;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26469;&#36741;&#21161;&#32534;&#20889;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32780;&#32593;&#32476;&#27983;&#35272;&#22120;&#26410;&#26469;&#21487;&#33021;&#20250;&#20351;&#29992;&#23427;&#26469;&#26412;&#22320;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#20462;&#22797;&#25439;&#22351;&#30340;&#32593;&#39029;&#12289;&#33410;&#30465;&#24102;&#23485;&#21644;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#21517;&#20026;WebDiffusion&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#35282;&#24230;&#27169;&#25311;&#30001;&#31283;&#23450;&#25193;&#25955;&#65288;&#19968;&#31181;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65289;&#39537;&#21160;&#30340;&#19975;&#32500;&#32593;&#12290;WebDiffusion&#36824;&#25903;&#25345;&#29992;&#25143;&#24847;&#35265;&#30340;&#20247;&#21253;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#21151;&#33021;&#35780;&#20272;&#20102;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is a cutting-edge technology capable of producing text, images, and various media content leveraging generative models and user prompts. Between 2022 and 2023, generative AI surged in popularity with a plethora of applications spanning from AI-powered movies to chatbots. In this paper, we delve into the potential of generative AI within the realm of the World Wide Web, specifically focusing on image generation. Web developers already harness generative AI to help crafting text and images, while Web browsers might use it in the future to locally generate images for tasks like repairing broken webpages, conserving bandwidth, and enhancing privacy. To explore this research area, we have developed WebDiffusion, a tool that allows to simulate a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. WebDiffusion further supports crowdsourcing of user opinions, which we use to evaluate the quality and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#39033;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17353</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#25991;&#21270;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#39033;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#29616;&#22312;&#26377;&#33021;&#21147;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#36328;&#25991;&#21270;&#29615;&#22659;&#26377;&#19968;&#20010;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20363;&#23376;&#26159;&#39135;&#35889;&#30340;&#36866;&#24212;&#65292;&#36825;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#32763;&#35793;&#65292;&#36824;&#21253;&#25324;&#23545;&#26576;&#20010;&#29305;&#23450;&#25991;&#21270;&#30340;&#39135;&#26448;&#12289;&#28921;&#39274;&#25216;&#24039;&#21644;&#33203;&#39135;&#20559;&#22909;&#30340;&#25484;&#25569;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28041;&#21450;&#20013;&#39184;&#21644;&#33521;&#35821;&#22269;&#23478;&#33756;&#31995;&#20043;&#38388;&#39135;&#35889;&#30340;&#32763;&#35793;&#21644;&#25991;&#21270;&#36866;&#24212;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#39033;&#35843;&#26597;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;CulturalRecipes&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#33258;&#21160;&#37197;&#23545;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;&#39135;&#35889;&#26500;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#36890;&#36807;&#20154;&#24037;&#32534;&#20889;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#22312;&#36825;&#20010;&#22797;&#26434;&#30340;&#36328;&#25991;&#21270;&#39135;&#35889;&#36866;&#24212;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;GPT-4&#21644;&#20854;&#20182;LLM&#12289;&#20256;&#32479;&#26426;&#22120;&#32763;&#35793;&#21644;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#21253;&#25324;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automati
&lt;/p&gt;</description></item><item><title>CQM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#21644;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.17330</link><description>&lt;p&gt;
CQM&#65306;&#20855;&#26377;&#37327;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CQM: Curriculum Reinforcement Learning with a Quantized World Model. (arXiv:2310.17330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17330
&lt;/p&gt;
&lt;p&gt;
CQM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#21644;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#65292;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35838;&#31243;&#22686;&#24378;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#20195;&#29702;&#20219;&#21153;&#30340;&#39034;&#24207;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#35838;&#31243;&#30446;&#26631;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#25351;&#23450;&#30340;&#30446;&#26631;&#31354;&#38388;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38480;&#21046;&#24182;&#25552;&#39640;&#35838;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#26041;&#27861;&#65292;&#23427;&#33258;&#21160;&#23450;&#20041;&#21253;&#21547;&#35838;&#31243;&#36807;&#31243;&#30340;&#20851;&#38190;&#20449;&#24687;&#30340;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#19978;&#25552;&#20986;&#35838;&#31243;&#30446;&#26631;&#12290;&#20026;&#20102;&#23450;&#20041;&#35821;&#20041;&#30446;&#26631;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;-&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#23558;&#36830;&#32493;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#36890;&#36807;&#22270;&#24418;&#24674;&#22797;&#31163;&#25955;&#35266;&#23519;&#32467;&#26524;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19981;&#30830;&#23450;&#24615;&#21644;&#26102;&#38388;&#36317;&#31163;&#24863;&#30693;&#30340;&#35838;&#31243;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#22312;&#33258;&#21160;&#32452;&#21512;&#30340;&#30446;&#26631;&#31354;&#38388;&#20013;&#25910;&#25947;&#21040;&#26368;&#32456;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C-Disentanglement&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#19988;&#21463;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#29983;&#25104;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17325</link><description>&lt;p&gt;
C-Disentanglement: &#24102;&#26377;&#28151;&#28102;&#22240;&#23376;&#24402;&#32435;&#20559;&#24046;&#30340;&#22240;&#26524;&#29420;&#31435;&#29983;&#25104;&#22240;&#23376;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C-Disentanglement&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#19988;&#21463;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#29983;&#25104;&#22240;&#23376;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#20551;&#35774;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#30001;&#20960;&#20010;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29983;&#25104;&#22240;&#23376;&#65288;&#21363;&#21464;&#24322;&#28304;&#65289;&#20135;&#29983;&#30340;&#65292;&#24182;&#26088;&#22312;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#23427;&#20204;&#12290;&#36825;&#20123;&#22240;&#23376;&#34987;&#26399;&#26395;&#26159;&#22240;&#26524;&#19978;&#35299;&#32544;&#30340;&#65292;&#24847;&#21619;&#30528;&#19981;&#21516;&#30340;&#22240;&#23376;&#34987;&#32534;&#30721;&#20026;&#21333;&#29420;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#24182;&#19988;&#19968;&#20010;&#22240;&#23376;&#30340;&#21464;&#21270;&#19981;&#20250;&#24433;&#21709;&#20854;&#20182;&#22240;&#23376;&#30340;&#20540;&#12290;&#19982;&#32479;&#35745;&#29420;&#31435;&#24615;&#30456;&#27604;&#65292;&#22240;&#26524;&#35299;&#32544;&#20801;&#35768;&#26356;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#21457;&#29616;&#36807;&#31243;&#20013;&#20551;&#35774;&#27809;&#26377;&#28151;&#28102;&#22240;&#32032;&#65292;&#21363;&#29983;&#25104;&#22240;&#23376;&#27809;&#26377;&#20849;&#21516;&#30340;&#21407;&#22240;&#65292;&#22240;&#27492;&#21482;&#33719;&#24471;&#32479;&#35745;&#29420;&#31435;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#22312;&#21457;&#29616;&#22240;&#26524;&#29983;&#25104;&#22240;&#23376;&#20013;&#24314;&#31435;&#28151;&#28102;&#22240;&#32032;&#30340;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#20123;&#22240;&#32032;&#26159;&#26080;&#27861;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#28151;&#28102;-&#35299;&#32544;&#65288;C-Di&#65289;&#30340;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Di
&lt;/p&gt;</description></item><item><title>FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.17306</link><description>&lt;p&gt;
FormaT5: &#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26465;&#20214;&#34920;&#26684;&#26684;&#24335;&#21270;&#30340;&#25277;&#26679;&#21644;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language. (arXiv:2310.17306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17306
&lt;/p&gt;
&lt;p&gt;
FormaT5&#26159;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#25551;&#36848;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#30340;&#26684;&#24335;&#21270;&#26159;&#21487;&#35270;&#21270;&#12289;&#23637;&#31034;&#21644;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32534;&#20889;&#25968;&#25454;&#30456;&#20851;&#30340;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#26469;&#33258;&#21160;&#26684;&#24335;&#21270;&#34920;&#26684;&#12290;&#20294;&#23545;&#29992;&#25143;&#26469;&#35828;&#65292;&#32534;&#20889;&#36825;&#26679;&#30340;&#35268;&#21017;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20182;&#20204;&#29702;&#35299;&#21644;&#23454;&#29616;&#24213;&#23618;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;FormaT5&#65292;&#21487;&#20197;&#26681;&#25454;&#30446;&#26631;&#34920;&#26684;&#21644;&#26399;&#26395;&#30340;&#26684;&#24335;&#36923;&#36753;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#26684;&#24335;&#35268;&#21017;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#25143;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#25110;&#21547;&#31946;&#30340;&#65292;&#36825;&#20351;&#24471;&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#38590;&#20197;&#22312;&#19968;&#27493;&#20013;&#20934;&#30830;&#23398;&#20064;&#21040;&#25152;&#38656;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35268;&#33539;&#19981;&#36275;&#30340;&#38382;&#39064;&#24182;&#20943;&#23569;&#21442;&#25968;&#38169;&#35823;&#65292;FormaT5&#36890;&#36807;&#25918;&#24323;&#30446;&#26631;&#30340;&#26041;&#24335;&#23398;&#20064;&#39044;&#27979;&#21344;&#20301;&#31526;&#12290;&#36825;&#20123;&#21344;&#20301;&#31526;&#21487;&#20197;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#25110;&#32773;&#24403;&#21487;&#29992;&#30340;&#34892;&#31034;&#20363;&#26102;&#65292;&#30001;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#32534;&#31243;&#31995;&#32479;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#35774;&#35745;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;&#32467;&#26524;&#26174;&#31034;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#29256;&#26412;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.17300</link><description>&lt;p&gt;
&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#29992;&#25143;&#20307;&#39564;&#65306;&#19968;&#39033;&#20851;&#20110;&#29992;&#25143;&#20307;&#39564;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience. (arXiv:2310.17300v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#22312;&#20005;&#32899;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#35774;&#35745;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;&#32467;&#26524;&#26174;&#31034;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#29256;&#26412;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#21442;&#19982;&#32773;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23545;&#35805;&#20195;&#29702;&#65288;ECAs&#65289;&#26159;&#20197;&#20855;&#35937;&#21270;&#35282;&#33394;&#24418;&#24335;&#21576;&#29616;&#30340;&#23545;&#35805;&#29992;&#25143;&#30028;&#38754;&#30340;&#33539;&#20363;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#32423;&#21035;&#30340;&#34920;&#29616;&#36924;&#30495;&#24230;&#65292;&#21363;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#12290;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#22522;&#20110;&#35821;&#38899;&#30340;ECAs&#22312;&#20005;&#32899;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#27934;&#23519;&#21644;&#35774;&#35745;&#24314;&#35758;&#12290;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#20010;&#23436;&#20840;&#32452;&#20869;&#12289;&#20108;&#22240;&#32032;&#35774;&#35745;&#65292;&#20849;&#26377;36&#21517;&#24615;&#21035;&#24179;&#34913;&#30340;&#21442;&#19982;&#32773;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36924;&#30495;&#21644;&#21160;&#30011;&#24418;&#35937;&#30340;&#29256;&#26412;&#37117;&#34987;&#35748;&#20026;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#24179;&#22343;&#24471;&#20998;&#20998;&#21035;&#20026;5.76&#21644;5.71&#12290;&#28982;&#32780;&#65292;69.4&#65285;&#30340;&#21442;&#19982;&#32773;&#34920;&#31034;&#20182;&#20204;&#26356;&#21916;&#27426;&#36924;&#30495;&#29256;&#26412;&#65292;25&#65285;&#30340;&#21442;&#19982;&#32773;&#34920;&#31034;&#20182;&#20204;&#26356;&#21916;&#27426;&#21160;&#30011;&#29256;&#26412;&#65292;&#36824;&#26377;5.6&#65285;&#30340;&#21442;&#19982;&#32773;&#27809;&#26377;&#34920;&#26126;&#20559;&#22909;&#12290;&#36924;&#30495;&#20195;&#29702;&#34987;&#35748;&#20026;&#26356;&#30495;&#23454;&#21644;&#31867;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied conversational agents (ECAs) are paradigms of conversational user interfaces in the form of embodied characters. While ECAs offer various manipulable features, this paper focuses on a study conducted to explore two distinct levels of presentation realism. The two agent versions are photorealistic and animated. The study aims to provide insights and design suggestions for speech-enabled ECAs within serious game environments. A within-subjects, two-by-two factorial design was employed for this research with a cohort of 36 participants balanced for gender. The results showed that both the photorealistic and the animated versions were perceived as highly usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4 per cent of the participants stated they preferred the photorealistic version, 25 per cent stated they preferred the animated version and 5.6 per cent had no stated preference. The photorealistic agents were perceived as more realistic and human-like, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17261</link><description>&lt;p&gt;
&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29399;&#21644;&#29483;&#30340;&#27604;&#20363;&#20026;1:1&#26102;&#65292;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#29399;&#21644;&#29483;&#20063;&#24212;&#26356;&#22909;&#22320;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21482;&#25552;&#20379;&#20102;&#8220;&#22810;&#26679;&#24615;&#8221;&#36825;&#20010;&#35299;&#37322;&#24615;&#20043;&#22806;&#30340;&#32500;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#12290;&#21333;&#23646;&#24615;&#24046;&#24322;&#65288;SaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#21333;&#20010;&#23646;&#24615;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#21452;&#23646;&#24615;&#24046;&#24322;&#65288;PaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#19968;&#23545;&#23646;&#24615;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#23646;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#22270;&#20687;&#30340;&#23646;&#24615;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;CLIP&#35780;&#20998;&#65288;HCS&#65289;&#65292;&#23427;&#36890;&#36807;&#27979;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#21521;&#37327;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous 
&lt;/p&gt;</description></item><item><title>IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17250</link><description>&lt;p&gt;
IDENAS: &#20869;&#37096;&#20381;&#36182;&#24615;&#25506;&#32034;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17250
&lt;/p&gt;
&lt;p&gt;
IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#21644;&#36827;&#34892;&#21508;&#31181;&#39044;&#27979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#31639;&#27861;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#65292;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#21306;&#21035;&#20197;&#21450;&#27169;&#22411;&#30340;&#24213;&#23618;&#20851;&#32852;&#65288;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#23618;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#29305;&#24449;&#36873;&#25321;&#24050;&#25104;&#20026;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IDENAS&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;NAS&#19982;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;1D&#20256;&#24863;&#22120;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#23436;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#20869;&#37096;&#20381;&#36182;&#24615;&#12290;IDENAS&#37319;&#29992;&#20102;&#20462;&#25913;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#21069;&#21521;&#25628;&#32034;&#65288;SFS&#65289;&#31639;&#27861;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#37197;&#32622;&#25628;&#32034;&#19982;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;IDENAS&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
&lt;/p&gt;</description></item><item><title>CROP&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#22870;&#21169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#36807;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#22870;&#21169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.17245</link><description>&lt;p&gt;
CROP: &#20445;&#23432;&#22870;&#21169;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CROP: Conservative Reward for Model-based Offline Policy Optimization. (arXiv:2310.17245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17245
&lt;/p&gt;
&lt;p&gt;
CROP&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#22870;&#21169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#36807;&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#22870;&#21169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25361;&#25112;&#26041;&#38754;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#26469;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#20013;&#23558;&#20445;&#23432;&#24615;&#24341;&#20837;&#27169;&#22411;&#25110;Q&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#22870;&#21169;&#20272;&#35745;&#20013;&#20445;&#23432;&#24615;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;CROP&#65292;&#35813;&#31639;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20445;&#23432;&#22320;&#20272;&#35745;&#22870;&#21169;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#23432;&#30340;&#22870;&#21169;&#20272;&#35745;&#65292;CROP&#21516;&#26102;&#26368;&#23567;&#21270;&#20272;&#35745;&#35823;&#24046;&#21644;&#38543;&#26426;&#21160;&#20316;&#30340;&#22870;&#21169;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#20445;&#23432;&#30340;&#22870;&#21169;&#26426;&#21046;&#23548;&#33268;...&#65288;&#25991;&#31456;&#25688;&#35201;&#26410;&#23436;&#65292;&#19979;&#21516;&#65289;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.17238</link><description>&lt;p&gt;
&#36890;&#36807;span&#21098;&#26525;&#21644;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;span&#21098;&#26525;&#26426;&#21046;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#36827;&#34892;&#39640;&#38454;&#24314;&#27169;&#65292;&#23454;&#29616;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;ERE&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#26368;&#36817;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;ERE&#27169;&#22411;&#22312;&#22810;&#20010;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#19981;&#32771;&#34385;&#39640;&#38454;&#20132;&#20114;&#65292;&#32780;&#39640;&#38454;&#24314;&#27169;&#21487;&#33021;&#20250;&#26377;&#30410;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#29992;&#20110;ERE&#65292;&#23427;&#26159;&#24314;&#31435;&#22312;PL-marker&#65288;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#27169;&#22411;&#65289;&#20043;&#19978;&#30340;&#12290;&#20026;&#20102;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#21484;&#22238;&#21098;&#26525;&#22120;&#26426;&#21046;&#23558;&#23454;&#20307;&#30340;&#35782;&#21035;&#21644;&#26631;&#27880;&#36127;&#25285;&#20174;NER&#27169;&#22359;&#36716;&#31227;&#21040;&#25105;&#20204;&#27169;&#22411;&#30340;&#32852;&#21512;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#38454;&#24314;&#27169;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#23454;&#20307;&#65288;&#30001;span&#21098;&#26525;&#22120;&#25552;&#20379;&#65289;&#65292;&#20197;&#21450;&#20854;&#20851;&#31995;&#65292;&#24182;&#19988;&#36229;&#36793;&#32534;&#30721;&#20102;&#20004;&#20010;&#19981;&#21516;&#20851;&#31995;&#20043;&#38388;&#25110;&#20851;&#31995;&#19982;&#20854;&#30456;&#20851;&#30340;&#20027;&#20307;&#21644;&#23486;&#35821;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25509;&#30528;&#25105;&#20204;&#36816;&#34892;...
&lt;/p&gt;
&lt;p&gt;
Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#21644;&#25913;&#36827;&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#23884;&#20837;&#12289;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#36716;&#25442;&#23884;&#20837;&#20197;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#25928;&#36873;&#25321;&#35757;&#32451;&#26679;&#20363;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.17228</link><description>&lt;p&gt;
TST$^\mathrm{R}$: &#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#36935;&#35265;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World. (arXiv:2310.17228v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#21644;&#25913;&#36827;&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26356;&#22823;&#30340;&#27169;&#22411;&#23884;&#20837;&#12289;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#36716;&#25442;&#23884;&#20837;&#20197;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#39640;&#25928;&#36873;&#25321;&#35757;&#32451;&#26679;&#20363;&#21644;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#30456;&#20284;&#24230;&#35843;&#25972;&#65288;TST&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;&#20195;&#30721;&#29983;&#25104;&#20013;&#36873;&#25321;&#30456;&#20851;&#20363;&#23376;&#20197;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20854;&#30446;&#26631;&#26159;&#20351;&#24471;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36866;&#24212;&#20004;&#20010;NL&#36755;&#20837;&#30340;&#30456;&#20284;&#24230;&#19982;&#20854;&#30456;&#20851;&#20195;&#30721;&#36755;&#20986;&#30340;&#30456;&#20284;&#24230;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24212;&#29992;&#21644;&#25913;&#36827;TST&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#21477;&#23376;&#36716;&#25442;&#22120;&#26367;&#25442;&#20026;&#26356;&#22823;&#27169;&#22411;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#35821;&#35328;&#20998;&#24067;&#30340;&#25935;&#24863;&#24615;&#65292;&#22686;&#21152;&#20102;&#21512;&#25104;&#31034;&#20363;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#23567;&#27169;&#22411;&#23558;&#36825;&#20123;&#23884;&#20837;&#36716;&#25442;&#21040;&#19968;&#20010;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#23884;&#20837;&#30456;&#20284;&#24230;&#21305;&#37197;&#20195;&#30721;&#30456;&#20284;&#24230;&#65292;&#20351;&#24471;&#27169;&#22411;&#20445;&#25345;&#40657;&#31665;&#29366;&#24577;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#21482;&#38656;&#36827;&#34892;&#23569;&#37327;&#30697;&#38453;&#20056;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#36873;&#25321;&#36739;&#23569;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#26469;&#35757;&#32451;TST&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25490;&#21517;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17217</link><description>&lt;p&gt;
&#36229;&#36234;MLE: &#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20984;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#20351;&#24471;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26159;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#26368;&#33021;&#35299;&#37322;&#35266;&#27979;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#12290;&#22312;&#25991;&#26412;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;MLE&#32463;&#24120;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#26032;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;MLE&#24182;&#19981;&#24635;&#26159;&#24517;&#35201;&#19988;&#26368;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38381;&#21512;&#22411;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#26368;&#21512;&#36866;&#30340;&#21709;&#24212;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#38656;&#35201;&#20351;&#29992;MLE&#26469;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20989;&#25968;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#31867;&#65292;&#20351;&#24471;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#38598;&#20013;&#20110;&#39640;&#27010;&#29575;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20272;&#35745;&#25972;&#20010;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23558;&#20984;&#20989;&#25968;&#24212;&#29992;&#20110;&#25439;&#22833;&#20989;&#25968;&#26102;&#30340;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#20984;&#20989;&#25968;&#21487;&#20197;&#20351;&#24471;&#26368;&#20248;&#39044;&#27979;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#38160;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#32508;&#36848;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#30340;&#29616;&#26377;&#36235;&#21183;&#12289;&#24773;&#24863;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.17212</link><description>&lt;p&gt;
&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition by Video: A review. (arXiv:2310.17212v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#31687;&#20851;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#32508;&#36848;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#30340;&#29616;&#26377;&#36235;&#21183;&#12289;&#24773;&#24863;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#20197;&#21450;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26159;&#24773;&#24863;&#35745;&#31639;&#30340;&#37325;&#35201;&#20998;&#25903;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#21487;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#26234;&#33021;&#21307;&#30103;&#31561;&#39046;&#22495;&#12290;&#23613;&#31649;&#24773;&#24863;&#35782;&#21035;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#25968;&#37327;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#24456;&#23569;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#25253;&#36947;&#30456;&#20851;&#30340;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;2015&#24180;&#33267;2023&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#65292;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#30740;&#31350;&#20013;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#29616;&#26377;&#36235;&#21183;&#12290;&#26412;&#25991;&#39318;&#20808;&#35752;&#35770;&#20102;&#20004;&#31181;&#20856;&#22411;&#30340;&#24773;&#24863;&#27169;&#22411;&#65292;&#28982;&#21518;&#20171;&#32461;&#20102;&#32463;&#24120;&#29992;&#20110;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#24211;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#21644;&#20998;&#31867;&#20102;&#29616;&#20195;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#20855;&#20307;&#32467;&#26500;&#21644;&#24615;&#33021;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Video emotion recognition is an important branch of affective computing, and its solutions can be applied in different fields such as human-computer interaction (HCI) and intelligent medical treatment. Although the number of papers published in the field of emotion recognition is increasing, there are few comprehensive literature reviews covering related research on video emotion recognition. Therefore, this paper selects articles published from 2015 to 2023 to systematize the existing trends in video emotion recognition in related studies. In this paper, we first talk about two typical emotion models, then we talk about databases that are frequently utilized for video emotion recognition, including unimodal databases and multimodal databases. Next, we look at and classify the specific structure and performance of modern unimodal and multimodal video emotion recognition methods, talk about the benefits and drawbacks of each, and then we compare them in detail in the tables. Further, we
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#27979;&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#30340;&#21464;&#21270;&#65292;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17207</link><description>&lt;p&gt;
&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Fusion using the Tsetlin Machine. (arXiv:2310.17207v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17207
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin&#26426;&#22120;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#34701;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#27979;&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#30340;&#21464;&#21270;&#65292;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Tsetlin Machine&#35780;&#20272;&#21644;&#34701;&#21512;&#22024;&#26434;&#21160;&#24577;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#27979;Tsetlin Machine&#23398;&#20064;&#21040;&#30340;&#36923;&#36753;&#23376;&#21477;&#22312;&#21160;&#24577;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#21464;&#21270;&#26469;&#35782;&#21035;&#21644;&#34701;&#21512;&#22122;&#22768;&#12290;&#36890;&#36807;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#23376;&#21477;&#30340;&#26435;&#37325;&#25110;&#20197;&#26032;&#30340;&#23376;&#21477;&#24418;&#24335;&#21453;&#26144;&#22122;&#22768;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel way of assessing and fusing noisy dynamic data using a Tsetlin Machine. Our approach consists in monitoring how explanations in form of logical clauses that a TM learns changes with possible noise in dynamic data. This way TM can recognize the noise by lowering weights of previously learned clauses, or reflect it in the form of new clauses. We also perform a comprehensive experimental study using notably different datasets that demonstrated high performance of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNCV&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#23454;&#29616;REINFORCE Leave-One-Out (RLOO)&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#65292;&#20248;&#21270;&#20102;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#24182;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2310.17200</link><description>&lt;p&gt;
&#29992;&#32593;&#32476;&#25511;&#21046;&#21464;&#37327;&#39535;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNCV&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#23454;&#29616;REINFORCE Leave-One-Out (RLOO)&#20316;&#20026;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#65292;&#20248;&#21270;&#20102;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#24182;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38754;&#20020;&#30528;&#35832;&#22914;&#27807;&#36890;&#24320;&#38144;&#22823;&#12289;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#25913;&#36827;&#19981;&#31283;&#23450;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#30001;&#24322;&#26500;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#24341;&#36215;&#30340;&#26799;&#24230;&#26041;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#25511;&#21046;&#21464;&#37327;&#65288;FedNCV&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;FedNCV&#26694;&#26550;&#30340;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#32423;&#21035;&#37117;&#37319;&#29992;REINFORCE Leave-One-Out&#65288;RLOO&#65289;&#20316;&#20026;&#22522;&#26412;&#25511;&#21046;&#21464;&#37327;&#21333;&#20803;&#12290;&#22312;&#23458;&#25143;&#31471;&#32423;&#21035;&#65292;RLOO&#25511;&#21046;&#21464;&#37327;&#29992;&#20110;&#20248;&#21270;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#65292;&#20943;&#36731;&#25968;&#25454;&#26679;&#26412;&#24341;&#20837;&#30340;&#26041;&#24046;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#65292;&#22522;&#20110;RLOO&#30340;&#20272;&#35745;&#37327;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#32858;&#21512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#20840;&#23616;&#26356;&#26032;&#12290;&#36825;&#31181;&#21452;&#20391;&#24212;&#29992;&#34987;&#24418;&#24335;&#21270;&#20026;&#32452;&#21512;&#25511;&#21046;&#21464;&#37327;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#20844;&#24335;&#26469;&#25429;&#25417;&#36825;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning, a decentralized approach to machine learning, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. These challenges primarily stem from the gradient variance due to heterogeneous client data distributions. To address this, we introduce a novel Networked Control Variates (FedNCV) framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO) as a fundamental control variate unit in the FedNCV framework, implemented at both client and server levels. At the client level, the RLOO control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. Once relayed to the server, the RLOO-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. This dual-side application is formalized as a linear combination of composite control variates. We provide a mathematical expression capturing this i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.17191</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23558;&#23454;&#20307;&#32465;&#23450;&#21040;&#19978;&#19979;&#25991;&#20013;?
&lt;/p&gt;
&lt;p&gt;
How do Language Models Bind Entities in Context?. (arXiv:2310.17191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#32465;&#23450;ID&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#23558;&#23454;&#20307;&#19982;&#23646;&#24615;&#36827;&#34892;&#26377;&#25928;&#22320;&#32465;&#23450;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#22914;&#20309;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27491;&#30830;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24517;&#39035;&#23558;&#23454;&#20307;&#19982;&#20854;&#23646;&#24615;&#36827;&#34892;&#32465;&#23450;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#25551;&#36848;&#8220;&#32511;&#33394;&#26041;&#22359;&#8221;&#21644;&#8220;&#34013;&#33394;&#22278;&#24418;&#8221;&#30340;&#19978;&#19979;&#25991;&#65292;LMs&#24517;&#39035;&#23558;&#24418;&#29366;&#19982;&#23427;&#20204;&#23545;&#24212;&#30340;&#39068;&#33394;&#36827;&#34892;&#32465;&#23450;&#12290;&#25105;&#20204;&#20998;&#26512;LM&#34920;&#31034;&#24182;&#30830;&#23450;&#32465;&#23450;ID&#26426;&#21046;&#65306;&#36825;&#26159;&#19968;&#31181;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;Pythia&#21644;LLaMA&#23478;&#26063;&#30340;&#27599;&#20010;&#36275;&#22815;&#22823;&#30340;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LMs&#20869;&#37096;&#28608;&#27963;&#36890;&#36807;&#23558;&#32465;&#23450;ID&#21521;&#37327;&#38468;&#21152;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#21644;&#23646;&#24615;&#19978;&#26469;&#34920;&#31034;&#32465;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#32465;&#23450;ID&#21521;&#37327;&#24418;&#25104;&#36830;&#32493;&#30340;&#23376;&#31354;&#38388;&#65292;&#22312;&#36825;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#32465;&#23450;ID&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#21306;&#21035;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;LMs&#22312;&#19978;&#19979;&#25991;&#20013;&#34920;&#31034;&#31526;&#21495;&#30693;&#35782;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;&#65292;&#20026;&#29702;&#35299;&#22823;&#35268;&#27169;LMs&#20013;&#30340;&#19968;&#33324;&#19978;&#19979;&#25991;&#25512;&#29702;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
&lt;/p&gt;</description></item><item><title>&#25237;&#24433;&#20202;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#32500;&#24230;&#21305;&#37197;&#21644;&#36923;&#36753;&#33976;&#39311;&#30340;&#24773;&#20917;&#19979;&#36215;&#21040;&#25913;&#21892;&#25928;&#26524;&#12290;&#23398;&#29983;&#20351;&#29992;&#25237;&#24433;&#20202;&#21487;&#20197;&#22312;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.17183</link><description>&lt;p&gt;
&#29702;&#35299;&#25237;&#24433;&#20202;&#23545;&#30693;&#35782;&#33976;&#39311;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Projectors in Knowledge Distillation. (arXiv:2310.17183v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17183
&lt;/p&gt;
&lt;p&gt;
&#25237;&#24433;&#20202;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#32500;&#24230;&#21305;&#37197;&#21644;&#36923;&#36753;&#33976;&#39311;&#30340;&#24773;&#20917;&#19979;&#36215;&#21040;&#25913;&#21892;&#25928;&#26524;&#12290;&#23398;&#29983;&#20351;&#29992;&#25237;&#24433;&#20202;&#21487;&#20197;&#22312;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#25945;&#24072;&#21644;&#23398;&#29983;&#32593;&#32476;&#20043;&#38388;&#30340;&#32500;&#24230;&#19981;&#21305;&#37197;&#65292;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#25237;&#24433;&#20202;&#26469;&#36827;&#34892;&#29305;&#24449;&#36716;&#25442;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#23398;&#29983;&#21644;&#25945;&#24072;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#32500;&#24230;&#65292;&#28155;&#21152;&#25237;&#24433;&#20202;&#20173;&#28982;&#26377;&#21161;&#20110;&#25552;&#39640;&#33976;&#39311;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#25105;&#20204;&#22312;&#26550;&#26500;&#20013;&#28155;&#21152;&#25237;&#24433;&#20202;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#36923;&#36753;&#33976;&#39311;&#30340;&#25928;&#26524;&#12290;&#37492;&#20110;&#36825;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#20197;&#21450;&#29616;&#26377;&#25991;&#29486;&#23545;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#25237;&#24433;&#20202;&#30340;&#29702;&#35299;&#30340;&#26222;&#36941;&#32570;&#20047;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25237;&#24433;&#20202;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#30340;&#38544;&#21547;&#20316;&#29992;&#65292;&#36804;&#20170;&#20026;&#27490;&#36825;&#20010;&#38382;&#39064;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#23398;&#29983;&#20855;&#26377;&#19982;&#25945;&#24072;&#30456;&#21516;&#30340;&#29305;&#24449;&#32500;&#24230;&#26102;&#65292;&#20351;&#29992;&#25237;&#24433;&#20202;&#30340;&#23398;&#29983;&#22312;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#27979;&#35797;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, during the knowledge distillation process (e.g. feature distillation), an additional projector is often required to perform feature transformation due to the dimension mismatch between the teacher and the student networks. Interestingly, we discovered that even if the student and the teacher have the same feature dimensions, adding a projector still helps to improve the distillation performance. In addition, projectors even improve logit distillation if we add them to the architecture too. Inspired by these surprising findings and the general lack of understanding of the projectors in the knowledge distillation process from existing literature, this paper investigates the implicit role that projectors play but so far have been overlooked. Our empirical study shows that the student with a projector (1) obtains a better trade-off between the training accuracy and the testing accuracy compared to the student without a projector when it has the same feature dimensions as th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17178</link><description>&lt;p&gt;
&#22270;&#24418;&#21270;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#37319;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#33021;&#22815;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22635;&#34917;&#20102;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21487;&#20197;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;3D&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#19968;&#20010;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#30340;2D&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#34109;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#26631;&#35760;&#20462;&#21098;&#21644;&#23436;&#25972;&#39044;&#35757;&#32451;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#24182;&#39044;&#27979;&#22270;&#20687;&#31867;&#21035;&#26631;&#31614;&#26469;&#23454;&#29616;&#21160;&#24577;&#35270;&#35273;transformers&#30340;&#21021;&#22987;&#21270;&#21644;&#21152;&#36895;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.17177</link><description>&lt;p&gt;
&#36890;&#36807;&#36974;&#34109;&#24494;&#35843;&#26469;&#24357;&#21512;&#26631;&#35760;&#20462;&#21098;&#21644;&#23436;&#25972;&#39044;&#35757;&#32451;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#34109;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#26631;&#35760;&#20462;&#21098;&#21644;&#23436;&#25972;&#39044;&#35757;&#32451;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#24182;&#39044;&#27979;&#22270;&#20687;&#31867;&#21035;&#26631;&#31614;&#26469;&#23454;&#29616;&#21160;&#24577;&#35270;&#35273;transformers&#30340;&#21021;&#22987;&#21270;&#21644;&#21152;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformers&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#21160;&#24577;&#35270;&#35273;transformers&#26469;&#36890;&#36807;&#20462;&#21098;&#22810;&#20313;&#30340;&#26631;&#35760;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#25913;&#36827;&#26631;&#35760;&#20462;&#21098;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#26412;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#23436;&#25972;&#30340;&#22270;&#20687;&#35757;&#32451;&#65292;&#21363;&#20351;&#29992;&#23436;&#25972;&#30340;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20445;&#30041;&#25972;&#20010;&#29305;&#24449;&#22270;&#65292;&#36825;&#19982;&#36880;&#28176;&#20943;&#23569;&#26631;&#35760;&#30340;&#21160;&#24577;&#27169;&#22411;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#21253;&#25324;&#35745;&#31639;&#27169;&#24335;&#12289;&#20449;&#24687;&#37327;&#21644;&#26631;&#35760;&#36873;&#25321;&#31574;&#30053;&#19978;&#30340;&#19981;&#19968;&#33268;&#12290;&#21463;&#21040;&#25191;&#34892;&#36974;&#34109;&#21644;&#37325;&#26500;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;MAE&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36974;&#34109;&#24494;&#35843;&#26469;&#24357;&#21512;&#29992;&#20110;&#21021;&#22987;&#21270;&#30340;&#39044;&#35757;&#32451;&#22522;&#26412;&#27169;&#22411;&#19982;&#22522;&#20110;&#26631;&#35760;&#20462;&#21098;&#30340;&#21160;&#24577;&#35270;&#35273;transformers&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#24182;&#39044;&#27979;&#22270;&#20687;&#31867;&#21035;&#26631;&#31614;&#26469;&#23436;&#25104;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of transformers on various computer vision tasks, they suffer from excessive memory and computational cost. Some works present dynamic vision transformers to accelerate inference by pruning redundant tokens. A key to improving token pruning is using well-trained models as initialization for faster convergence and better performance. However, current base models usually adopt full image training, i.e., using full images as inputs and keeping the whole feature maps through the forward process, which causes inconsistencies with dynamic models that gradually reduce tokens, including calculation pattern, information amount and token selection strategy inconsistencies. Inspired by MAE which performs masking and reconstruction self-supervised task, we devise masked fine-tuning to bridge the gaps between pre-trained base models used for initialization and token pruning based dynamic vision transformers, by masking image patches and predicting the image class label based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17176</link><description>&lt;p&gt;
&#20174;&#20840;&#26223;X&#23556;&#32447;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#22312;&#29616;&#20195;&#21475;&#33108;&#20445;&#20581;&#20013;&#26159;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#29273;&#40831;&#31181;&#26893;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#26681;&#25454;FUSegNet&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#21019;&#38754;&#20998;&#21106;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#20110;&#32593;&#26684;&#30340;&#27880;&#24847;&#21147;&#38376;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24341;&#20837;&#23450;&#21521;&#36793;&#30028;&#26694;&#65288;OBB&#65289;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#29273;&#40831;&#23450;&#20301;&#20272;&#35745;&#12290;&#22312;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;DNS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;543&#20010;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#26368;&#39640;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24471;&#20998;82.43%&#65292;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#24471;&#20998;90.37%&#65292;&#22312;OBB&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26059;&#36716;&#30340;&#20132;&#24182;&#27604;&#65288;RIoU&#65289;&#24471;&#20998;82.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#24182;&#30452;&#25509;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.17167</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#25913;&#36827;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#24182;&#30452;&#25509;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#36129;&#29486;&#65292;&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#36890;&#36807;&#20197;&#22270;&#20687;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#22235;&#20998;&#20043;&#19968;&#22278;&#24359;&#19978;&#30340;&#35282;&#24230;&#37325;&#26032;&#21442;&#25968;&#21270;&#25193;&#25955;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#35774;&#32622;&#20256;&#32479;&#30340; $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$&#12290;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#28040;&#38500;&#20102;&#20004;&#20010;&#22855;&#24322;&#28857;&#65292;&#24182;&#20801;&#35768;&#23558;&#25193;&#25955;&#28436;&#21270;&#34920;&#36798;&#20026;&#19968;&#20010;&#33391;&#22909;&#34892;&#20026;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#20174;&#32780;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#26356;&#39640;&#38454;&#30340;ODE&#27714;&#35299;&#22120;&#65292;&#22914;Runge-Kutta&#26041;&#27861;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#30452;&#25509;&#20351;&#29992;&#25105;&#20204;&#30340;&#32593;&#32476;&#20272;&#35745;&#22270;&#20687;&#65288;$\mathbf{x}_0$&#65289;&#21644;&#22122;&#22768;&#65288;$\mathbf{\epsilon}$&#65289;&#65292;&#36825;&#20351;&#24471;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#26356;&#26032;&#27493;&#39588;&#35745;&#31639;&#26356;&#21152;&#31283;&#23450;&#65292;&#22240;&#20026;&#22312;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#21644;&#22122;&#22768;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#20123;&#21464;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;</title><link>http://arxiv.org/abs/2310.17162</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#38899;&#20048;&#38899;&#39057;&#39046;&#22495;&#20986;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#39640;&#36136;&#37327;&#38899;&#20048;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#65292;&#24182;&#19988;&#19968;&#20123;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#22312;&#38899;&#20048;&#19978;&#30340;&#25511;&#21046;&#33021;&#21147;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#36890;&#36807;&#20803;&#25968;&#25454;&#65288;&#22914;&#27468;&#25163;&#21644;&#20048;&#22120;&#65289;&#25110;&#39640;&#32423;&#34920;&#31034;&#65288;&#22914;&#27969;&#27966;&#21644;&#24773;&#24863;&#65289;&#38388;&#25509;&#22320;&#25551;&#36848;&#38899;&#20048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36827;&#19968;&#27493;&#25552;&#20379;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Coco-Mulla&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#20102;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#38899;&#39057;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#65292;&#21442;&#25968;&#35843;&#20248;&#30340;&#27604;&#20363;&#19981;&#21040;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
&lt;/p&gt;</description></item><item><title>CosmosDSR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#12290;&#22312;&#20351;&#29992;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;YOLOv3&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;CosmosDSR&#21644;LKF&#37117;&#33021;&#20934;&#30830;&#22320;&#36319;&#36394;&#21355;&#26143;&#12290;</title><link>http://arxiv.org/abs/2310.17158</link><description>&lt;p&gt;
CosmosDSR -- &#19968;&#31181;&#20351;&#29992;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
CosmosDSR -- a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter. (arXiv:2310.17158v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17158
&lt;/p&gt;
&lt;p&gt;
CosmosDSR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#36319;&#36394;&#36712;&#36947;&#30862;&#29255;&#12290;&#22312;&#20351;&#29992;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;YOLOv3&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;CosmosDSR&#21644;LKF&#37117;&#33021;&#20934;&#30830;&#22320;&#36319;&#36394;&#21355;&#26143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kessler&#32508;&#21512;&#24449;&#26159;&#25351;&#39057;&#32321;&#30340;&#22826;&#31354;&#27963;&#21160;&#20135;&#29983;&#30340;&#21319;&#32423;&#30340;&#22826;&#31354;&#30862;&#29255;&#65292;&#23041;&#32961;&#21040;&#26410;&#26469;&#30340;&#22826;&#31354;&#25506;&#32034;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;YOLO&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#32452;&#21512;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#39033;&#30446;&#24341;&#20837;&#20102;CosmosDSR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;YOLOv3&#19982;&#26080;&#21619;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;&#24207;&#21015;&#22270;&#20687;&#20013;&#36319;&#36394;&#21355;&#26143;&#65292;&#19982;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30456;&#27604;&#12290;&#20351;&#29992;&#21346;&#26862;&#22561;&#22823;&#23398;&#30340;SPARK&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;YOLOv3&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#20102;&#25152;&#26377;&#21355;&#26143;&#31867;&#21035;&#65288;mAP=97.18&#65285;&#65292;F1=0.95&#65289;&#65292;&#20986;&#29616;&#20102;&#23569;&#37327;&#38169;&#35823;&#65288;TP=4163&#65292;FP=209&#65292;FN=237&#65289;&#12290;CosmosDSR&#21644;LKF&#37117;&#20934;&#30830;&#36319;&#36394;&#21355;&#26143;&#65288;UKF&#65306;MSE=2.83/RMSE=1.66&#65292;LKF&#65306;...
&lt;/p&gt;
&lt;p&gt;
The Kessler syndrome refers to the escalating space debris from frequent space activities, threatening future space exploration. Addressing this issue is vital. Several AI models, including Convolutional Neural Networks (CNN), Kernel Principal Component Analysis (KPCA), and Model-Agnostic Meta-Learning (MAML), have been assessed with various data types. Earlier studies highlighted the combination of the YOLO object detector and a linear Kalman filter for object detection and tracking. Building on this, our project introduces CosmosDSR, a novel methodology combining YOLOv3 with an Unscented Kalman Filter for tracking satellites in sequential images, compared to a linear Kalman filter. Using the SPARK dataset from the University of Luxembourg for training and testing, the YOLOv3 precisely detected and classified all satellite categories (mAP=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237). Both CosmosDSR and the LKF tracked satellites accurately (UKF: MSE=2.83/RMSE=1.66, LKF: 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17149</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#22478;&#24066;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26234;&#33021;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#65292;&#22240;&#20854;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#22478;&#24066;&#36164;&#28304;&#20998;&#37197;&#21644;&#25919;&#31574;&#21046;&#23450;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STExplainer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#24378;STGNNs&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#21516;&#26102;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19982;&#20301;&#32622;&#20449;&#24687;&#34701;&#21512;&#23618;&#20316;&#20026;STG&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#38598;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;GIB&#65289;&#21407;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35299;&#37322;&#30446;&#26631;&#30340;&#32467;&#26500;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#20363;&#21270;GIB&#21407;&#21017;&#26469;&#25552;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph neural networks (STGNNs) have gained popularity as a powerful tool for effectively modeling spatio-temporal dependencies in diverse real-world urban applications, including intelligent transportation and public safety. However, the black-box nature of STGNNs limits their interpretability, hindering their application in scenarios related to urban resource allocation and policy formulation. To bridge this gap, we propose an Explainable Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances STGNNs with inherent explainability, enabling them to provide accurate predictions and faithful explanations simultaneously. Our framework integrates a unified spatio-temporal graph attention network with a positional information fusion layer as the STG encoder and decoder, respectively. Furthermore, we propose a structure distillation approach based on the Graph Information Bottleneck (GIB) principle with an explainable objective, which is instantiated by the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#21152;&#26435;&#30340;&#26032;&#22411;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.17146</link><description>&lt;p&gt;
&#21322;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#22686;&#24378;&#37325;&#35201;&#24615;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17146
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#21644;&#21152;&#26435;&#30340;&#26032;&#22411;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#39046;&#22495;&#26102;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#20102;&#35299;&#26032;&#31574;&#30053;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#32593;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#22312;&#26412;&#36136;&#19978;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#31163;&#32447;&#25968;&#25454;&#21487;&#33021;&#19981;&#21453;&#26144;&#30001;&#20110;&#24212;&#29992;&#26032;&#31574;&#30053;&#32780;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#26681;&#25454;&#26032;&#31574;&#30053;&#25910;&#38598;&#36712;&#36857;&#36827;&#34892;&#22312;&#32447;&#35780;&#20272;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#39046;&#22495;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#35780;&#20272;&#26694;&#26550;&#65292;&#20316;&#20026;&#31163;&#32447;&#21644;&#22312;&#32447;&#35780;&#20272;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#20854;&#20013;&#20154;&#31867;&#29992;&#25143;&#25552;&#20379;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#30340;&#27880;&#37322;&#12290;&#34429;&#28982;&#35825;&#20154;&#22320;&#31616;&#21333;&#22320;&#29992;&#36825;&#20123;&#27880;&#37322;&#26469;&#22686;&#21152;&#29616;&#26377;&#25968;&#25454;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#22825;&#30495;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26377;&#20559;&#30340;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21644;&#26032;&#39062;&#21152;&#26435;&#30340;OPE&#20272;&#35745;&#22120;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#26469;&#35299;&#20915;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#21327;&#21516;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.17140</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#35268;&#21010;&#21644;&#20195;&#30721;&#29983;&#25104;&#22312;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17140
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#26469;&#35299;&#20915;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#31995;&#32479;&#22312;&#21327;&#21516;&#21442;&#32771;&#35299;&#26512;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#20195;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#65292;LLMs&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#24456;&#38590;&#24341;&#23548;&#20854;&#26397;&#30528;&#20219;&#21153;&#30446;&#26631;&#21069;&#36827;&#65292;&#24182;&#19988;&#26080;&#27861;&#22788;&#29702;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#23454;&#38469;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#21512;LLMs&#21644;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#20197;&#21450;&#22522;&#20110;&#23454;&#38469;&#30340;&#20195;&#30721;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#38405;&#35835;&#22120;&#21644;&#35268;&#21010;&#22120;&#32452;&#25104;&#65306;&#38405;&#35835;&#22120;&#21033;&#29992;LLMs&#23558;&#21512;&#20316;&#20249;&#20276;&#30340;&#35805;&#35821;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#65292;&#35843;&#29992;&#25191;&#34892;&#22522;&#20110;&#23454;&#38469;&#30340;&#20989;&#25968;&#12290;&#36716;&#25442;&#21518;&#30340;&#20195;&#30721;&#30340;&#36755;&#20986;&#34987;&#23384;&#20648;&#20197;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#65292;&#32780;&#31526;&#21495;&#21270;&#35268;&#21010;&#22120;&#30830;&#23450;&#19979;&#19968;&#20010;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#35201;&#27714;&#39640;&#30340;OneCommon&#23545;&#35805;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#35299;&#20915;&#25955;&#28857;&#22270;&#20687;&#30340;&#21327;&#21516;&#21442;&#32771;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21253;&#25324;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#25913;&#21892;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102; CoRe Challenge 2023 &#20013;&#25552;&#20132;&#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.17136</link><description>&lt;p&gt;
Core Challenge 2023: &#27714;&#35299;&#22120;&#21644;&#22270;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Core Challenge 2023: Solver and Graph Descriptions. (arXiv:2310.17136v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102; CoRe Challenge 2023 &#20013;&#25552;&#20132;&#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25910;&#38598;&#20102;&#25552;&#20132;&#32473; CoRe Challenge 2023 &#30340;&#27714;&#35299;&#22120;&#21644; ISR &#23454;&#20363;&#30340;&#25152;&#26377;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper collects all descriptions of solvers and ISR instances submitted to CoRe Challenge 2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#30340;&#26041;&#24335;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17133</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#38382;&#31572;&#23545;&#30340;&#26041;&#24335;&#23558;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;(MMT)&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#26816;&#39564;&#20102;MMT&#31995;&#32479;&#22312;&#25991;&#26412;&#36755;&#20837;&#23436;&#25972;&#26102;&#23545;&#35270;&#35273;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38477;&#20302;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#29616;&#35937;&#28304;&#20110;&#36328;&#27169;&#24577;&#20132;&#20114;&#19981;&#36275;&#65292;&#32780;&#19981;&#26159;&#22270;&#20687;&#20449;&#24687;&#20887;&#20313;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20174;&#28304;&#25991;&#26412;&#29983;&#25104;&#24182;&#34892;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26679;&#24335;&#23545;&#65292;&#20419;&#36827;&#26356;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23545;MMT&#20013;&#30340;&#25506;&#27979;&#20449;&#21495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;VQA&#26679;&#24335;&#25968;&#25454;&#65292;&#21019;&#24314;&#20102;Multi30K-VQA&#25968;&#25454;&#38598;&#12290;&#24341;&#20837;&#20102;MMT-VQA&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#24335;&#25506;&#27979;&#20449;&#21495;&#34701;&#20837;MMT&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;&#65306;\url{https://github.com/libeineu/MMT-VQA}&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;GNN&#26410;&#20805;&#20998;&#21033;&#29992;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#30340;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#20351;GNN&#33021;&#22815;&#20805;&#20998;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.17132</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#37322;&#25918;GNN&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the potential of GNNs via Bi-directional Knowledge Transfer. (arXiv:2310.17132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;GNN&#26410;&#20805;&#20998;&#21033;&#29992;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#30340;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#20351;GNN&#33021;&#22815;&#20805;&#20998;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29305;&#24449;&#20256;&#25773;&#26426;&#21046;&#26469;&#25552;&#39640;GNN&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#24449;&#36716;&#25442;&#65292;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#25805;&#20316;&#65292;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#20856;&#22411;GNN&#20013;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;&#24847;&#22806;&#30340;&#26159;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;GNN&#27809;&#26377;&#23436;&#20840;&#37322;&#25918;&#20986;&#20869;&#22312;&#30340;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#30693;&#35782;&#20256;&#36882;&#65288;BiKT&#65289;&#65292;&#19968;&#31181;&#25554;&#25300;&#24335;&#26041;&#27861;&#65292;&#26088;&#22312;&#37322;&#25918;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#20462;&#25913;&#21407;&#22987;&#26550;&#26500;&#12290;&#23558;&#29305;&#24449;&#36716;&#25442;&#25805;&#20316;&#35270;&#20026;&#19982;&#21407;&#22987;GNN&#20849;&#20139;&#21442;&#25968;&#30340;&#27966;&#29983;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#35813;&#27169;&#22411;&#30340;&#30452;&#25509;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#25299;&#25169;&#26080;&#20851;&#30340;&#30693;&#35782;&#21453;&#39304;&#65292;&#36827;&#19968;&#27493;&#25351;&#23548;&#20102;GNN&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the message-passing paradigm, there has been an amount of research proposing diverse and impressive feature propagation mechanisms to improve the performance of GNNs. However, less focus has been put on feature transformation, another major operation of the message-passing framework. In this paper, we first empirically investigate the performance of the feature transformation operation in several typical GNNs. Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation. By this observation, we propose the Bi-directional Knowledge Transfer (BiKT), a plug-and-play approach to unleash the potential of the feature transformation operations without modifying the original architecture. Taking the feature transformation operation as a derived representation learning model that shares parameters with the original GNN, the direct prediction by this model provides a topological-agnostic knowledge feedback that can further instru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.17120</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#20998;&#27573;
&lt;/p&gt;
&lt;p&gt;
Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#27492;&#31867;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#20998;&#27573;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20351;&#29992;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#23558;&#25991;&#26723;&#25110;&#23545;&#35805;&#26681;&#25454;&#20854;&#35821;&#20041;&#32467;&#26500;&#20998;&#35299;&#20026;&#22810;&#20010;&#36830;&#32493;&#29255;&#27573;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20851;&#20110;&#20027;&#39064;&#20998;&#27573;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#20998;&#27573;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20027;&#39064;&#20998;&#27573;&#27169;&#22411;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;a&#65289;&#30446;&#21069;&#22312;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#25991;&#26412;&#35821;&#26009;&#24211;&#65288;&#22914;Wiki-727K&#65289;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#23545;&#20110;&#22312;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#19978;&#30340;&#21487;&#20256;&#36882;&#24615;&#24182;&#19981;&#26377;&#24110;&#21161;&#12290;&#65288;b&#65289;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#30456;&#23545;&#23567;&#35268;&#27169;&#30340;&#30446;&#26631;&#38750;&#32467;&#26500;&#21270;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#20998;&#27573;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23581;&#35797;&#22810;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#36827;&#34892;&#25105;&#20204;&#30340;&#20027;&#39064;&#20998;&#27573;&#26041;&#27861;&#30340;&#24378;&#21270;&#27979;&#35797;&#65292;&#20197;&#20943;&#36731;&#38750;&#32467;&#26500;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#30340;&#19981;&#24179;&#34913;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;Fo
&lt;/p&gt;
&lt;p&gt;
Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.17091</link><description>&lt;p&gt;
&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#30340;&#38544;&#34109;&#32593;&#32476;&#25915;&#20987;&#30340;&#26816;&#27979;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach. (arXiv:2310.17091v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37197;&#22791;&#20102;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#20854;&#20182;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#30340;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38024;&#23545;&#36825;&#20123;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#32593;&#32476;&#25915;&#20987;&#28508;&#22312;&#39118;&#38505;&#20063;&#20986;&#29616;&#20102;&#12290;&#34429;&#28982;&#24378;&#21046;&#36710;&#36742;&#21457;&#29983;&#30896;&#25758;&#30340;&#26126;&#26174;&#25915;&#20987;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20294;&#26356;&#38544;&#34109;&#30340;&#25915;&#20987;&#65292;&#21482;&#30053;&#24494;&#25913;&#21464;&#34892;&#39542;&#34892;&#20026;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#33539;&#22260;&#20869;&#25317;&#22581;&#12289;&#29123;&#27833;&#28040;&#32791;&#22686;&#21152;&#65292;&#29978;&#33267;&#22686;&#21152;&#30896;&#25758;&#39118;&#38505;&#65292;&#20294;&#24456;&#38590;&#34987;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#33021;&#30340;&#19977;&#31181;&#32593;&#32476;&#25915;&#20987;&#31867;&#22411;&#65306;&#24694;&#24847;&#25805;&#32437;&#36710;&#36742;&#25511;&#21046;&#21629;&#20196;&#12289;&#23545;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#34394;&#20551;&#27880;&#20837;&#25915;&#20987;&#21644;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;&#20010;&#20307;&#36710;&#36742;&#65288;&#24494;&#35266;&#65289;&#21644;&#20132;&#36890;&#27969;&#65288;&#23439;&#35266;&#65289;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of vehicles equipped with advanced driver-assistance systems, such as adaptive cruise control (ACC) and other automated driving features, the potential for cyberattacks on these automated vehicles (AVs) has emerged. While overt attacks that force vehicles to collide may be easily identified, more insidious attacks, which only slightly alter driving behavior, can result in network-wide increases in congestion, fuel consumption, and even crash risk without being easily detected. To address the detection of such attacks, we first present a traffic model framework for three types of potential cyberattacks: malicious manipulation of vehicle control commands, false data injection attacks on sensor measurements, and denial-of-service (DoS) attacks. We then investigate the impacts of these attacks at both the individual vehicle (micro) and traffic flow (macro) levels. A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identifica
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.</title><link>http://arxiv.org/abs/2310.17072</link><description>&lt;p&gt;
&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives. (arXiv:2310.17072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17072
&lt;/p&gt;
&lt;p&gt;
Isometric Motion Manifold Primitives (IMMP) is proposed to address the degradation of Motion Manifold Primitive (MMP) performance due to geometric distortion in the latent space. IMMP preserves the geometry of the manifold in the latent coordinate space using a Riemannian metric, and experimental results show that IMMP significantly outperforms existing MMP methods.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;MMP&#65289;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#19968;&#31995;&#21015;&#36830;&#32493;&#36712;&#36857;&#27969;&#24418;&#65292;&#27599;&#19968;&#20010;&#36712;&#36857;&#27969;&#24418;&#37117;&#33021;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#30001;&#23545;&#27969;&#24418;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#35299;&#30721;&#20989;&#25968;&#20197;&#21450;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#23494;&#24230;&#32452;&#25104;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#30001;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20960;&#20309;&#25197;&#26354;&#65292;MMP&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;--&#36890;&#36807;&#21464;&#24418;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#30456;&#20284;&#30340;&#36816;&#21160;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#27861;&#30456;&#37051;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#36317;&#36816;&#21160;&#27969;&#24418;&#22522;&#20803;&#65288;IMMP&#65289;&#65292;&#20854;&#28508;&#22312;&#22352;&#26631;&#31354;&#38388;&#20445;&#25345;&#20102;&#27969;&#24418;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#21644;&#20351;&#29992;&#20102;&#19968;&#20010;Riemannian&#24230;&#37327;&#65292;&#29992;&#20110;&#36816;&#21160;&#31354;&#38388;&#65288;&#21363;&#65292;&#21442;&#25968;&#21270;&#26354;&#32447;&#31354;&#38388;&#65289;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CurveGeom Riemannian&#24230;&#37327;&#12290;&#23545;&#20110;&#24179;&#38754;&#38556;&#30861;&#36991;&#35753;&#36816;&#21160;&#21644;&#25512;&#21160;&#25805;&#32437;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IMMP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;MMP&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Gabe-YHLee/IMMP&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.17064</link><description>&lt;p&gt;
math-PVS:&#19968;&#20010;&#23558;&#31185;&#23398;&#20986;&#29256;&#29289;&#26144;&#23556;&#21040;PVS&#29702;&#35770;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories. (arXiv:2310.17064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#22312;&#25968;&#23398;&#21457;&#29616;&#26041;&#38754;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#24341;&#23548;&#29468;&#24819;&#29983;&#25104;&#65292;&#26500;&#36896;&#21453;&#20363;&#65292;&#21327;&#21161;&#24418;&#24335;&#21270;&#25968;&#23398;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#25968;&#23398;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#31561;&#31561;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#35745;&#31639;&#26426;&#36827;&#34892;&#35814;&#23613;&#30340;&#25968;&#23398;&#35777;&#26126;&#25628;&#32034;&#65292;&#20294;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#35745;&#31639;&#24179;&#21488;&#23450;&#20301;&#20026;&#25968;&#23398;&#30740;&#31350;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#36129;&#29486;&#32773;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#23558;&#23450;&#29702;&#35777;&#26126;&#31995;&#32479;&#19982;&#22522;&#30784;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#22312;&#24418;&#24335;&#21270;&#39640;&#32423;&#25968;&#23398;&#27010;&#24565;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25209;&#21028;&#24615;&#22320;&#23457;&#26597;&#21644;&#26816;&#26597;&#30740;&#31350;&#35770;&#25991;&#20013;&#25968;&#23398;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20869;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#26469;&#23398;&#20064;&#21487;&#37325;&#22797;&#30340;&#35821;&#38899;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;ICC&#27491;&#21017;&#21270;&#22120;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#34917;&#20805;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#23884;&#20837;&#30340;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17049</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20869;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21487;&#37325;&#22797;&#30340;&#35821;&#38899;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Repeatable Speech Embeddings Using An Intra-class Correlation Regularizer. (arXiv:2310.17049v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20869;&#30456;&#20851;&#27491;&#21017;&#21270;&#22120;&#26469;&#23398;&#20064;&#21487;&#37325;&#22797;&#30340;&#35821;&#38899;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;ICC&#27491;&#21017;&#21270;&#22120;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#34917;&#20805;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#23884;&#20837;&#30340;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22909;&#30340;&#30417;&#30563;&#23884;&#20837;&#29992;&#20110;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20165;&#23545;&#24863;&#20852;&#36259;&#30340;&#26631;&#31614;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;&#28151;&#28102;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#21033;&#29992;&#27979;&#37327;&#29702;&#35770;&#20013;&#30340;&#37325;&#22797;&#24615;&#27010;&#24565;&#26469;&#25551;&#36848;&#36825;&#20010;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#31867;&#20869;&#30456;&#20851;&#31995;&#25968;&#65288;ICC&#65289;&#26469;&#35780;&#20272;&#23884;&#20837;&#30340;&#37325;&#22797;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21363;ICC&#27491;&#21017;&#21270;&#22120;&#65292;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#34917;&#20805;&#32452;&#20214;&#65292;&#24341;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#37325;&#22797;&#24615;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#26469;&#35299;&#37322;&#20026;&#20160;&#20040;ICC&#27491;&#21017;&#21270;&#22120;&#22312;&#20943;&#23567;&#31867;&#20869;&#21464;&#24322;&#24615;&#26041;&#38754;&#27604;&#20165;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;ICC&#27491;&#21017;&#21270;&#22120;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#38899;&#20219;&#21153;&#65306;&#35828;&#35805;&#20154;&#39564;&#35777;&#12289;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#20197;&#21450;&#29992;&#20110;&#26816;&#27979;&#22768;&#24102;&#21151;&#33021;&#32010;&#20081;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;ICC&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A good supervised embedding for a specific machine learning task is only sensitive to changes in the label of interest and is invariant to other confounding factors. We leverage the concept of repeatability from measurement theory to describe this property and propose to use the intra-class correlation coefficient (ICC) to evaluate the repeatability of embeddings. We then propose a novel regularizer, the ICC regularizer, as a complementary component for contrastive losses to guide deep neural networks to produce embeddings with higher repeatability. We use simulated data to explain why the ICC regularizer works better on minimizing the intra-class variance than the contrastive loss alone. We implement the ICC regularizer and apply it to three speech tasks: speaker verification, voice style conversion, and a clinical application for detecting dysphonic voice. The experimental results demonstrate that adding an ICC regularizer can improve the repeatability of learned embeddings compared 
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;</title><link>http://arxiv.org/abs/2310.17041</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25163;&#26415;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#21482;&#23545;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#37096;&#20998;&#23618;&#36827;&#34892;&#32454;&#35843;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#36873;&#25321;&#24615;&#24494;&#35843;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#30740;&#31350;&#31361;&#20986;&#34920;&#26126;&#65292;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#65288;&#20351;&#29992;&#25152;&#26377;&#21442;&#25968;&#25110;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65289;&#24448;&#24448;&#26159;&#23558;&#20854;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#30340;&#40664;&#35748;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#65292;&#20165;&#32454;&#35843;&#37096;&#20998;&#23618;&#21363;&#21487;&#33719;&#24471;&#25509;&#36817;&#29978;&#33267;&#20248;&#20110;&#32454;&#35843;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#25152;&#26377;&#23618;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#65288;FIM&#35780;&#20998;&#65289;&#30340;&#39640;&#25928;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#29992;&#20110;&#36873;&#25321;&#24615;&#24494;&#35843;&#30340;&#20505;&#36873;&#23618;&#12290;&#25105;&#20204;&#22312;GLUE&#21644;SuperGLUE&#20219;&#21153;&#20197;&#21450;&#19981;&#21516;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#19978;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#65292;&#36825;&#20010;&#24230;&#37327;&#21487;&#20197;&#26377;&#25928;&#36873;&#25321;&#23548;&#33268;&#24378;&#22823;&#19979;&#28216;&#24615;&#33021;&#30340;&#23618;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#19982;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#24212;&#30340;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#36890;&#24120;&#23616;&#37096;&#21270;&#22312;&#23569;&#25968;&#23618;&#20869;&#65292;&#21482;&#35843;&#25972;&#36825;&#20123;&#23618;&#23545;&#20110;&#24378;&#22823;&#30340;&#24615;&#33021;&#23601;&#36275;&#22815;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning all the layers of a pre-trained neural language encoder (either using all the parameters or using parameter-efficient methods) is often the de-facto way of adapting it to a new task. We show evidence that for different downstream language tasks, fine-tuning only a subset of layers is sufficient to obtain performance that is close to and often better than fine-tuning all the layers in the language encoder. We propose an efficient metric based on the diagonal of the Fisher information matrix (FIM score), to select the candidate layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE tasks and across distinct language encoders, that this metric can effectively select layers leading to a strong downstream performance. Our work highlights that task-specific information corresponding to a given downstream task is often localized within a few layers, and tuning only those is sufficient for strong performance. Additionally, we demonstrate the robustness of the 
&lt;/p&gt;</description></item><item><title>netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2310.17025</link><description>&lt;p&gt;
netFound: &#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
netFound: Foundation Model for Network Security. (arXiv:2310.17025v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17025
&lt;/p&gt;
&lt;p&gt;
netFound&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#35757;&#32451;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20256;&#32479;&#24037;&#20316;&#27969;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#65292;&#20294;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#21644;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38459;&#30861;&#20102;&#29305;&#24449;&#36873;&#25321;&#65292;&#23548;&#33268;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#20851;&#38190;&#20851;&#31995;&#21644;&#26377;&#25928;&#27867;&#21270;&#12290;&#21463;&#21040;GPT-4&#21644;Vision Transformers&#31561;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;netFound&#65292;&#19968;&#20010;&#32593;&#32476;&#23433;&#20840;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#31639;&#27861;&#23545;&#29616;&#26377;&#30340;&#26410;&#26631;&#35760;&#32593;&#32476;&#25968;&#25454;&#21253;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;netFound&#30340;&#35774;&#35745;&#34701;&#21512;&#20102;&#32593;&#32476;&#27969;&#37327;&#30340;&#23618;&#27425;&#21270;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#38544;&#34255;&#30340;&#32593;&#32476;&#19978;&#19979;&#25991;&#65292;&#21253;&#25324;&#24212;&#29992;&#36923;&#36753;&#12289;&#36890;&#20449;&#21327;&#35758;&#21644;&#32593;&#32476;&#26465;&#20214;&#12290;&#26377;&#20102;&#36825;&#20010;&#39044;&#35757;&#32451;&#22522;&#30784;&#65292;&#21363;&#20351;&#22788;&#29702;&#36136;&#37327;&#20302;&#12289;&#26377;&#38480;&#21644;&#22024;&#26434;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#23545;netFound&#36827;&#34892;&#24494;&#35843;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;netFound&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ML for network security, traditional workflows rely on high-quality labeled data and manual feature engineering, but limited datasets and human expertise hinder feature selection, leading to models struggling to capture crucial relationships and generalize effectively. Inspired by recent advancements in ML application domains like GPT-4 and Vision Transformers, we have developed netFound, a foundational model for network security. This model undergoes pre-training using self-supervised algorithms applied to readily available unlabeled network packet traces. netFound's design incorporates hierarchical and multi-modal attributes of network traffic, effectively capturing hidden networking contexts, including application logic, communication protocols, and network conditions.  With this pre-trained foundation in place, we can fine-tune netFound for a wide array of downstream tasks, even when dealing with low-quality, limited, and noisy labeled data. Our experiments demonstrate netFound'
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;PRISMA&#26694;&#26550;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;534&#31687;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.17017</link><description>&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#35270;&#35282;&#30340;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives. (arXiv:2310.17017v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;PRISMA&#26694;&#26550;&#32508;&#36848;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;534&#31687;&#35770;&#25991;&#65292;&#21457;&#29616;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#23545;&#35805;&#20195;&#29702;&#65288;&#20063;&#31216;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20854;&#25552;&#20379;&#23545;&#37027;&#20123;&#38754;&#20020;&#24515;&#29702;&#20581;&#24247;&#25361;&#25112;&#30340;&#20154;&#21487;&#21450;&#24615;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;&#20043;&#21069;&#23545;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#20027;&#35201;&#32771;&#34385;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#25110;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#23548;&#33268;&#29702;&#35299;&#19978;&#30340;&#20998;&#35010;&#65292;&#38459;&#30861;&#20102;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26377;&#30410;&#30693;&#35782;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#20351;&#29992;PRISMA&#26694;&#26550;&#36827;&#34892;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#23457;&#26597;&#20102;534&#31687;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#21457;&#34920;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#32508;&#36848;&#25581;&#31034;&#20102;136&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#24314;&#31435;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#22810;&#31181;&#24314;&#27169;&#21644;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#35770;&#25991;&#20391;&#37325;&#20110;LLM&#25216;&#26415;&#21644;&#20351;&#29992;&#33258;&#21160;&#24230;&#37327;&#35780;&#20272;&#21709;&#24212;&#36136;&#37327;&#65292;&#23545;&#24212;&#29992;&#24212;&#29992;&#20851;&#27880;&#36739;&#23569;&#65292;&#32780;&#21307;&#23398;&#35770;&#25991;&#21017;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#23545;&#35805;&#20195;&#29702;&#21644;&#32467;&#26524;&#24230;&#37327;&#26469;&#34913;&#37327;&#21442;&#19982;&#32773;&#30340;&#20581;&#24247;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of parti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#21407;&#22411;&#32593;&#32476;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#37325;&#35201;&#32500;&#24230;&#26469;&#25552;&#39640;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17010</link><description>&lt;p&gt;
&#36825;&#23601;&#20687;&#37027;&#26679;&#38405;&#35835;&#65306;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#21407;&#22411;&#32593;&#32476;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#37325;&#35201;&#32500;&#24230;&#26469;&#25552;&#39640;&#30456;&#20284;&#24230;&#35745;&#31639;&#65292;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#21644;&#35299;&#37322;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24615;&#20915;&#31574;&#65292;&#36890;&#36807;&#23398;&#20064;&#30340;&#21407;&#22411;&#30340;&#30456;&#20284;&#24615;&#26469;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#23613;&#31649;&#23427;&#20027;&#35201;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#21407;&#22411;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#30340;&#21152;&#26435;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#32858;&#28966;&#20110;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#20449;&#24687;&#32500;&#24230;&#26469;&#22686;&#24378;&#30456;&#20284;&#24230;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#65292;&#20174;&#21407;&#22411;&#21644;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#21333;&#35789;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;&#22312;AG News&#21644;RT Polarity&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#19982;&#22522;&#20110;&#21512;&#29702;&#24615;&#30340;&#36882;&#24402;&#21367;&#31215;&#30456;&#27604;&#65292;&#36824;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype learning, a popular machine learning method designed for inherently interpretable decisions, leverages similarities to learned prototypes for classifying new data. While it is mainly applied in computer vision, in this work, we build upon prior research and further explore the extension of prototypical networks to natural language processing. We introduce a learned weighted similarity measure that enhances the similarity computation by focusing on informative dimensions of pre-trained sentence embeddings. Additionally, we propose a post-hoc explainability mechanism that extracts prediction-relevant words from both the prototype and input sentences. Finally, we empirically demonstrate that our proposed method not only improves predictive performance on the AG News and RT Polarity datasets over a previous prototype-based approach, but also improves the faithfulness of explanations compared to rationale-based recurrent convolutions.
&lt;/p&gt;</description></item><item><title>STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16990</link><description>&lt;p&gt;
STEER: &#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
STEER: Semantic Turn Extension-Expansion Recognition for Voice Assistants. (arXiv:2310.16990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16990
&lt;/p&gt;
&lt;p&gt;
STEER&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#38899;&#21161;&#25163;&#30340;&#35821;&#20041;&#36716;&#21521;&#25193;&#23637;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#36716;&#21521;&#24847;&#22270;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21161;&#25163;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#36716;&#21521;&#26159;&#25351;&#29992;&#25143;&#21457;&#20986;&#21518;&#32493;&#21629;&#20196;&#65292;&#35797;&#22270;&#24341;&#23548;&#25110;&#28548;&#28165;&#20043;&#21069;&#30340;&#25351;&#20196;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STEER&#65292;&#19968;&#20010;&#36716;&#21521;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21518;&#32493;&#21629;&#20196;&#26159;&#21542;&#26159;&#29992;&#25143;&#20225;&#22270;&#36716;&#21521;&#20043;&#21069;&#25351;&#20196;&#30340;&#23581;&#35797;&#12290;&#30001;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#26500;&#24314;&#29992;&#20110;&#36716;&#21521;&#26696;&#20363;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#21551;&#21457;&#24335;&#35268;&#21017;&#26469;&#37319;&#26679;&#36873;&#25321;&#21152;&#20837;&#20351;&#29992;&#25968;&#25454;&#65292;&#36817;&#20284;&#27491;&#36127;&#26679;&#26412;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#35782;&#21035;&#36716;&#21521;&#24847;&#22270;&#30340;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#25105;&#20204;&#37319;&#26679;&#30340;&#25968;&#25454;&#19978;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;STEER&#32467;&#21512;&#25105;&#20204;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#22312;&#20154;&#24037;&#35780;&#20272;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#19982;&#30495;&#23454;&#30340;&#36716;&#21521;&#22330;&#26223;&#30456;&#21305;&#37197;&#12290;&#38500;&#20102;&#20165;&#20381;&#36182;&#29992;&#25143;&#30340;&#36716;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;STEER+&#65292;&#36825;&#26159;&#27169;&#22411;&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of a voice assistant system, steering refers to the phenomenon in which a user issues a follow-up command attempting to direct or clarify a previous turn. We propose STEER, a steering detection model that predicts whether a follow-up turn is a user's attempt to steer the previous command. Constructing a training dataset for steering use cases poses challenges due to the cold-start problem. To overcome this, we developed heuristic rules to sample opt-in usage data, approximating positive and negative samples without any annotation. Our experimental results show promising performance in identifying steering intent, with over 95% accuracy on our sampled data. Moreover, STEER, in conjunction with our sampling strategy, aligns effectively with real-world steering scenarios, as evidenced by its strong zero-shot performance on a human-graded evaluation set. In addition to relying solely on user transcripts as input, we introduce STEER+, an enhanced version of the model. STEER+ 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.16978</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24847;&#20041;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Significance of Machine Learning in Clinical Disease Diagnosis: A Review. (arXiv:2310.16978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20020;&#24202;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21508;&#31181;&#30142;&#30149;&#26426;&#21046;&#30340;&#22797;&#26434;&#24615;&#21644;&#24739;&#32773;&#30151;&#29366;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#30340;&#30142;&#30149;&#35786;&#26029;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#21307;&#29983;&#21644;&#24739;&#32773;&#27491;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#19968;&#38376;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23398;&#31185;&#65292;&#26469;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;ML&#21644;AI&#26041;&#27861;&#65292;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#26041;&#21487;&#20197;&#33719;&#24471;&#22686;&#24378;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#38024;&#23545;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#25351;&#26631;&#20013;&#25913;&#21892;&#24515;&#29575;&#25968;&#25454;&#20256;&#36755;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;ML&#31639;&#27861;&#65292;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;ML&#30340;&#30142;&#30149;&#35786;&#26029;&#65288;MLBDD&#65289;&#30340;&#26368;&#26032;&#36235;&#21183;&#21644;&#26041;&#27861;&#12290;&#32771;&#34385;&#30340;&#22240;&#32032;&#21253;&#25324;&#31639;&#27861;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global need for effective disease diagnosis remains substantial, given the complexities of various disease mechanisms and diverse patient symptoms. To tackle these challenges, researchers, physicians, and patients are turning to machine learning (ML), an artificial intelligence (AI) discipline, to develop solutions. By leveraging sophisticated ML and AI methods, healthcare stakeholders gain enhanced diagnostic and treatment capabilities. However, there is a scarcity of research focused on ML algorithms for enhancing the accuracy and computational efficiency. This research investigates the capacity of machine learning algorithms to improve the transmission of heart rate data in time series healthcare metrics, concentrating particularly on optimizing accuracy and efficiency. By exploring various ML algorithms used in healthcare applications, the review presents the latest trends and approaches in ML-based disease diagnosis (MLBDD). The factors under consideration include the algorith
&lt;/p&gt;</description></item><item><title>CL-MASR&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.16931</link><description>&lt;p&gt;
CL-MASR&#65306;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#38899;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CL-MASR: A Continual Learning Benchmark for Multilingual ASR. (arXiv:2310.16931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16931
&lt;/p&gt;
&lt;p&gt;
CL-MASR&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#35821;&#38899;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#22914;Whisper&#65292;&#20351;&#24471;&#33021;&#22815;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#38899;&#39057;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#36890;&#24120;&#22312;&#21333;&#20010;&#35821;&#35328;&#25110;&#22810;&#20219;&#21153;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#25345;&#32493;&#23398;&#20064;&#26032;&#35821;&#35328;&#30340;&#25361;&#25112;&#12290;&#20851;&#20110;&#22914;&#20309;&#28155;&#21152;&#26032;&#35821;&#35328;&#32780;&#19981;&#20002;&#22833;&#20808;&#21069;&#25968;&#25454;&#20013;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#22810;&#35821;&#38899;ASR&#30340;&#25345;&#32493;&#23398;&#20064;&#36824;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;CL-MASR&#65292;&#19968;&#20010;&#19987;&#20026;&#22312;&#25345;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#22810;&#35821;&#38899;ASR&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#12290;CL-MASR&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#24120;&#35265;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23398;&#20064;&#26032;&#35821;&#35328;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model. However, current state-of-the-art ASR models are typically evaluated on individual languages or in a multi-task setting, overlooking the challenge of continually learning new languages. There is insufficient research on how to add new languages without losing valuable information from previous data. Furthermore, existing continual learning benchmarks focus mostly on vision and language tasks, leaving continual learning for multilingual ASR largely unexplored. To bridge this gap, we propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting. CL-MASR provides a diverse set of continual learning methods implemented on top of large-scale pretrained ASR models, along with common metrics to assess the effectiveness of learning new languages while addressing the issue of catastrophic forgetting
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#24179;&#22374;&#26368;&#20302;&#27700;&#21360;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#25239;&#30333;&#30418;&#25915;&#20987;&#65292;&#20445;&#25252;GAN&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#36890;&#36807;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;GAN&#30340;&#35757;&#32451;&#20013;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#29255;&#21253;&#21547;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#30340;&#38543;&#26426;&#22122;&#22768;&#65292;&#20351;&#27169;&#22411;&#25910;&#25947;&#21040;&#24191;&#27867;&#24179;&#22374;&#30340;&#27700;&#21360;&#25439;&#22833;&#26368;&#20302;&#28857;&#65292;&#25552;&#39640;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16919</link><description>&lt;p&gt;
&#24191;&#27867;&#24179;&#22374;&#26368;&#20302;&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;GAN&#30340;&#40065;&#26834;&#24615;&#24402;&#23646;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs. (arXiv:2310.16919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#27867;&#24179;&#22374;&#26368;&#20302;&#27700;&#21360;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#25239;&#30333;&#30418;&#25915;&#20987;&#65292;&#20445;&#25252;GAN&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#36890;&#36807;&#23558;&#27700;&#21360;&#23884;&#20837;&#21040;GAN&#30340;&#35757;&#32451;&#20013;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#29255;&#21253;&#21547;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#30340;&#38543;&#26426;&#22122;&#22768;&#65292;&#20351;&#27169;&#22411;&#25910;&#25947;&#21040;&#24191;&#27867;&#24179;&#22374;&#30340;&#27700;&#21360;&#25439;&#22833;&#26368;&#20302;&#28857;&#65292;&#25552;&#39640;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20301;&#26080;&#30418;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;GAN&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25913;&#36827;&#20102;&#23545;&#30333;&#30418;&#25915;&#20987;&#65292;&#22914;&#24494;&#35843;&#12289;&#21098;&#26525;&#12289;&#37327;&#21270;&#21644;&#26367;&#20195;&#27169;&#22411;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#27700;&#21360;&#26159;&#36890;&#36807;&#22312;GAN&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#39069;&#22806;&#30340;&#27700;&#21360;&#25439;&#22833;&#39033;&#36827;&#34892;&#23884;&#20837;&#30340;&#65292;&#30830;&#20445;GAN&#29983;&#25104;&#30340;&#22270;&#29255;&#21253;&#21547;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#65292;&#21487;&#20197;&#30001;&#39044;&#35757;&#32451;&#30340;&#27700;&#21360;&#35299;&#30721;&#22120;&#25552;&#21462;&#20986;&#26469;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#30333;&#30418;&#27169;&#22411;&#32423;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#30830;&#20445;&#27169;&#22411;&#25910;&#25947;&#21040;&#27700;&#21360;&#25439;&#22833;&#39033;&#30340;&#24191;&#27867;&#24179;&#22374;&#26368;&#20302;&#28857;&#65292;&#36825;&#26679;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#30340;&#20462;&#25913;&#37117;&#19981;&#20250;&#25830;&#38500;&#27700;&#21360;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21521;&#29983;&#25104;&#22120;&#30340;&#21442;&#25968;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#21521;&#37327;&#65292;&#24182;&#35201;&#27714;&#27700;&#21360;&#25439;&#22833;&#39033;&#22312;&#22122;&#22768;&#23384;&#22312;&#26102;&#23613;&#21487;&#33021;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#20010;&#36807;&#31243;&#24378;&#36843;&#29983;&#25104;&#22120;&#25910;&#25947;&#21040;&#27700;&#21360;&#25216;&#26415;&#30340;&#24191;&#27867;&#24179;&#22374;&#26368;&#20302;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16867</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#33041;&#37096;&#35760;&#24405;&#65292;&#36827;&#34892;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#33258;&#21160;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#33021;&#22815;&#21033;&#29992;&#26102;&#39057;&#29305;&#24449;&#65292;&#20174;&#21407;&#22987;&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;&#39057;&#35889;&#22270;&#12290;&#22312;&#25506;&#32034;&#20102;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#32622;&#21518;&#65292;&#36873;&#25321;&#20102;&#36866;&#21512;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;Wasserstein GAN with Gradient Penalty(WGAN-GP)&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#29983;&#25104;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20351;&#29992;VAE&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#22312;&#20934;&#30830;&#24230;&#19978;&#25552;&#39640;&#20102;3.0&#65285;&#65292;&#36798;&#21040;&#20102;99.0&#65285;&#65292;&#21516;&#26102;&#24471;&#21040;&#20102;&#26356;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#20449;&#20219;&#32570;&#20047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#21644;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;DLBCL&#24739;&#32773;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16863</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#22810;&#30149;&#21464;DLBCL&#27835;&#30103;&#21453;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images. (arXiv:2310.16863v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20020;&#24202;&#21644;&#22270;&#20687;&#25968;&#25454;&#26469;&#39044;&#27979;DLBCL&#24739;&#32773;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#28459;&#22823;B&#32454;&#32990;&#28107;&#24052;&#30244;&#65288;DLBCL&#65289;&#26159;&#28041;&#21450;&#19968;&#20010;&#25110;&#22810;&#20010;&#28107;&#24052;&#32467;&#21644;&#28107;&#24052;&#22806;&#37096;&#20301;&#28857;&#30340;&#28107;&#24052;&#30284;&#30151;&#12290;&#20854;&#35786;&#26029;&#21644;&#38543;&#35775;&#20381;&#36182;&#20110;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;PET&#65289;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#22312;&#35786;&#26029;&#21518;&#65292;&#26631;&#20934;&#19968;&#32447;&#27835;&#30103;&#38750;&#21453;&#24212;&#24739;&#32773;&#30340;&#25968;&#37327;&#20173;&#28982;&#24456;&#22823;&#65288;30-40%&#65289;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#26041;&#27861;&#65292;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#24739;&#32773;&#30340;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#20020;&#24202;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#38656;&#35201;&#36866;&#24212;&#24615;&#27835;&#30103;&#30340;&#39640;&#39118;&#38505;&#24739;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#30149;&#21464;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#20197;&#21450;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#26377;&#25928;&#22320;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#35813;&#27169;&#22411;&#22312;&#19968;&#20010;&#31169;&#20154;&#21069;&#30651;&#24615;&#22810;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;583&#21517;&#24739;&#32773;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#22522;&#20110;&#20020;&#24202;&#12289;&#22270;&#20687;&#25110;&#20020;&#24202;&#21644;&#22270;&#20687;&#30340;&#32463;&#20856;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffuse Large B-cell Lymphoma (DLBCL) is a lymphatic cancer involving one or more lymph nodes and extranodal sites. Its diagnostic and follow-up rely on Positron Emission Tomography (PET) and Computed Tomography (CT). After diagnosis, the number of nonresponding patients to standard front-line therapy remains significant (30-40%). This work aims to develop a computer-aided approach to identify high-risk patients requiring adapted treatment by efficiently exploiting all the information available for each patient, including both clinical and image data. We propose a method based on recent graph neural networks that combine imaging information from multiple lesions, and a cross-attention module to integrate different data modalities efficiently. The model is trained and evaluated on a private prospective multicentric dataset of 583 patients. Experimental results show that our proposed method outperforms classical supervised methods based on either clinical, imaging or both clinical and im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#34913;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#20041;&#21644;&#32467;&#26500;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36793;&#25928;&#29992;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#21407;&#22987;&#26377;&#31526;&#21495;&#22270;&#12290;</title><link>http://arxiv.org/abs/2310.16862</link><description>&lt;p&gt;
&#24179;&#34913;&#22686;&#24378;&#19982;&#36793;&#25928;&#29992;&#36807;&#28388;&#22120;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Balancing Augmentation with Edge-Utility Filter for Signed GNNs. (arXiv:2310.16862v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24179;&#34913;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#20041;&#21644;&#32467;&#26500;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36793;&#25928;&#29992;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#21407;&#22987;&#26377;&#31526;&#21495;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#24341;&#36215;&#20102;&#26356;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#26159;&#30001;&#27491;&#36127;&#20004;&#31181;&#31867;&#22411;&#30340;&#36793;&#32452;&#25104;&#30340;&#12290;&#36127;&#36793;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;SGNN&#22312;&#20004;&#20010;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#12290;&#20854;&#19968;&#26159;&#35821;&#20041;&#19981;&#24179;&#34913;&#65292;&#22240;&#20026;&#36127;&#36793;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#25552;&#20379;&#28508;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20854;&#20108;&#26159;&#32467;&#26500;&#19981;&#24179;&#34913;&#65292;&#20363;&#22914;&#19981;&#24179;&#34913;&#30340;&#19977;&#35282;&#24418;&#65292;&#23427;&#20204;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;SGNN&#30340;&#19978;&#36848;&#20004;&#20010;&#26041;&#38754;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#35745;&#31639;&#36127;&#36793;&#22312;&#19981;&#24179;&#34913;&#32467;&#26500;&#20013;&#30340;&#20986;&#29616;&#27425;&#25968;&#26469;&#34913;&#37327;&#27599;&#20010;&#36127;&#36793;&#30340;&#25928;&#29992;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#65288;1&#65289;&#36793;&#25200;&#21160;&#35843;&#33410;&#22120;&#24179;&#34913;&#27491;&#36127;&#36793;&#30340;&#25968;&#37327;&#65292;&#24182;&#30830;&#23450;&#25200;&#21160;&#36793;&#19982;&#21407;&#22987;&#36793;&#30340;&#27604;&#29575;&#21644;&#65288;2&#65289;&#36793;&#25928;&#29992;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#21407;&#22987;&#26377;&#31526;&#21495;&#22270;&#65292;&#20197;&#21435;&#38500;&#36127;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed graph neural networks (SGNNs) has recently drawn more attention as many real-world networks are signed networks containing two types of edges: positive and negative. The existence of negative edges affects the SGNN robustness on two aspects. One is the semantic imbalance as the negative edges are usually hard to obtain though they can provide potentially useful information. The other is the structural unbalance, e.g. unbalanced triangles, an indication of incompatible relationship among nodes. In this paper, we propose a balancing augmentation method to address the above two aspects for SGNNs. Firstly, the utility of each negative edge is measured by calculating its occurrence in unbalanced structures. Secondly, the original signed graph is selectively augmented with the use of (1) an edge perturbation regulator to balance the number of positive and negative edges and to determine the ratio of perturbed edges to original edges and (2) an edge utility filter to remove the negativ
&lt;/p&gt;</description></item><item><title>CP-BCS&#26159;&#19968;&#20010;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#20266;&#20195;&#30721;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20989;&#25968;&#25688;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21093;&#31163;&#20102;&#31526;&#21495;&#34920;&#21644;&#35843;&#35797;&#20449;&#24687;&#30340;&#20108;&#36827;&#21046;&#12290;&#23427;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#23398;&#20064;&#20108;&#36827;&#21046;&#20989;&#25968;&#30340;&#25191;&#34892;&#34892;&#20026;&#21644;&#36923;&#36753;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.16853</link><description>&lt;p&gt;
CP-BCS&#65306;&#30001;&#25511;&#21046;&#27969;&#22270;&#21644;&#20266;&#20195;&#30721;&#24341;&#23548;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code. (arXiv:2310.16853v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16853
&lt;/p&gt;
&lt;p&gt;
CP-BCS&#26159;&#19968;&#20010;&#22522;&#20110;&#25511;&#21046;&#27969;&#22270;&#21644;&#20266;&#20195;&#30721;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20989;&#25968;&#25688;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21093;&#31163;&#20102;&#31526;&#21495;&#34920;&#21644;&#35843;&#35797;&#20449;&#24687;&#30340;&#20108;&#36827;&#21046;&#12290;&#23427;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#23398;&#20064;&#20108;&#36827;&#21046;&#20989;&#25968;&#30340;&#25191;&#34892;&#34892;&#20026;&#21644;&#36923;&#36753;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20026;&#20108;&#36827;&#21046;&#29983;&#25104;&#20989;&#25968;&#25688;&#35201;&#26159;&#19968;&#39033;&#38750;&#24120;&#26377;&#20215;&#20540;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#23558;&#20302;&#32423;&#35821;&#35328;&#65288;&#27719;&#32534;&#20195;&#30721;&#65289;&#30340;&#25191;&#34892;&#34892;&#20026;&#21644;&#35821;&#20041;&#36716;&#21270;&#20026;&#21487;&#35835;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#20851;&#20110;&#29702;&#35299;&#27719;&#32534;&#20195;&#30721;&#30340;&#24037;&#20316;&#37117;&#26159;&#20026;&#20102;&#29983;&#25104;&#20989;&#25968;&#21517;&#31216;&#65292;&#20854;&#20013;&#21253;&#21547;&#35768;&#22810;&#32553;&#20889;&#26415;&#35821;&#65292;&#20351;&#24471;&#23427;&#20204;&#20173;&#28982;&#20196;&#20154;&#22256;&#24785;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20026;&#20108;&#36827;&#21046;&#20989;&#25968;&#29983;&#25104;&#23436;&#25972;&#30340;&#25688;&#35201;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21093;&#31163;&#20102;&#31526;&#21495;&#34920;&#21644;&#35843;&#35797;&#20449;&#24687;&#30340;&#20108;&#36827;&#21046;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#27719;&#32534;&#20195;&#30721;&#30340;&#35821;&#20041;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CP-BCS&#30340;&#25511;&#21046;&#27969;&#22270;&#21644;&#20266;&#20195;&#30721;&#24341;&#23548;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#25688;&#35201;&#26694;&#26550;&#12290;CP-BCS&#21033;&#29992;&#21452;&#21521;&#25351;&#20196;&#32423;&#25511;&#21046;&#27969;&#22270;&#21644;&#20266;&#20195;&#30721;&#65292;&#32467;&#21512;&#19987;&#23478;&#30693;&#35782;&#26469;&#23398;&#20064;&#20840;&#38754;&#30340;&#20108;&#36827;&#21046;&#20989;&#25968;&#25191;&#34892;&#34892;&#20026;&#21644;&#36923;&#36753;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#20108;&#36827;&#21046;&#20248;&#21270;&#26696;&#20363;&#19978;&#35780;&#20272;&#20102;CP-BCS&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating function summaries for binaries is an extremely valuable but challenging task, since it involves translating the execution behavior and semantics of the low-level language (assembly code) into human-readable natural language. However, most current works on understanding assembly code are oriented towards generating function names, which involve numerous abbreviations that make them still confusing. To bridge this gap, we focus on generating complete summaries for binary functions, especially for stripped binary (no symbol table and debug information in reality). To fully exploit the semantics of assembly code, we present a control flow graph and pseudo code guided binary code summarization framework called CP-BCS. CP-BCS utilizes a bidirectional instruction-level control flow graph and pseudo code that incorporates expert knowledge to learn the comprehensive binary function execution behavior and logic semantics. We evaluate CP-BCS on 3 different binary optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.16842</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#20915;&#23884;&#20837;&#24335;FPGA&#20013;LSTM&#21333;&#20803;&#30340;&#21534;&#21520;&#37327;&#29942;&#39048;&#65292;&#22686;&#24378;&#33021;&#25928;
&lt;/p&gt;
&lt;p&gt;
Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;LSTM&#21333;&#20803;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#22312;FPGA&#19978;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#36739;&#20302;&#30340;&#33021;&#32791;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#21534;&#21520;&#37327;&#21644;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20013;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#23884;&#20837;&#24335;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#19968;&#32500;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#12290;&#36807;&#21435;&#65292;&#32463;&#24120;&#20351;&#29992;CNN&#22240;&#20026;&#23427;&#20204;&#23545;&#20110;&#29305;&#27530;&#30340;&#23884;&#20837;&#24335;&#30828;&#20214;&#27604;&#22914;FPGA&#26469;&#35828;&#24456;&#23481;&#26131;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32456;&#31471;&#35774;&#22791;&#19978;&#36827;&#34892;&#33021;&#25928;&#25512;&#26029;&#30340;&#26032;&#22411;LSTM&#21333;&#20803;&#20248;&#21270;&#26041;&#27861;&#12290;&#20197;&#20132;&#36890;&#36895;&#24230;&#39044;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20248;&#21270;&#21518;&#30340;LSTM&#21333;&#20803;&#30340;&#31616;&#21333;LSTM&#27169;&#22411;&#22312;FPGA XC7S15&#65288;&#26469;&#33258;Spartan-7&#31995;&#21015;&#65289;&#19978;&#27599;&#31186;&#21487;&#23454;&#29616;17534&#20010;&#25512;&#26029;&#65292;&#20165;&#28040;&#32791;&#27599;&#20010;&#25512;&#26029;3.8&#24494;&#28966;&#32819;&#30340;&#33021;&#37327;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#30340;&#21534;&#21520;&#37327;&#33267;&#23569;&#25552;&#39640;&#20102;5.4&#20493;&#65292;&#33021;&#25928;&#25552;&#39640;&#20102;1.37&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16779</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Diffusion Denoised Smoothing. (arXiv:2310.16779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#35843;&#25972;&#20197;&#23454;&#29616;&#24179;&#28369;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#38543;&#26426;&#24179;&#28369;&#24050;&#25104;&#20026;&#23569;&#25968;&#20960;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20026;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#8220;&#21435;&#22122;&#21644;&#20998;&#31867;&#8221;&#27969;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#21435;&#22122;&#24179;&#28369;&#65292;&#22312;&#20219;&#20309;&#20998;&#31867;&#22120;&#19978;&#25191;&#34892;&#38543;&#26426;&#24179;&#28369;&#65292;&#21069;&#25552;&#26159;&#26377;&#19968;&#20010;&#20934;&#30830;&#30340;&#21435;&#22122;&#22120;&#21487;&#29992;&#65292;&#27604;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21435;&#22122;&#24179;&#28369;&#30340;&#20934;&#30830;&#24230;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#36136;&#30097;&#21738;&#31181;&#25193;&#25955;&#27169;&#22411;&#30340;&#34920;&#31034;&#24418;&#24335;&#33021;&#22815;&#26368;&#22823;&#21270;&#21435;&#22122;&#24179;&#28369;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#65292;&#26088;&#22312;&#23454;&#29616;&#20849;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20849;&#20139;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#21516;&#26102;&#20063;&#20026;&#20854;&#35748;&#35777;&#40065;&#26834;&#24615;&#34917;&#20607;&#20934;&#30830;&#24230;&#30340;&#25104;&#26412;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with recent diffusion models, randomized smoothing has become one of a few tangible approaches that offers adversarial robustness to models at scale, e.g., those of large pre-trained models. Specifically, one can perform randomized smoothing on any classifier via a simple "denoise-and-classify" pipeline, so-called denoised smoothing, given that an accurate denoiser is available - such as diffusion model. In this paper, we investigate the trade-off between accuracy and certified robustness of denoised smoothing: for example, we question on which representation of diffusion model would maximize the certified robustness of denoised smoothing. We consider a new objective that aims collective robustness of smoothed classifiers across multiple noise levels at a shared diffusion model, which also suggests a new way to compensate the cost of accuracy in randomized smoothing for its certified robustness. This objective motivates us to fine-tune diffusion model (a) to perform consistent de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>SkyMath&#26159;&#19968;&#20010;13&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#23427;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16713</link><description>&lt;p&gt;
SkyMath&#65306;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
SkyMath: Technical Report. (arXiv:2310.16713v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16713
&lt;/p&gt;
&lt;p&gt;
SkyMath&#26159;&#19968;&#20010;13&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#23427;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;SOTA&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#35299;&#20915;&#38382;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#25968;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SkyMath&#12290;&#36890;&#36807;&#33258;&#27604;&#36739;&#24494;&#35843;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#21319;&#20102;Skywork-13B-Base&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;GSM8K&#19978;&#65292;SkyMath&#36229;&#36807;&#20102;&#25152;&#26377;&#24050;&#30693;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#26032;&#30340;SOTA&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown great potential to solve varieties of natural language processing (NLP) tasks, including mathematical reasoning. In this work, we present SkyMath, a large language model for mathematics with 13 billion parameters. By applying self-compare fine-tuning, we have enhanced mathematical reasoning abilities of Skywork-13B-Base remarkably. On GSM8K, SkyMath outperforms all known open-source models of similar size and has established a new SOTA performance.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26234;&#33021;DDoS&#26816;&#27979;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#33258;&#27835;&#26680;&#24515;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;DDoS&#26816;&#27979;&#26550;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#21644;Yet Another Next Generation&#27169;&#22411;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#25104;&#21151;&#22320;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;97%&#30340;&#30495;&#23454;&#20998;&#31867;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12924</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26234;&#33021;DDoS&#26816;&#27979;&#26426;&#21046;&#29992;&#20110;&#33258;&#27835;&#26680;&#24515;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Enabled Intelligent DDoS Detection Mechanism for Autonomous Core Networks. (arXiv:2310.12924v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26234;&#33021;DDoS&#26816;&#27979;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#33258;&#27835;&#26680;&#24515;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;DDoS&#26816;&#27979;&#26550;&#26500;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#27169;&#22359;&#21644;Yet Another Next Generation&#27169;&#22411;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#25104;&#21151;&#22320;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#24182;&#20855;&#26377;97%&#30340;&#30495;&#23454;&#20998;&#31867;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65288;DDoS&#65289;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#22788;&#29702;&#39640;&#24230;&#32858;&#21512;&#30340;&#25968;&#25454;&#36895;&#29575;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;ISP&#65289;&#26680;&#24515;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26234;&#33021;DDoS&#26816;&#27979;&#26426;&#21046;&#65292;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#33258;&#27835;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#20026;ISP&#26680;&#24515;&#32593;&#32476;&#35774;&#35745;&#20102;&#19968;&#20010;DDoS&#26816;&#27979;&#26550;&#26500;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;Yet Another Next Generation&#65288;YANG&#65289;&#27169;&#22411;&#21644;&#33258;&#21160;&#29305;&#24449;&#36873;&#25321;&#65288;AutoFS&#65289;&#27169;&#22359;&#26469;&#22788;&#29702;&#26680;&#24515;&#32593;&#32476;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#23454;&#26102;&#21644;&#39640;&#25928;&#22320;&#26356;&#26032;&#27169;&#22411;&#65292;&#24555;&#36895;&#25913;&#36827;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30830;&#20445;&#20934;&#30830;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#25104;&#21151;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#24182;&#19988;&#23558;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#23398;&#20064;&#27169;&#22411;&#26356;&#26032;&#30340;&#30495;&#23454;&#20998;&#31867;&#29575;&#36798;&#21040;&#20102;97&#65285;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#22823;&#32422;
&lt;/p&gt;
&lt;p&gt;
Existing distributed denial of service attack (DDoS) solutions cannot handle highly aggregated data rates; thus, they are unsuitable for Internet service provider (ISP) core networks. This article proposes a digital twin-enabled intelligent DDoS detection mechanism using an online learning method for autonomous systems. Our contributions are three-fold: we first design a DDoS detection architecture based on the digital twin for ISP core networks. We implemented a Yet Another Next Generation (YANG) model and an automated feature selection (AutoFS) module to handle core network data. We used an online learning approach to update the model instantly and efficiently, improve the learning model quickly, and ensure accurate predictions. Finally, we reveal that our proposed solution successfully detects DDoS attacks and updates the feature selection method and learning model with a true classification rate of ninety-seven percent. Our proposed solution can estimate the attack within approxima
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36719;&#20214;&#23450;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#32593;&#32476;&#24863;&#30693;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32593;&#32476;&#38480;&#21046;&#29615;&#22659;&#19979;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#24182;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.12914</link><description>&lt;p&gt;
&#38754;&#21521;&#36719;&#20214;&#23450;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#32593;&#32476;&#24863;&#30693;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Network-Aware AutoML Framework for Software-Defined Sensor Networks. (arXiv:2310.12914v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12914
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36719;&#20214;&#23450;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#32593;&#32476;&#24863;&#30693;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32593;&#32476;&#38480;&#21046;&#29615;&#22659;&#19979;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#24182;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24403;&#21069;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65288;DDoS&#65289;&#30340;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#39069;&#22806;&#30340;&#22522;&#30784;&#35774;&#26045;&#26469;&#22788;&#29702;&#39640;&#32858;&#21512;&#25968;&#25454;&#36895;&#29575;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#20256;&#24863;&#22120;&#32593;&#32476;&#25110;&#29289;&#32852;&#32593;&#12290;&#27492;&#22806;&#65292;&#36719;&#20214;&#23450;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23433;&#20840;&#26550;&#26500;&#38656;&#35201;&#27880;&#24847;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#28431;&#27934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32593;&#32476;&#24863;&#30693;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;DDoS&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26681;&#25454;&#21464;&#37327;&#27969;&#37327;&#36127;&#36733;&#12289;&#24322;&#26500;&#27969;&#37327;&#36895;&#29575;&#21644;&#26816;&#27979;&#26102;&#38388;&#31561;&#25351;&#26631;&#65292;&#22312;&#32593;&#32476;&#38480;&#21046;&#29615;&#22659;&#19979;&#36873;&#25321;&#29702;&#24819;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;DDoS&#25915;&#20987;&#65292;&#21516;&#26102;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(i)&#25105;&#20204;&#39318;&#27425;&#22312;DDoS&#26816;&#27979;&#33539;&#22260;&#20869;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25928;&#29575;&#19982;&#32593;&#32476;/&#27969;&#37327;&#29366;&#24577;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;(ii)&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#36719;&#20214;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
As the current detection solutions of distributed denial of service attacks (DDoS) need additional infrastructures to handle high aggregate data rates, they are not suitable for sensor networks or the Internet of Things. Besides, the security architecture of software-defined sensor networks needs to pay attention to the vulnerabilities of both software-defined networks and sensor networks. In this paper, we propose a network-aware automated machine learning (AutoML) framework which detects DDoS attacks in software-defined sensor networks. Our framework selects an ideal machine learning algorithm to detect DDoS attacks in network-constrained environments, using metrics such as variable traffic load, heterogeneous traffic rate, and detection time while preventing over-fitting. Our contributions are two-fold: (i) we first investigate the trade-off between the efficiency of ML algorithms and network/traffic state in the scope of DDoS detection. (ii) we design and implement a software archi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#33021;&#28023;&#28207;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;&#20256;&#32479;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#26159;&#26377;&#38480;&#30340;&#65292;&#32780;&#34588;&#32592;&#21017;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#34892;&#20026;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#34394;&#25311;&#34588;&#32592;&#30340;&#36924;&#30495;&#24615;&#65292;&#21560;&#24341;&#26356;&#22810;&#30340;&#25915;&#20987;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.12880</link><description>&lt;p&gt;
TwinPot: &#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#30340;&#26234;&#33021;&#28023;&#28207;
&lt;/p&gt;
&lt;p&gt;
TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports. (arXiv:2310.12880v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#33021;&#28023;&#28207;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;&#20256;&#32479;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#26159;&#26377;&#38480;&#30340;&#65292;&#32780;&#34588;&#32592;&#21017;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#34892;&#20026;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#34394;&#25311;&#34588;&#32592;&#30340;&#36924;&#30495;&#24615;&#65292;&#21560;&#24341;&#26356;&#22810;&#30340;&#25915;&#20987;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21313;&#24180;&#65292;&#38543;&#30528;&#25928;&#29575;&#38656;&#27714;&#30340;&#19978;&#21319;&#21644;&#36135;&#29289;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#19979;&#19968;&#20195;&#28207;&#21475;&#30340;&#27010;&#24565;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#22312;&#36825;&#20010;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#21644;&#35774;&#26045;&#30340;&#26032;&#26102;&#20195;&#65292;&#24456;&#26126;&#26174;&#32593;&#32476;&#23433;&#20840;&#24050;&#32463;&#25104;&#20026;&#28023;&#28207;&#21644;&#28023;&#20107;&#24403;&#23616;&#36817;&#26399;&#26368;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#26159;&#22823;&#22810;&#25968;&#28207;&#21475;&#35758;&#31243;&#19978;&#30340;&#39318;&#35201;&#20851;&#27880;&#28857;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#20813;&#21463;&#26377;&#23475;&#23454;&#20307;&#30340;&#20405;&#23475;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26356;&#36879;&#26126;&#22320;&#36816;&#34892;&#65292;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21482;&#33021;&#35266;&#23519;&#12289;&#26816;&#26597;&#21644;&#20102;&#35299;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34588;&#32592;&#26159;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#34588;&#32592;&#21487;&#20197;&#26159;&#34394;&#25311;&#30340;&#25110;&#29289;&#29702;&#30340;&#12290;&#34394;&#25311;&#34588;&#32592;&#24517;&#39035;&#26356;&#21152;&#36924;&#30495;&#20197;&#21560;&#24341;&#25915;&#20987;&#32773;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22909;&#30340;&#39640;&#24230;&#36924;&#30495;&#24615;&#12290;&#20026;&#27492;&#65292;&#21487;&#20197;&#37319;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;(DT)&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of next-generation ports has become more apparent in the last ten years in response to the challenge posed by the rising demand for efficiency and the ever-increasing volume of goods. In this new era of intelligent infrastructure and facilities, it is evident that cyber-security has recently received the most significant attention from the seaport and maritime authorities, and it is a primary concern on the agenda of most ports. Traditional security solutions can be applied to safeguard IoT and Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security researchers can only watch, examine, and learn about the behaviors of attackers if these solutions operate more transparently. Herein, honeypots are potential solutions since they offer valuable information about the attackers. It can be virtual or physical. Virtual honeypots must be more realistic to entice attackers, necessitating better high-fidelity. To this end, Digital Twin (DT) technology can be employed t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11191</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#65306;&#36890;&#36807;&#38750;&#20856;&#22411;&#35757;&#32451;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#20248;&#21270;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#24357;&#21512;&#27807;&#36890;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#26377;&#29992;&#30340;&#24212;&#29992;&#65292;&#24050;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#31616;&#21270;&#26041;&#27861;&#26377;&#26102;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#21507;&#20111;&#25439;&#22833;&#24178;&#22270;&#29255;&#21050;&#28608;&#29983;&#25104;&#26356;&#31616;&#21333;&#30340;&#26415;&#35821;&#65292;&#20197;&#21450;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35835;&#24615;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#25991;&#26412;&#31616;&#21270;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;</title><link>http://arxiv.org/abs/2310.02255</link><description>&lt;p&gt;
MathVista: &#29992;GPT-4V&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models. (arXiv:2310.02255v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;&#22330;&#26223;&#20013;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30456;&#23545;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#19978;&#25552;&#21319;&#20102;15.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MathVista&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#25968;&#23398;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#25361;&#25112;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#21253;&#21547;&#20102;6141&#20010;&#20363;&#23376;&#65292;&#20854;&#20013;&#26377;28&#20010;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;3&#20010;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#65288;&#21363;IQTest&#12289;FunctionQA&#21644;PaperQA&#65289;&#12290;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#31934;&#32454;&#30340;&#12289;&#28145;&#20837;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#32452;&#21512;&#25512;&#29702;&#65292;&#36825;&#20123;&#37117;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#36890;&#36807;MathVista&#65292;&#25105;&#20204;&#23545;12&#20010;&#33879;&#21517;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;GPT-4V&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#29575;&#20026;49.9%&#65292;&#26126;&#26174;&#20248;&#20110;&#31532;&#20108;&#21517;&#30340;Bard&#27169;&#22411;&#65292;&#30456;&#24046;15.1%&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01551</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#25321;&#30340;&#33021;&#21147;&#20248;&#21270;&#20915;&#31574;&#26641;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Choices in Decision Tree Learning. (arXiv:2310.01551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#25321;&#33021;&#21147;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;Top-$k$&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#36138;&#23146;&#31639;&#27861;&#21644;&#26368;&#20248;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23545;&#26631;&#20934;&#21644;&#32463;&#39564;&#25104;&#21151;&#30340;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;ID3&#12289;C4.5&#21644;CART&#65289;&#36827;&#34892;&#25512;&#24191;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20973;&#20511;&#36138;&#23146;&#30340;&#29305;&#24615;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65306;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#22522;&#20110;&#26368;&#20339;&#23646;&#24615;&#36827;&#34892;&#21010;&#20998;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Top-$k$&#21017;&#32771;&#34385;$k$&#20010;&#26368;&#20339;&#23646;&#24615;&#20316;&#20026;&#21487;&#33021;&#30340;&#21010;&#20998;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#20010;&#26368;&#20339;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25512;&#24191;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#8220;&#36138;&#23146;&#23618;&#27425;&#23450;&#29702;&#8221;&#65292;&#23545;&#20110;&#27599;&#20010;$k \in \mathbb{N}$&#65292;Top-$(k+1)$&#27604;Top-$k$&#26356;&#21152;&#24378;&#22823;&#65306;&#22312;&#26576;&#20123;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21069;&#32773;&#21487;&#20197;&#36798;&#21040;$1-\varepsilon$&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#21518;&#32773;&#21482;&#33021;&#36798;&#21040;$\frac1{2}+\varepsilon$&#30340;&#20934;&#30830;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Top-$k$&#31639;&#27861;&#22312;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#20248;&#20110;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#32463;&#20856;&#36138;&#23146;&#31639;&#27861;&#21644;&#36739;&#26032;&#30340;&#8220;&#26368;&#20248;&#20915;&#31574;&#26641;&#8221;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple generalization of standard and empirically successful decision tree learning algorithms such as ID3, C4.5, and CART. These algorithms, which have been central to machine learning for decades, are greedy in nature: they grow a decision tree by iteratively splitting on the best attribute. Our algorithm, Top-$k$, considers the $k$ best attributes as possible splits instead of just the single best attribute. We demonstrate, theoretically and empirically, the power of this simple generalization. We first prove a {\sl greediness hierarchy theorem} showing that for every $k \in \mathbb{N}$, Top-$(k+1)$ can be dramatically more powerful than Top-$k$: there are data distributions for which the former achieves accuracy $1-\varepsilon$, whereas the latter only achieves accuracy $\frac1{2}+\varepsilon$. We then show, through extensive experiments, that Top-$k$ outperforms the two main approaches to decision tree learning: classic greedy algorithms and more recent "optimal decis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.00100</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;--&#25688;&#35201;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Multilingual Natural Language ProcessingModel for Radiology Reports -- The Summary is all you need!. (arXiv:2310.00100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#22312;&#22810;&#35821;&#35328;&#20013;&#24635;&#32467;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#26410;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#19988;&#33021;&#22815;&#24212;&#29992;&#20110;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21360;&#35937;&#37096;&#20998;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#22312;&#21521;&#21307;&#29983;&#20256;&#36798;&#36825;&#20123;&#21457;&#29616;&#26102;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#29983;&#26469;&#35828;&#65292;&#20934;&#22791;&#36825;&#20123;&#25688;&#35201;&#26082;&#32791;&#26102;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#22815;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#24635;&#32467;&#36825;&#20123;&#25253;&#21578;&#30340;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#34701;&#21512;&#26469;&#33258;&#19981;&#21516;&#26063;&#35028;&#32972;&#26223;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20110;&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#19978;&#24494;&#35843;&#65292;&#33258;&#21160;&#21270;&#22320;&#29983;&#25104;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#25918;&#23556;&#23398;&#21360;&#35937;&#65292;&#20197;&#24635;&#32467;&#33521;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#24503;&#35821;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#12290;&#22312;&#19968;&#39033;&#30450;&#27979;&#20013;&#65292;&#20004;&#20301;&#26377;&#25191;&#19994;&#36164;&#26684;&#30340;&#25918;&#23556;&#31185;&#21307;&#29983;&#34920;&#31034;&#65292;&#23545;&#20110;&#33267;&#23569;70%&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#20854;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality 
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07593</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22797;&#26434;&#23398;&#20064;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#26102;&#65292;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#31227;&#38500;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26159;&#21442;&#32771;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#32479;&#35745;&#20445;&#35777;&#26469;&#39564;&#35777;&#21464;&#37327;&#21253;&#21547;&#24615;&#26102;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#20351;&#29992;&#21464;&#37327;&#32622;&#25442;&#26041;&#26696;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#23481;&#26131;&#23558;&#19981;&#37325;&#35201;&#30340;&#21464;&#37327;&#35823;&#35782;&#21035;&#20026;&#37325;&#35201;&#21464;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#30740;&#31350;&#26465;&#20214;&#32622;&#25442;&#37325;&#35201;&#24615;&#65288;Conditional Permutation Importance&#65292;CPI&#65289;&#65292;&#23427;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;CPI&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;I&#22411;&#38169;&#35823;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;CPI&#22987;&#32456;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;COMEDIAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01270</link><description>&lt;p&gt;
COMEDIAN: &#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;Transformer&#36827;&#34892;&#21160;&#20316;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers. (arXiv:2309.01270v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01270
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;COMEDIAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;COMEDIAN&#65292;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#21021;&#22987;&#21270;&#26102;&#31354;Transformer&#36827;&#34892;&#21160;&#20316;&#23450;&#20301;&#30340;&#26032;&#22411;&#27969;&#31243;&#12290;&#21160;&#20316;&#23450;&#20301;&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#25139;&#32423;&#21035;&#30340;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65292;&#20854;&#20013;&#26377;&#20004;&#20010;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#30701;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#23545;&#31354;&#38388;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#21021;&#22987;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21021;&#22987;&#21270;&#19968;&#20010;&#26102;&#24207;Transformer&#65292;&#36890;&#36807;&#19982;&#27599;&#20010;&#30701;&#35270;&#39057;&#29255;&#27573;&#23545;&#40784;&#30340;&#39044;&#35745;&#31639;&#29305;&#24449;&#24211;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#22686;&#24378;&#31354;&#38388;Transformer&#30340;&#36755;&#20986;&#19982;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#26368;&#21518;&#19968;&#27493;&#65292;&#25105;&#20204;&#23545;Transformer&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#21160;&#20316;&#23450;&#20301;&#20219;&#21153;&#12290;&#22312;SoccerNet-v2&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;COMEDIAN&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#39044;&#35757;&#32451;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#25105;&#20204;&#39044;&#35757;&#32451;&#27969;&#31243;&#30340;&#20960;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present COMEDIAN, a novel pipeline to initialize spatiotemporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14284</link><description>&lt;p&gt;
LLM&#24378;&#21270;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
LLM Powered Sim-to-real Transfer for Traffic Signal Control. (arXiv:2308.14284v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#34892;&#21160;&#36716;&#25442;&#65292;&#35299;&#20915;&#20102;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#30340;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#20379;&#39640;&#25928;&#30340;&#20132;&#36890;&#21644;&#20943;&#36731;&#25317;&#22581;&#28010;&#36153;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24403;&#22312;&#20223;&#30495;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#25166;&#26681;&#34892;&#21160;&#36716;&#25442;&#65292;&#26469;&#29702;&#35299;&#21644;&#25551;&#36848;&#31995;&#32479;&#21160;&#24577;&#12290;&#36890;&#36807;&#25509;&#21463;&#22635;&#31354;&#25552;&#31034;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#21487;&#20197;&#35775;&#38382;&#30340;&#19978;&#19979;&#25991;&#22635;&#20889;&#31572;&#26696;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24212;&#29992;&#20110;&#23545;&#31995;&#32479;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks aiming to provide efficient transportation and mitigate congestion waste. In recent, promising results have been attained by Reinforcement Learning (RL) methods through trial and error in simulators, bringing confidence in solving cities' congestion headaches. However, there still exist performance gaps when simulator-trained policies are deployed to the real world. This issue is mainly introduced by the system dynamic difference between the training simulator and the real-world environments. The Large Language Models (LLMs) are trained on mass knowledge and proved to be equipped with astonishing inference abilities. In this work, we leverage LLMs to understand and profile the system dynamics by a prompt-based grounded action transformation. Accepting the cloze prompt template, and then filling in the answer based on accessible context, the pre-trained LLM's inference ability is exploited and applied to understa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13633</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30333;&#21270;&#65306;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24863;&#35273;&#21306;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#20854;&#20010;&#20307;&#21709;&#24212;&#30340;&#26041;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#20197;&#21450;&#20943;&#23569;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#36716;&#25442;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#30333;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#30340;&#26426;&#21046;&#27169;&#22411;&#21482;&#20351;&#29992;&#31361;&#35302;&#21487;&#22609;&#24615;&#25110;&#22686;&#30410;&#35843;&#21046;&#20316;&#20026;&#36866;&#24212;&#30340;&#29983;&#29289;&#22522;&#36136;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#33539;&#24615;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#31361;&#35302;&#21487;&#22609;&#24615;&#21644;&#22686;&#30410;&#35843;&#21046;&#30340;&#35745;&#31639;&#35282;&#33394;&#26469;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#30333;&#21270;&#12290;&#22686;&#30410;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#26681;&#25454;&#24403;&#21069;&#30340;&#32479;&#35745;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#31361;&#35302;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#35843;&#25972;&#65292;&#23398;&#20064;&#36755;&#20837;&#32479;&#35745;&#20013;&#19982;&#24773;&#22659;&#26080;&#20851;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#33258;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05891</link><description>&lt;p&gt;
&#21463;PID&#25511;&#21046;&#22120;&#21551;&#21457;&#30340;&#20559;&#24046;&#24402;&#32435;&#27861;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21463;&#21040;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65292;&#20197;&#24179;&#34913;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#28789;&#27963;&#24615;&#19982;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#25968;&#25454;&#33258;&#24049;&#23398;&#20064;&#25511;&#21046;&#31995;&#32479;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;RL&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#36890;&#24120;&#19981;&#21487;&#35266;&#27979;&#12290;&#24403;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#26102;&#65292;&#31574;&#30053;&#38656;&#35201;&#21033;&#29992;&#35266;&#23519;&#21382;&#21490;&#26469;&#25512;&#26029;&#24403;&#21069;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#20351;&#24471;&#31574;&#30053;&#19981;&#20250;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#22312;&#21382;&#21490;&#35760;&#24405;&#32534;&#30721;&#22120;&#28789;&#27963;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#35201;&#23545;&#29615;&#22659;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#24179;&#34913;&#65292;&#25105;&#20204;&#23547;&#27714;PID&#25511;&#21046;&#22120;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26029;&#23450;PID&#25511;&#21046;&#22120;&#30340;&#25104;&#21151;&#34920;&#26126;&#65292;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#21482;&#38656;&#35201;&#27714;&#21644;&#21644;&#27714;&#24046;&#26469;&#32047;&#31215;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#20010;&#21407;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#32534;&#30721;&#21382;&#21490;&#35760;&#24405;&#30340;&#26550;&#26500;&#65306;&#19968;&#31181;&#30452;&#25509;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has shown immense potential for learning to control systems through data alone. However, one challenge deep RL faces is that the full state of the system is often not observable. When this is the case, the policy needs to leverage the history of observations to infer the current state. At the same time, differences between the training and testing environments makes it critical for the policy not to overfit to the sequence of observations it sees at training time. As such, there is an important balancing act between having the history encoder be flexible enough to extract relevant information, yet be robust to changes in the environment. To strike this balance, we look to the PID controller for inspiration. We assert the PID controller's success shows that only summing and differencing are needed to accumulate information over time for many control tasks. Following this principle, we propose two architectures for encoding history: one that directly uses
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24207;&#21015;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#19968;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#21160;&#24577;&#29366;&#24577;&#24207;&#21015;&#39044;&#27979;&#21644;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2307.04721</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#27169;&#24335;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as General Pattern Machines. (arXiv:2307.04721v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04721
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24207;&#21015;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#19968;&#23450;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#22914;&#21160;&#24577;&#29366;&#24577;&#24207;&#21015;&#39044;&#27979;&#21644;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;&#22797;&#26434;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#20174;&#30001;&#27010;&#29575;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;PCFG&#65289;&#38543;&#26426;&#29983;&#25104;&#30340;&#20219;&#24847;&#24207;&#21015;&#65292;&#21040;&#22312;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#20013;&#21457;&#29616;&#30340;&#26356;&#20016;&#23500;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#20197;ASCII&#33402;&#26415;&#30340;&#24418;&#24335;&#25552;&#31034;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#20174;&#35789;&#27719;&#34920;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#20196;&#29260;&#34920;&#31034;&#24207;&#21015;&#65292;&#27169;&#24335;&#23436;&#25104;&#33021;&#21147;&#20063;&#21487;&#20197;&#37096;&#20998;&#20445;&#30041;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;LLMs&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24207;&#21015;&#27169;&#22411;&#22120;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#39537;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#38646;&#26679;&#26412;&#33021;&#21147;&#22914;&#20309;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#20174;&#23545;&#34920;&#31034;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29366;&#24577;&#30340;&#25968;&#23383;&#24207;&#21015;&#36827;&#34892;&#22806;&#25512;&#65292;&#20197;&#23436;&#25104;&#31616;&#21333;&#30340;&#36816;&#21160;&#65292;&#21040;&#20197;&#22870;&#21169;&#26465;&#20214;&#36712;&#36857;&#30340;&#26368;&#23567;&#21040;&#26368;&#22823;&#25552;&#31034;&#26041;&#24335;&#65292;&#33021;&#22815;&#21457;&#29616;&#21644;&#34920;&#31034;&#38381;&#29615;&#31574;&#30053;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#30340;&#25511;&#21046;&#31995;&#32479;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstraction and Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#33410;&#28857;&#20998;&#31867;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#33410;&#28857;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20063;&#20250;&#20943;&#23569;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#24773;&#20917;&#37027;&#20040;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2307.01951</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#31070;&#32463;&#22604;&#38519;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks. (arXiv:2307.01951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#33410;&#28857;&#20998;&#31867;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#28436;&#21270;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#22312;&#33410;&#28857;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20063;&#20250;&#20943;&#23569;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#24773;&#20917;&#37027;&#20040;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;GNNs&#20013;&#22270;&#25299;&#25169;&#21644;&#29305;&#24449;&#28436;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20197;&#22522;&#20110;&#33410;&#28857;&#30340;&#20998;&#31867;&#20026;&#20027;&#39064;&#65292;&#20197;&#38543;&#26426;&#22359;&#27169;&#22411;&#22270;&#19978;&#30340;&#31038;&#21306;&#26816;&#27979;&#20026;&#20363;&#65292;&#36890;&#36807;&#8220;&#31070;&#32463;&#22604;&#38519;&#8221;&#29616;&#35937;&#26469;&#25506;&#32034;&#29305;&#24449;&#28436;&#21270;&#12290;&#24403;&#35757;&#32451;&#22522;&#20110;&#23454;&#20363;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#36229;&#36807;&#38646;&#35757;&#32451;&#35823;&#24046;&#28857;&#26102;&#65292;&#31070;&#32463;&#22604;&#38519;&#34920;&#29616;&#20026;&#26368;&#28145;&#23618;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#24615;&#20943;&#23569;&#65292;&#24182;&#19988;&#31867;&#22343;&#20540;&#19982;&#29305;&#23450;&#30340;&#23545;&#31216;&#32467;&#26500;&#26356;&#21152;&#23545;&#40784;&#12290;&#25105;&#20204;&#20808;&#20174;&#23454;&#35777;&#30740;&#31350;&#24320;&#22987;&#65292;&#26174;&#31034;&#31867;&#20869;&#21464;&#24322;&#24615;&#30340;&#20943;&#23569;&#22312;&#22522;&#20110;&#33410;&#28857;&#30340;&#20998;&#31867;&#29615;&#22659;&#20013;&#20063;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#19981;&#21450;&#22522;&#20110;&#23454;&#20363;&#30340;&#26696;&#20363;&#37027;&#20040;&#26126;&#26174;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#36825;&#31181;&#21306;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#19981;&#32771;&#34385;&#28608;&#27963;&#65292;&#22270;&#25299;&#25169;&#20449;&#24687;&#20063;&#33021;&#23548;&#33268;&#29305;&#24449;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become increasingly popular for classification tasks on graph-structured data. Yet, the interplay between graph topology and feature evolution in GNNs is not well understood. In this paper, we focus on node-wise classification, illustrated with community detection on stochastic block model graphs, and explore the feature evolution through the lens of the "Neural Collapse" (NC) phenomenon. When training instance-wise deep classifiers (e.g. for image classification) beyond the zero training error point, NC demonstrates a reduction in the deepest features' within-class variability and an increased alignment of their class means to certain symmetric structures. We start with an empirical study that shows that a decrease in within-class variability is also prevalent in the node-wise classification setting, however, not to the extent observed in the instance-wise case. Then, we theoretically study this distinction. Specifically, we show that even an "optimis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14685</link><description>&lt;p&gt;
DiffSketcher: &#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. (arXiv:2306.14685v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#32032;&#25551;&#21512;&#25104;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;DiffSketcher&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#36125;&#22622;&#23572;&#26354;&#32447;&#21644;&#25193;&#25955;&#27169;&#22411;&#25439;&#22833;&#26469;&#29983;&#25104;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21152;&#24555;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSketcher&#30340;&#32032;&#25551;&#36136;&#37327;&#39640;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20027;&#35201;&#35757;&#32451;&#20110;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#24341;&#23548;&#32032;&#25551;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffSketcher&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21019;&#24314;&#30690;&#37327;&#21270;&#30340;&#25163;&#32472;&#32032;&#25551;&#12290;DiffSketcher&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#23637;&#29256;&#26412;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#19968;&#32452;&#36125;&#22622;&#23572;&#26354;&#32447;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#26629;&#26684;&#32423;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#26469;&#20248;&#21270;&#21442;&#25968;&#21270;&#30340;&#30690;&#37327;&#32032;&#25551;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#31508;&#30011;&#21021;&#22987;&#21270;&#20197;&#21152;&#24555;&#36895;&#24230;&#12290;&#29983;&#25104;&#30340;&#32032;&#25551;&#23637;&#31034;&#20102;&#22810;&#23618;&#27425;&#30340;&#25277;&#35937;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#34987;&#32472;&#21046;&#20027;&#39064;&#30340;&#21487;&#35782;&#21035;&#24615;&#12289;&#22522;&#26412;&#32467;&#26500;&#21644;&#37325;&#35201;&#30340;&#35270;&#35273;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffSketcher&#30340;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though trained mainly on images, we discover that pretrained diffusion models show impressive power in guiding sketch synthesis. In this paper, we present DiffSketcher, an innovative algorithm that creates vectorized free-hand sketches using natural language input. DiffSketcher is developed based on a pre-trained text-to-image diffusion model. It performs the task by directly optimizing a set of Bezier curves with an extended version of the score distillation sampling (SDS) loss, which allows us to use a raster-level diffusion model as a prior for optimizing a parametric vectorized sketch generator. Furthermore, we explore attention maps embedded in the diffusion model for effective stroke initialization to speed up the generation process. The generated sketches demonstrate multiple levels of abstraction while maintaining recognizability, underlying structure, and essential visual details of the subject drawn. Our experiments show that DiffSketcher achieves greater quality than pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07092</link><description>&lt;p&gt;
&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tuning Legged Locomotion Controllers via Safe Bayesian Optimization. (arXiv:2306.07092v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#20197;&#31616;&#21270;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#30828;&#20214;&#24179;&#21488;&#20013;&#37096;&#32626;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#26080;&#27169;&#22411;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#25511;&#21046;&#22686;&#30410;&#65292;&#35299;&#20915;&#20102;&#22312;&#25511;&#21046;&#21046;&#23450;&#20013;&#20351;&#29992;&#30340;&#31616;&#21270;&#27169;&#22411;&#19982;&#23454;&#38469;&#31995;&#32479;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#21487;&#33021;&#23433;&#20840;&#21306;&#22495;&#20869;&#39640;&#25928;&#20248;&#21270;&#21442;&#25968;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#19982;&#26426;&#22120;&#20154;&#30340;&#21361;&#38505;&#20132;&#20114;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#20197;&#23558;&#19981;&#21516;&#30340;&#27493;&#24577;&#21442;&#25968;&#20316;&#20026;&#32972;&#26223;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#23433;&#20840;&#22320;&#35843;&#25972;&#22810;&#26679;&#21270;&#27493;&#24577;&#27169;&#24335;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#27493;&#24577;&#20013;&#35843;&#25972;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#65292;&#35813;&#31639;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a data-driven strategy to streamline the deployment of model-based controllers in legged robotic hardware platforms. Our approach leverages a model-free safe learning algorithm to automate the tuning of control gains, addressing the mismatch between the simplified model used in the control formulation and the real system. This method substantially mitigates the risk of hazardous interactions with the robot by sample-efficiently optimizing parameters within a probably safe region. Additionally, we extend the applicability of our approach to incorporate the different gait parameters as contexts, leading to a safe, sample-efficient exploration algorithm capable of tuning a motion controller for diverse gait patterns. We validate our method through simulation and hardware experiments, where we demonstrate that the algorithm obtains superior performance on tuning a model-based motion controller for multiple gaits safely.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#35745;&#31639;&#25928;&#29575;&#20302;&#21644;&#38590;&#20197;&#19982;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20860;&#23481;&#12290;EDP&#36890;&#36807;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#26469;&#36991;&#20813;&#36816;&#34892;&#37319;&#26679;&#38142;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.20081</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Diffusion Policies for Offline Reinforcement Learning. (arXiv:2305.20081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#35745;&#31639;&#25928;&#29575;&#20302;&#21644;&#38590;&#20197;&#19982;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20860;&#23481;&#12290;EDP&#36890;&#36807;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#26469;&#36991;&#20813;&#36816;&#34892;&#37319;&#26679;&#38142;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#21442;&#25968;&#21270;&#26159;&#20851;&#38190;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#65292;Diffusion-QL&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#21442;&#25968;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#36827;&#34892;&#37319;&#26679;&#65292;&#20294;&#26159;Diffusion-QL&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;1&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#36827;&#34892;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;2&#65289;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20284;&#28982;&#20989;&#25968;&#38590;&#20197;&#35745;&#31639;&#65292;&#19982;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65289;&#19981;&#20860;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#25193;&#25955;&#31574;&#30053;&#65288;EDP&#65289;&#26469;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;EDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#20174;&#25439;&#22351;&#30340;&#21160;&#20316;&#20013;&#36817;&#20284;&#26500;&#24314;&#21160;&#20316;&#65292;&#36991;&#20813;&#20102;&#36816;&#34892;&#37319;&#26679;&#38142;&#12290;&#25105;&#20204;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EDP&#33021;&#22815;&#20943;&#23569;&#25193;&#25955;&#30340;&#28508;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) aims to learn optimal policies from offline datasets, where the parameterization of policies is crucial but often overlooked. Recently, Diffsuion-QL significantly boosts the performance of offline RL by representing a policy with a diffusion model, whose success relies on a parametrized Markov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers from two critical limitations. 1) It is computationally inefficient to forward and backward through the whole Markov chain during training. 2) It is incompatible with maximum likelihood-based RL algorithms (e.g., policy gradient methods) as the likelihood of diffusion models is intractable. Therefore, we propose efficient diffusion policy (EDP) to overcome these two challenges. EDP approximately constructs actions from corrupted ones at training to avoid running the sampling chain. We conduct extensive experiments on the D4RL benchmark. The results show that EDP can reduce the diffusion po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16427</link><description>&lt;p&gt;
&#31070;&#32463;&#65288;&#20999;&#21521;&#26680;&#65289;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65306;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#23427;&#25429;&#25417;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#26399;&#38388;&#30340;&#28436;&#21270;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#29616;&#35937;&#65292;&#23427;&#25351;&#30340;&#26159;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;DNN&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20013;&#23545;&#31216;&#24615;&#21644;&#32467;&#26500;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#32463;&#39564;NTK&#19982;&#31867;&#26631;&#31614;&#23545;&#40784;&#24182;&#24418;&#25104;&#22359;&#29366;&#32467;&#26500;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#26356;&#24378;&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#35757;&#32451;&#30340;DNN&#21160;&#24577;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#19981;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#21160;&#24577;&#30340;&#26412;&#36136;&#65292;&#24182;&#29992;&#23427;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#31181;&#24120;&#35265;DNN&#26550;&#26500;&#21644;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#25968;&#20540;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#21450;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.15080</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#21644;&#20923;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models. (arXiv:2305.15080v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#21450;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#21050;&#28608;&#20102;&#23545;&#23558;&#20854;&#24212;&#29992;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#28526;&#27969;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#25277;&#35937;&#22270;&#20687;&#26631;&#39064;&#21644;&#20419;&#36827;&#33258;&#28982;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#25991;&#26412;&#20016;&#23500;&#30340;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38405;&#35835;&#27169;&#22411;&#65288;Cream&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#25429;&#25417;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;-&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;Cream&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36741;&#21161;&#32534;&#30721;&#22120;&#65292;&#24182;&#20511;&#21161;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#25216;&#26415;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#20013;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#35821;&#35328;&#20449;&#24687;&#30340;&#26356;&#26377;&#25928;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20026;&#26356;&#22797;&#26434;&#30340;&#25991;&#26723;&#26234;&#33021;&#21161;&#25163;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#35270;&#35273;&#23450;&#20301;&#19978;&#19979;&#25991;&#20013;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-sit
&lt;/p&gt;</description></item><item><title>Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers&#26159;&#31561;&#25928;&#19988;&#39640;&#25928;&#30340;Pre-LN Transformers&#26550;&#26500;&#65292;&#21487;&#20197;&#32479;&#19968;&#20351;&#29992;&#20004;&#31181;&#20027;&#27969;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;LayerNorm&#21644;RMSNorm&#65292;&#20174;&#32780;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.14858</link><description>&lt;p&gt;
Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers: &#31561;&#25928;&#21644;&#39640;&#25928;&#30340;Pre-LN Transformers
&lt;/p&gt;
&lt;p&gt;
Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. (arXiv:2305.14858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14858
&lt;/p&gt;
&lt;p&gt;
Pre-RMSNorm&#21644;Pre-CRMSNorm Transformers&#26159;&#31561;&#25928;&#19988;&#39640;&#25928;&#30340;Pre-LN Transformers&#26550;&#26500;&#65292;&#21487;&#20197;&#32479;&#19968;&#20351;&#29992;&#20004;&#31181;&#20027;&#27969;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;LayerNorm&#21644;RMSNorm&#65292;&#20174;&#32780;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#24402;&#19968;&#21270;&#25216;&#26415;&#65292;&#22914;Layer Normalization&#65288;LayerNorm&#65292;LN&#65289;&#21644;Root Mean Square Normalization&#65288;RMSNorm&#65289;&#65292;&#22312;&#21152;&#36895;&#21644;&#31283;&#23450;Transformer&#30340;&#35757;&#32451;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;LayerNorm&#23545;&#36755;&#20837;&#21521;&#37327;&#36827;&#34892;&#37325;&#26032;&#20013;&#24515;&#21270;&#21644;&#37325;&#26032;&#32553;&#25918;&#65292;&#32780;RMSNorm&#20165;&#25353;&#20854;RMS&#20540;&#37325;&#26032;&#32553;&#25918;&#21521;&#37327;&#12290;&#23613;&#31649;RMSNorm&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#65292;&#20294;&#21487;&#33021;&#20250;&#25439;&#23475;Transformer&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#30446;&#21069;&#20851;&#20110;&#39318;&#36873;&#24402;&#19968;&#21270;&#25216;&#26415;&#23578;&#26080;&#20849;&#35782;&#65292;&#22240;&#20026;&#19968;&#20123;&#27169;&#22411;&#20351;&#29992;LayerNorm&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#20351;&#29992;RMSNorm&#65292;&#23588;&#20854;&#26159;&#26368;&#36817;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#23558;&#20855;&#26377;&#19968;&#31181;&#24402;&#19968;&#21270;&#30340;Transformer&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#31867;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30446;&#21069;&#20004;&#31181;&#24402;&#19968;&#21270;&#31867;&#22411;&#20043;&#38388;&#23384;&#22312;&#20105;&#35758;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#32479;&#19968;&#20004;&#31181;&#20027;&#27969;Transformer&#26550;&#26500;&#65292;Pre-LN&#21644;Pre-RMSNorm Transformers&#12290;&#36890;&#36807;&#21435;&#38500;&#22266;&#26377;&#30340;&#20887;&#20313;&#22343;&#20540;&#20449;&#24687;&#26469;&#23454;&#29616;&#31561;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean informat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22312;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#26102;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26102;&#19982;QA&#31995;&#32479;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#65292;&#29992;&#25143;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#38169;&#35823;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.14331</link><description>&lt;p&gt;
&#29992;&#25143;&#23545;&#20110;QA&#31995;&#32479;&#30340;&#32972;&#26223;&#20449;&#24687;&#20381;&#36182;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What Else Do I Need to Know? The Effect of Background Information on Users' Reliance on QA Systems. (arXiv:2305.14331v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#25143;&#22312;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#26102;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26102;&#19982;QA&#31995;&#32479;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#36825;&#20123;&#20449;&#24687;&#65292;&#29992;&#25143;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#23545;&#38169;&#35823;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#31995;&#32479;&#22312;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#20165;&#38480;&#20110;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#26469;&#38480;&#21046;&#27169;&#22411;&#30340;&#30693;&#35782;&#25110;&#25512;&#29702;&#26159;&#19981;&#21487;&#33021;&#30340;&#19988;&#36890;&#24120;&#20063;&#19981;&#21487;&#21462;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#31572;&#26696;&#30340;&#20449;&#24687;&#19982;&#29992;&#25143;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#31572;&#26696;&#30340;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26469;&#35780;&#20272;&#39044;&#27979;&#26102;&#29992;&#25143;&#22914;&#20309;&#19982;QA&#31995;&#32479;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35810;&#38382;&#26159;&#21542;&#28155;&#21152;&#24517;&#35201;&#30340;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#20943;&#23569;&#29992;&#25143;&#23545;&#39044;&#27979;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#32570;&#20047;&#36275;&#22815;&#20449;&#24687;&#26469;&#35780;&#20272;&#27169;&#22411;&#27491;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20173;&#28982;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#26377;&#21161;&#20110;&#29992;&#25143;&#26356;&#22909;&#22320;&#21457;&#29616;&#27169;&#22411;&#38169;&#35823;&#65292;&#20943;&#23569;&#23545;&#19981;&#27491;&#30830;&#39044;&#27979;&#30340;&#20381;&#36182;&#12290;&#32780;&#32972;&#26223;&#20449;&#24687;&#30340;&#28155;&#21152;&#20063;&#21487;&#33021;&#22686;&#21152;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP systems have shown impressive performance at answering questions by retrieving relevant context. However, with the increasingly large models, it is impossible and often undesirable to constrain models' knowledge or reasoning to only the retrieved context. This leads to a mismatch between the information that the models access to derive the answer and the information that is available to the user to assess the model predicted answer. In this work, we study how users interact with QA systems in the absence of sufficient information to assess their predictions. Further, we ask whether adding the requisite background helps mitigate users' over-reliance on predictions. Our study reveals that users rely on model predictions even in the absence of sufficient information needed to assess the model's correctness. Providing the relevant background, however, helps users better catch model errors, reducing over-reliance on incorrect predictions. On the flip side, background information also in
&lt;/p&gt;</description></item><item><title>Dynosaur&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#65292;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;API&#25104;&#26412;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.14327</link><description>&lt;p&gt;
Dynosaur: &#19968;&#31181;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#25972;&#29702;&#30340;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. (arXiv:2305.14327v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14327
&lt;/p&gt;
&lt;p&gt;
Dynosaur&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#65292;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;API&#25104;&#26412;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29702;&#35299;&#25351;&#20196;&#21644;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#25163;&#21160;&#27880;&#37322;&#65292;&#35201;&#20040;&#20351;&#29992;LLM&#65288;&#22914;GPT&#31995;&#21015;&#65289;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#23558;&#25351;&#20196;&#19982;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#20851;&#32852;&#36215;&#26469;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dynosaur&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#25972;&#29702;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#30340;&#21160;&#24577;&#22686;&#38271;&#27169;&#24335;&#12290;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#33258;&#21160;&#26500;&#24314;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#30340;&#25968;&#25454;&#23383;&#27573;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;Dynosaur&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20248;&#28857;&#65306;1&#65289;&#20943;&#23569;&#20102;&#29983;&#25104;&#25351;&#20196;&#30340;API&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#35843;&#29992;GPT-3.5-turbo&#29983;&#25104;80&#19975;&#20010;&#25351;&#20196;&#35843;&#20248;&#26679;&#26412;&#30340;&#25104;&#26412;&#20302;&#20110;12&#32654;&#20803;&#65289;&#65307;2&#65289;&#20026;&#25351;&#20196;&#35843;&#20248;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#34920;&#29616;&#20248;&#20110;Alpaca&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged to enhance the capabilities of large language models (LLMs) to comprehend instructions and generate appropriate responses. Existing methods either manually annotate or employ LLM (e.g., GPT-series) to generate data for instruction tuning. However, they often overlook associating instructions with existing annotated datasets. In this paper, we propose Dynosaur, a dynamic growth paradigm for the automatic curation of instruction-tuning data. Based on the metadata of existing datasets, we use LLMs to automatically construct instruction-tuning data by identifying relevant data fields and generating appropriate instructions.  By leveraging the existing annotated datasets, Dynosaur offers several advantages: 1) it reduces the API cost for generating instructions (e.g., it costs less than $12 USD by calling GPT-3.5-turbo for generating 800K instruction tuning samples; 2) it provides high-quality data for instruction tuning (e.g., it performs better than Alpaca a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13850</link><description>&lt;p&gt;
&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document. (arXiv:2305.13850v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13850
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#30340;&#36830;&#32493;&#36845;&#20195;&#30340;&#26041;&#24335;&#21435;&#25429;&#33719;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#20851;&#31995;&#25277;&#21462;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#25552;&#21462;&#65288;VRE&#65289;&#26088;&#22312;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#23454;&#20307;&#29305;&#24449;&#21333;&#29420;&#39044;&#27979;&#27599;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#24573;&#30053;&#20102;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32570;&#20047;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#21487;&#33021;&#20351;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#38271;&#31243;&#20851;&#31995;&#65292;&#24182;&#23481;&#26131;&#20135;&#29983;&#20914;&#31361;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GOSE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25429;&#33719;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32473;&#23450;&#25991;&#26723;&#30340;&#25195;&#25551;&#22270;&#20687;&#65292;GOSE&#39318;&#20808;&#23545;&#23454;&#20307;&#23545;&#29983;&#25104;&#21021;&#27493;&#30340;&#20851;&#31995;&#39044;&#27979;&#12290;&#31532;&#20108;&#65292;&#22312;&#20808;&#21069;&#36845;&#20195;&#30340;&#39044;&#27979;&#32467;&#26524;&#22522;&#30784;&#19978;&#65292;GOSE&#21033;&#29992;&#20840;&#23616;&#32467;&#26500;&#30693;&#35782;&#36827;&#19968;&#27493;&#25972;&#21512;&#23454;&#20307;&#34920;&#31034;&#12290;&#36825;&#31181;&#8220;&#29983;&#25104;-&#25429;&#33719;-&#25972;&#21512;&#8221;&#27169;&#24335;&#34987;&#22810;&#27425;&#25191;&#34892;&#65292;&#20197;&#20415;&#23454;&#20307;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#33021;&#22815;&#34987;&#24456;&#22909;&#22320;&#25429;&#33719;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This "generate-capture-incorporate" schema is performed multiple times so that entit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13632</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Hallucinations in Multilingual Summarisation. (arXiv:2305.13632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#25688;&#35201;&#20013;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21152;&#26435;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#25688;&#35201;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#34429;&#28982;&#33258;&#21160;&#20135;&#29983;&#30340;&#25688;&#35201;&#21487;&#33021;&#27969;&#30021;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23545;&#21407;&#22987;&#25991;&#26723;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#22914;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#31361;&#20986;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#33521;&#35821;&#65292;&#22240;&#27492;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#29978;&#33267;&#34913;&#37327;&#36825;&#31181;&#29616;&#35937;&#30340;&#31243;&#24230;&#20063;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;mFACT&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#33521;&#35821;&#30340;&#24544;&#23454;&#24615;&#27979;&#37327;&#32467;&#26524;&#20013;&#20511;&#37492;&#32763;&#35793;&#22522;&#30784;&#30693;&#35782;&#20026;&#38750;&#33521;&#35821;&#25688;&#35201;&#35780;&#20272;&#20854;&#24544;&#23454;&#24615;&#12290;&#28982;&#21518;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36328;&#35821;&#35328;&#36716;&#31227;&#20943;&#23569;&#24187;&#35273;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20056;&#20197;&#20854;&#24544;&#23454;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#22810;&#31181;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;mFACT&#26159;&#26368;&#36866;&#21512;&#26816;&#27979;&#24187;&#35273;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#25552;&#20986;&#30340;&#21152;&#26435;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations pose a significant challenge to the reliability of neural models for abstractive summarisation. While automatically generated summaries may be fluent, they often lack faithfulness to the original document. This issue becomes even more pronounced in low-resource settings, such as cross-lingual transfer. With the existing faithful metrics focusing on English, even measuring the extent of this phenomenon in cross-lingual settings is hard. To address this, we first develop a novel metric, mFACT, evaluating the faithfulness of non-English summaries, leveraging translation-based transfer from multiple English faithfulness metrics. We then propose a simple but effective method to reduce hallucinations with a cross-lingual transfer, which weighs the loss of each training example by its faithfulness score. Through extensive experiments in multiple languages, we demonstrate that mFACT is the metric that is most suited to detect hallucinations. Moreover, we find that our proposed l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13552</link><description>&lt;p&gt;
&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65306;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;
&lt;/p&gt;
&lt;p&gt;
Squared Neural Families: A New Class of Tractable Density Models. (arXiv:2305.13552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13552
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#31867;&#8212;&#8212;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65292;&#20854;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#21644;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#25351;&#25968;&#26063;&#65292;&#20855;&#26377;&#38381;&#24615;&#26465;&#20214;&#25512;&#26029;&#21644;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#24067;&#30340;&#28789;&#27963;&#27169;&#22411;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#20998;&#24067;&#31867;&#21035;&#65292;&#31216;&#20026;&#24179;&#26041;&#31070;&#32463;&#20998;&#24067;&#26063;&#65288;SNEFY&#65289;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;2&#33539;&#25968;&#36827;&#34892;&#24179;&#26041;&#24182;&#22522;&#20110;&#26576;&#20010;&#22522;&#30784;&#24230;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#31867;&#20284;&#20110;&#26080;&#31351;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#24191;&#27867;&#32852;&#31995;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#65292;SNEFY&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#22240;&#27492;&#26159;&#28789;&#27963;&#19988;&#23436;&#20840;&#21487;&#35745;&#31639;&#23494;&#24230;&#27169;&#22411;&#12290;SNEFY&#20005;&#26684;&#25512;&#24191;&#20102;&#32463;&#20856;&#30340;&#25351;&#25968;&#26063;&#65292;&#23545;&#20110;&#26465;&#20214;&#25512;&#26029;&#20855;&#26377;&#38381;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23494;&#24230;&#20272;&#35745;&#21644;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible models for probability distributions are an essential ingredient in many machine learning tasks. We develop and investigate a new class of probability distributions, which we call a Squared Neural Family (SNEFY), formed by squaring the 2-norm of a neural network and normalising it with respect to a base measure. Following the reasoning similar to the well established connections between infinitely wide neural networks and Gaussian processes, we show that SNEFYs admit a closed form normalising constants in many cases of interest, thereby resulting in flexible yet fully tractable density models. SNEFYs strictly generalise classical exponential families, are closed under conditioning, and have tractable marginal distributions. Their utility is illustrated on a variety of density estimation and conditional density estimation tasks. Software available at https://github.com/RussellTsuchida/snefy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.13507</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65306;&#19968;&#20221;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Automated Fact-Checking: A Survey. (arXiv:2305.13507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#30340;&#26694;&#26550;&#65292;&#24182;&#21253;&#25324;&#20102;&#29420;&#29305;&#30340;&#23376;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#36825;&#22235;&#31181;&#27169;&#24577;&#30340;&#29616;&#23454;&#24212;&#29992;&#12290;&#32426;&#24405;&#20102;&#30456;&#20851;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20449;&#24687;&#65292;&#21363;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#20197;&#22810;&#31181;&#24418;&#24335;&#20256;&#36798;&#65292;&#20363;&#22914;&#24102;&#26377;&#26631;&#39064;&#30340;&#22270;&#20687;&#12290; &#23427;&#34987;&#20154;&#20204;&#35270;&#20026;&#26356;&#21487;&#20449;&#65292;&#27604;&#20854;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#24212;&#29289;&#25193;&#25955;&#36895;&#24230;&#26356;&#24555;&#65292;&#33539;&#22260;&#26356;&#24191;&#12290; &#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#28041;&#21450;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#65288;AFC&#65289;&#65292;&#20294;&#20197;&#24448;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#25991;&#26412;&#35823;&#23548;&#26041;&#38754;&#12290; &#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#25324;&#22810;&#27169;&#24577;&#35823;&#23548;&#29420;&#29305;&#23376;&#20219;&#21153;&#22312;&#20869;&#30340;AFC&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19978;&#35752;&#35770;&#20102;&#19981;&#21516;&#31038;&#21306;&#25152;&#21457;&#23637;&#30340;&#30456;&#20851;&#26415;&#35821;&#12290; &#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23384;&#22312;&#30340;&#22235;&#31181;&#27169;&#24577;&#65306;&#25991;&#26412;&#65292;&#22270;&#20687;&#65292;&#38899;&#39057;&#21644;&#35270;&#39057;&#12290; &#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20934;&#21644;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation, i.e. factually incorrect information, is often conveyed in multiple modalities, e.g. an image accompanied by a caption. It is perceived as more credible by humans, and spreads faster and wider than its text-only counterparts. While an increasing body of research investigates automated fact-checking (AFC), previous surveys mostly focus on textual misinformation. In this survey, we conceptualise a framework for AFC including subtasks unique to multimodal misinformation. Furthermore, we discuss related terminological developed in different communities in the context of our framework. We focus on four modalities prevalent in real-world fact-checking: text, image, audio, and video. We survey benchmarks and models, and discuss limitations and promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.04798</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#23545;&#35805;&#24335;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-grained Hypergraph Interest Modeling for Conversational Recommendation. (arXiv:2305.04798v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#19981;&#21516;&#35282;&#24230;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#12290;&#37319;&#29992;&#36229;&#22270;&#32467;&#26500;&#34920;&#31034;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#25429;&#25417;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#36718;&#23545;&#35805;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#30340;&#21363;&#26102;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#24456;&#22810;&#26377;&#25928;&#30340;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#20294;&#22823;&#22810;&#25968;&#20173;&#28982;&#38598;&#20013;&#22312;&#24403;&#21069;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19978;&#65292;&#36890;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#21033;&#29992;&#21382;&#21490;&#23545;&#35805;&#25968;&#25454;&#26469;&#20016;&#23500;&#24403;&#21069;&#23545;&#35805;&#30340;&#26377;&#38480;&#19978;&#19979;&#25991;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#31890;&#24230;&#36229;&#22270;&#20852;&#36259;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#22797;&#26434;&#21382;&#21490;&#25968;&#25454;&#19979;&#30340;&#29992;&#25143;&#20852;&#36259;&#12290;&#20316;&#20026;&#26680;&#24515;&#24605;&#24819;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#26469;&#34920;&#31034;&#21382;&#21490;&#23545;&#35805;&#20013;&#22797;&#26434;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#36229;&#22270;&#32467;&#26500;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#21382;&#21490;&#23545;&#35805;&#20250;&#35805;&#65292;&#24182;&#24418;&#25104;&#19968;&#20010;&#22522;&#20110;&#20250;&#35805;&#30340;&#36229;&#22270;&#65292;&#35813;&#36229;&#22270;&#25429;&#25417;&#20102;&#31895;&#31890;&#24230;&#30340;&#20250;&#35805;&#32423;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender system (CRS) interacts with users through multi-turn dialogues in natural language, which aims to provide high-quality recommendations for user's instant information need. Although great efforts have been made to develop effective CRS, most of them still focus on the contextual information from the current dialogue, usually suffering from the data scarcity issue. Therefore, we consider leveraging historical dialogue data to enrich the limited contexts of the current dialogue session.  In this paper, we propose a novel multi-grained hypergraph interest modeling approach to capture user interest beneath intricate historical data from different perspectives. As the core idea, we employ hypergraph to represent complicated semantic relations underlying historical dialogues. In our approach, we first employ the hypergraph structure to model users' historical dialogue sessions and form a session-based hypergraph, which captures coarse-grained, session-level relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#65292;MVSD&#65292;&#35813;&#26694;&#26550;&#23558;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#27963;&#21160;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#19988;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24378;&#22522;&#32447;&#30340;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.11411</link><description>&lt;p&gt;
&#29992;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#32593;&#32476;&#26816;&#27979;&#24433;&#35780;&#20013;&#30340;&#21095;&#36879;
&lt;/p&gt;
&lt;p&gt;
Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks. (arXiv:2304.11411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#65292;MVSD&#65292;&#35813;&#26694;&#26550;&#23558;&#30005;&#24433;&#30693;&#35782;&#21644;&#29992;&#25143;&#27963;&#21160;&#32435;&#20837;&#32771;&#34385;&#65292;&#24182;&#19988;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#24378;&#22522;&#32447;&#30340;&#30456;&#20851;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#30005;&#24433;&#35780;&#35770;&#24179;&#21488;&#20026;&#30005;&#24433;&#20135;&#19994;&#21644;&#20844;&#20247;&#25552;&#20379;&#20247;&#21253;&#21453;&#39304;&#65292;&#20294;&#21095;&#36879;&#35780;&#35770;&#20005;&#37325;&#25439;&#23475;&#20102;&#29992;&#25143;&#20307;&#39564;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#30740;&#31350;&#20197;&#33258;&#21160;&#35782;&#21035;&#21095;&#36879;&#65292;&#20294;&#20854;&#20165;&#20851;&#27880;&#20110;&#35780;&#35770;&#20869;&#23481;&#26412;&#36523;&#65292;&#32780;&#24378;&#22823;&#30340;&#21095;&#36879;&#26816;&#27979;&#38656;&#35201;&#23558;&#35780;&#35770;&#25918;&#20837;&#20851;&#20110;&#30005;&#24433;&#30340;&#20107;&#23454;&#21644;&#30693;&#35782;&#12289;&#29992;&#25143;&#22312;&#30005;&#24433;&#35780;&#35770;&#24179;&#21488;&#19978;&#30340;&#34892;&#20026;&#31561;&#19978;&#19979;&#25991;&#20013;&#12290;&#22240;&#27492;&#65292;&#22312;&#25105;&#20204;&#39318;&#20808;&#25972;&#29702;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#21095;&#36879;&#26816;&#27979;&#25968;&#25454;&#38598;LCS&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#26368;&#26032;&#30005;&#24433;&#30693;&#35782;&#24211;UKM&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MVSD&#65292;&#19968;&#31181;&#32771;&#34385;&#21040;&#26377;&#20851;&#30005;&#24433;&#21644;&#29992;&#25143;&#27963;&#21160;&#30340;&#22806;&#37096;&#30693;&#35782;&#30340;&#26032;&#22411;&#22810;&#35270;&#35282;&#21095;&#36879;&#26816;&#27979;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MVSD&#26500;&#24314;&#19977;&#20010;&#20114;&#32852;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#26469;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#26469;&#28304;&#21450;&#20854;&#22810;&#35270;&#22270;&#23646;&#24615;&#65292;&#32780;&#25105;&#20204;&#35774;&#35745;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;HGCN&#65289;&#26469;&#34701;&#21512;&#22810;&#35270;&#22270;&#23884;&#20837;&#24182;&#39044;&#27979;&#21095;&#36879;&#12290;&#25105;&#20204;&#22312;LCS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;MVSD&#22312;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#30005;&#24433;&#35780;&#35770;&#32593;&#31449;&#19978;&#23637;&#31034;&#20102;MVSD&#22312;&#30495;&#23454;&#21095;&#36879;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content itself, while robust spoiler detection requires putting the review into the context of facts and knowledge regarding movies, user behavior on film review platforms, and more. In light of these challenges, we first curate a large-scale network-based spoiler detection dataset LCS and a comprehensive and up-to-date movie knowledge base UKM. We then propose MVSD, a novel Multi-View Spoiler Detection framework that takes into account the external knowledge about movies and user activities on movie review platforms. Specifically, MVSD constructs three interconnecting heterogeneous information networks to model diverse data sources and their multi-view attributes, while we design and employ a novel heter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#27880;&#21464;&#21270;&#21040;&#25551;&#36848;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#25551;&#36848;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#30340;&#21464;&#21270;&#25551;&#36848;&#19981;&#21516;&#65292;&#36965;&#24863;&#21464;&#21270;&#25551;&#36848;&#38656;&#35201;&#25429;&#25417;&#24433;&#21709;&#22240;&#32032;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.01091</link><description>&lt;p&gt;
Remote Sensing Change Captioning&#30340;&#20851;&#27880;&#32593;&#32476;&#65306;&#23545;&#36828;&#31243;&#24863;&#30693;&#21464;&#21270;&#25551;&#36848;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Changes to Captions: An Attentive Network for Remote Sensing Change Captioning. (arXiv:2304.01091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#27880;&#21464;&#21270;&#21040;&#25551;&#36848;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#25551;&#36848;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#30340;&#21464;&#21270;&#25551;&#36848;&#19981;&#21516;&#65292;&#36965;&#24863;&#21464;&#21270;&#25551;&#36848;&#38656;&#35201;&#25429;&#25417;&#24433;&#21709;&#22240;&#32032;&#30340;&#26368;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20808;&#36827;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#21644;&#20998;&#26512;&#36965;&#24863;&#22270;&#20687;&#12290;&#20934;&#30830;&#25551;&#36848;&#22810;&#26102;&#30456;&#36965;&#24863;&#22270;&#20687;&#20013;&#21457;&#29983;&#30340;&#21464;&#21270;&#23545;&#20110;&#22320;&#29702;&#31354;&#38388;&#29702;&#35299;&#21644;&#22303;&#22320;&#35268;&#21010;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#21464;&#21270;&#25551;&#36848;&#20219;&#21153;&#19981;&#21516;&#65292;&#36965;&#24863;&#21464;&#21270;&#25551;&#36848;&#26088;&#22312;&#25429;&#25417;&#26368;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#19981;&#21463;&#29031;&#26126;&#12289;&#23395;&#33410;&#25928;&#24212;&#21644;&#22797;&#26434;&#22320;&#29289;&#35206;&#30422;&#31561;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20934;&#30830;&#25551;&#36848;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#22270;&#20687;&#20197;&#21450;&#36965;&#24863;&#22270;&#20687;&#30340;&#21464;&#21270;&#25551;&#36848;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#20934;&#30830;&#25551;&#36848;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Chg2Cap&#30340;&#20851;&#27880;&#21464;&#21270;&#21040;&#25551;&#36848;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#20004;&#26102;&#30456;&#30340;&#36965;&#24863;&#22270;&#20687;&#12290;&#35813;&#32593;&#32476;&#30001;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advanced research has focused on the direct learning and analysis of remote sensing images using natural language processing (NLP) techniques. The ability to accurately describe changes occurring in multi-temporal remote sensing images is becoming increasingly important for geospatial understanding and land planning. Unlike natural image change captioning tasks, remote sensing change captioning aims to capture the most significant changes, irrespective of various influential factors such as illumination, seasonal effects, and complex land covers. In this study, we highlight the significance of accurately describing changes in remote sensing images and present a comparison of the change captioning task for natural and synthetic images and remote sensing images. To address the challenge of generating accurate captions, we propose an attentive changes-to-captions network, called Chg2Cap for short, for bi-temporal remote sensing images. The network comprises three main com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.15027</link><description>&lt;p&gt;
&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data. (arXiv:2303.15027v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#26159;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#31995;&#32479;&#21464;&#37327;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20027;&#35201;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#26412;&#25991;&#23545;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24120;&#29992;&#26415;&#35821;&#65292;&#28982;&#21518;&#20840;&#38754;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#24037;&#20855;&#25110;&#36719;&#20214;&#21253;&#20197;&#21450;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#19968;&#20123;&#24120;&#35265;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;TriPlaneNet&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;EG3D&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#29616;&#26377;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.13497</link><description>&lt;p&gt;
TriPlaneNet&#65306;&#19968;&#31181;EG3D&#21453;&#28436;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;TriPlaneNet&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;EG3D&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#29616;&#26377;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;NeRF&#30340;GAN&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#26032;&#39062;&#30340;&#35270;&#35282;&#28210;&#26579;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#33021;&#22815;&#37325;&#26032;&#28210;&#26579;&#25110;&#20462;&#25913;&#29616;&#26377;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#65292;&#24517;&#39035;&#35299;&#20915;&#19968;&#20010;&#21453;&#38382;&#39064;&#12290;&#23613;&#31649;&#23545;&#20110;2D GAN&#21453;&#28436;&#32780;&#35328;&#65292;&#36890;&#29992;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;3D GAN&#21453;&#28436;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;3D&#19968;&#33268;&#30340;&#28210;&#26579;&#12290;&#32780;&#20687;StyleGAN&#36825;&#26679;&#30340;&#24555;&#36895;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21487;&#33021;&#20063;&#19981;&#22826;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#20026;EG3D&#29983;&#25104;&#27169;&#22411;&#24341;&#20837;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24357;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#25152;&#26174;&#31034;&#30340;&#37027;&#26679;&#65292;&#28210;&#26579;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11888</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36328;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving. (arXiv:2303.11888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#21644;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#20013;&#12290;&#25991;&#20013;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#20132;&#36890;&#35268;&#21017;&#30340;&#36981;&#23432;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#24335;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#31561;&#20219;&#21153;&#30340;&#20934;&#30830;&#24230;&#24050;&#32463;&#36229;&#36807;&#20154;&#31867;&#12290;&#33258;&#21160;&#39550;&#39542;&#20316;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#24443;&#24213;&#25913;&#21464;&#26410;&#26469;&#30340;&#20132;&#36890;&#21644;&#20986;&#34892;&#26041;&#24335;&#12290;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#30001;&#20110;&#20854;&#22810;&#32500;&#24863;&#30693;&#21644;&#38598;&#25104;&#33021;&#21147;&#30340;&#28508;&#21147;&#32780;&#25104;&#20026;&#24403;&#21069;&#30740;&#31350;&#30340;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#32423;&#22810;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#23548;&#33322;&#21644;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20027;&#35201;&#20851;&#27880;&#20110;&#28608;&#20809;&#38647;&#36798;&#21644;RGB&#20449;&#24687;&#30340;&#34701;&#21512;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#33021;&#21147;&#24182;&#32479;&#19968;&#27169;&#20223;&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of Pattern Recognition and Computer Vision technologies, tasks like object detection or semantic segmentation have achieved even better accuracy than human beings. Based on these solid foundations, autonomous driving is becoming an important research direction, aiming to revolute the future of transportation and mobility. Sensors are critical to autonomous driving's security and feasibility to perceive the surrounding environment. Multi-Sensor fusion has become a current research hot spot because of its potential for multidimensional perception and integration ability. In this paper, we propose a novel feature-level multi-sensor fusion technology for end-to-end autonomous driving navigation with imitation learning. Our paper mainly focuses on fusion technologies for Lidar and RGB information. We also provide a brand-new penalty-based imitation learning method to reinforce the model's compliance with traffic rules and unify the objective of imitation learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11249</link><description>&lt;p&gt;
&#20160;&#20040;&#35753;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65311;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
What Makes Data Suitable for a Locally Connected Neural Network? A Necessary and Sufficient Condition Based on Quantum Entanglement. (arXiv:2303.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#29992;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21028;&#23450;&#25968;&#25454;&#36866;&#21512;&#20110;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#30456;&#24212;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#36866;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#26469;&#33258;&#37327;&#23376;&#29289;&#29702;&#23398;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#38024;&#23545;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#23616;&#37096;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#20869;&#30340;&#24191;&#27867;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#22312;&#26576;&#20123;&#29305;&#24449;&#30340;&#35268;&#33539;&#21010;&#20998;&#19979;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#25509;&#21463;&#20302;&#37327;&#23376;&#32416;&#32544;&#26102;&#65292;&#29305;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25165;&#33021;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#35813;&#25968;&#25454;&#20998;&#24067;&#12290;&#20316;&#20026;&#26412;&#32467;&#26524;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25968;&#25454;&#20998;&#24067;&#36866;&#21512;&#23616;&#37096;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#24191;&#27867;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#32416;&#32544;&#23558;&#40723;&#21169;&#24418;&#24335;&#25512;&#29702;&#30340;&#29289;&#29702;&#24037;&#20855;&#26469;&#36827;&#19968;&#27493;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The question of what makes a data distribution suitable for deep learning is a fundamental open problem. Focusing on locally connected neural networks (a prevalent family of architectures that includes convolutional and recurrent neural networks as well as local self-attention models), we address this problem by adopting theoretical tools from quantum physics. Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low quantum entanglement under certain canonical partitions of features. As a practical application of this result, we derive a preprocessing method for enhancing the suitability of a data distribution to locally connected neural networks. Experiments with widespread models over various datasets demonstrate our findings. We hope that our use of quantum entanglement will encourage further adoption of tools from physics for formally reasoning about 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03284</link><description>&lt;p&gt;
"Wasserstein Believer:&#36890;&#36807;&#21487;&#38752;&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;"
&lt;/p&gt;
&lt;p&gt;
The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26159;&#24314;&#27169;&#20195;&#29702;&#26080;&#27861;&#24863;&#30693;&#21040;&#23436;&#25972;&#29366;&#24577;&#30340;&#29615;&#22659;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#38656;&#35201;&#32771;&#34385;&#36807;&#21435;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#36827;&#34892;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21382;&#21490;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20165;&#20165;&#35760;&#20303;&#23436;&#25972;&#21382;&#21490;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20445;&#25345;&#27169;&#25311;&#30495;&#23454;&#29366;&#24577;&#30340;&#32622;&#20449;&#27010;&#29575;&#20998;&#24067;&#21487;&#20197;&#20316;&#20026;&#21382;&#21490;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#35201;&#35775;&#38382;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#35266;&#23519;-&#34892;&#21160;&#21382;&#21490;&#20197;&#23398;&#20064;&#20805;&#20998;&#30340;&#32479;&#35745;&#37327;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#25104;&#21151;&#30340;&#20445;&#35777;&#24182;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Wasserstein Belief Updater &#65292;&#36825;&#26159;&#19968;&#31181;RL&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#38754;&#23545;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#26102;&#30340;&#22256;&#38590;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.00905</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open-World Object Manipulation using Pre-trained Vision-Language Models. (arXiv:2303.00905v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#22312;&#38754;&#23545;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#26102;&#30340;&#22256;&#38590;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#25353;&#29031;&#20154;&#20204;&#30340;&#25351;&#20196;&#34892;&#21160;&#65292;&#23427;&#20204;&#24517;&#39035;&#33021;&#22815;&#23558;&#20154;&#31867;&#35789;&#27719;&#20013;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#65306;&#8220;&#20320;&#33021;&#32473;&#25105;&#25343;&#26469;&#31881;&#33394;&#30340;&#27611;&#32466;&#40120;&#40060;&#21527;&#65311;&#8221;&#65289;&#19982;&#23427;&#20204;&#30340;&#24863;&#30693;&#35266;&#23519;&#21644;&#34892;&#21160;&#30456;&#36830;&#25509;&#12290;&#36825;&#23545;&#26426;&#22120;&#20154;&#26469;&#35828;&#24102;&#26469;&#20102;&#19968;&#20010;&#26126;&#26174;&#22256;&#38590;&#30340;&#25361;&#25112;&#65306;&#23613;&#31649;&#26426;&#22120;&#20154;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#26426;&#22120;&#20154;&#20174;&#31532;&#19968;&#25163;&#32463;&#39564;&#20013;&#23398;&#20064;&#21040;&#35768;&#22810;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20294;&#26426;&#22120;&#20154;&#19981;&#21487;&#33021;&#20146;&#36523;&#20307;&#39564;&#21040;&#28085;&#30422;&#25152;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24076;&#26395;&#26426;&#22120;&#20154;&#30340;&#31574;&#30053;&#33021;&#22815;&#24863;&#30693;&#21644;&#25342;&#21462;&#31881;&#33394;&#30340;&#27611;&#32466;&#40120;&#40060;&#65292;&#21363;&#20351;&#23427;&#20197;&#21069;&#20174;&#26410;&#35265;&#36807;&#19982;&#27611;&#32466;&#40120;&#40060;&#20114;&#21160;&#30340;&#25968;&#25454;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20114;&#32852;&#32593;&#19978;&#30340;&#38745;&#24577;&#25968;&#25454;&#25317;&#26377;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#19988;&#36825;&#20123;&#20449;&#24687;&#34987;&#25429;&#25417;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#31574;&#30053;&#19982;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25509;&#21475;&#36830;&#25509;&#65292;&#20197;&#20415;&#35753;&#26426;&#22120;&#20154;&#23436;&#25104;&#28041;&#21450;&#26426;&#22120;&#20154;&#20197;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#29289;&#20307;&#31867;&#21035;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to follow instructions from people, they must be able to connect the rich semantic information in human vocabulary, e.g. "can you get me the pink stuffed whale?" to their sensory observations and actions. This brings up a notably difficult challenge for robots: while robot learning approaches allow robots to learn many different behaviors from first-hand experience, it is impractical for robots to have first-hand experiences that span all of this semantic information. We would like a robot's policy to be able to perceive and pick up the pink stuffed whale, even if it has never seen any data interacting with a stuffed whale before. Fortunately, static data on the internet has vast semantic information, and this information is captured in pre-trained vision-language models. In this paper, we study whether we can interface robot policies with these pre-trained models, with the aim of allowing robots to complete instructions involving object categories that the robot has never s
&lt;/p&gt;</description></item><item><title>DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06961</link><description>&lt;p&gt;
DualStreamFoveaNet: &#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#29992;&#20110;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06961
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#23545;&#20110;&#20998;&#26512;&#35270;&#32593;&#33180;&#30142;&#30149;&#20197;&#39044;&#38450;&#19981;&#21487;&#36870;&#35270;&#21147;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#34429;&#28982;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20013;&#22830;&#20985;&#28857;&#21608;&#22260;&#23616;&#37096;&#35299;&#21078;&#26631;&#35760;&#30340;&#32570;&#22833;&#12289;&#19981;&#33021;&#40065;&#26834;&#22320;&#22788;&#29702;&#30149;&#21464;&#35270;&#32593;&#33180;&#22270;&#20687;&#21644;&#22270;&#20687;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#31216;&#20026;DualStreamFoveaNet (DSFN)&#29992;&#20110;&#22810;&#32447;&#32034;&#34701;&#21512;&#12290;&#35813;&#26550;&#26500;&#26126;&#30830;&#22320;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#26469;&#23454;&#29616;&#38271;&#31243;&#36830;&#25509;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#25105;&#20204;&#22312;&#21452;&#27969;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#33258;&#23398;&#20064;&#30340;&#35299;&#21078;&#20449;&#24687;&#65292;&#26356;&#27880;&#37325;&#20998;&#24067;&#22312;&#34880;&#31649;&#27839;&#32447;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate fovea localization is essential for analyzing retinal diseases to prevent irreversible vision loss. While current deep learning-based methods outperform traditional ones, they still face challenges such as the lack of local anatomical landmarks around the fovea, the inability to robustly handle diseased retinal images, and the variations in image conditions. In this paper, we propose a novel transformer-based architecture called DualStreamFoveaNet (DSFN) for multi-cue fusion. This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization. We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers. Our extensive experiments show that the proposed architecture achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2302.02209</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#30340;&#38142;&#36335;&#39044;&#27979;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Link Prediction via Relational Weisfeiler-Leman. (arXiv:2302.02209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#23545;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#31616;&#21333;&#22270;&#19978;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#20294;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#31995;&#32479;&#24615;&#30340;&#29702;&#35299;&#65292;&#20197;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#31561;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#35282;&#12289;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#27169;&#22411;&#65292;&#24182;&#35299;&#38145;&#20102;&#19968;&#31995;&#21015;&#20854;&#20182;&#27169;&#22411;&#12290;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#65292;&#34920;&#24449;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#20998;&#26512;&#34987;&#25193;&#23637;&#20197;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#25429;&#25417;&#30340;&#20989;&#25968;&#31867;&#36827;&#34892;&#31934;&#30830;&#36923;&#36753;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#29702;&#35770;&#21457;&#29616;&#35299;&#37322;&#20102;&#19968;&#20123;&#24191;&#27867;&#37319;&#29992;&#30340;&#23454;&#38469;&#35774;&#35745;&#36873;&#25321;&#30340;&#20248;&#28857;&#65292;&#24182;&#24471;&#21040;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are prominent models for representation learning over graph-structured data. While the capabilities and limitations of these models are well-understood for simple graphs, our understanding remains incomplete in the context of knowledge graphs. Our goal is to provide a systematic understanding of the landscape of graph neural networks for knowledge graphs pertaining to the prominent task of link prediction. Our analysis entails a unifying perspective on seemingly unrelated models and unlocks a series of other models. The expressive power of various models is characterized via a corresponding relational Weisfeiler-Leman algorithm. This analysis is extended to provide a precise logical characterization of the class of functions captured by a class of graph neural networks. The theoretical findings presented in this paper explain the benefits of some widely employed practical design choices, which are validated empirically.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#19981;&#20381;&#36182;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12593</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#39118;&#38505;&#21388;&#24694;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Risk-Averse Model Uncertainty for Distributionally Robust Safe Reinforcement Learning. (arXiv:2301.12593v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#22788;&#29702;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#65292;&#20855;&#26377;&#39640;&#25928;&#19988;&#19981;&#20381;&#36182;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#39046;&#22495;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23433;&#20840;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#36807;&#28193;&#27169;&#22411;&#30340;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#30456;&#24178;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#26469;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37319;&#21462;&#39118;&#38505;&#21388;&#24694;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#23427;&#31561;&#20215;&#20110;&#19968;&#31867;&#29305;&#23450;&#30340;&#20998;&#24067;&#40065;&#26834;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#36825;&#20010;&#26694;&#26550;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#34920;&#36798;&#19981;&#28041;&#21450;&#26497;&#23567;&#21270;&#26368;&#22823;&#20248;&#21270;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#12289;&#19981;&#20381;&#36182;&#27169;&#22411;&#22320;&#22312;&#21333;&#20010;&#35757;&#32451;&#29615;&#22659;&#20013;&#20165;&#38656;&#35201;&#26631;&#20934;&#25968;&#25454;&#25910;&#38598;&#26469;&#23454;&#26045;&#12290;&#22312;&#20855;&#26377;&#23433;&#20840;&#32422;&#26463;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#37096;&#32626;&#26102;&#33021;&#22815;&#20135;&#29983;&#31283;&#20581;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world domains require safe decision making in uncertain environments. In this work, we introduce a deep reinforcement learning framework for approaching this important problem. We consider a distribution over transition models, and apply a risk-averse perspective towards model uncertainty through the use of coherent distortion risk measures. We provide robustness guarantees for this framework by showing it is equivalent to a specific class of distributionally robust safe reinforcement learning problems. Unlike existing approaches to robustness in deep reinforcement learning, however, our formulation does not involve minimax optimization. This leads to an efficient, model-free implementation of our approach that only requires standard data collection from a single training environment. In experiments on continuous control tasks with safety constraints, we demonstrate that our framework produces robust performance and safety at deployment time across a range of perturbed test e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01488</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#20214;&#30693;&#35782;&#65306;&#19981;&#21487;&#33021;&#24615;&#21644;&#19981;&#22826;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Event knowledge in large language models: the gap between the impossible and the unlikely. (arXiv:2212.01488v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01488
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25317;&#26377;&#20016;&#23500;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#27604;&#19981;&#21487;&#33021;&#20107;&#20214;&#30340;&#25551;&#36848;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35821;&#26009;&#24211;&#20013;&#30340;&#35789;&#20849;&#29616;&#27169;&#24335;&#21253;&#21547;&#30528;&#24847;&#24819;&#19981;&#21040;&#30340;&#27010;&#24565;&#30693;&#35782;&#12290;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#35789;&#35821;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#27169;&#24335;&#65292;&#22312;&#38656;&#35201;&#19990;&#30028;&#30693;&#35782;&#30340;&#21508;&#31181;&#35821;&#20041;&#20219;&#21153;&#19978;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20851;&#20110;LLMs&#30340;&#35821;&#20041;&#33021;&#21147;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#23427;&#20204;&#26159;&#21542;&#33719;&#24471;&#20102;&#24120;&#35265;&#20107;&#20214;&#30340;&#19968;&#33324;&#21270;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20116;&#20010;&#39044;&#35757;&#32451;&#30340;LLMs&#65288;&#20174;2018&#24180;&#30340;BERT&#21040;2023&#24180;&#30340;MPT&#65289;&#26159;&#21542;&#27604;&#21516;&#19968;&#20107;&#20214;&#30340;&#19981;&#22826;&#21487;&#33021;&#30340;&#29256;&#26412;&#26356;&#21487;&#33021;&#22320;&#20998;&#37197;&#32473;&#21512;&#29702;&#30340;&#20195;&#29702;-&#24739;&#32773;&#30456;&#20114;&#20316;&#29992;&#12290;&#20351;&#29992;&#19977;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#26368;&#23567;&#21477;&#23545;&#38598;&#21512;&#65288;&#24635;&#25968;n=1,215&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;LLMs&#25317;&#26377;&#30456;&#24403;&#22823;&#30340;&#20107;&#20214;&#30693;&#35782;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#20960;&#20046;&#24635;&#26159;&#23558;&#21487;&#33021;&#20107;&#20214;&#19982;&#19981;&#21487;&#33021;&#20107;&#20214;&#30456;&#27604;&#36171;&#20104;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#65288;&#25945;&#24072;&#20080;&#20102;&#31508;&#35760;&#26412;&#30005;&#33041;&#30456;&#23545;&#20110;&#31508;&#35760;&#26412;&#30005;&#33041;&#20080;&#20102;&#25945;&#24072;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word co-occurrence patterns in language corpora contain a surprising amount of conceptual knowledge. Large language models (LLMs), trained to predict words in context, leverage these patterns to achieve impressive performance on diverse semantic tasks requiring world knowledge. An important but understudied question about LLMs' semantic abilities is whether they acquire generalized knowledge of common events. Here, we test whether five pre-trained LLMs (from 2018's BERT to 2023's MPT) assign higher likelihood to plausible descriptions of agent-patient interactions than to minimally different implausible versions of the same event. Using three curated sets of minimal sentence pairs (total n=1,215), we found that pre-trained LLMs possess substantial event knowledge, outperforming other distributional language models. In particular, they almost always assign higher likelihood to possible vs. impossible events (The teacher bought the laptop vs. The laptop bought the teacher). However, LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25919;&#24220;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26631;&#20934;&#21270;&#25805;&#20316;&#31243;&#24207;&#21644;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#26399;&#26395;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22810;&#23398;&#31185;&#30740;&#31350;&#32773;&#22312;&#27010;&#24565;&#19978;&#30340;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.17218</link><description>&lt;p&gt;
&#25919;&#24220;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#27010;&#24565;&#65292;&#26631;&#20934;&#21644;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence in government: Concepts, standards, and a unified framework. (arXiv:2210.17218v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#25919;&#24220;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#26631;&#20934;&#21270;&#25805;&#20316;&#31243;&#24207;&#21644;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#26399;&#26395;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22810;&#23398;&#31185;&#30740;&#31350;&#32773;&#22312;&#27010;&#24565;&#19978;&#30340;&#30862;&#29255;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#25919;&#24220;&#25913;&#21464;&#30340;&#24076;&#26395;&#21464;&#24471;&#33021;&#22815;&#23454;&#29616;&#12290;&#37492;&#20110;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20808;&#36827;&#33021;&#21147;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#20351;&#29992;&#26631;&#20934;&#25805;&#20316;&#31243;&#24207;&#23558;&#20854;&#23884;&#20837;&#65292;&#24182;&#31526;&#21512;&#31038;&#20250;&#30340;&#35268;&#33539;&#26399;&#26395;&#12290;&#22810;&#20010;&#39046;&#22495;&#30340;&#23398;&#32773;&#24320;&#22987;&#27010;&#24565;&#21270;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#19981;&#21516;&#24418;&#24335;&#65292;&#24378;&#35843;&#20854;&#28508;&#22312;&#30340;&#22909;&#22788;&#21644;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20173;&#28982;&#30862;&#29255;&#21270;&#65292;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#22914;&#20844;&#20849;&#31649;&#29702;&#21644;&#25919;&#27835;&#31185;&#23398;&#65292;&#20197;&#21450;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#39046;&#22495;&#37117;&#22312;&#30456;&#23545;&#23396;&#31435;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#27010;&#24565;&#12290;&#34429;&#28982;&#26377;&#21628;&#21505;&#23545;&#25919;&#24220;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#23558;&#20154;&#24037;&#26234;&#33021;&#23884;&#20837;&#20844;&#20849;&#39046;&#22495;&#30340;&#21518;&#26524;&#25152;&#38656;&#30340;&#29702;&#35770;&#35266;&#28857;&#30340;&#20840;&#38754;&#32508;&#21512;&#25551;&#36848;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI), especially in generative language modelling, hold the promise of transforming government. Given the advanced capabilities of new AI systems, it is critical that these are embedded using standard operational procedures, clear epistemic criteria, and behave in alignment with the normative expectations of society. Scholars in multiple domains have subsequently begun to conceptualize the different forms that AI applications may take, highlighting both their potential benefits and pitfalls. However, the literature remains fragmented, with researchers in social science disciplines like public administration and political science, and the fast-moving fields of AI, ML, and robotics, all developing concepts in relative isolation. Although there are calls to formalize the emerging study of AI in government, a balanced account that captures the full depth of theoretical perspectives needed to understand the consequences of embedding AI into a publi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#24335;&#22810;&#35270;&#22270;&#31435;&#20307;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#37096;&#20998;&#24102;&#26377;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;MVS&#20013;&#30340;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.11699</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#28145;&#24230;&#22810;&#35270;&#22270;&#31435;&#20307;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23398;&#20064;&#24335;&#22810;&#35270;&#22270;&#31435;&#20307;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#37096;&#20998;&#24102;&#26377;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;MVS&#20013;&#30340;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22810;&#35270;&#22270;&#31435;&#20307;(MVS)&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20026;&#20102;&#23558;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#28857;(&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;)&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#20943;&#23569;&#23545;&#26114;&#36149;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20165;&#26377;&#19968;&#23567;&#37096;&#20998;MVS&#25968;&#25454;&#38468;&#24102;&#31264;&#23494;&#28145;&#24230;&#20934;&#30830;&#20540;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#23398;&#20064;&#24335;MVS&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#22270;&#20013;&#30340;&#22330;&#26223;&#24040;&#22823;&#21464;&#21270;&#21644;&#28789;&#27963;&#35774;&#32622;&#65292;&#36825;&#21487;&#33021;&#30772;&#22351;&#20102;&#32463;&#20856;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#26412;&#20551;&#35774;&#65292;&#21363;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#26631;&#31614;&#25968;&#25454;&#20849;&#20139;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;MVS&#38382;&#39064;&#20013;&#31216;&#20026;&#21322;&#30417;&#30563;&#20998;&#24067;&#38388;&#38553;&#22810;&#20041;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#20998;&#24067;&#22686;&#24378;MVS&#26694;&#26550;&#65292;&#21363;SDA-MVS&#12290;&#23545;&#20110;MVS&#25968;&#25454;&#20013;&#22522;&#26412;&#20551;&#35774;&#36866;&#29992;&#30340;&#31616;&#21333;&#24773;&#20917;&#65292;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#40723;&#21169;&#27169;&#22411;&#39044;&#27979;&#22312;&#21407;&#22987;&#26679;&#26412;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Significant progress has been witnessed in learning-based Multi-view Stereo (MVS) under supervised and unsupervised settings. To combine their respective merits in accuracy and completeness, meantime reducing the demand for expensive labeled data, this paper explores the problem of learning-based MVS in a semi-supervised setting that only a tiny part of the MVS data is attached with dense depth ground truth. However, due to huge variation of scenarios and flexible settings in views, it may break the basic assumption in classic semi-supervised learning, that unlabeled data and labeled data share the same label space and data distribution, named as semi-supervised distribution-gap ambiguity in the MVS problem. To handle these issues, we propose a novel semi-supervised distribution-augmented MVS framework, namely SDA-MVS. For the simple case that the basic assumption works in MVS data, consistency regularization encourages the model predictions to be consistent between original sample and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;&#65288;LAN&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20915;&#26550;&#26500;&#21644;&#20013;&#24515;&#21270;&#35780;&#35770;&#23478;&#26469;&#23398;&#20064;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#24182;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.12458</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.12458
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#20248;&#21183;&#32593;&#32476;&#65288;LAN&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23545;&#20915;&#26550;&#26500;&#21644;&#20013;&#24515;&#21270;&#35780;&#35770;&#23478;&#26469;&#23398;&#20064;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#24182;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25104;&#21151;&#30340;&#31163;&#31574;&#30053;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#23547;&#25214;&#20998;&#35299;&#30340;&#20540;&#20989;&#25968;&#65292;&#23548;&#33268;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;LAN&#31639;&#27861;&#24314;&#31435;&#22312;&#29420;&#31435;Q&#23398;&#20064;&#32773;&#30340;&#32467;&#26500;&#22522;&#30784;&#19978;&#65292;&#37319;&#29992;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20915;&#26550;&#26500;&#36890;&#36807;&#20010;&#20307;&#20248;&#21183;&#20989;&#25968;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#23398;&#20064;&#20998;&#25955;&#30340;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#12290;&#36890;&#36807;&#19968;&#20010;&#20013;&#24515;&#21270;&#30340;&#35780;&#35770;&#23478;&#31283;&#23450;&#23398;&#20064;&#65292;&#35780;&#35770;&#23478;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20943;&#23569;&#20010;&#20307;&#20248;&#21183;&#30340;&#31227;&#21160;&#30446;&#26631;&#38382;&#39064;&#12290;&#35780;&#35770;&#23478;&#30340;&#32593;&#32476;&#22823;&#23567;&#19982;&#26234;&#33021;&#20307;&#25968;&#37327;&#26080;&#20851;&#65292;&#22312;&#23398;&#20064;&#21518;&#34987;&#20002;&#24323;&#12290;&#22312;StarCraft II&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;LAN&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#20110;&#26234;&#33021;&#20307;&#25968;&#37327;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#65292;&#20026;MARL&#30740;&#31350;&#24320;&#36767;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.
&lt;/p&gt;</description></item><item><title>&#8220;&#37327;&#23376;&#27169;&#31946;&#8221;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#37327;&#23376;&#36719;&#20214;&#31034;&#20363;&#65292;&#22312;&#35745;&#31639;&#26426;&#28216;&#25103;&#12289;&#38899;&#20048;&#21644;&#33402;&#26415;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19982;&#26368;&#37325;&#35201;&#29992;&#25143;&#30340;&#35752;&#35770;&#65292;&#30830;&#23450;&#20102;&#35813;&#26041;&#27861;&#26368;&#26377;&#29992;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24615;&#19982;&#37327;&#23376;&#29616;&#35937;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2112.01646</link><description>&lt;p&gt;
&#30740;&#31350;&#8220;&#37327;&#23376;&#27169;&#31946;&#8221;&#25216;&#26415;&#30340;&#26377;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the usefulness of Quantum Blur. (arXiv:2112.01646v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01646
&lt;/p&gt;
&lt;p&gt;
&#8220;&#37327;&#23376;&#27169;&#31946;&#8221;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#37327;&#23376;&#36719;&#20214;&#31034;&#20363;&#65292;&#22312;&#35745;&#31639;&#26426;&#28216;&#25103;&#12289;&#38899;&#20048;&#21644;&#33402;&#26415;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19982;&#26368;&#37325;&#35201;&#29992;&#25143;&#30340;&#35752;&#35770;&#65292;&#30830;&#23450;&#20102;&#35813;&#26041;&#27861;&#26368;&#26377;&#29992;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20123;&#29305;&#24615;&#19982;&#37327;&#23376;&#29616;&#35937;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36317;&#31163;&#37327;&#23376;&#35745;&#31639;&#23436;&#20840;&#36229;&#36234;&#20256;&#32479;&#35745;&#31639;&#36824;&#38656;&#35201;&#19968;&#20123;&#24180;&#20221;&#65292;&#20294;&#24050;&#32463;&#25552;&#20379;&#20102;&#21487;&#20197;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25506;&#32034;&#24615;&#20219;&#21153;&#30340;&#36164;&#28304;&#12290;&#36825;&#21253;&#25324;&#22312;&#35745;&#31639;&#26426;&#28216;&#25103;&#12289;&#38899;&#20048;&#21644;&#33402;&#26415;&#20013;&#36827;&#34892;&#31243;&#24207;&#29983;&#25104;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#25152;&#35859;&#30340;&#8220;&#37327;&#23376;&#27169;&#31946;&#8221;&#26041;&#27861;&#20195;&#34920;&#20102;&#36825;&#19968;&#25506;&#32034;&#20043;&#26053;&#30340;&#31532;&#19968;&#27493;&#65292;&#20026;&#25105;&#20204;&#23637;&#31034;&#20102;&#37327;&#23376;&#36719;&#20214;&#22914;&#20309;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#8220;&#37327;&#23376;&#27169;&#31946;&#8221;&#26041;&#27861;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#27169;&#31946;&#25928;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#36890;&#36807;&#19982;&#35813;&#26041;&#27861;&#30340;&#26368;&#37325;&#35201;&#29992;&#25143;&#30340;&#35752;&#35770;&#26469;&#25351;&#23548;&#30340;&#65292;&#30446;&#30340;&#26159;&#30830;&#23450;&#21738;&#20123;&#29305;&#24615;&#34987;&#35748;&#20026;&#26368;&#26377;&#29992;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#29305;&#24615;&#22914;&#20309;&#21462;&#20915;&#20110;&#37327;&#23376;&#29616;&#35937;&#30340;&#21472;&#21152;&#21644;&#32416;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though some years remain before quantum computation can fully outperform conventional computation, it already provides resources that can be used for exploratory purposes in various fields. This includes certain tasks for procedural generation in computer games, music and art. The so-called `Quantum Blur' method represents the first step on this journey, providing a simple proof-of-principle example of how quantum software can be useful in these areas today. Here we analyse the `Quantum Blur' method and compare it to conventional blur effects. This investigation was guided by discussions with the most prominent user of the method, to determine which features were found most useful. In particular we determine how these features depend on the quantum phenomena of superposition and entanglement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2109.14251</link><description>&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#24341;&#23548;&#30340;&#22478;&#24066;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Road Network Guided Fine-Grained Urban Traffic Flow Inference. (arXiv:2109.14251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#20132;&#36890;&#27969;&#37327;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#65292;&#26222;&#36941;&#36866;&#29992;&#20110;&#22478;&#24066;&#20132;&#36890;&#27969;&#37327;&#30417;&#27979;&#21644;&#35843;&#25511;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25512;&#26029;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#37327;&#26159;&#19968;&#20010;&#26032;&#20852;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26497;&#22823;&#22320;&#20943;&#23569;&#25152;&#38656;&#20132;&#36890;&#30417;&#27979;&#20256;&#24863;&#22120;&#30340;&#25968;&#37327;&#20197;&#33410;&#30465;&#25104;&#26412;&#12290;&#26412;&#25991;&#21457;&#29616;&#20132;&#36890;&#27969;&#37327;&#19982;&#36947;&#36335;&#32593;&#32476;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#23436;&#20840;&#24573;&#30053;&#20102;&#36825;&#19968;&#28857;&#65292;&#25110;&#32773;&#20165;&#23558;&#20854;&#35270;&#20026;&#22806;&#37096;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#32593;&#24863;&#30693;&#20132;&#36890;&#27969;&#37327;&#25918;&#22823;&#22120;&#65288;RATFM&#65289;&#65292;&#23427;&#26126;&#30830;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20840;&#38754;&#23398;&#20064;&#32454;&#31890;&#24230;&#20132;&#36890;&#27969;&#30340;&#36947;&#36335;&#24863;&#30693;&#31354;&#38388;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26041;&#21521;1D&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#36947;&#36335;&#32593;&#32476;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#21644;&#31895;&#31890;&#24230;&#27969;&#37327;&#29305;&#24449;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35268;&#33539;&#21270;&#36947;&#36335;&#30456;&#20851;&#20132;&#36890;&#27969;&#30340;&#30701;&#36317;&#31163;&#31354;&#38388;&#20998;&#24067;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36947;&#36335;&#32593;&#32476;&#29305;&#24449;&#20316;&#20026;&#26597;&#35810;&#26469;&#25429;&#33719;&#38271;&#36317;&#31163;&#36335;&#27573;&#20132;&#36890;&#27969;&#37327;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate inference of fine-grained traffic flow from coarse-grained one is an emerging yet crucial problem, which can help greatly reduce the number of the required traffic monitoring sensors for cost savings. In this work, we notice that traffic flow has a high correlation with road network, which was either completely ignored or simply treated as an external factor in previous works.To facilitate this problem, we propose a novel Road-Aware Traffic Flow Magnifier (RATFM) that explicitly exploits the prior knowledge of road networks to fully learn the road-aware spatial distribution of fine-grained traffic flow. Specifically, a multi-directional 1D convolutional layer is first introduced to extract the semantic feature of the road network. Subsequently, we incorporate the road network feature and coarse-grained flow feature to regularize the short-range spatial distribution modeling of road-relative traffic flow. Furthermore, we take the road network feature as a query to capture the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2104.02206</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#27969;&#23398;&#20064;&#30340;&#35843;&#20248;&#32452;&#21512;&#29305;&#24449;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CRUMB&#30340;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#36890;&#36807;&#23384;&#20648;&#20869;&#23384;&#22359;&#30340;&#32034;&#24341;&#26469;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#65292;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#22823;&#33041;&#20174;&#30636;&#26102;&#30340;&#19990;&#30028;&#32463;&#39564;&#20013;&#25552;&#21462;&#20986;&#25345;&#20037;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#30693;&#35782;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36828;&#36828;&#19981;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#27700;&#24179;&#65306;&#24403;&#34987;&#35201;&#27714;&#36890;&#36807;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#35757;&#32451;&#38750;&#37325;&#22797;&#35270;&#39057;&#24103;&#26469;&#23398;&#20064;&#23545;&#35937;&#20998;&#31867;&#26102;&#65288;&#22312;&#32447;&#27969;&#23398;&#20064;&#65289;&#65292;&#37027;&#20123;&#33021;&#22815;&#20174;&#37325;&#26032;&#25490;&#21015;&#30340;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#23398;&#20064;&#26032;&#30340;&#21050;&#28608;&#26102;&#20250;&#28798;&#38590;&#24615;&#22320;&#36951;&#24536;&#26087;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;Compositional Replay Using Memory Blocks (CRUMB)&#65292;&#36890;&#36807;&#37325;&#25918;&#36890;&#36807;&#37325;&#26032;&#32452;&#21512;&#36890;&#29992;&#37096;&#20998;&#37325;&#24314;&#30340;&#29305;&#24449;&#22270;&#26469;&#32531;&#35299;&#36951;&#24536;&#38382;&#39064;&#12290;CRUMB&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20018;&#32852;&#21487;&#35757;&#32451;&#21644;&#21487;&#37325;&#29992;&#30340;&#8220;&#20869;&#23384;&#22359;&#8221;&#21521;&#37327;&#65292;&#20197;&#32452;&#21512;&#26041;&#24335;&#37325;&#24314;&#29305;&#24449;&#22270;&#24352;&#37327;&#65292;&#23601;&#20687;&#38754;&#21253;&#23633;&#32452;&#21512;&#25104;&#19968;&#20010;&#38754;&#21253;&#19968;&#26679;&#12290;CRUMB&#23384;&#20648;&#29992;&#20110;&#37325;&#24314;&#26032;&#21050;&#28608;&#30340;&#20869;&#23384;&#22359;&#32034;&#24341;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#33021;&#22815;&#22238;&#25918;&#29305;&#23450;&#30340;&#35760;&#24518;&#12290;&#36825;&#31181;&#37325;&#24314;&#26426;&#21046;&#36824;&#21487;&#20197;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable "memory block" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2010.01992</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.01992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22810;&#20219;&#21153;&#34920;&#31034;&#65288;MTR&#65289;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#30446;&#26631;&#26159;&#21033;&#29992;&#28304;&#20219;&#21153;&#26469;&#23398;&#20064;&#19968;&#20010;&#34920;&#31034;&#65292;&#20943;&#23569;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;MTR&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#26694;&#26550;&#20869;&#23545;&#27969;&#34892;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26799;&#24230;&#20248;&#21270;&#21644;&#24230;&#37327;&#20248;&#21270;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26681;&#26412;&#21306;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#35299;&#37322;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#24471;&#21040;&#30340;&#35265;&#35299;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35889;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23558;MTR&#29702;&#35770;&#30340;&#26368;&#26032;&#23398;&#20064;&#30028;&#38480;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#39318;&#27425;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.
&lt;/p&gt;</description></item></channel></rss>