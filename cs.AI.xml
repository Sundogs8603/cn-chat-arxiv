<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IM-3D&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#30452;&#25509;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.08682</link><description>&lt;p&gt;
IM-3D&#65306;&#29992;&#20110;&#39640;&#36136;&#37327;3D&#29983;&#25104;&#30340;&#36845;&#20195;&#22810;&#35270;&#35282;&#25193;&#25955;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IM-3D&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#29983;&#25104;&#22120;&#21644;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#30452;&#25509;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#22120;&#37117;&#26159;&#22522;&#20110;&#24050;&#35757;&#32451;&#36807;&#30340;&#25968;&#21313;&#20159;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#26500;&#24314;&#30340;&#12290;&#23427;&#20204;&#20351;&#29992;Score Distillation Sampling (SDS)&#30340;&#21464;&#20307;&#65292;&#36825;&#31181;&#26041;&#27861;&#36739;&#24930;&#12289;&#19981;&#22826;&#31283;&#23450;&#19988;&#23481;&#26131;&#20135;&#29983;&#20266;&#24433;&#12290;&#19968;&#31181;&#32531;&#35299;&#26041;&#27861;&#26159;&#23558;2D&#29983;&#25104;&#22120;&#24494;&#35843;&#20026;&#22810;&#35270;&#35282;&#24863;&#30693;&#65292;&#21487;&#20197;&#24110;&#21161;&#28040;&#38500;&#20266;&#24433;&#65292;&#25110;&#32773;&#19982;&#37325;&#26500;&#32593;&#32476;&#32467;&#21512;&#65292;&#30452;&#25509;&#36755;&#20986;3D&#23545;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#36890;&#36807;&#32771;&#34385;&#35270;&#39057;&#32780;&#38750;&#22270;&#20687;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#22810;&#35270;&#35282;&#29983;&#25104;&#12290;&#32467;&#21512;&#19968;&#31181;&#20351;&#29992;&#39640;&#26031;&#24179;&#38138;&#30340;3D&#37325;&#26500;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#40065;&#26834;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#30452;&#25509;&#20174;&#29983;&#25104;&#30340;&#35270;&#22270;&#36755;&#20986;&#39640;&#36136;&#37327;&#30340;3D&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;IM-3D&#23558;2D&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#35745;&#31639;&#27425;&#25968;&#20943;&#23569;&#20102;10-100&#20493;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#27969;&#27700;&#32447;&#12289;&#26356;&#22909;&#30340;&#36136;&#37327;&#12289;&#26356;&#23569;&#30340;&#20960;&#20309;&#19981;&#19968;&#33268;&#24615;&#21644;&#26356;&#39640;&#30340;&#21487;&#29992;3D&#36164;&#20135;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08679</link><description>&lt;p&gt;
COLD-Attack: &#29992;&#20110;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#20855;&#26377;&#38544;&#31192;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;LLM&#36234;&#29425;&#12290;&#36890;&#36807;&#24314;&#31435;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#19982;&#25915;&#20987;&#29983;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#37319;&#29992;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#30340;&#25511;&#21046;&#35201;&#27714;&#19979;&#25628;&#32034;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#36234;&#29425;&#30340;&#27880;&#24847;&#21147;&#36234;&#26469;&#36234;&#22810;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#24615;&#65292;&#26377;&#24517;&#35201;&#32771;&#34385;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#36234;&#29425;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#20197;&#21450;&#24773;&#24863;/&#39118;&#26684;&#21464;&#21270;&#65292;&#22240;&#27492;&#30740;&#31350;&#21487;&#25511;&#24615;&#36234;&#29425;&#26159;&#26377;&#30410;&#30340;&#65292;&#21363;&#22914;&#20309;&#23545;LLM&#25915;&#20987;&#36827;&#34892;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#24418;&#24335;&#21270;&#20102;&#21487;&#25511;&#24615;&#25915;&#20987;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#19982;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20043;&#38388;&#30340;&#26032;&#22411;&#20851;&#32852;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;&#20027;&#39064;&#12290;&#22522;&#20110;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#33021;&#37327;&#38480;&#21046;&#35299;&#30721;&#19982;Langevin&#21160;&#21147;&#23398;&#65288;COLD&#65289;&#30340;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;COLD-Attack&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#19988;&#33258;&#21160;&#21270;&#22320;&#25628;&#32034;&#21508;&#31181;&#25511;&#21046;&#35201;&#27714;&#19979;&#30340;&#23545;&#25239;&#24615;LLM&#25915;&#20987;&#65292;&#20363;&#22914;&#27969;&#30021;&#24615;&#12289;&#38544;&#31192;&#24615;&#12289;&#24773;&#24863;&#21644;&#24038;&#21491;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jailbreaks on Large language models (LLMs) have recently received increasing attention. For a comprehensive assessment of LLM safety, it is essential to consider jailbreaks with diverse attributes, such as contextual coherence and sentiment/stylistic variations, and hence it is beneficial to study controllable jailbreaking, i.e. how to enforce control on LLM attacks. In this paper, we formally formulate the controllable attack generation problem, and build a novel connection between this problem and controllable text generation, a well-explored topic of natural language processing. Based on this connection, we adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD), a state-of-the-art, highly efficient algorithm in controllable text generation, and introduce the COLD-Attack framework which unifies and automates the search of adversarial LLM attacks under a variety of control requirements such as fluency, stealthiness, sentiment, and left-right-coherence. The controlla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08672</link><description>&lt;p&gt;
&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#22312;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Assessment and Selection under Temporal Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21512;&#25104;&#19981;&#21516;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#25968;&#25454;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#24403;&#21069;&#26102;&#26399;&#21644;&#21382;&#21490;&#26102;&#26399;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35780;&#20272;&#19982;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#30693;&#21644;&#21487;&#33021;&#20219;&#24847;&#30340;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#28378;&#21160;&#31383;&#21475;&#26041;&#27861;&#26469;&#20272;&#35745;&#32473;&#23450;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#36825;&#31181;&#31574;&#30053;&#36824;&#36890;&#36807;&#20272;&#35745;&#20004;&#20010;&#20505;&#36873;&#27169;&#22411;&#20043;&#38388;&#30340;&#27867;&#21270;&#35823;&#24046;&#24046;&#24322;&#26469;&#26041;&#20415;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20004;&#20004;&#27604;&#36739;&#25972;&#21512;&#21040;&#21333;&#22330;&#28120;&#27760;&#36187;&#20013;&#65292;&#20174;&#20505;&#36873;&#27169;&#22411;&#38598;&#21512;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#27169;&#22411;&#36873;&#25321;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#25968;&#25454;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.08671</link><description>&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#22312;&#21305;&#37197;&#23616;&#37096;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Semi-Dense Detector-Free Methods Good at Matching Local Features?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;SAM&#65292;&#24182;&#21457;&#29616;SAM&#22312;&#20301;&#23039;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20316;&#32773;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#31264;&#23494;&#26080;&#26816;&#27979;&#22120;&#26041;&#27861;&#65288;SDF&#65289;&#65292;&#22914;LoFTR&#65292;&#30446;&#21069;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#21305;&#37197;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;SDF&#26041;&#27861;&#34987;&#35757;&#32451;&#29992;&#20110;&#22312;&#20004;&#24133;&#22270;&#20687;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20960;&#20046;&#21482;&#20351;&#29992;&#30456;&#23545;&#20301;&#23039;&#20272;&#35745;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#22312;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#30340;&#33021;&#21147;&#21644;&#20272;&#35745;&#20301;&#23039;&#36136;&#37327;&#20043;&#38388;&#30340;&#32852;&#31995;&#24471;&#21040;&#30340;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#21305;&#37197;&#26550;&#26500;&#65288;SAM&#65289;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;MegaDepth&#21644;HPatches&#65289;&#19978;&#23637;&#31034;&#19968;&#20010;&#36870;&#30452;&#35273;&#30340;&#32467;&#26524;&#65306;&#19968;&#26041;&#38754;&#65292;SAM&#22312;&#20301;&#23039;/&#21333;&#24212;&#24615;&#20272;&#35745;&#25351;&#26631;&#26041;&#38754;&#35201;&#20040;&#20248;&#20110;SDF&#26041;&#27861;&#65292;&#35201;&#20040;&#19982;&#20043;&#30456;&#24403;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;SDF&#26041;&#27861;&#22312;&#21305;&#37197;&#20934;&#30830;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;SAM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#21305;&#37197;&#20934;&#30830;&#24230;&#30340;&#35745;&#31639;&#38480;&#21046;&#22312;&#32441;&#29702;&#21306;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Semi-dense detector-free approaches (SDF), such as LoFTR, are currently among the most popular image matching methods. While SDF methods are trained to establish correspondences between two images, their performances are almost exclusively evaluated using relative pose estimation metrics. Thus, the link between their ability to establish correspondences and the quality of the resulting estimated pose has thus far received little attention. This paper is a first attempt to study this link. We start with proposing a novel structured attention-based image matching architecture (SAM). It allows us to show a counter-intuitive result on two datasets (MegaDepth and HPatches): on the one hand SAM either outperforms or is on par with SDF methods in terms of pose/homography estimation metrics, but on the other hand SDF approaches are significantly better than SAM in terms of matching accuracy. We then propose to limit the computation of the matching accuracy to textured regions, and show that in
&lt;/p&gt;</description></item><item><title>Rec-GPT4V&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20316;&#20026;&#29992;&#25143;&#20559;&#22909;&#20449;&#24687;&#24182;&#32467;&#21512;&#22270;&#20687;&#25688;&#35201;&#21644;&#39033;&#30446;&#26631;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22270;&#20687;&#21160;&#24577;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.08670</link><description>&lt;p&gt;
Rec-GPT4V: &#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08670
&lt;/p&gt;
&lt;p&gt;
Rec-GPT4V&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20316;&#20026;&#29992;&#25143;&#20559;&#22909;&#20449;&#24687;&#24182;&#32467;&#21512;&#22270;&#20687;&#25688;&#35201;&#21644;&#39033;&#30446;&#26631;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22270;&#20687;&#21160;&#24577;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#21457;&#23637;&#20026;&#20256;&#32479;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#29087;&#32451;&#29702;&#35299;&#38745;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;LVLM&#22312;&#35813;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#21463;&#21040;&#20197;&#19979;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#30001;&#20110;LVLM&#26159;&#20174;&#22823;&#37327;&#36890;&#29992;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#24471;&#21040;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#29992;&#25143;&#20559;&#22909;&#30693;&#35782;&#12290;&#20854;&#27425;&#65292;&#22312;&#28041;&#21450;&#31163;&#25955;&#12289;&#22024;&#26434;&#21644;&#20887;&#20313;&#22270;&#20687;&#24207;&#21015;&#30340;&#24773;&#26223;&#20013;&#65292;LVLM&#22312;&#22788;&#29702;&#22810;&#20010;&#22270;&#20687;&#21160;&#24577;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rec-GPT4V&#30340;&#26032;&#39062;&#25512;&#29702;&#26041;&#26696;&#65306;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#35270;&#35273;&#25688;&#35201;&#24605;&#32500;&#65288;VST&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#29992;&#25143;&#21382;&#21490;&#20316;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20419;&#20351;LVLM&#29983;&#25104;&#39033;&#30446;&#22270;&#20687;&#25688;&#35201;&#65292;&#24182;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#21644;&#39033;&#30446;&#26631;&#39064;&#26469;&#26597;&#35810;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large vision-language models (LVLMs) offers the potential to address challenges faced by traditional multimodal recommendations thanks to their proficient understanding of static images and textual dynamics. However, the application of LVLMs in this field is still limited due to the following complexities: First, LVLMs lack user preference knowledge as they are trained from vast general datasets. Second, LVLMs suffer setbacks in addressing multiple image dynamics in scenarios involving discrete, noisy, and redundant image sequences. To overcome these issues, we propose the novel reasoning scheme named Rec-GPT4V: Visual-Summary Thought (VST) of leveraging large vision-language models for multimodal recommendation. We utilize user history as in-context user preferences to address the first challenge. Next, we prompt LVLMs to generate item image summaries and utilize image comprehension in natural language space combined with item titles to query the user preferences ov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.08658</link><description>&lt;p&gt;
&#26368;&#21518;&#30340;JITAI&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#25918;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#65306;&#22312;&#21069;&#30651;&#24615;&#24515;&#33039;&#24247;&#22797;&#29615;&#22659;&#20013;&#20419;&#36827;&#20307;&#32946;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#12290;JITAIs&#34987;&#35270;&#20026;&#21487;&#25345;&#32493;&#34892;&#20026;&#25913;&#21464;&#30340;&#20851;&#38190;&#26426;&#21046;&#65292;&#23558;&#24178;&#39044;&#25514;&#26045;&#26681;&#25454;&#20010;&#20307;&#30340;&#24403;&#21069;&#24773;&#22659;&#21644;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;JITAI&#23454;&#26045;&#20013;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#31649;&#29702;&#22810;&#21442;&#25968;&#31995;&#32479;&#22256;&#38590;&#20197;&#21450;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36890;&#36807;LLMs&#23454;&#29616;JITAI&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22312;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#20419;&#36827;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#30340;&#29616;&#20195;&#26368;&#39640;&#24615;&#33021;&#27169;&#22411;&#8220;GPT-4&#8221;&#30340;&#23454;&#20363;&#20316;&#20026;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;JITAIs&#30340;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24635;&#20849;450&#20010;&#24314;&#35758;&#30340;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message c
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;QuGAP-&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.08648</link><description>&lt;p&gt;
&#20026;&#37327;&#23376;&#20998;&#31867;&#22120;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Generating Universal Adversarial Perturbations for Quantum Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;QuGAP-&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#20316;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#33021;&#21147;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20687;&#32463;&#20856;&#30340;&#27169;&#22411;&#19968;&#26679;&#65292;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQC&#65289;&#30340;QML&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#22312;&#37327;&#23376;&#39046;&#22495;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#20102;&#23384;&#22312;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#65288;UAP&#65289;&#65292;&#24182;&#19988;&#24050;&#22312;&#37327;&#23376;&#20998;&#31867;&#22120;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuGAP&#65306;&#19968;&#20010;&#20026;&#37327;&#23376;&#20998;&#31867;&#22120;&#29983;&#25104;UAP&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;PQC&#30340;&#20998;&#31867;&#22120;&#30340;&#21152;&#24615;UAP&#30340;&#27010;&#24565;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#65288;QuGAP-A&#65289;&#26469;&#21046;&#36896;&#21152;&#24615;UAP&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#20102;&#37327;&#23376;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#31867;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#21387;&#32553;&#26041;&#27861;&#29983;&#25104;&#24186;&#27491;UAP&#30340;&#26032;&#26041;&#27861;&#65288;QuGAP-U&#65289;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML) has emerged as a promising field of research, aiming to leverage the capabilities of quantum computing to enhance existing machine learning methodologies. Recent studies have revealed that, like their classical counterparts, QML models based on Parametrized Quantum Circuits (PQCs) are also vulnerable to adversarial attacks. Moreover, the existence of Universal Adversarial Perturbations (UAPs) in the quantum domain has been demonstrated theoretically in the context of quantum classifiers. In this work, we introduce QuGAP: a novel framework for generating UAPs for quantum classifiers. We conceptualize the notion of additive UAPs for PQC-based classifiers and theoretically demonstrate their existence. We then utilize generative models (QuGAP-A) to craft additive UAPs and experimentally show that quantum classifiers are susceptible to such attacks. Moreover, we formulate a new method for generating unitary UAPs (QuGAP-U) using quantum generative models and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#27010;&#29575;&#21270;&#25551;&#36848;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08646</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#25512;&#29702;&#25277;&#35937;&#30340;&#32479;&#19968;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#27010;&#29575;&#21270;&#25551;&#36848;&#30340;&#32479;&#19968;&#35299;&#37322;&#65292;&#24182;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#23545;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#22823;&#33041;&#21151;&#33021;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27010;&#29575;&#21270;&#35299;&#37322;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#31526;&#21495;&#25512;&#29702;&#36827;&#34892;&#25551;&#36848;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#32463;&#39564;&#19978;&#30340;&#25512;&#29702;&#20851;&#31995;&#12289;&#26368;&#22823;&#19968;&#33268;&#38598;&#12289;&#26368;&#22823;&#21487;&#33021;&#38598;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23545;&#23427;&#20204;&#36827;&#34892;&#25551;&#36848;&#12290;&#36825;&#20010;&#29702;&#35770;&#20026;&#23454;&#29616;&#20154;&#31867;&#31867;&#20284;&#30340;&#26426;&#22120;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08640</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#22270;&#35889;&#19978;&#39044;&#27979;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Forecasting high-impact research topics via machine learning on evolving knowledge graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08640
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#26410;&#21457;&#24067;&#30740;&#31350;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30001;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#26500;&#24314;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#32467;&#21512;&#35770;&#25991;&#20869;&#23481;&#21644;&#21382;&#21490;&#24341;&#29992;&#30340;&#20449;&#24687;&#65292;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#26410;&#26469;&#30340;&#28436;&#21270;&#32593;&#32476;&#21160;&#24577;&#21644;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20986;&#29256;&#29289;&#30340;&#25351;&#25968;&#22686;&#38271;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26500;&#25104;&#20102;&#20005;&#23803;&#25361;&#25112;&#12290;&#23427;&#36843;&#20351;&#30740;&#31350;&#32773;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#26356;&#29421;&#31364;&#30340;&#23376;&#39046;&#22495;&#19978;&#65292;&#20351;&#24471;&#21457;&#29616;&#20854;&#20182;&#39046;&#22495;&#30340;&#26032;&#39062;&#19988;&#26377;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#24819;&#27861;&#21644;&#21512;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#34429;&#28982;&#26377;&#21150;&#27861;&#39044;&#27979;&#31185;&#23398;&#35770;&#25991;&#26410;&#26469;&#30340;&#24341;&#29992;&#27425;&#25968;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#31561;&#21040;&#30740;&#31350;&#23436;&#25104;&#24182;&#19988;&#35770;&#25991;&#20889;&#25104;&#21518;&#25165;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#26679;&#23601;&#38169;&#36807;&#20102;&#24819;&#27861;&#26500;&#24605;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#39044;&#27979;&#20174;&#26410;&#34987;&#30740;&#31350;&#32773;&#21457;&#24067;&#30340;&#24819;&#27861;&#30340;&#24433;&#21709;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;2100&#19975;&#31687;&#31185;&#23398;&#35770;&#25991;&#12290;&#23427;&#32467;&#21512;&#20102;&#20174;&#35770;&#25991;&#20869;&#23481;&#20013;&#21019;&#24314;&#30340;&#35821;&#20041;&#32593;&#32476;&#21644;&#20174;&#21382;&#21490;&#24341;&#29992;&#20013;&#21019;&#24314;&#30340;&#24433;&#21709;&#32593;&#32476;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#28436;&#21270;&#32593;&#32476;&#30340;&#21160;&#24577;&#24773;&#20917;&#65292;&#20174;&#32780;&#39044;&#27979;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#39044;&#26399;&#36825;&#31181;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#32773;&#21457;&#29616;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#21076;&#38500;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.08611</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Cost-Sensitive Transformer Model for Prognostics Under Highly Imbalanced Industrial Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#24230;&#19981;&#24179;&#34913;&#24037;&#19994;&#25968;&#25454;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#36890;&#36807;&#21076;&#38500;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#24471;&#30410;&#20110;&#20256;&#24863;&#22120;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#20351;&#24471;&#22823;&#37327;&#25968;&#25454;&#30340;&#25910;&#38598;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#39044;&#27979;&#26041;&#38754;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#22833;&#20540;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#25104;&#26412;&#25935;&#24863;&#24615;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#24212;&#29992;&#20256;&#32479;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#26412;&#25935;&#24863;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#23427;&#26159;&#20316;&#20026;&#19968;&#20010;&#31995;&#32479;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#65292;&#36824;&#25972;&#21512;&#20102;&#28151;&#21512;&#37325;&#37319;&#26679;&#22120;&#21644;&#22522;&#20110;&#22238;&#24402;&#30340;&#25554;&#34917;&#22120;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;Scania&#21345;&#36710;&#30340;APS&#25925;&#38556;&#25968;&#25454;&#38598;&#21644;SECOM&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21076;&#38500;&#23454;&#39564;&#26469;&#20998;&#26512;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#19981;&#21516;&#32452;&#20214;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid influx of data-driven models into the industrial sector has been facilitated by the proliferation of sensor technology, enabling the collection of vast quantities of data. However, leveraging these models for failure detection and prognosis poses significant challenges, including issues like missing values and class imbalances. Moreover, the cost sensitivity associated with industrial operations further complicates the application of conventional models in this context. This paper introduces a novel cost-sensitive transformer model developed as part of a systematic workflow, which also integrates a hybrid resampler and a regression-based imputer. After subjecting our approach to rigorous testing using the APS failure dataset from Scania trucks and the SECOM dataset, we observed a substantial enhancement in performance compared to state-of-the-art methods. Moreover, we conduct an ablation study to analyze the contributions of different components in our proposed method. Our fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08593</link><description>&lt;p&gt;
&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#65306;&#23454;&#26102;&#20174;&#20132;&#26131;&#22270;&#20013;&#25552;&#21462;&#22522;&#20110;&#23376;&#22270;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#22270;&#20013;&#30340;&#20856;&#22411;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#34987;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#29992;&#20110;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#65292;&#22914;&#27927;&#38065;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#24211;&#21033;&#29992;&#22810;&#26680;&#24182;&#34892;&#24615;&#65292;&#32500;&#25252;&#19968;&#20010;&#21160;&#24577;&#30340;&#20869;&#23384;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25366;&#25496;&#20256;&#20837;&#20132;&#26131;&#27969;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#27969;&#30340;&#26041;&#24335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21512;&#25104;&#21453;&#27927;&#38065;&#65288;AML&#65289;&#21644;&#30495;&#23454;&#30340;&#20197;&#22826;&#22346;&#38035;&#40060;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#38750;&#27861;&#20132;&#26131;&#30340;&#27604;&#20363;&#38750;&#24120;&#23567;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#25105;&#20204;&#30340;&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FESS Loss&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;Dice&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25552;&#39640;&#31354;&#38388;&#31934;&#24230;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08582</link><description>&lt;p&gt;
FESS Loss&#65306;&#29992;&#20110;&#20248;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
FESS Loss: Feature-Enhanced Spatial Segmentation Loss for Optimizing Medical Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FESS Loss&#30340;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;Dice&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25552;&#39640;&#31354;&#38388;&#31934;&#24230;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#23545;&#20110;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30740;&#31350;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#28041;&#21450;&#23558;&#22270;&#20687;&#21010;&#20998;&#20026;&#22810;&#20010;&#21306;&#22495;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#25110;&#30149;&#29702;&#32467;&#26500;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#31354;&#38388;&#31934;&#24230;&#21644;&#20840;&#38754;&#29305;&#24449;&#34920;&#31034;&#20043;&#38388;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#29305;&#24449;&#31354;&#38388;&#20998;&#21106;&#25439;&#22833;&#65288;FESS Loss&#65289;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#20248;&#21183;&#65288;&#22312;&#21307;&#23398;&#25104;&#20687;&#30340;&#24494;&#22937;&#39046;&#22495;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#29305;&#24449;&#65289;&#19982;Dice&#25439;&#22833;&#30340;&#31354;&#38388;&#20934;&#30830;&#24615;&#30456;&#32467;&#21512;&#12290;&#30446;&#26631;&#26159;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#22686;&#24378;&#31354;&#38388;&#31934;&#24230;&#21644;&#22522;&#20110;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;FESS Loss&#20195;&#34920;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#12289;&#26356;&#31934;&#32454;&#30340;&#20998;&#21106;&#36807;&#31243;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation is a critical process in the field of medical imaging, playing a pivotal role in diagnosis, treatment, and research. It involves partitioning of an image into multiple regions, representing distinct anatomical or pathological structures. Conventional methods often grapple with the challenge of balancing spatial precision and comprehensive feature representation due to their reliance on traditional loss functions. To overcome this, we propose Feature-Enhanced Spatial Segmentation Loss (FESS Loss), that integrates the benefits of contrastive learning (which extracts intricate features, particularly in the nuanced domain of medical imaging) with the spatial accuracy inherent in the Dice loss. The objective is to augment both spatial precision and feature-based representation in the segmentation of medical images. FESS Loss signifies a notable advancement, offering a more accurate and refined segmentation process, ultimately contributing to heightened precision i
&lt;/p&gt;</description></item><item><title>FedLPS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#21644;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08578</link><description>&lt;p&gt;
FedLPS: &#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
FedLPS: Heterogeneous Federated Learning for Multiple Tasks with Local Parameter Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08578
&lt;/p&gt;
&lt;p&gt;
FedLPS&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#21644;&#25552;&#39640;&#37096;&#32626;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#19978;&#20849;&#21516;&#20248;&#21270;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;FL&#36991;&#20813;&#20102;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#22686;&#24378;&#20102;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#23454;&#38469;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;FL&#20173;&#28982;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#12289;&#22810;&#20219;&#21153;&#37096;&#32626;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#20943;&#23569;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;FL&#35757;&#32451;&#25104;&#26412;&#65292;&#24573;&#30053;&#20102;&#22312;&#24322;&#26500;FL&#22330;&#26223;&#20013;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20855;&#26377;&#26412;&#22320;&#21442;&#25968;&#20849;&#20139;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65288;FedLPS&#65289;&#30340;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;FedLPS&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#21407;&#29702;&#65292;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#23454;&#29616;&#22810;&#20219;&#21153;&#37096;&#32626;&#65292;&#36890;&#36807;&#23558;&#26412;&#22320;&#27169;&#22411;&#21010;&#20998;&#20026;&#21487;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising solution in Edge Computing (EC) environments to process the proliferation of data generated by edge devices. By collaboratively optimizing the global machine learning models on distributed edge devices, FL circumvents the need for transmitting raw data and enhances user privacy. Despite practical successes, FL still confronts significant challenges including constrained edge device resources, multiple tasks deployment, and data heterogeneity. However, existing studies focus on mitigating the FL training costs of each single task whereas neglecting the resource consumption across multiple tasks in heterogeneous FL scenarios. In this paper, we propose Heterogeneous Federated Learning with Local Parameter Sharing (FedLPS) to fill this gap. FedLPS leverages principles from transfer learning to facilitate the deployment of multiple tasks on a single device by dividing the local model into a shareable encoder and task-specific encoders. To f
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#37319;&#38598;&#38381;&#28304;&#27169;&#22411;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#24320;&#28304;&#32534;&#30721;&#22120;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.08570</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Online Foundation Model Selection in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#22312;&#32447;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#37319;&#38598;&#38381;&#28304;&#27169;&#22411;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#24320;&#28304;&#32534;&#30721;&#22120;&#21644;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#19978;&#19979;&#25991;&#29305;&#24449;&#26469;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#21518;&#65292;&#22522;&#30784;&#27169;&#22411;&#26368;&#36817;&#25193;&#23637;&#21040;&#20102;&#26426;&#22120;&#20154;&#23398;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#33719;&#24471;&#65306;&#24320;&#28304;&#25110;&#20184;&#36153;&#30340;&#38381;&#28304;&#36873;&#39033;&#12290;&#29992;&#25143;&#21516;&#26102;&#26377;&#20004;&#31181;&#36873;&#25321;&#26102;&#65292;&#20182;&#20204;&#20250;&#38754;&#20020;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#26377;&#25928;&#20294;&#26114;&#36149;&#30340;&#38381;&#28304;&#27169;&#22411;&#21644;&#20813;&#36153;&#20294;&#21151;&#33021;&#36739;&#24369;&#30340;&#24320;&#28304;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#30001;&#20110;&#20174;&#38381;&#28304;&#27169;&#22411;&#20013;&#25910;&#38598;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#36739;&#39640;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#19978;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#21516;&#26102;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#23558;&#24320;&#28304;&#32534;&#30721;&#22120;&#19982;&#22788;&#29702;&#35813;&#19978;&#19979;&#25991;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#32534;&#30721;&#22120;&#23558;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#24067;&#25552;&#28860;&#25104;&#20302;&#32500;&#29305;&#24449;&#65292;&#21363;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have recently expanded into robotics after excelling in computer vision and natural language processing. The models are accessible in two ways: open-source or paid, closed-source options. Users with access to both face a problem when deciding between effective yet costly closed-source models and free but less powerful open-source alternatives. We call it the model selection problem. Existing supervised-learning methods are impractical due to the high cost of collecting extensive training data from closed-source models. Hence, we focus on the online learning setting where algorithms learn while collecting data, eliminating the need for large pre-collected datasets. We thus formulate a user-centric online model selection problem and propose a novel solution that combines an open-source encoder to output context and an online learning algorithm that processes this context. The encoder distills vast data distributions into low-dimensional features, i.e., the context, with
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#21322;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#21253;&#25324;&#20256;&#32479;&#29305;&#24449;&#21644;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30340;&#26694;&#26550;&#26469;&#32771;&#23519;21&#20010;&#39046;&#20808;&#30340;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#65292;&#24182;&#20998;&#26512;&#20102;11&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#12289;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.08565</link><description>&lt;p&gt;
&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Literature Reviews: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#20851;&#27880;&#20102;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#30340;&#21322;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#21253;&#25324;&#20256;&#32479;&#29305;&#24449;&#21644;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30340;&#26694;&#26550;&#26469;&#32771;&#23519;21&#20010;&#39046;&#20808;&#30340;&#25991;&#29486;&#32508;&#36848;&#24037;&#20855;&#65292;&#24182;&#20998;&#26512;&#20102;11&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#23398;&#26415;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#12289;&#20027;&#35201;&#30740;&#31350;&#25361;&#25112;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;SLR&#26159;&#19968;&#31181;&#20005;&#35880;&#26377;&#24207;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25972;&#21512;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#12290;&#35768;&#22810;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#36741;&#21161;&#21644;&#37096;&#20998;&#33258;&#21160;&#21270;SLR&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#35282;&#33394;&#26174;&#31034;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26356;&#26377;&#25928;&#25903;&#25345;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#26397;&#30528;&#25991;&#29486;&#32508;&#36848;&#30340;&#21322;&#33258;&#21160;&#21270;&#21019;&#24314;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;SLR&#30340;&#21322;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31579;&#36873;&#21644;&#25552;&#21462;&#38454;&#27573;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#23558;23&#20010;&#20256;&#32479;&#29305;&#24449;&#19982;11&#20010;&#20154;&#24037;&#26234;&#33021;&#29305;&#24449;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23545;21&#20010;&#39046;&#20808;&#30340;SLR&#24037;&#20855;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#29486;&#25628;&#32034;&#21644;&#36741;&#21161;&#23398;&#26415;&#20889;&#20316;&#30340;11&#20010;&#26368;&#26032;&#24037;&#20855;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36235;&#21183;&#65292;&#27010;&#36848;&#20102;&#20027;&#35201;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions f
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65288;MoLA&#65289;&#65292;&#29992;&#20110;Transformer-based&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#39640;&#23618;&#38656;&#35201;&#26356;&#22810;&#30340;LoRA&#19987;&#23478;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08562</link><description>&lt;p&gt;
Higher Layers Need More LoRA Experts
&lt;/p&gt;
&lt;p&gt;
Higher Layers Need More LoRA Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65288;MoLA&#65289;&#65292;&#29992;&#20110;Transformer-based&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#39640;&#23618;&#38656;&#35201;&#26356;&#22810;&#30340;LoRA&#19987;&#23478;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#25216;&#26415;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#20173;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#21162;&#21147;&#25972;&#21512;&#20102;LoRA&#21644;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#20197;&#25552;&#39640;PEFT&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#26377;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#25913;&#36827;&#24102;&#26377;MoE&#30340;LoRA&#30340;&#25928;&#29575;&#30340;&#30740;&#31350;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MoE&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#19987;&#23478;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#36824;&#23384;&#22312;&#19968;&#20123;&#20887;&#20313;&#12290;&#36825;&#20010;&#35770;&#26029;&#26159;&#21542;&#20063;&#36866;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;MoE&#26041;&#27861;&#65292;&#31216;&#20026;MoLA&#65288;\textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation&#65289;&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#27169;&#22411;&#23618;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;LoRA&#19987;&#23478;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#20855;&#26377;&#19981;&#21516;&#23618;&#32423;&#19987;&#23478;&#37197;&#32622;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#23545;&#20845;&#20010;&#30693;&#21517;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24120;&#35782;&#38382;&#31572;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PEFT) techniques like low-rank adaptation (LoRA) offer training efficiency on Large Language Models, but their impact on model performance remains limited. Recent efforts integrate LoRA and Mixture-of-Experts (MoE) to improve the performance of PEFT methods. Despite promising results, research on improving the efficiency of LoRA with MoE is still in its early stages. Recent studies have shown that experts in the MoE architecture have different strengths and also exhibit some redundancy. Does this statement also apply to parameter-efficient MoE? In this paper, we introduce a novel parameter-efficient MoE method, \textit{\textbf{M}oE-L\textbf{o}RA with \textbf{L}ayer-wise Expert \textbf{A}llocation (MoLA)} for Transformer-based models, where each model layer has the flexibility to employ a varying number of LoRA experts. We investigate several architectures with varying layer-wise expert configurations. Experiments on six well-known NLP and commonsense QA benc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#22797;&#20844;&#24179;&#20998;&#21106;&#38382;&#39064;&#20013;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#22914;&#26524;Bob&#36807;&#20110;&#20559;&#22909;&#26576;&#19968;&#22359;&#34507;&#31957;&#65292;Alice&#21033;&#29992;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#21487;&#20197;&#31995;&#32479;&#24615;&#22320;&#23545;Bob&#23454;&#26045;&#21093;&#21066;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#26356;&#22810;&#36164;&#28304;&#20221;&#39069;&#12290;</title><link>https://arxiv.org/abs/2402.08547</link><description>&lt;p&gt;
&#25581;&#31034;&#20102;&#22312;&#37325;&#22797;&#30340;&#34507;&#31957;&#20999;&#21106;&#20013;&#33521;&#21191;&#31454;&#20105;&#30340;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08547
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#37325;&#22797;&#20844;&#24179;&#20998;&#21106;&#38382;&#39064;&#20013;&#30340;&#31454;&#20105;&#31574;&#30053;&#65292;&#21457;&#29616;&#22914;&#26524;Bob&#36807;&#20110;&#20559;&#22909;&#26576;&#19968;&#22359;&#34507;&#31957;&#65292;Alice&#21033;&#29992;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#21487;&#20197;&#31995;&#32479;&#24615;&#22320;&#23545;Bob&#23454;&#26045;&#21093;&#21066;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#26356;&#22810;&#36164;&#28304;&#20221;&#39069;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#29609;&#23478;&#65292;&#20998;&#21035;&#20195;&#34920;Alice&#21644;Bob&#65292;&#36890;&#36807;&#23545;&#34507;&#31957;&#30340;&#20010;&#20154;&#20272;&#20540;&#36827;&#34892;&#20102;&#37325;&#22797;&#30340;&#20844;&#24179;&#20998;&#21106;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20250;&#20986;&#29616;&#19968;&#22359;&#19982;&#20043;&#21069;&#36718;&#27425;&#30456;&#21516;&#30340;&#26032;&#34507;&#31957;&#12290;Alice&#22312;&#33258;&#24049;&#36873;&#25321;&#30340;&#19968;&#20010;&#28857;&#19978;&#20999;&#21106;&#34507;&#31957;&#65292;&#32780;Bob&#36873;&#25321;&#24038;&#36793;&#30340;&#37096;&#20998;&#25110;&#21491;&#36793;&#30340;&#37096;&#20998;&#65292;&#25226;&#21097;&#20313;&#30340;&#32473;Alice&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#21464;&#31181;&#65306;&#39034;&#24207;&#27169;&#24335;&#65292;Bob&#22312;&#36873;&#25321;&#24038;/&#21491;&#20043;&#21069;&#35266;&#23519;Alice&#30340;&#20999;&#21106;&#28857;&#65307;&#21516;&#26102;&#27169;&#24335;&#65292;&#20182;&#21482;&#22312;&#20570;&#20986;&#36873;&#25321;&#21518;&#35266;&#23519;&#22905;&#30340;&#20999;&#21106;&#28857;&#12290;&#21516;&#26102;&#27169;&#24335;&#26159;&#30001;Aumann&#21644;Maschler&#65288;1995&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;Bob&#20960;&#20046;&#26159;&#30446;&#20809;&#30701;&#27973;&#30340;&#65292;&#24182;&#19988;&#32463;&#24120;&#36873;&#25321;&#33258;&#24049;&#21916;&#27426;&#30340;&#37096;&#20998;&#65292;&#37027;&#20040;&#20182;&#21487;&#20197;&#34987;Alice&#36890;&#36807;&#31867;&#20284;&#20110;&#20108;&#20998;&#26597;&#25214;&#30340;&#31574;&#30053;&#31995;&#32479;&#24615;&#22320;&#21033;&#29992;&#12290;&#36825;&#20010;&#31574;&#30053;&#20351;&#24471;Alice&#21487;&#20197;&#36234;&#26469;&#36234;&#31934;&#30830;&#22320;&#27169;&#25311;Bob&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#22312;&#26102;&#38388;&#19978;&#33719;&#24471;&#19981;&#25104;&#27604;&#20363;&#30340;&#36164;&#28304;&#20221;&#39069;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#29609;&#23478;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the setting of repeated fair division between two players, denoted Alice and Bob, with private valuations over a cake. In each round, a new cake arrives, which is identical to the ones in previous rounds. Alice cuts the cake at a point of her choice, while Bob chooses the left piece or the right piece, leaving the remainder for Alice. We consider two versions: sequential, where Bob observes Alice's cut point before choosing left/right, and simultaneous, where he only observes her cut point after making his choice. The simultaneous version was first considered by Aumann and Maschler (1995).   We observe that if Bob is almost myopic and chooses his favorite piece too often, then he can be systematically exploited by Alice through a strategy akin to a binary search. This strategy allows Alice to approximate Bob's preferences with increasing precision, thereby securing a disproportionate share of the resource over time.   We analyze the limits of how much a player can exploit t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.08530</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21518;&#32493;&#34920;&#31034;&#30340;&#20998;&#24067;&#24335;&#31867;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Distributional Analogue to the Successor Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#31163;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#26469;&#25551;&#36848;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#21518;&#26524;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23558;&#36716;&#25442;&#32467;&#26500;&#21644;&#22870;&#21169;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#19982;&#21518;&#32493;&#34920;&#31034;&#65288;SR&#65289;&#25551;&#36848;&#25353;&#29031;&#32473;&#23450;&#31574;&#30053;&#34892;&#20026;&#30340;&#26399;&#26395;&#21518;&#26524;&#31867;&#20284;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#21518;&#32487;&#24230;&#37327;&#65288;SM&#65289;&#25551;&#36848;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#20998;&#24067;&#24335;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20998;&#24067;&#24335;SM&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#24067;&#30340;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#20998;&#24067;&#24335;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20998;&#24067;&#24335;SM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#23618;&#27425;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#19968;&#20123;&#29420;&#31435;&#26377;&#20215;&#20540;&#30340;&#23398;&#20064;&#29366;&#24577;&#29983;&#25104;&#27169;&#22411;&#30340;&#31639;&#27861;&#25216;&#26415;&#12290;&#20316;&#20026;&#20998;&#24067;&#24335;SM&#26377;&#29992;&#24615;&#30340;&#20363;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20351;&#24471;&#38646;&#26679;&#26412;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#22312;&#20197;&#21069;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi
&lt;/p&gt;</description></item><item><title>&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#26469;&#23545;&#24433;&#21709;&#36827;&#34892;&#24418;&#24335;&#21270;&#30340;&#29305;&#24449;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08514</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Influence in Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08514
&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#38382;&#39064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#26469;&#23545;&#24433;&#21709;&#36827;&#34892;&#24418;&#24335;&#21270;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;MDP&#36335;&#24452;&#964;&#65292;&#36825;&#31181;&#25512;&#29702;&#20801;&#35768;&#25105;&#20204;&#24471;&#20986;&#25551;&#36848;&#964;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#964;'&#65292;&#25551;&#36848;&#20102;&#22312;&#19982;&#964;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#20316;&#24207;&#21015;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#964;&#30340;&#8220;&#22914;&#26524;&#26159;&#36825;&#31181;&#24773;&#20917;&#8221;&#30340;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21453;&#20107;&#23454;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#38543;&#26102;&#38388;&#21457;&#29983;&#20559;&#31163;&#65292;&#35266;&#23519;&#964;&#21487;&#33021;&#19981;&#20877;&#24433;&#21709;&#21453;&#20107;&#23454;&#19990;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#20998;&#26512;&#19981;&#20877;&#38024;&#23545;&#20010;&#20307;&#35266;&#23519;&#32467;&#26524;&#65292;&#32780;&#26159;&#20135;&#29983;&#24178;&#39044;&#24615;&#32467;&#26524;&#32780;&#38750;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#29305;&#21035;&#24433;&#21709;&#27969;&#34892;&#30340;Gumbel-max&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#36825;&#31181;&#27169;&#22411;&#29992;&#20110;MDP&#21453;&#20107;&#23454;&#65292;&#20294;&#30452;&#21040;&#29616;&#22312;&#19968;&#30452;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27604;&#36739;&#21453;&#20107;&#23454;&#21644;&#24178;&#39044;&#20998;&#24067;&#30340;&#24433;&#21709;&#30340;&#24418;&#24335;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#26500;&#24314;&#21453;&#20107;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work addresses a fundamental problem in the context of counterfactual inference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, this kind of inference allows us to derive counterfactual paths $\tau'$ describing what-if versions of $\tau$ obtained under different action sequences than those observed in $\tau$. However, as the counterfactual states and actions deviate from the observed ones over time, the observation $\tau$ may no longer influence the counterfactual world, meaning that the analysis is no longer tailored to the individual observation, resulting in interventional outcomes rather than counterfactual ones. Even though this issue specifically affects the popular Gumbel-max structural causal model used for MDP counterfactuals, it has remained overlooked until now. In this work, we introduce a formal characterisation of influence based on comparing counterfactual and interventional distributions. We devise an algorithm to construct counterfactual models that
&lt;/p&gt;</description></item><item><title>AmEx-MCTS introduces a novel formulation by decoupling value updates, visit count updates, and the selected path in Monte-Carlo tree search, allowing exclusion of already explored regions. This enables a broader search with the same computational resources while maintaining the utility of visit counts for exploration-exploitation balancing and quality metrics within MCTS.</title><link>https://arxiv.org/abs/2402.08511</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#27880;&#20110;&#26410;&#30693;&#39046;&#22495;&#26469;&#25918;&#22823;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Amplifying Exploration in Monte-Carlo Tree Search by Focusing on the Unknown
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08511
&lt;/p&gt;
&lt;p&gt;
AmEx-MCTS introduces a novel formulation by decoupling value updates, visit count updates, and the selected path in Monte-Carlo tree search, allowing exclusion of already explored regions. This enables a broader search with the same computational resources while maintaining the utility of visit counts for exploration-exploitation balancing and quality metrics within MCTS.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#26377;&#25928;&#30340;&#23454;&#26102;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26377;&#31574;&#30053;&#22320;&#20998;&#37197;&#32473;&#25628;&#32034;&#26641;&#20013;&#26377;&#24076;&#26395;&#30340;&#37096;&#20998;&#65292;&#20351;&#24471;&#23427;&#25104;&#20026;&#22823;&#35268;&#27169;&#25628;&#32034;&#31354;&#38388;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20173;&#28982;&#23384;&#22312;&#26368;&#26377;&#24076;&#26395;&#30340;&#36335;&#24452;&#26102;&#65292;MCTS&#32463;&#24120;&#22312;&#37325;&#26032;&#35780;&#20272;&#20043;&#21069;&#25506;&#32034;&#36807;&#30340;&#21306;&#22495;&#19978;&#28040;&#32791;&#26377;&#38480;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AmEx-MCTS&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;MCTS&#20844;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;AmEx-MCTS&#30340;&#26680;&#24515;&#26159;&#22312;&#26641;&#25628;&#32034;&#36807;&#31243;&#20013;&#35299;&#32806;&#20540;&#26356;&#26032;&#12289;&#35775;&#38382;&#35745;&#25968;&#26356;&#26032;&#21644;&#36873;&#23450;&#36335;&#24452;&#65292;&#20174;&#32780;&#20351;&#24471;&#24050;&#25506;&#32034;&#30340;&#23376;&#26641;&#25110;&#21494;&#33410;&#28857;&#21487;&#20197;&#34987;&#25490;&#38500;&#12290;&#36825;&#31181;&#20998;&#31163;&#20445;&#25345;&#20102;&#35775;&#38382;&#35745;&#25968;&#22312;MCTS&#20013;&#29992;&#20110;&#25506;&#32034;-&#24320;&#21457;&#24179;&#34913;&#21644;&#36136;&#37327;&#25351;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;&#32467;&#26524;&#22686;&#24378;&#20102;&#20351;&#29992;&#30456;&#21516;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#26356;&#24191;&#27867;&#25628;&#32034;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;MCTS&#30340;&#22522;&#26412;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast amount of applications. It strategically allocates computational resources to focus on promising segments of the search tree, making it a very attractive search algorithm in large search spaces. However, it often expends its limited resources on reevaluating previously explored regions when they remain the most promising path. Our proposed methodology, denoted as AmEx-MCTS, solves this problem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the decoupling of value updates, visit count updates, and the selected path during the tree search, thereby enabling the exclusion of already explored subtrees or leaves. This segregation preserves the utility of visit counts for both exploration-exploitation balancing and quality metrics within MCTS. The resultant augmentation facilitates in a considerably broader search using identical computational resources, preserving the essential characteristics of M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25512;&#26029;&#32473;&#23450;SPARQL CONSTRUCT&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#36755;&#20986;&#22270;&#24418;&#19978;&#25104;&#31435;&#30340;&#24418;&#29366;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#36755;&#20837;&#22270;&#30340;&#24418;&#29366;&#32422;&#26463;&#21644;&#26597;&#35810;&#27169;&#26495;&#21487;&#33021;&#26045;&#21152;&#30340;&#26032;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.08509</link><description>&lt;p&gt;
&#20174;&#24418;&#29366;&#21040;&#24418;&#29366;&#65306;&#25512;&#29702;SPARQL CONSTRUCT&#26597;&#35810;&#32467;&#26524;&#30340;SHACL&#24418;&#29366;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
From Shapes to Shapes: Inferring SHACL Shapes for Results of SPARQL CONSTRUCT Queries (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25512;&#26029;&#32473;&#23450;SPARQL CONSTRUCT&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#36755;&#20986;&#22270;&#24418;&#19978;&#25104;&#31435;&#30340;&#24418;&#29366;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#36755;&#20837;&#22270;&#30340;&#24418;&#29366;&#32422;&#26463;&#21644;&#26597;&#35810;&#27169;&#26495;&#21487;&#33021;&#26045;&#21152;&#30340;&#26032;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SPARQL CONSTRUCT&#26597;&#35810;&#20801;&#35768;&#25351;&#23450;&#25968;&#25454;&#22788;&#29702;&#27969;&#27700;&#32447;&#65292;&#23558;&#32473;&#23450;&#30340;&#36755;&#20837;&#22270;&#24418;&#36716;&#25442;&#20026;&#26032;&#30340;&#36755;&#20986;&#22270;&#24418;&#12290;&#36890;&#36807;SHACL&#24418;&#29366;&#23545;&#22270;&#36827;&#34892;&#32422;&#26463;&#29616;&#22312;&#24050;&#32463;&#24456;&#24120;&#35265;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20102;&#35299;&#20182;&#20204;&#21487;&#20197;&#39044;&#26399;&#21738;&#20123;&#25968;&#25454;&#20197;&#21450;&#19981;&#20250;&#26377;&#21738;&#20123;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#30693;&#36947;&#29305;&#23450;&#30340;&#36755;&#20837;&#25968;&#25454;&#65292;&#22312;&#25968;&#25454;&#22788;&#29702;&#27969;&#27700;&#32447;&#32467;&#26463;&#26102;&#20102;&#35299;&#21738;&#20123;&#22270;&#25968;&#25454;&#21487;&#20197;&#39044;&#26399;&#23558;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#36755;&#20837;&#22270;&#30340;&#24418;&#29366;&#32422;&#26463;&#21487;&#33021;&#20250;&#24433;&#21709;&#36755;&#20986;&#22270;&#65292;&#20294;&#21487;&#33021;&#19981;&#20877;&#30452;&#25509;&#36866;&#29992;&#65292;&#24182;&#19988;&#26597;&#35810;&#27169;&#26495;&#21487;&#33021;&#20250;&#26045;&#21152;&#26032;&#30340;&#24418;&#29366;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#20110;&#32473;&#23450;&#30340;SPARQL CONSTRUCT&#26597;&#35810;&#30340;&#25152;&#26377;&#21487;&#33021;&#36755;&#20986;&#22270;&#24418;&#19978;&#25104;&#31435;&#30340;&#24418;&#29366;&#32422;&#26463;&#30340;&#25512;&#23548;&#12290;&#25105;&#20204;&#20551;&#35774;SPARQL CONSTRUCT&#26597;&#35810;&#26159;&#22266;&#23450;&#30340;&#65292;&#20363;&#22914;&#20316;&#20026;&#19968;&#20010;&#31243;&#24207;&#30340;&#19968;&#37096;&#20998;&#65292;&#32780;&#36755;&#20837;&#22270;&#36981;&#23432;&#36755;&#20837;&#24418;&#29366;&#32422;&#26463;&#65292;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#19988;&#22240;&#27492;&#22823;&#22810;&#25968;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;SPARQL CONSTRUCT&#26597;&#35810;&#30340;&#19968;&#20010;&#29255;&#27573;&#65288;SCCQ&#65289;&#21644;&#19968;&#20010;&#29255;&#27573;&#30340;S
&lt;/p&gt;
&lt;p&gt;
SPARQL CONSTRUCT queries allow for the specification of data processing pipelines that transform given input graphs into new output graphs. It is now common to constrain graphs through SHACL shapes allowing users to understand which data they can expect and which not. However, it becomes challenging to understand what graph data can be expected at the end of a data processing pipeline without knowing the particular input data: Shape constraints on the input graph may affect the output graph, but may no longer apply literally, and new shapes may be imposed by the query template. In this paper, we study the derivation of shape constraints that hold on all possible output graphs of a given SPARQL CONSTRUCT query. We assume that the SPARQL CONSTRUCT query is fixed, e.g., being part of a program, whereas the input graphs adhere to input shape constraints but may otherwise vary over time and, thus, are mostly unknown. We study a fragment of SPARQL CONSTRUCT queries (SCCQ) and a fragment of S
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20351;&#29992;&#27874;&#22763;&#39039;&#32928;&#20934;&#22791;&#35780;&#20998;&#65288;BBPS&#65289;&#36827;&#34892;&#32467;&#32928;&#38236;&#35780;&#20272;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20854;&#20934;&#30830;&#29575;&#36739;&#20869;&#38236;&#21307;&#29983;&#20302;&#65292;&#20294;&#20173;&#26377;&#25104;&#20026;&#36741;&#21161;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08492</link><description>&lt;p&gt;
ChatGPT&#22312;&#22238;&#31572;&#19982;&#27874;&#22763;&#39039;&#32928;&#20934;&#22791;&#35780;&#20998;&#30456;&#20851;&#38382;&#39064;&#26102;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Application of ChatGPT in Responding to Questions Related to the Boston Bowel Preparation Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20351;&#29992;&#27874;&#22763;&#39039;&#32928;&#20934;&#22791;&#35780;&#20998;&#65288;BBPS&#65289;&#36827;&#34892;&#32467;&#32928;&#38236;&#35780;&#20272;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20854;&#20934;&#30830;&#29575;&#36739;&#20869;&#38236;&#21307;&#29983;&#20302;&#65292;&#20294;&#20173;&#26377;&#25104;&#20026;&#36741;&#21161;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#32467;&#32928;&#38236;&#26816;&#26597;&#26159;&#32963;&#32928;&#23398;&#20013;&#30340;&#20851;&#38190;&#35786;&#26029;&#24037;&#20855;&#65292;&#20005;&#37325;&#20381;&#36182;&#33391;&#22909;&#30340;&#32928;&#20934;&#22791;&#12290;ChatGPT&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#20852;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20063;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#20351;&#29992;&#27874;&#22763;&#39039;&#32928;&#20934;&#22791;&#35780;&#20998;&#65288;BBPS&#65289;&#36827;&#34892;&#32467;&#32928;&#38236;&#35780;&#20272;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#22238;&#39038;&#24615;&#25910;&#38598;&#20102;2020&#24180;&#33267;2023&#24180;&#38388;&#30340;233&#20010;&#32467;&#32928;&#38236;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#30001;3&#21517;&#36164;&#28145;&#20869;&#38236;&#21307;&#29983;&#21644;3&#21517;&#21021;&#32423;&#20869;&#38236;&#21307;&#29983;&#20351;&#29992;BBPS&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;ChatGPT&#20063;&#23545;&#36825;&#20123;&#22270;&#20687;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20998;&#20026;&#19977;&#32452;&#24182;&#36827;&#34892;&#20102;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#36890;&#36807;&#20004;&#36718;&#27979;&#35797;&#35780;&#20272;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#65306;&#22312;&#21021;&#22987;&#36718;&#20013;&#65292;ChatGPT&#30340;&#20934;&#30830;&#29575;&#22312;48.93%&#21644;62.66%&#20043;&#38388;&#21464;&#21270;&#65292;&#20302;&#20110;&#20869;&#38236;&#21307;&#29983;&#30340;&#20934;&#30830;&#29575;&#65288;76.68%&#21040;77.83%&#65289;&#12290;ChatGPT&#30340;Kappa&#20540;&#20026;0.52&#21040;0.53&#65292;&#32780;&#20869;&#38236;&#21307;&#29983;&#30340;Kappa&#20540;&#20026;0.75&#21040;0.87&#12290;&#32467;&#35770;&#65306;&#23613;&#31649;ChatGPT&#22312;&#20351;&#29992;BBPS&#36827;&#34892;&#32467;&#32928;&#38236;&#35780;&#20272;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#20294;&#23427;&#20173;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#36741;&#21161;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Colonoscopy, a crucial diagnostic tool in gastroenterology, depends heavily on superior bowel preparation. ChatGPT, a large language model with emergent intelligence which also exhibits potential in medical applications. This study aims to assess the accuracy and consistency of ChatGPT in using the Boston Bowel Preparation Scale (BBPS) for colonoscopy assessment. Methods: We retrospectively collected 233 colonoscopy images from 2020 to 2023. These images were evaluated using the BBPS by 3 senior endoscopists and 3 novice endoscopists. Additionally, ChatGPT also assessed these images, having been divided into three groups and undergone specific Fine-tuning. Consistency was evaluated through two rounds of testing. Results: In the initial round, ChatGPT's accuracy varied between 48.93% and 62.66%, trailing the endoscopists' accuracy of 76.68% to 77.83%. Kappa values for ChatGPT was between 0.52 and 0.53, compared to 0.75 to 0.87 for the endoscopists. Conclusion: While ChatGPT 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08491</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#30340;&#24067;&#23572;&#27169;&#22411;&#21560;&#24341;&#23376;&#26223;&#35266;&#20013;&#30340;&#25511;&#21046;&#36941;&#21382;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#21487;&#29992;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#19981;&#21516;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#28287;&#23454;&#39564;&#21457;&#29616;&#37325;&#32534;&#31243;&#31574;&#30053;&#30340;&#25928;&#29575;&#21463;&#21040;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#20415;&#24110;&#21161;&#35782;&#21035;&#37325;&#32534;&#31243;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#26694;&#26550;&#30340;BNs&#21644;PBNs&#20197;&#21450;&#24322;&#27493;&#26356;&#26032;&#27169;&#24335;&#19979;&#21046;&#23450;&#20102;&#19968;&#20010;&#25511;&#21046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20266;&#21560;&#24341;&#23376;&#29366;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#19978;&#26377;&#36229;&#36807;99%&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#21364;&#23436;&#20840;&#22833;&#36133;&#12290;&#36890;&#36807;&#32447;&#24615;&#36817;&#20284;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08473</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#31995;&#32479;&#24615;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;Transformer&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#36259;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#19978;&#26377;&#36229;&#36807;99%&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#21364;&#23436;&#20840;&#22833;&#36133;&#12290;&#36890;&#36807;&#32447;&#24615;&#36817;&#20284;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#37322;&#36825;&#20123;&#24046;&#24322;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;(&#38646;&#26679;&#26412;)&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#36825;&#20123;&#27169;&#22411;&#30446;&#21069;&#23578;&#26410;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#12290;&#34429;&#28982;&#25506;&#27979;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#29305;&#23450;&#23646;&#24615;&#65292;&#20294;&#34920;&#31034;&#31354;&#38388;&#30340;&#32467;&#26500;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#24615;&#30340;&#21051;&#30011;&#65307;&#22240;&#27492;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#26679;&#30340;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21644;&#36807;&#24230;&#25512;&#24191;&#21040;&#36229;&#20986;&#25968;&#25454;&#38598;&#33539;&#22260;&#30340;&#26032;&#36755;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#25506;&#32034;&#24120;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;Imagenette&#25968;&#25454;&#38598;&#34920;&#26126;&#65292;&#23613;&#31649;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;&#36229;&#36807;99%&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#20013;&#23436;&#20840;&#22833;&#36133;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#36817;&#20284;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#20123;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#21478;&#19968;&#20010;&#27169;&#22411;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20197;&#25903;&#25345;&#36825;&#20010;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets. However, these models are poorly understood due to their complexity and size. While probing-based methods are widely used to understand specific properties, the structures of the representation space are not systematically characterized; consequently, it is unclear how such models generalize and overgeneralize to new inputs beyond datasets. In this paper, based on a new gradient descent optimization method, we are able to explore the embedding space of a commonly used vision-language model. Using the Imagenette dataset, we show that while the model achieves over 99\% zero-shot classification performance, it fails systematic evaluations completely. Using a linear approximation, we provide a framework to explain the striking differences. We have also obtained similar results using a different model to support that o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;STNWeb&#20013;&#65292;&#23637;&#31034;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20840;&#38754;&#30340;&#25253;&#21578;&#21644;&#22270;&#34920;&#65292;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#38477;&#20302;&#20351;&#29992;&#35813;&#24037;&#20855;&#30340;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2402.08472</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#20998;&#26512;&#20248;&#21270;&#31639;&#27861;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for the Automated Analysis of Optimization Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;STNWeb&#20013;&#65292;&#23637;&#31034;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20840;&#38754;&#30340;&#25253;&#21578;&#21644;&#22270;&#34920;&#65292;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#38477;&#20302;&#20351;&#29992;&#35813;&#24037;&#20855;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21644;&#20195;&#30721;&#30340;&#33021;&#21147;&#20351;&#20854;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;STNWeb&#20013;&#65292;&#23637;&#31034;&#20854;&#22312;&#20248;&#21270;&#31639;&#27861;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;STNWeb&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#25628;&#32034;&#36712;&#36857;&#32593;&#32476;&#65288;STNs&#65289;&#30340;&#22522;&#20110;Web&#30340;&#24037;&#20855;&#65292;STNs&#26159;&#20248;&#21270;&#31639;&#27861;&#34892;&#20026;&#30340;&#21487;&#35270;&#21270;&#12290;&#34429;&#28982;STNWeb&#29983;&#25104;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#23545;&#20110;&#31639;&#27861;&#35774;&#35745;&#32773;&#26469;&#35828;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#26159;&#24448;&#24448;&#38656;&#35201;&#19968;&#23450;&#30340;&#20808;&#39564;&#30693;&#35782;&#25165;&#33021;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#30693;&#35782;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;LLMs&#65288;&#29305;&#21035;&#26159;GPT-4&#65289;&#25972;&#21512;&#21040;STNWeb&#20013;&#65292;&#29983;&#25104;&#20840;&#38754;&#30340;&#20070;&#38754;&#25253;&#21578;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#22270;&#34920;&#65292;&#20174;&#32780;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#24182;&#20943;&#23569;&#30740;&#31350;&#30028;&#20351;&#29992;&#35813;&#24037;&#20855;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#20248;&#21270;&#31038;&#21306;&#20013;&#30340;&#20854;&#20182;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of Large Language Models (LLMs) to generate high-quality text and code has fuelled their rise in popularity. In this paper, we aim to demonstrate the potential of LLMs within the realm of optimization algorithms by integrating them into STNWeb. This is a web-based tool for the generation of Search Trajectory Networks (STNs), which are visualizations of optimization algorithm behavior. Although visualizations produced by STNWeb can be very informative for algorithm designers, they often require a certain level of prior knowledge to be interpreted. In an attempt to bridge this knowledge gap, we have incorporated LLMs, specifically GPT-4, into STNWeb to produce extensive written reports, complemented by automatically generated plots, thereby enhancing the user experience and reducing the barriers to the adoption of this tool by the research community. Moreover, our approach can be expanded to other tools from the optimization community, showcasing the versatility and potential
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21451;&#22909;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;&#65292;&#24182;&#25552;&#20379;&#22312;&#32447;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.08470</link><description>&lt;p&gt;
&#38754;&#21521;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#30340;&#24182;&#34892;&#21451;&#22909;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel-friendly Spatio-Temporal Graph Learning for Photovoltaic Degradation Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#21451;&#22909;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35268;&#27169;&#21270;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;&#65292;&#24182;&#25552;&#20379;&#22312;&#32447;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#36235;&#21183;&#20998;&#26512;&#26041;&#27861;(ST-GTrend)&#65292;&#29992;&#20110;&#36827;&#34892;&#20809;&#20239;&#30005;&#21147;&#32593;&#32476;&#30340;&#38598;&#32676;&#32423;&#24615;&#33021;&#34928;&#20943;&#20998;&#26512;&#12290;&#20809;&#20239;&#30005;&#31449;&#24050;&#25104;&#20026;&#20840;&#29699;&#21487;&#25345;&#32493;&#33021;&#28304;&#29983;&#20135;&#26684;&#23616;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20934;&#30830;&#20272;&#35745;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#23545;&#20110;&#20854;&#20316;&#20026;&#21457;&#30005;&#25216;&#26415;&#21644;&#37329;&#34701;&#36164;&#20135;&#30340;&#21487;&#34892;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#35780;&#20272;&#20809;&#20239;&#31995;&#32479;&#30340;&#27700;&#24179;&#21270;&#33021;&#28304;&#25104;&#26412;(LCOE)&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20102;&#35299;&#21644;&#20272;&#35745;&#22823;&#35268;&#27169;&#20809;&#20239;&#36870;&#21464;&#22120;&#30340;&#38271;&#26399;&#24615;&#33021;&#25439;&#22833;&#29575;(PLR)&#12290;ST-GTrend&#38598;&#25104;&#20102;&#26102;&#31354;&#19968;&#33268;&#24615;&#21644;&#22270;&#27880;&#24847;&#21147;&#65292;&#23558;PLR&#20316;&#20026;&#38271;&#26399;&#30340;&#8220;&#32769;&#21270;&#8221;&#36235;&#21183;&#19982;&#20809;&#20239;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22810;&#20010;&#27874;&#21160;&#39033;&#20998;&#31163;&#24320;&#26469;&#12290;&#20026;&#20102;&#24212;&#23545;&#26102;&#24207;&#20013;&#22810;&#26679;&#30340;&#34928;&#20943;&#27169;&#24335;&#65292;ST-GTrend&#37319;&#29992;&#24182;&#34892;&#22270;&#33258;&#32534;&#30721;&#22120;&#25968;&#32452;&#21516;&#26102;&#25552;&#21462;&#32769;&#21270;&#21644;&#27874;&#21160;&#39033;&#12290;ST-GTrend&#21457;&#24067;&#20102;&#19968;&#20010;&#22312;&#32447;&#20809;&#20239;&#34928;&#20943;&#20998;&#26512;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25913;&#36827;&#20809;&#20239;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Spatio-Temporal Graph Neural Network empowered trend analysis approach (ST-GTrend) to perform fleet-level performance degradation analysis for Photovoltaic (PV) power networks. PV power stations have become an integral component to the global sustainable energy production landscape. Accurately estimating the performance of PV systems is critical to their feasibility as a power generation technology and as a financial asset. One of the most challenging problems in assessing the Levelized Cost of Energy (LCOE) of a PV system is to understand and estimate the long-term Performance Loss Rate (PLR) for large fleets of PV inverters. ST-GTrend integrates spatio-temporal coherence and graph attention to separate PLR as a long-term "aging" trend from multiple fluctuation terms in the PV input data. To cope with diverse degradation patterns in timeseries, ST-GTrend adopts a paralleled graph autoencoder array to extract aging and fluctuation terms simultaneously. ST-GTrend impo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08466</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#22521;&#35757;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#31867;&#24341;&#23548;&#19982;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#20851;&#21361;&#23475;&#26356;&#24378;&#22823;&#30340;&#27835;&#29702;&#30340;&#28909;&#24773;&#21628;&#22768;&#27491;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#24341;&#36215;&#31649;&#29702;&#23398;&#32773;&#25152;&#31216;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#26041;&#27861;&#30340;&#37319;&#29992;&#12290;&#32654;&#22269;&#21644;&#27431;&#27954;&#30340;&#26368;&#26032;&#20513;&#35758;&#20197;&#21450;&#22269;&#38469;&#26631;&#20934;&#21270;&#32452;&#32455;&#37319;&#32435;&#30340;&#37325;&#35201;&#33258;&#25105;&#30417;&#31649;&#26631;&#20934;&#37117;&#20849;&#21516;&#20855;&#26377;&#19968;&#20010;&#26680;&#24515;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#22522;&#20110;&#31649;&#29702;&#30340;&#20513;&#35758;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#20154;&#31867;&#23545;AI&#24037;&#20855;&#30340;&#22521;&#35757;&#21644;&#24320;&#21457;&#30340;&#30417;&#30563;&#26469;&#28608;&#21169;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20010;&#26032;&#20852;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#33539;&#24335;&#26102;&#20195;&#20013;&#65292;&#38656;&#35201;&#23545;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#25216;&#26415;&#36827;&#34892;&#23436;&#21892;&#21644;&#31995;&#32479;&#21270;&#12290;&#22914;&#26524;&#35748;&#30495;&#23545;&#24453;&#65292;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#21487;&#20197;&#20943;&#36731;&#19968;&#20123;&#23545;AI&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#21387;&#21147;&#65292;&#20197;&#20154;&#31867;&#30452;&#35273;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#24182;&#26356;&#22909;&#22320;&#28385;&#36275;&#23545;&#20844;&#24179;&#24615;&#21644;&#26377;&#25928;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#22270;&#29983;&#25104;&#30340;&#35282;&#24230;&#26159;&#21542;&#23384;&#22312;&#33021;&#22815;&#21306;&#20998;&#25152;&#26377;3D&#22270;&#24418;&#30340;k&#65292;&#20197;&#21450;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;3D&#22270;&#24418;&#65292;k-WL&#30340;&#21516;&#26500;&#21028;&#21035;&#33021;&#21147;&#26159;&#21542;&#20005;&#26684;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2402.08429</link><description>&lt;p&gt;
&#33021;&#22815;&#21306;&#20998;&#25152;&#26377;3D&#22270;&#24418;&#30340;&#26159;3-&#65288;F&#65289;WL&#26041;&#27861;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is 3-(F)WL Enough to Distinguish All 3D Graphs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#22270;&#29983;&#25104;&#30340;&#35282;&#24230;&#26159;&#21542;&#23384;&#22312;&#33021;&#22815;&#21306;&#20998;&#25152;&#26377;3D&#22270;&#24418;&#30340;k&#65292;&#20197;&#21450;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;3D&#22270;&#24418;&#65292;k-WL&#30340;&#21516;&#26500;&#21028;&#21035;&#33021;&#21147;&#26159;&#21542;&#20005;&#26684;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21516;&#26500;&#38382;&#39064;&#26159;&#22270;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#65306;&#20998;&#26512;&#20004;&#20010;&#21270;&#23398;&#20998;&#23376;&#30340;&#30456;&#20284;&#24615;&#65292;&#25110;&#32773;&#30740;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;WL&#27979;&#35797;&#26159;&#19968;&#31181;&#21028;&#26029;&#20004;&#20010;&#22270;&#26159;&#21542;&#21516;&#26500;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#19981;&#33021;&#21306;&#20998;&#25152;&#26377;&#38750;&#21516;&#26500;&#22270;&#12290;&#20316;&#20026;WL&#30340;&#25913;&#36827;&#65292;k-WL&#20855;&#26377;&#26356;&#24378;&#30340;&#21516;&#26500;&#21028;&#21035;&#33021;&#21147;&#65292;&#19988;&#38543;&#30528;k&#30340;&#22686;&#21152;&#65292;&#20854;&#21028;&#21035;&#33021;&#21147;&#20005;&#26684;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;3D&#22270;&#24418;&#65292;k-WL&#30340;&#21516;&#26500;&#21028;&#21035;&#33021;&#21147;&#26159;&#21542;&#20005;&#26684;&#22686;&#21152;&#65292;&#25110;&#32773;&#26159;&#21542;&#23384;&#22312;&#33021;&#22815;&#21306;&#20998;&#25152;&#26377;3D&#22270;&#24418;&#30340;k&#65292;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#22270;&#29983;&#25104;&#30340;&#35270;&#35282;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of graph isomorphism is an important but challenging problem in the field of graph analysis, for example: analyzing the similarity of two chemical molecules, or studying the expressive ability of graph neural networks. WL test is a method to judge whether two graphs are isomorphic, but it cannot distinguish all non-isomorphic graphs. As an improvement of WL, k-WL has stronger isomorphism discrimination ability, and as k increases, its discrimination ability is strictly increasing. However, whether the isomorphic discrimination power of k-WL is strictly increasing for more complex 3D graphs, or whether there exists k that can discriminate all 3D graphs, remains unexplored. This paper attempts to explore this problem from the perspective of graph generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26893;&#20837;&#35760;&#24518;&#30340;Neural Decision Tree&#65288;eMem-NDT&#65289;&#25506;&#32034;&#30446;&#26631;&#36710;&#36742;&#34892;&#20026;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#20915;&#31574;&#26641;&#20013;&#23545;&#21382;&#21490;&#36710;&#36742;&#34892;&#20026;&#29305;&#24449;&#30340;&#34892;&#20026;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#20998;&#32452;&#21644;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.08423</link><description>&lt;p&gt;
&#36890;&#36807;&#26893;&#20837;&#35760;&#24518;&#30340;Neural Decision Tree&#23454;&#29616;&#36710;&#36742;&#34892;&#20026;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vehicle Behavior Prediction by Episodic-Memory Implanted NDT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26893;&#20837;&#35760;&#24518;&#30340;Neural Decision Tree&#65288;eMem-NDT&#65289;&#25506;&#32034;&#30446;&#26631;&#36710;&#36742;&#34892;&#20026;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#20915;&#31574;&#26641;&#20013;&#23545;&#21382;&#21490;&#36710;&#36742;&#34892;&#20026;&#29305;&#24449;&#30340;&#34892;&#20026;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#20998;&#32452;&#21644;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#39044;&#27979;&#30446;&#26631;&#36710;&#36742;&#30340;&#34892;&#20026;&#65288;&#24038;&#36716;&#12289;&#20572;&#36710;&#31561;&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20570;&#20986;&#23433;&#20840;&#20915;&#31574;&#21644;&#36991;&#20813;&#20107;&#25925;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#20248;&#31168;&#32780;&#20934;&#30830;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#40657;&#30418;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#20449;&#20219;&#12290;&#26412;&#25991;&#36890;&#36807;&#26893;&#20837;&#35760;&#24518;&#30340;Neural Decision Tree&#65288;&#31616;&#31216;eMem-NDT&#65289;&#25506;&#32034;&#30446;&#26631;&#36710;&#36742;&#34892;&#20026;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;eMem-NDT&#30340;&#32467;&#26500;&#36890;&#36807;&#23545;&#36710;&#36742;&#34892;&#20026;&#25551;&#36848;&#30340;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#20998;&#23618;&#32858;&#31867;&#26500;&#24314;&#12290;eMem-NDT&#26159;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#31070;&#32463;&#25903;&#25345;&#37096;&#20998;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27169;&#22411;&#30340;&#36719;&#26368;&#22823;&#23618;&#26367;&#25442;&#20026;eMem-NDT&#65292;&#22312;&#31070;&#32463;&#20915;&#31574;&#26641;&#19978;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#21382;&#21490;&#36710;&#36742;&#34892;&#20026;&#29305;&#24449;&#30340;&#34892;&#20026;&#35760;&#24518;&#21407;&#22411;&#36827;&#34892;&#20998;&#32452;&#21644;&#23545;&#40784;&#12290;eMem-NDT&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#30001;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#65292;&#29992;&#20110;&#23545;&#40784;&#34892;&#20026;&#35760;&#24518;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, predicting the behavior (turning left, stopping, etc.) of target vehicles is crucial for the self-driving vehicle to make safe decisions and avoid accidents. Existing deep learning-based methods have shown excellent and accurate performance, but the black-box nature makes it untrustworthy to apply them in practical use. In this work, we explore the interpretability of behavior prediction of target vehicles by an Episodic Memory implanted Neural Decision Tree (abbrev. eMem-NDT). The structure of eMem-NDT is constructed by hierarchically clustering the text embedding of vehicle behavior descriptions. eMem-NDT is a neural-backed part of a pre-trained deep learning model by changing the soft-max layer of the deep model to eMem-NDT, for grouping and aligning the memory prototypes of the historical vehicle behavior features in training data on a neural decision tree. Each leaf node of eMem-NDT is modeled by a neural network for aligning the behavior memory prototypes. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;7T&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#19982;&#20302;&#22330;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#22312;7T&#32570;&#22833;&#29615;&#22659;&#19979;&#36827;&#34892;&#33041;&#22270;&#20687;&#20998;&#21106;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#34701;&#20837;&#65292;&#21033;&#29992;&#24378;&#24230;&#24341;&#23548;&#29305;&#24449;&#65292;&#21487;&#20197;&#35782;&#21035;&#38590;&#20197;&#35782;&#21035;&#30340;&#32454;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.08409</link><description>&lt;p&gt;
&#23558;&#36229;&#39640;&#22330;&#22270;&#20687;&#30340;&#29305;&#24449;&#36801;&#31227;&#21040;&#20302;&#22330;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#36827;&#34892;&#24378;&#24230;&#24341;&#23548;&#30340;&#33041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Transferring Ultrahigh-Field Representations for Intensity-Guided Brain Segmentation of Low-Field Magnetic Resonance Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;7T&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#19982;&#20302;&#22330;&#30913;&#20849;&#25391;&#25104;&#20687;&#29305;&#24449;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#22312;7T&#32570;&#22833;&#29615;&#22659;&#19979;&#36827;&#34892;&#33041;&#22270;&#20687;&#20998;&#21106;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#34701;&#21512;&#21644;&#34701;&#20837;&#65292;&#21033;&#29992;&#24378;&#24230;&#24341;&#23548;&#29305;&#24449;&#65292;&#21487;&#20197;&#35782;&#21035;&#38590;&#20197;&#35782;&#21035;&#30340;&#32454;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#39640;&#22330;(7T)&#30913;&#20849;&#25391;&#25104;&#20687;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#21078;&#32454;&#33410;&#21644;&#23545;&#27604;&#24230;&#65292;&#28982;&#32780;&#20854;&#39640;&#25104;&#26412;&#21644;&#36739;&#20302;&#21487;&#35775;&#38382;&#24615;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;7T&#22270;&#20687;&#30340;&#29305;&#24449;&#19982;&#20302;&#22330;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#34701;&#21512;&#65292;&#23454;&#29616;&#22312;7T&#32570;&#22833;&#29615;&#22659;&#19979;&#30340;&#33041;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#25552;&#21462;&#20302;&#22330;&#22270;&#20687;&#30340;7T-like&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#31934;&#32454;&#35843;&#25972;&#65292;&#20351;&#20854;&#26377;&#25928;&#22320;&#34701;&#20837;&#20302;&#22330;&#22270;&#20687;&#30340;&#29305;&#24449;&#20013;&#12290;&#21033;&#29992;&#20174;&#34701;&#21512;&#21644;&#34701;&#20837;&#20013;&#33719;&#24471;&#30340;&#24378;&#24230;&#24341;&#23548;&#29305;&#24449;&#65292;&#20998;&#21106;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#36890;&#24120;&#22312;&#20165;&#20381;&#36182;&#20302;&#22330;&#22270;&#20687;&#26102;&#38590;&#20197;&#35782;&#21035;&#30340;&#32454;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrahigh-field (UHF) magnetic resonance imaging (MRI), i.e., 7T MRI, provides superior anatomical details of internal brain structures owing to its enhanced signal-to-noise ratio and susceptibility-induced contrast. However, the widespread use of 7T MRI is limited by its high cost and lower accessibility compared to low-field (LF) MRI. This study proposes a deep-learning framework that systematically fuses the input LF magnetic resonance feature representations with the inferred 7T-like feature representations for brain image segmentation tasks in a 7T-absent environment. Specifically, our adaptive fusion module aggregates 7T-like features derived from the LF image by a pre-trained network and then refines them to be effectively assimilable UHF guidance into LF image features. Using intensity-guided features obtained from such aggregation and assimilation, segmentation models can recognize subtle structural representations that are usually difficult to recognize when relying only on L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOSS-GAT&#30340;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37319;&#29992;&#20102;&#26631;&#31614;&#20256;&#25773;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08401</link><description>&lt;p&gt;
LOSS-GAT: &#26631;&#31614;&#20256;&#25773;&#21644;&#19968;&#31867;&#21322;&#30417;&#30563;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LOSS-GAT: Label Propagation and One-Class Semi-Supervised Graph Attention Network for Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOSS-GAT&#30340;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#37319;&#29992;&#20102;&#26631;&#31614;&#20256;&#25773;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#31038;&#20132;&#32593;&#32476;&#30340;&#26102;&#20195;&#65292;&#34394;&#20551;&#26032;&#38395;&#30340;&#24555;&#36895;&#20256;&#25773;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#23041;&#32961;&#65292;&#22312;&#20154;&#20204;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35782;&#21035;&#20551;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#26159;&#26631;&#35760;&#26032;&#38395;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#19968;&#20010;&#31867;&#23398;&#20064;&#65288;OCL&#65289;&#26041;&#27861;&#21487;&#20197;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#21512;&#36866;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#21487;&#20197;&#35775;&#38382;&#21040;&#19981;&#21516;&#20869;&#23481;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#19988;&#22270;&#19978;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#33410;&#28857;&#30340;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21322;&#30417;&#30563;&#21644;&#19968;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#31216;&#20026;LOSS-GAT&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of widespread social networks, the rapid dissemination of fake news has emerged as a significant threat, inflicting detrimental consequences across various dimensions of people's lives. Machine learning and deep learning approaches have been extensively employed for identifying fake news. However, a significant challenge in identifying fake news is the limited availability of labeled news datasets. Therefore, the One-Class Learning (OCL) approach, utilizing only a small set of labeled data from the interest class, can be a suitable approach to address this challenge. On the other hand, representing data as a graph enables access to diverse content and structural information, and label propagation methods on graphs can be effective in predicting node labels. In this paper, we adopt a graph-based model for data representation and introduce a semi-supervised and one-class approach for fake news detection, called LOSS-GAT. Initially, we employ a two-step label propagation algori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08384</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#23398;&#20064;&#65306;&#23454;&#29616;&#21160;&#24577;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Selective Learning: Towards Robust Calibration with Dynamic Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35823;&#26657;&#20934;&#25351;&#30340;&#26159;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#26159;&#30001;&#36807;&#25311;&#21512;&#38382;&#39064;&#24341;&#36215;&#30340;&#65292;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#29305;&#28857;&#26159;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#25152;&#26377;&#20869;&#23481;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36827;&#34892;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#24182;&#32531;&#35299;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#29702;&#35299;&#20026;&#23547;&#25214;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#21487;&#20449;&#24230;&#26469;&#36866;&#24212;&#23454;&#38469;&#26631;&#31614;&#65292;&#21516;&#26102;&#36890;&#36807;&#38477;&#20302;&#21487;&#20449;&#24230;&#26469;&#26368;&#22823;&#21270;&#39044;&#27979;&#27010;&#29575;&#30340;&#29109;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#23548;&#33268;&#30446;&#26631;&#20914;&#31361;&#65288;&#22686;&#21152;&#20294;&#20063;&#38477;&#20302;&#21487;&#20449;&#24230;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#36991;&#20813;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LE-PDE-UQ&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08383</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#20840;&#23616;&#28436;&#21270;&#23454;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#36870;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Forward and Inverse Problems of PDEs via Latent Global Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LE-PDE-UQ&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#22312;&#36895;&#24230;&#26041;&#38754;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24448;&#24448;&#33021;&#22815;&#23454;&#29616;10&#21040;1000&#20493;&#30340;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21363;&#32570;&#20047;&#23545;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#20851;&#38190;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39640;&#25928;&#21644;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#21040;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28508;&#22312;&#28436;&#21270;&#65288;LE-PDE-UQ&#65289;&#65292;&#20026;&#21069;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#36171;&#20104;&#20102;&#28145;&#24230;&#23398;&#20064;&#26367;&#20195;&#27169;&#22411;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;LE-PDE-UQ&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#26469;&#28436;&#21270;&#31995;&#32479;&#30340;&#29366;&#24577;&#21644;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based surrogate models have demonstrated remarkable advantages over classical solvers in terms of speed, often achieving speedups of 10 to 1000 times over traditional partial differential equation (PDE) solvers. However, a significant challenge hindering their widespread adoption in both scientific and industrial domains is the lack of understanding about their prediction uncertainties, particularly in scenarios that involve critical decision making. To address this limitation, we propose a method that integrates efficient and precise uncertainty quantification into a deep learning-based surrogate model. Our method, termed Latent Evolution of PDEs with Uncertainty Quantification (LE-PDE-UQ), endows deep learning-based surrogate models with robust and efficient uncertainty quantification capabilities for both forward and inverse problems. LE-PDE-UQ leverages latent vectors within a latent space to evolve both the system's state and its corresponding uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#29992;&#20110;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08373</link><description>&lt;p&gt;
&#21160;&#24577;&#31574;&#30053;&#19979;&#22810;&#27493;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification for Dynamic Strategies in Multi-Step Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#29992;&#20110;&#22810;&#27493;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22810;&#27493;&#39044;&#27979;&#65288;MSF&#65289;&#26159;&#20960;&#20046;&#25152;&#26377;&#26102;&#38388;&#39046;&#22495;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#20010;&#26410;&#26469;&#26102;&#38388;&#27493;&#39588;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#31181;&#39044;&#27979;&#65292;&#24517;&#39035;&#20551;&#35774;&#26102;&#38388;&#21160;&#24577;&#30340;&#36882;&#24402;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#20551;&#35774;&#34987;&#31216;&#20026;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35780;&#20272;&#26410;&#35265;&#25968;&#25454;&#20043;&#21069;&#65292;&#19981;&#28165;&#26970;&#21738;&#31181;&#39044;&#27979;&#31574;&#30053;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#22810;&#27493;&#39044;&#27979;&#26041;&#27861;&#20351;&#29992;&#21333;&#20010;&#65288;&#22266;&#23450;&#65289;&#30340;&#39044;&#27979;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#26368;&#20248;&#39044;&#27979;&#31574;&#30053;&#30340;&#23454;&#20363;&#32423;&#21035;&#21464;&#21270;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;MSF&#30340;&#21160;&#24577;&#31574;&#30053;&#65288;DyStrat&#65289;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#35268;&#27169;&#12289;&#39046;&#22495;&#21644;&#22810;&#27493;&#39044;&#27979;&#38271;&#24230;&#30340;10&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#30340;&#20998;&#31867;&#22120;&#26102;&#65292;DyStrat&#22312;94%&#30340;&#26102;&#38388;&#20869;&#20248;&#20110;&#26368;&#20339;&#22266;&#23450;&#31574;&#30053;&#65292;&#24179;&#22343;&#22343;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step forecasting (MSF) in time-series, the ability to make predictions multiple time steps into the future, is fundamental to almost all temporal domains. To make such forecasts, one must assume the recursive complexity of the temporal dynamics. Such assumptions are referred to as the forecasting strategy used to train a predictive model. Previous work shows that it is not clear which forecasting strategy is optimal a priori to evaluating on unseen data. Furthermore, current approaches to MSF use a single (fixed) forecasting strategy.   In this paper, we characterise the instance-level variance of optimal forecasting strategies and propose Dynamic Strategies (DyStrat) for MSF. We experiment using 10 datasets from different scales, domains, and lengths of multi-step horizons. When using a random-forest-based classifier, DyStrat outperforms the best fixed strategy, which is not knowable a priori, 94% of the time, with an average reduction in mean-squared error of 11%. Our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#29616;&#19968;&#27425;&#24615;&#27169;&#20223;&#21644;&#38646;&#27425;&#36866;&#24212;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#25512;&#26029;&#20986;&#35821;&#20041;&#25216;&#33021;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32463;&#36807;&#20248;&#21270;&#30340;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#36866;&#24212;&#29615;&#22659;&#20013;&#38544;&#21547;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#19968;&#27425;&#24615;&#27169;&#20223;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08369</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36890;&#36807;&#22810;&#27169;&#24577;&#25216;&#33021;&#23454;&#29616;&#19968;&#27425;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#29616;&#19968;&#27425;&#24615;&#27169;&#20223;&#21644;&#38646;&#27425;&#36866;&#24212;&#12290;&#36890;&#36807;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#25512;&#26029;&#20986;&#35821;&#20041;&#25216;&#33021;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#32463;&#36807;&#20248;&#21270;&#30340;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#36866;&#24212;&#29615;&#22659;&#20013;&#38544;&#21547;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;&#19968;&#27425;&#24615;&#27169;&#20223;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#24615;&#27169;&#20223;&#26159;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#37319;&#29992;&#23427;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#26159;&#24456;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36825;&#31181;&#29615;&#22659;&#20855;&#26377;&#39640;&#22495;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22797;&#26434;&#20219;&#21153;&#30340;&#32452;&#21512;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25216;&#33021;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#27425;&#24615;&#27169;&#20223;&#21644;&#38646;&#27425;&#36866;&#24212;&#65307;&#36890;&#36807;&#21333;&#20010;&#28436;&#31034;&#26469;&#25512;&#26029;&#22797;&#26434;&#30340;&#26410;&#35265;&#20219;&#21153;&#20013;&#30340;&#35821;&#20041;&#25216;&#33021;&#24207;&#21015;&#65292;&#28982;&#21518;&#23558;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#25216;&#33021;&#36716;&#21270;&#20026;&#32463;&#36807;&#20248;&#21270;&#30340;&#34892;&#21160;&#24207;&#21015;&#65292;&#20197;&#36866;&#24212;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#29615;&#22659;&#38544;&#21547;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20174;&#31163;&#32447;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#35821;&#20041;&#25216;&#33021;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#25216;&#33021;&#22312;&#35270;&#35273;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#19978;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#23454;&#29616;&#38646;&#27425;&#25216;&#33021;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#19968;&#27425;&#24615;&#27169;&#20223;&#22330;&#26223;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#20110;&#25193;&#23637;&#30340;&#22810;&#38454;&#27573;&#20803;&#19990;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.08349</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#29992;&#25143;&#26597;&#35810;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#35780;&#20272;&#20102;&#22312;&#23454;&#36341;&#20013;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#30340;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#30340;&#22269;&#38469;&#39033;&#30446;&#38598;&#20013;&#35780;&#20272;&#65292;&#23545;&#19968;&#20010;&#22312;FIFA World Cup&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#30340;&#30495;&#23454;&#37096;&#32626;&#30340;FootballDB&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#65288;&#20063;&#31216;&#20026;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#65289;&#24050;&#25104;&#20026;&#24357;&#21512;&#29992;&#25143;&#33021;&#21147;&#19982;&#22522;&#20110;SQL&#30340;&#25968;&#25454;&#35775;&#38382;&#20043;&#38388;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#35831;&#27714;&#36716;&#21270;&#20026;&#29305;&#23450;&#25968;&#25454;&#24211;&#30340;&#26377;&#25928;SQL&#35821;&#21477;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#24471;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24120;&#24120;&#26159;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19981;&#26029;&#21462;&#24471;&#26032;&#30340;&#39640;&#20998;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#12289;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#19981;&#21516;&#25968;&#25454;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#26126;&#26174;&#32570;&#20047;&#12290;&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22810;&#24180;&#22269;&#38469;&#39033;&#30446;&#20851;&#20110;&#25991;&#26412;&#21040;SQL&#30028;&#38754;&#30340;&#38598;&#20013;&#35780;&#20272;&#65292;&#25552;&#20379;&#20102;&#23545;&#25991;&#26412;&#21040;SQL&#31995;&#32479;&#22312;&#23454;&#36341;&#20013;&#25968;&#25454;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#39318;&#27425;&#28145;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20110;FootballDB&#30340;&#30495;&#23454;&#37096;&#32626;&#65292;&#35813;&#31995;&#32479;&#22312;FIFA World Cup&#30340;&#32972;&#26223;&#19979;&#36830;&#32493;&#36816;&#34892;&#20102;9&#20010;&#26376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA Wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08341</link><description>&lt;p&gt;
&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#65306;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Eliciting Big Five Personality Traits in Large Language Models: A Textual Analysis with Classifier-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20998;&#31867;&#22120;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#36755;&#20837;&#25552;&#31034;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#65292;&#20197;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#36755;&#20837;&#30340;&#19981;&#21516;&#25552;&#31034;&#32780;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23545;&#21050;&#28608;&#20570;&#20986;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25307;&#32856;&#32972;&#26223;&#19979;&#34987;&#24212;&#32856;&#32773;&#21644;&#38599;&#20027;&#24191;&#27867;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20063;&#24341;&#21457;&#20102;&#20247;&#22810;&#20262;&#29702;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19982;&#36825;&#20123;&#8220;&#40657;&#30418;&#23376;&#8221;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#26377;&#20851;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#35843;&#26597;LLMs&#30340;&#20154;&#26684;&#29305;&#36136;&#26469;&#22686;&#21152;&#20854;&#36879;&#26126;&#24230;&#65292;&#20294;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35201;&#27714;&#27169;&#22411;&#26469;&#23436;&#25104;&#20154;&#26684;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#19981;&#21516;&#36755;&#20837;&#25552;&#31034;&#19979;&#27169;&#22411;&#30340;&#36755;&#20986;&#21464;&#21270;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#24120;&#35265;&#38754;&#35797;&#38382;&#39064;&#21644;&#26088;&#22312;&#24341;&#21457;&#29305;&#23450;&#30340;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#26032;&#39062;&#30340;&#35843;&#26597;&#26041;&#27861;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#23481;&#26131;&#28608;&#27963;&#29305;&#23450;&#20154;&#26684;&#29305;&#36136;&#65292;&#24182;&#26681;&#25454;&#20854;&#36755;&#20986;&#20013;&#30340;&#35821;&#35328;&#26469;&#35780;&#20272;&#20854;&#20154;&#26684;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21453;&#22797;&#25552;&#20379;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly being utilized by both candidates and employers in the recruitment context. However, with this comes numerous ethical concerns, particularly related to the lack of transparency in these "black-box" models. Although previous studies have sought to increase the transparency of these models by investigating the personality traits of LLMs, many of the previous studies have provided them with personality assessments to complete. On the other hand, this study seeks to obtain a better understanding of such models by examining their output variations based on different input prompts. Specifically, we use a novel elicitation approach using prompts derived from common interview questions, as well as prompts designed to elicit particular Big Five personality traits to examine whether the models were susceptible to trait-activation like humans are, to measure their personality based on the language used in their outputs. To do so, we repeatedly prompte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#36755;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08324</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36890;&#36807;&#31283;&#23450;&#20998;&#24067;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification via Stable Distribution Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08324
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#36755;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#31283;&#23450;&#27010;&#29575;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23616;&#37096;&#32447;&#24615;&#21270;&#65292;&#22312;ReLU&#38750;&#32447;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#26159;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#26368;&#20248;&#36817;&#20284;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#39640;&#26031;&#21644;&#26607;&#35199;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#20854;&#36755;&#20986;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#20256;&#25773;&#20998;&#24067;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26657;&#20934;&#32622;&#20449;&#21306;&#38388;&#21644;&#36873;&#25321;&#24615;&#39044;&#27979;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#12290;&#32467;&#26524;&#34920;&#26126;&#20102;&#20256;&#25773;&#20998;&#24067;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35832;&#22914;&#30697;&#21305;&#37197;&#31561;&#20854;&#20182;&#26041;&#27861;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach for propagating stable probability distributions through neural networks. Our method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data. The results demonstrate a broad applicability of propagating distributions and show the advantages of our method over other approaches such as moment matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#38382;&#39064;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;378&#20010;&#35268;&#33539;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#65292;&#24182;&#24635;&#32467;&#20102;&#20262;&#29702;&#20105;&#35758;&#30340;&#26680;&#24515;&#20869;&#23481;&#65292;&#21253;&#25324;&#20844;&#27491;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#26377;&#23475;&#20869;&#23481;&#12289;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08323</link><description>&lt;p&gt;
&#26144;&#23556;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;: &#19968;&#39033;&#20840;&#38754;&#30340;&#33539;&#22260;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Mapping the Ethics of Generative AI: A Comprehensive Scoping Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#38382;&#39064;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;378&#20010;&#35268;&#33539;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#25490;&#24207;&#65292;&#24182;&#24635;&#32467;&#20102;&#20262;&#29702;&#20105;&#35758;&#30340;&#26680;&#24515;&#20869;&#23481;&#65292;&#21253;&#25324;&#20844;&#27491;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#26377;&#23475;&#20869;&#23481;&#12289;&#38544;&#31169;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#21644;&#23427;&#22312;&#31038;&#20250;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#20262;&#29702;&#24433;&#21709;&#21644;&#39118;&#38505;&#30340;&#28608;&#28872;&#36777;&#35770;&#12290;&#36825;&#20123;&#39118;&#38505;&#24448;&#24448;&#19982;&#20256;&#32479;&#30340;&#21028;&#21035;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#20102;&#32508;&#21512;&#36817;&#26399;&#30340;&#35752;&#35770;&#24182;&#26144;&#23556;&#20854;&#35268;&#33539;&#27010;&#24565;&#65292;&#25105;&#20204;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#19968;&#39033;&#33539;&#22260;&#35843;&#26597;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;19&#20010;&#20027;&#39064;&#39046;&#22495;&#20013;378&#20010;&#35268;&#33539;&#38382;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25353;&#29031;&#23427;&#20204;&#22312;&#25991;&#29486;&#20013;&#30340;&#26222;&#36941;&#24615;&#36827;&#34892;&#20102;&#25490;&#24207;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#23398;&#32773;&#12289;&#23454;&#36341;&#32773;&#25110;&#20915;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#21387;&#32553;&#20102;&#22260;&#32469;&#20844;&#27491;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#26377;&#23475;&#20869;&#23481;&#12289;&#24187;&#35273;&#12289;&#38544;&#31169;&#12289;&#20114;&#21160;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#31038;&#20250;&#24433;&#21709;&#31561;&#20262;&#29702;&#20105;&#35758;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#24179;&#34913;&#24773;&#20917;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#32463;&#35777;&#23454;&#30340;&#39118;&#38505;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21512;&#25104;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#20010;&#32032;&#25551;&#23454;&#29616;&#20102;&#19968;&#23545;&#22810;&#30340;&#25991;&#21270;&#33402;&#26415;&#21697;&#19977;&#32500;&#20960;&#20309;&#37325;&#24314;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36755;&#20837;&#36827;&#34892;&#25351;&#23548;&#65292;&#23545;&#20110;&#21382;&#21490;&#32032;&#25551;&#31561;&#19981;&#22826;&#34920;&#29616;&#21147;&#24378;&#30340;&#34920;&#31034;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20165;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36866;&#24212;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#20114;&#21160;&#22320;&#37325;&#24314;&#20002;&#22833;&#33402;&#26415;&#21697;&#30340;&#21487;&#33021;&#22806;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.08310</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#25991;&#21270;&#33402;&#26415;&#21697;&#30340;&#19968;&#23545;&#22810;&#19977;&#32500;&#20960;&#20309;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
One-to-many Reconstruction of 3D Geometry of cultural Artifacts using a synthetically trained Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08310
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21512;&#25104;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#20010;&#32032;&#25551;&#23454;&#29616;&#20102;&#19968;&#23545;&#22810;&#30340;&#25991;&#21270;&#33402;&#26415;&#21697;&#19977;&#32500;&#20960;&#20309;&#37325;&#24314;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#36755;&#20837;&#36827;&#34892;&#25351;&#23548;&#65292;&#23545;&#20110;&#21382;&#21490;&#32032;&#25551;&#31561;&#19981;&#22826;&#34920;&#29616;&#21147;&#24378;&#30340;&#34920;&#31034;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20165;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36866;&#24212;&#65292;&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#20114;&#21160;&#22320;&#37325;&#24314;&#20002;&#22833;&#33402;&#26415;&#21697;&#30340;&#21487;&#33021;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21333;&#20010;&#22270;&#20687;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29616;&#20195;&#26041;&#27861;&#38024;&#23545;&#19968;&#33324;&#29289;&#20307;&#22522;&#20110;&#30495;&#23454;&#29031;&#29255;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#21382;&#21490;&#32032;&#25551;&#31561;&#19981;&#22826;&#34920;&#29616;&#21147;&#24378;&#30340;&#34920;&#31034;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#21487;&#20197;&#20174;&#21333;&#20010;&#32032;&#25551;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35814;&#32454;&#19977;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#31561;&#22810;&#27169;&#24577;&#36755;&#20837;&#36827;&#34892;&#25351;&#23548;&#12290;&#23427;&#20165;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#65288;&#22914;&#39302;&#38271;&#65289;&#19982;&#20043;&#20132;&#20114;&#22320;&#37325;&#24314;&#20002;&#22833;&#33402;&#26415;&#21697;&#30340;&#21487;&#33021;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 3D shape of an object using a single image is a difficult problem. Modern approaches achieve good results for general objects, based on real photographs, but worse results on less expressive representations such as historic sketches. Our automated approach generates a variety of detailed 3D representation from a single sketch, depicting a medieval statue, and can be guided by multi-modal inputs, such as text prompts. It relies solely on synthetic data for training, making it adoptable even in cases of only small numbers of training examples. Our solution allows domain experts such as a curators to interactively reconstruct potential appearances of lost artifacts.
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#21628;&#21505;&#20572;&#19979;&#26469;&#24605;&#32771;&#25105;&#20204;&#24819;&#35201;&#36827;&#34892;&#20160;&#20040;&#26679;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#65292;&#36991;&#20813;&#19981;&#20005;&#35880;&#21644;&#19981;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#39564;&#65292;&#25913;&#21464;&#20197;&#25903;&#25345;&#20808;&#21069;&#20449;&#24565;&#20026;&#30446;&#30340;&#30340;&#24577;&#24230;&#65292;&#25552;&#20513;&#23545;&#20010;&#20154;&#21644;&#31038;&#32676;&#30340;&#30495;&#35802;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#21453;&#24605;&#12290;</title><link>https://arxiv.org/abs/2402.08298</link><description>&lt;p&gt;
&#20572;&#19979;&#26469;&#24605;&#32771;&#65306;&#25105;&#20204;&#24819;&#35201;&#36827;&#34892;&#20160;&#20040;&#26679;&#30340;&#30740;&#31350;&#65311;
&lt;/p&gt;
&lt;p&gt;
Time to Stop and Think: What kind of research do we want to do?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#21628;&#21505;&#20572;&#19979;&#26469;&#24605;&#32771;&#25105;&#20204;&#24819;&#35201;&#36827;&#34892;&#20160;&#20040;&#26679;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#39564;&#35774;&#35745;&#26041;&#38754;&#65292;&#36991;&#20813;&#19981;&#20005;&#35880;&#21644;&#19981;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#39564;&#65292;&#25913;&#21464;&#20197;&#25903;&#25345;&#20808;&#21069;&#20449;&#24565;&#20026;&#30446;&#30340;&#30340;&#24577;&#24230;&#65292;&#25552;&#20513;&#23545;&#20010;&#20154;&#21644;&#31038;&#32676;&#30340;&#30495;&#35802;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#22266;&#26377;&#37096;&#20998;&#65292;&#23427;&#21487;&#20197;&#25910;&#38598;&#23450;&#37327;&#35266;&#23519;&#25968;&#25454;&#65292;&#39564;&#35777;&#20551;&#35774;&#65292;&#24182;&#20026;&#20854;&#37325;&#26032;&#21046;&#23450;&#25552;&#20379;&#35777;&#25454;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#24517;&#39035;&#19982;&#30740;&#31350;&#30446;&#30340;&#19968;&#33268;&#65292;&#36866;&#24403;&#22320;&#22788;&#29702;&#27599;&#31181;&#24773;&#20917;&#19979;&#30340;&#30456;&#20851;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25991;&#29486;&#20013;&#20805;&#26021;&#30528;&#19968;&#20123;&#23454;&#39564;&#32570;&#20047;&#20005;&#35880;&#24615;&#21644;&#35828;&#26381;&#21147;&#30340;&#30740;&#31350;&#65292;&#24448;&#24448;&#26159;&#20026;&#20102;&#25903;&#25345;&#20808;&#21069;&#30340;&#20449;&#24565;&#65292;&#32780;&#19981;&#26159;&#22238;&#31572;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#25991;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#39046;&#22495;&#65292;&#22240;&#20026;&#36825;&#26159;&#25105;&#20204;&#20027;&#35201;&#30340;&#24037;&#20316;&#39046;&#22495;&#65292;&#20063;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#23384;&#22312;&#19981;&#24403;&#34892;&#20026;&#30340;&#22320;&#26041;&#12290;&#21363;&#20351;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23558;&#37325;&#28857;&#38480;&#21046;&#22312;&#30740;&#31350;&#30340;&#23454;&#39564;&#37096;&#20998;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#28608;&#21457;&#23545;&#25105;&#20204;&#24037;&#20316;&#30340;&#30495;&#35802;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#24341;&#21457;&#20010;&#20154;&#21644;&#31038;&#32676;&#23618;&#38754;&#30340;&#21453;&#24605;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation is an intrinsic part of research in artificial intelligence since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions.   In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Even if we limit the focus of this manuscript to the experimental part of the research, our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#26041;&#27861;&#22312;&#21009;&#20107;&#26696;&#20214;&#35843;&#26597;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25512;&#26029;&#20982;&#25163;&#30340;&#21160;&#26426;&#12289;&#26426;&#20250;&#21644;&#26041;&#27861;&#26469;&#23547;&#25214;&#32618;&#29359;&#12290;</title><link>https://arxiv.org/abs/2402.08284</link><description>&lt;p&gt;
&#19968;&#31181;&#36923;&#36753;&#26041;&#27861;&#22312;&#21009;&#20107;&#26696;&#20214;&#35843;&#26597;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Logical Approach to Criminal Case Investigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#26041;&#27861;&#22312;&#21009;&#20107;&#26696;&#20214;&#35843;&#26597;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25512;&#26029;&#20982;&#25163;&#30340;&#21160;&#26426;&#12289;&#26426;&#20250;&#21644;&#26041;&#27861;&#26469;&#23547;&#25214;&#32618;&#29359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#35299;&#37322;&#20854;&#32467;&#35770;&#21407;&#22240;&#30340;&#23646;&#24615;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;XAI&#26377;&#26395;&#22312;&#27861;&#21307;&#31185;&#23398;&#21644;&#21496;&#27861;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#22312;&#24403;&#21069;&#30340;&#27861;&#21307;&#21644;&#21009;&#20107;&#35843;&#26597;&#29615;&#22659;&#20013;&#65292;&#19987;&#23478;&#20204;&#38754;&#20020;&#30528;&#22823;&#37327;&#25968;&#25454;&#12289;&#28151;&#20081;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#30340;&#23567;&#35777;&#25454;&#21644;&#20256;&#32479;&#23454;&#39564;&#23460;&#32467;&#26500;&#20197;&#21450;&#26377;&#26102;&#19981;&#36275;&#30340;&#30693;&#35782;&#31561;&#35768;&#22810;&#25361;&#25112;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#21487;&#33021;&#23548;&#33268;&#35843;&#26597;&#22833;&#36133;&#21644;&#21496;&#27861;&#22833;&#35823;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36923;&#36753;&#26041;&#27861;&#22312;&#29359;&#32618;&#29616;&#22330;&#35843;&#26597;&#20013;&#30340;&#24212;&#29992;&#12290;&#24212;&#29992;&#30340;&#20027;&#39064;&#26159;&#12298;&#26001;&#32441;&#24102;&#20043;&#35868;&#12299;&#36825;&#37096;&#31119;&#23572;&#25705;&#26031;&#30701;&#31687;&#23567;&#35828;&#12290;&#24212;&#29992;&#30340;&#25968;&#25454;&#26159;&#20026;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#25361;&#25112;&#21019;&#24314;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#26029;&#27599;&#20010;&#20855;&#26377;&#21160;&#26426;&#12289;&#26426;&#20250;&#21644;&#26041;&#27861;&#30340;&#20154;&#26469;&#23547;&#25214;&#20982;&#25163;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
XAI (eXplanable AI) techniques that have the property of explaining the reasons for their conclusions, i.e. explainability or interpretability, are attracting attention. XAI is expected to be used in the development of forensic science and the justice system. In today's forensic and criminal investigation environment, experts face many challenges due to large amounts of data, small pieces of evidence in a chaotic and complex environment, traditional laboratory structures and sometimes inadequate knowledge. All these can lead to failed investigations and miscarriages of justice. In this paper, we describe the application of one logical approach to crime scene investigation. The subject of the application is ``The Adventure of the Speckled Band'' from the Sherlock Holmes short stories. The applied data is the knowledge graph created for the Knowledge Graph Reasoning Challenge. We tried to find the murderer by inferring each person with the motive, opportunity, and method. We created an o
&lt;/p&gt;</description></item><item><title>Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08280</link><description>&lt;p&gt;
Pix2Code&#65306;&#23398;&#20064;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Pix2Code: Learning to Compose Neural Visual Concepts as Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08280
&lt;/p&gt;
&lt;p&gt;
Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20174;&#22270;&#20687;&#20013;&#25277;&#35937;&#27010;&#24565;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#23558;&#35270;&#35273;&#24863;&#30693;&#21644;&#36890;&#29992;&#20851;&#31995;&#25512;&#29702;&#36827;&#34892;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#20351;&#24471;&#20154;&#31867;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#33021;&#20462;&#27491;&#38169;&#35823;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Code&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#31243;&#24207;&#21512;&#25104;&#25193;&#23637;&#21040;&#35270;&#35273;&#20851;&#31995;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#26126;&#30830;&#30340;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#35780;&#20272;&#20102;Pix2Code&#30340;&#22810;&#26679;&#29305;&#24615;&#65292;&#20174;&#32780;&#27979;&#35797;&#20854;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.08267</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#36741;&#21161;&#25439;&#22833;&#25913;&#36827;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#65288;ICM&#65289;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27169;&#22411;&#32780;&#19981;&#26159;&#20154;&#30524;&#35270;&#35273;&#26469;&#21387;&#32553;&#22270;&#20687;&#20197;&#20379;&#26426;&#22120;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#22312;ICM&#20013;&#65292;&#32534;&#30721;&#22120;&#35782;&#21035;&#21644;&#21387;&#32553;&#23545;&#20110;&#26426;&#22120;&#35782;&#21035;&#20219;&#21153;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23398;&#20064;&#22411;ICM&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#22522;&#20110;&#20219;&#21153;&#25439;&#22833;&#30340;&#21387;&#32553;&#27169;&#22411;&#20248;&#21270;&#21644;&#22522;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#27604;&#29305;&#20998;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#35782;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#35782;&#21035;&#27169;&#22411;&#24456;&#28145;&#26102;&#65292;&#20351;&#29992;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#22522;&#20110;ROI&#30340;&#26041;&#27861;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#36890;&#24120;&#20250;&#22686;&#21152;&#39069;&#22806;&#24320;&#38144;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22411;ICM&#27169;&#22411;&#30340;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32534;&#30721;&#22120;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#26469;&#25552;&#39640;&#20854;&#35782;&#21035;&#33021;&#21147;&#21644;&#36895;&#29575;-&#22833;&#30495;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;27.7%&#21644;20.3%&#30340;Bjontegaard Delta&#36895;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;MOOCs&#20013;&#24179;&#34913;&#26174;&#24335;&#21644;&#38544;&#24335;&#20851;&#31995;&#36827;&#34892;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;MOOCs&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HIN)&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#21644;&#23398;&#20064;&#38544;&#24335;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.08256</link><description>&lt;p&gt;
&#22312;MOOC&#20013;&#20197;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#24314;&#27169;&#24179;&#34913;&#26174;&#24335;&#21644;&#38544;&#24335;&#20851;&#31995;&#65292;&#29992;&#20110;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Modeling Balanced Explicit and Implicit Relations with Contrastive Learning for Knowledge Concept Recommendation in MOOCs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;MOOCs&#20013;&#24179;&#34913;&#26174;&#24335;&#21644;&#38544;&#24335;&#20851;&#31995;&#36827;&#34892;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;MOOCs&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HIN)&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#21644;&#23398;&#20064;&#38544;&#24335;&#20851;&#31995;&#65292;&#20174;&#32780;&#25552;&#39640;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#30340;&#24615;&#33021;&#24182;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24320;&#25918;&#22312;&#32447;&#35838;&#31243;(MOOCs)&#20013;&#65292;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;MOOC&#24179;&#21488;&#19978;&#29992;&#25143;&#21644;&#30693;&#35782;&#27010;&#24565;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#22312;&#29992;&#25143;&#30340;&#23398;&#20064;&#27963;&#21160;&#20013;&#20250;&#20135;&#29983;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#20852;&#36259;&#25110;&#30456;&#21516;&#30340;&#30693;&#35782;&#27700;&#24179;&#65289;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#65292;&#24182;&#19988;&#36825;&#20123;&#20851;&#31995;&#26412;&#36523;&#24456;&#38590;&#23398;&#20064;&#21644;&#34920;&#31034;&#65292;&#23548;&#33268;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#34920;&#29616;&#19981;&#20339;&#65292;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;MOOCs&#20013;&#34920;&#31034;&#21644;&#24179;&#34913;&#26174;&#24335;&#21644;&#38544;&#24335;&#20851;&#31995;&#36827;&#34892;&#30693;&#35782;&#27010;&#24565;&#25512;&#33616;&#65288;CL-KCRec&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;MOOCs&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HIN)&#36827;&#34892;&#24314;&#27169;&#26469;&#26500;&#24314;&#19968;&#20010;HIN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge concept recommendation in Massive Open Online Courses (MOOCs) is a significant issue that has garnered widespread attention. Existing methods primarily rely on the explicit relations between users and knowledge concepts on the MOOC platforms for recommendation. However, there are numerous implicit relations (e.g., shared interests or same knowledge levels between users) generated within the users' learning activities on the MOOC platforms. Existing methods fail to consider these implicit relations, and these relations themselves are difficult to learn and represent, causing poor performance in knowledge concept recommendation and an inability to meet users' personalized needs. To address this issue, we propose a novel framework based on contrastive learning, which can represent and balance the explicit and implicit relations for knowledge concept recommendation in MOOCs (CL-KCRec). Specifically, we first construct a MOOCs heterogeneous information network (HIN) by modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#26497;&#38480;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#30340;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#29992;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08255</link><description>&lt;p&gt;
Distal Interference: &#25506;&#32034;&#22522;&#20110;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Distal Interference: Exploring the Limits of Model-Based Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#26497;&#38480;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#30340;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#29992;&#20197;&#35299;&#20915;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25353;&#39034;&#24207;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#25345;&#32493;&#23398;&#20064;&#34987;&#31216;&#20026;&#28798;&#38590;&#24615;&#24178;&#25200;&#25110;&#36951;&#24536;&#30340;&#38459;&#30861;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#28798;&#38590;&#24615;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#26799;&#24230;&#19979;&#38477;&#21644;&#36828;&#36317;&#31163;&#36755;&#20837;&#28857;&#20043;&#38388;&#37325;&#21472;&#34920;&#31034;&#22914;&#20309;&#23548;&#33268;&#36828;&#36317;&#31163;&#24178;&#25200;&#21644;&#28798;&#38590;&#24615;&#24178;&#25200;&#12290;&#36828;&#36317;&#31163;&#24178;&#25200;&#26159;&#25351;&#22312;&#23545;&#22495;&#30340;&#23376;&#38598;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26102;&#65292;&#23545;&#22495;&#30340;&#20854;&#20182;&#23376;&#38598;&#36896;&#25104;&#38750;&#23616;&#37096;&#21464;&#21270;&#30340;&#29616;&#35937;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#36828;&#36317;&#31163;&#24178;&#25200;&#30340;&#22343;&#21248;&#21487;&#35757;&#32451;&#27169;&#22411;&#24517;&#39035;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#35268;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ABEL-Spline&#30340;&#26032;&#22411;&#21453;&#23545;&#31216;&#26377;&#30028;&#25351;&#25968;&#23618;B-spline ANN&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#36817;&#20284;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20855;&#26377;&#22343;&#21248;&#21487;&#35757;&#32451;&#24615;&#12289;&#22810;&#39033;&#24335;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is the sequential learning of different tasks by a machine learning model. Continual learning is known to be hindered by catastrophic interference or forgetting, i.e. rapid unlearning of earlier learned tasks when new tasks are learned. Despite their practical success, artificial neural networks (ANNs) are prone to catastrophic interference. This study analyses how gradient descent and overlapping representations between distant input points lead to distal interference and catastrophic interference. Distal interference refers to the phenomenon where training a model on a subset of the domain leads to non-local changes on other subsets of the domain. This study shows that uniformly trainable models without distal interference must be exponentially large. A novel antisymmetric bounded exponential layer B-spline ANN architecture named ABEL-Spline is proposed that can approximate any continuous function, is uniformly trainable, has polynomial computational complexity, an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#24320;&#21457;AI&#27169;&#22411;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#20197;&#30830;&#20445;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#24212;&#29992;AI&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08250</link><description>&lt;p&gt;
&#26368;&#36817;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey of recent methods for addressing AI fairness and bias in biomedicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08250
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#24320;&#21457;AI&#27169;&#22411;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#25514;&#26045;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#20197;&#30830;&#20445;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#24212;&#29992;AI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20855;&#26377;&#25913;&#38761;&#20020;&#24202;&#23454;&#36341;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#25163;&#26415;&#20915;&#31574;&#65292;&#21516;&#26102;&#20943;&#23569;&#25104;&#26412;&#21644;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#20250;&#24310;&#32493;&#31038;&#20250;&#19981;&#24179;&#31561;&#25110;&#23637;&#31034;&#22522;&#20110;&#31181;&#26063;&#25110;&#24615;&#21035;&#30340;&#20559;&#35265;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#21457;&#29983;&#22312;AI&#27169;&#22411;&#30340;&#24320;&#21457;&#20043;&#21069;&#12289;&#26399;&#38388;&#25110;&#20043;&#21518;&#65292;&#22240;&#27492;&#20102;&#35299;&#21644;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#23545;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#24212;&#29992;AI&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#32531;&#35299;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36817;&#26399;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19981;&#21516;&#21435;&#20559;&#26041;&#27861;&#30340;&#25991;&#29486;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#35299;&#20915;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;PubMed&#12289;ACM&#25968;&#23383;&#22270;&#20070;&#39302;&#21644;IEEE Xplore&#19978;&#25191;&#34892;&#20102;&#25991;&#29486;&#25628;&#32034;&#65292;&#20869;&#23481;&#28085;&#30422;&#20102;&#30456;&#20851;&#25991;&#31456;&#30340;&#21457;&#24067;&#22312;1&#26376;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems have the potential to revolutionize clinical practices, including improving diagnostic accuracy and surgical decision-making, while also reducing costs and manpower. However, it is important to recognize that these systems may perpetuate social inequities or demonstrate biases, such as those based on race or gender. Such biases can occur before, during, or after the development of AI models, making it critical to understand and address potential biases to enable the accurate and reliable application of AI models in clinical settings. To mitigate bias concerns during model development, we surveyed recent publications on different debiasing methods in the fields of biomedical natural language processing (NLP) or computer vision (CV). Then we discussed the methods that have been applied in the biomedical domain to address bias. We performed our literature search on PubMed, ACM digital library, and IEEE Xplore of relevant articles published between Janu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#20154;&#26426;&#21327;&#20316;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#26080;&#20154;&#26426;&#30340;&#35270;&#28857;&#65292;&#24182;&#23558;&#36335;&#24452;&#35268;&#21010;&#36716;&#21270;&#20026;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#21487;&#34892;&#30340;&#24033;&#26816;&#36335;&#24452;&#65292;&#32780;&#19988;&#33021;&#22815;&#20943;&#23569;&#36335;&#24452;&#38271;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08246</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#26426;&#21512;&#20316;&#24033;&#26816;&#36335;&#24452;&#35268;&#21010;&#30340;&#34433;&#32676;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ant Colony Optimization for Cooperative Inspection Path Planning Using Multiple Unmanned Aerial Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#20154;&#26426;&#21327;&#20316;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#26080;&#20154;&#26426;&#30340;&#35270;&#28857;&#65292;&#24182;&#23558;&#36335;&#24452;&#35268;&#21010;&#36716;&#21270;&#20026;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#21487;&#34892;&#30340;&#24033;&#26816;&#36335;&#24452;&#65292;&#32780;&#19988;&#33021;&#22815;&#20943;&#23569;&#36335;&#24452;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32676;&#38598;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#26469;&#22788;&#29702;&#26080;&#20154;&#26426;&#21327;&#20316;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23545;&#20110;&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#24033;&#26816;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;&#29983;&#25104;&#26080;&#20154;&#26426;&#30340;&#35270;&#28857;&#12290;&#35270;&#28857;&#30340;&#35745;&#31639;&#32771;&#34385;&#20102;&#19982;&#26080;&#20154;&#26426;&#32534;&#38431;&#27169;&#22411;&#12289;&#30456;&#26426;&#21442;&#25968;&#21644;&#25968;&#25454;&#21518;&#22788;&#29702;&#35201;&#27714;&#30456;&#20851;&#30340;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#35270;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#23558;&#36335;&#24452;&#35268;&#21010;&#34920;&#31034;&#20026;&#25193;&#23637;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65292;&#24182;&#23450;&#20041;&#26032;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#26368;&#21518;&#20351;&#29992;&#34433;&#32676;&#20248;&#21270;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#24471;&#21040;&#26368;&#20248;&#30340;&#24033;&#26816;&#36335;&#24452;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#32467;&#26500;&#30340;&#19977;&#32500;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#20026;&#26080;&#20154;&#26426;&#29983;&#25104;&#21487;&#34892;&#30340;&#24033;&#26816;&#36335;&#24452;&#65292;&#32780;&#19988;&#22312;&#22797;&#26434;&#32467;&#26500;&#19978;&#23558;&#36335;&#24452;&#38271;&#24230;&#20943;&#23569;&#20102;29.47&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new swarm intelligence-based approach to deal with the cooperative path planning problem of unmanned aerial vehicles (UAVs), which is essential for the automatic inspection of infrastructure. The approach uses a 3D model of the structure to generate viewpoints for the UAVs. The calculation of the viewpoints considers the constraints related to the UAV formation model, camera parameters, and requirements for data post-processing. The viewpoints are then used as input to formulate the path planning as an extended traveling salesman problem and the definition of a new cost function. Ant colony optimization is finally used to solve the problem to yield optimal inspection paths. Experiments with 3D models of real structures have been conducted to evaluate the performance of the proposed approach. The results show that our system is not only capable of generating feasible inspection paths for UAVs but also reducing the path length by 29.47\% for complex structures when 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#19982;&#20844;&#24179;&#33021;&#21147;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08242</link><description>&lt;p&gt;
&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#20154;&#30740;&#21457;&#20013;&#30340;&#20844;&#24179;&#25935;&#25463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Equitable Agile Research and Development of AI and Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08242
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#19982;&#20844;&#24179;&#33021;&#21147;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26041;&#27861;&#24448;&#24448;&#20250;&#22797;&#21046;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#20559;&#35265;&#21644;&#25104;&#35265;&#65292;AI&#26426;&#22120;&#20154;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#20855;&#22791;&#38754;&#37096;&#35782;&#21035;&#21151;&#33021;&#30340;&#26426;&#22120;&#20154;&#26410;&#33021;&#23558;&#40657;&#20154;&#22899;&#24615;&#35782;&#21035;&#20026;&#20154;&#31867;&#65292;&#32780;&#20854;&#20182;&#26426;&#22120;&#20154;&#21017;&#20165;&#26681;&#25454;&#22806;&#34920;&#23558;&#20154;&#20204;&#65292;&#22914;&#40657;&#20154;&#30007;&#24615;&#65292;&#24402;&#31867;&#20026;&#32618;&#29359;&#12290;&#22312;&#8220;AI&#20379;&#24212;&#38142;&#8221;&#20013;&#65292;&#19968;&#31181;&#8220;&#27169;&#22359;&#21270;&#25991;&#21270;&#8221;&#24847;&#21619;&#30528;&#20260;&#23475;&#34987;&#35748;&#20026;&#26159;&#8220;&#36229;&#20986;&#33539;&#22260;&#8221;&#30340;&#65292;&#25110;&#32773;&#26159;&#21035;&#20154;&#30340;&#36131;&#20219;&#12290;&#20107;&#20214;&#30340;&#21457;&#29983;&#26159; routine enough&#65288;incidentdatabase.ai &#21015;&#20030;&#20102;2000&#22810;&#20010;&#20363;&#23376;&#65289;&#20197;&#34920;&#26126;&#24456;&#23569;&#26377;&#32452;&#32455;&#33021;&#22815;&#23436;&#20840;&#23562;&#37325;&#20154;&#20204;&#30340;&#26435;&#21033;&#65307;&#23454;&#29616;&#25152;&#22768;&#31216;&#30340;&#20844;&#24179;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21253;&#23481;&#24615;&#65288;EDI&#25110;DEI&#65289;&#30446;&#26631;&#65307;&#25110;&#32773;&#35782;&#21035;&#28982;&#21518;&#35299;&#20915;&#36825;&#20123;&#32452;&#32455;&#21644;&#24037;&#20214;&#20013;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#24191;&#27867;&#23454;&#36341;&#30340;&#30740;&#21457;&#39033;&#30446;&#31649;&#29702;&#26041;&#27861;&#65292;&#20197;&#24314;&#31435;&#32452;&#32455;&#30340;&#20844;&#24179;&#33021;&#21147;&#24182;&#26356;&#22909;&#22320;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) and 'Artificial Intelligence' ('AI') methods tend to replicate and amplify existing biases and prejudices, as do Robots with AI. For example, robots with facial recognition have failed to identify Black Women as human, while others have categorized people, such as Black Men, as criminals based on appearance alone. A 'culture of modularity' means harms are perceived as 'out of scope', or someone else's responsibility, throughout employment positions in the 'AI supply chain'. Incidents are routine enough (incidentdatabase.ai lists over 2000 examples) to indicate that few organizations are capable of completely respecting peoples' rights; meeting claimed equity, diversity, and inclusion (EDI or DEI) goals; or recognizing and then addressing such failures in their organizations and artifacts. We propose a framework for adapting widely practiced Research and Development (R&amp;D) project management methodologies to build organizational equity capabilities and better integr
&lt;/p&gt;</description></item><item><title>BERT4FCA&#26159;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#21644;BERT&#36827;&#34892;&#20108;&#37096;&#22270;&#38142;&#25509;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;FCA&#25552;&#21462;&#30340;&#26368;&#22823;&#21452;&#21521;&#22242;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08236</link><description>&lt;p&gt;
BERT4FCA&#65306;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#21644;BERT&#36827;&#34892;&#20108;&#37096;&#22270;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept Analysis and BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08236
&lt;/p&gt;
&lt;p&gt;
BERT4FCA&#26159;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#21644;BERT&#36827;&#34892;&#20108;&#37096;&#22270;&#38142;&#25509;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;FCA&#25552;&#21462;&#30340;&#26368;&#22823;&#21452;&#21521;&#22242;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BERT4FCA&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20108;&#37096;&#22270;&#32593;&#32476;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#65288;FCA&#65289;&#21644;BERT&#12290;&#20108;&#37096;&#22270;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#35299;&#20915;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#22909;&#21451;&#25512;&#33616;&#21644;&#20316;&#32773;-&#35770;&#25991;&#32593;&#32476;&#20013;&#30340;&#21512;&#20316;&#39044;&#27979;&#31561;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20108;&#37096;&#22270;&#32593;&#32476;&#20013;&#65292;&#26368;&#22823;&#30340;&#21452;&#21521;&#22242;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;FCA&#26469;&#25552;&#21462;&#23427;&#20204;&#12290;&#19968;&#20123;&#22522;&#20110;FCA&#30340;&#20108;&#37096;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#25429;&#25417;&#21040;&#25552;&#21462;&#30340;&#26368;&#22823;&#21452;&#21521;&#22242;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;BERT&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;FCA&#25552;&#21462;&#30340;&#26368;&#22823;&#21452;&#21521;&#22242;&#20013;&#23398;&#20064;&#26356;&#22810;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#30340;&#20108;&#37096;&#22270;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose BERT4FCA, a novel method for link prediction in bipartite networks, using formal concept analysis (FCA) and BERT. Link prediction in bipartite networks is an important task that can solve various practical problems like friend recommendation in social networks and co-authorship prediction in author-paper networks. Recent research has found that in bipartite networks, maximal bi-cliques provide important information for link prediction, and they can be extracted by FCA. Some FCA-based bipartite link prediction methods have achieved good performance. However, we figured out that their performance could be further improved because these methods did not fully capture the rich information of the extracted maximal bi-cliques. To address this limitation, we propose an approach using BERT, which can learn more information from the maximal bi-cliques extracted by FCA and use them to make link prediction. We conduct experiments on three real-world bipartite networks and demonstrate th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08228</link><description>&lt;p&gt;
&#30740;&#31350;GNN&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65306;&#20174;&#26550;&#26500;&#35282;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20110;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#23545;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#30340;&#25506;&#32034;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#20004;&#20010;&#8220;&#27169;&#22411;&#26080;&#20851;&#8221;&#35282;&#24230;&#19978;&#65306;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#30340;GNN&#27169;&#22411;&#26550;&#26500;&#23545;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#20114;&#29420;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;OOD&#25512;&#24191;&#65292;&#24182;&#23545;&#29616;&#20195;GNN&#30340;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
&lt;/p&gt;</description></item><item><title>BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.08219</link><description>&lt;p&gt;
BBox-Adapter: &#36731;&#37327;&#32423;&#36866;&#37197;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08219
&lt;/p&gt;
&lt;p&gt;
BBox-Adapter&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#25490;&#21517;&#24335;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#21644;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#36879;&#26126;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#21644;Gemini&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#20219;&#21153;&#30340;&#35201;&#27714;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21442;&#25968;&#12289;&#23884;&#20837;&#21644;&#36755;&#20986;&#27010;&#29575;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#21482;&#33021;&#36890;&#36807;&#23427;&#20204;&#30340;API&#26381;&#21153;&#36866;&#24212;&#36825;&#20123;&#40657;&#30418;LLMs&#65292;&#36825;&#24341;&#21457;&#20102;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#21644;&#25104;&#26412;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BBox-Adapter&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#29992;&#20110;&#40657;&#30418;LLMs&#30340;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#12290;BBox-Adapter&#36890;&#36807;&#23558;&#30446;&#26631;&#25968;&#25454;&#35270;&#20026;&#27491;&#26679;&#26412;&#65292;&#23558;&#28304;&#25968;&#25454;&#35270;&#20026;&#36127;&#26679;&#26412;&#26469;&#21306;&#20998;&#30446;&#26631;&#21644;&#28304;&#22495;&#25968;&#25454;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#22122;&#38899;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#25439;&#22833;&#26469;&#25552;&#39640;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#24809;&#32602;&#28304;&#22495;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20855;&#26377;&#22312;&#32447;&#36866;&#24212;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#23558;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#12289;&#20154;&#31867;&#25110;AI&#21453;&#39304;&#30340;&#23454;&#26102;&#27491;&#26679;&#26412;&#37319;&#26679;&#19982;&#20808;&#21069;&#36866;&#24212;&#30340;&#36127;&#26679;&#26412;&#25968;&#25454;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;BBox-Adapter&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#40657;&#30418;LLMs&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini for specific tasks is challenging. Due to the opacity in their parameters, embeddings, and even output probabilities, existing fine-tuning adaptation methods are inapplicable. Consequently, adapting these black-box LLMs is only possible through their API services, raising concerns about transparency, privacy, and cost. To address these challenges, we introduce BBox-Adapter, a novel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target and source domain data by treating target data as positive and source data as negative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to promote the likelihood of target domain data while penalizing that of the source domain. Furthermore, it features an online adaptation mechanism, which incorporates real-time positive data sampling from ground-truth, human, or AI feedback, coupled with negative data from previous adaptations. Extensive ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#25152;&#20986;&#29616;&#30340;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#30340;&#35748;&#30693;&#20998;&#25903;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#26426;&#21046;&#19982;&#20154;&#33041;&#23553;&#38145;&#26426;&#21046;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08211</link><description>&lt;p&gt;
&#24403;&#22312;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;Transformer&#26426;&#21046;&#27169;&#20223;&#39069;&#39030;&#22238;&#36335;&#23553;&#38145;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#25152;&#20986;&#29616;&#30340;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#22914;&#20309;&#35299;&#20915;&#22797;&#26434;&#30340;&#35748;&#30693;&#20998;&#25903;&#20219;&#21153;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#26426;&#21046;&#19982;&#20154;&#33041;&#23553;&#38145;&#26426;&#21046;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#38656;&#35201;&#22797;&#26434;&#30340;&#8220;&#35748;&#30693;&#20998;&#25903;&#8221;&#65288;&#25110;&#22312;&#23454;&#29616;&#20854;&#20182;&#30446;&#26631;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#19968;&#20010;&#30446;&#26631;&#30340;&#36861;&#27714;&#65289;&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#25104;&#21151;&#23436;&#25104;&#36825;&#26679;&#30340;&#20219;&#21153;&#34987;&#35748;&#20026;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#39069;&#39030;&#22238;&#36335;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#36890;&#36807;&#36873;&#25321;&#24615;&#8220;&#23553;&#38145;&#8221;&#23454;&#29616;&#20102;&#23545;&#20449;&#24687;&#30340;&#26356;&#26032;&#21644;&#35835;&#21462;&#65292;&#36825;&#20123;&#20449;&#24687;&#20197;&#31070;&#32463;&#20803;&#22242;&#31751;&#30340;&#24418;&#24335;&#23384;&#20648;&#22312;&#35760;&#24518;&#30340;&#19981;&#21516;&#8220;&#22320;&#22336;&#8221;&#19978;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#24182;&#27809;&#26377;&#26377;&#24847;&#22320;&#20869;&#32622;&#36825;&#26679;&#30340;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#20197;&#21450;&#20026;&#27492;&#32780;&#20986;&#29616;&#30340;&#26426;&#21046;&#26159;&#21542;&#19982;&#20154;&#33041;&#30340;&#23553;&#38145;&#26426;&#21046;&#30456;&#20284;&#37117;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20165;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22522;&#26412;Transformer&#19978;&#35757;&#32451;&#19968;&#20010;&#21463;&#21040;&#24037;&#20316;&#35760;&#24518;&#30740;&#31350;&#20219;&#21153;&#21551;&#21457;&#30340;&#31616;&#21333;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex "cognitive branching" -- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct "addresses" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#28165;&#27927;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08209</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#38408;&#20540;&#25968;&#25454;Shapley&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;
&lt;/p&gt;
&lt;p&gt;
Thresholding Data Shapley for Data Cleansing Using Multi-Armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#28165;&#27927;&#30340;&#35745;&#31639;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28165;&#27927;&#26088;&#22312;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21024;&#38500;&#19968;&#32452;&#26377;&#23475;&#23454;&#20363;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25968;&#25454;Shapley&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#23454;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#65307;&#28982;&#32780;&#65292;&#23427;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#25152;&#26377;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38408;&#20540;&#36172;&#21338;&#26426;&#31639;&#27861;&#24555;&#36895;&#35782;&#21035;&#20855;&#26377;&#20302;&#25968;&#25454;Shapley&#20540;&#30340;&#23454;&#20363;&#23376;&#38598;&#30340;&#36845;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#22914;&#26524;&#36827;&#34892;&#36275;&#22815;&#22810;&#30340;&#36845;&#20195;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#36873;&#25321;&#26377;&#23475;&#23454;&#20363;&#12290;&#20351;&#29992;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data cleansing aims to improve model performance by removing a set of harmful instances from the training dataset. Data Shapley is a common theoretically guaranteed method to evaluate the contribution of each instance to model performance; however, it requires training on all subsets of the training data, which is computationally expensive. In this paper, we propose an iterativemethod to fast identify a subset of instances with low data Shapley values by using the thresholding bandit algorithm. We provide a theoretical guarantee that the proposed method can accurately select harmful instances if a sufficiently large number of iterations is conducted. Empirical evaluation using various models and datasets demonstrated that the proposed method efficiently improved the computational speed while maintaining the model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27867;&#21270;&#38382;&#39064;&#20197;&#21450;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08208</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#22266;&#26377;&#22810;&#26679;&#21270;&#20887;&#20313;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Inherent Diverse Redundant Safety Mechanisms for AI-based Software Elements in Automotive Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#27867;&#21270;&#38382;&#39064;&#20197;&#21450;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#21644;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#20803;&#32032;&#12290;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#22797;&#26434;&#21644;&#39640;&#32500;&#29615;&#22659;&#20013;&#25191;&#34892;&#23454;&#26102;&#20851;&#38190;&#21151;&#33021;&#65292;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#20915;&#31574;&#20219;&#21153;&#65292;&#22914;&#36816;&#21160;&#35268;&#21010;&#12289;&#36710;&#36947;&#20445;&#25345;&#21644;&#32039;&#24613;&#21046;&#21160;&#12290;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;AI&#27169;&#22411;&#22312;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#22914;&#20309;&#36827;&#34892;&#27867;&#21270;&#12290;&#36825;&#31181;&#27867;&#21270;&#38382;&#39064;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#21464;&#24471;&#26126;&#26174;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19981;&#22312;&#20854;&#35757;&#32451;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#34920;&#31034;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#38754;&#20020;&#20998;&#24067;&#25110;&#39046;&#22495;&#36716;&#31227;&#65292;AI&#31995;&#32479;&#20173;&#24517;&#39035;&#26377;&#25928;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#36807;&#24230;&#33258;&#20449;&#30340;AI&#27169;&#22411;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#19968;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the role and challenges of Artificial Intelligence (AI) algorithms, specifically AI-based software elements, in autonomous driving systems. These AI systems are fundamental in executing real-time critical functions in complex and high-dimensional environments. They handle vital tasks like multi-modal perception, cognition, and decision-making tasks such as motion planning, lane keeping, and emergency braking. A primary concern relates to the ability (and necessity) of AI models to generalize beyond their initial training data. This generalization issue becomes evident in real-time scenarios, where models frequently encounter inputs not represented in their training or validation data. In such cases, AI systems must still function effectively despite facing distributional or domain shifts. This paper investigates the risk associated with overconfident AI models in safety-critical applications like autonomous driving. To mitigate these risks, methods for training AI m
&lt;/p&gt;</description></item><item><title>PSC-CPI&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.08198</link><description>&lt;p&gt;
PSC-CPI: &#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and Generalizable Compound-Protein Interaction Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08198
&lt;/p&gt;
&lt;p&gt;
PSC-CPI&#26159;&#19968;&#31181;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#29992;&#20110;&#39640;&#25928;&#19988;&#20855;&#26377;&#21487;&#25512;&#24191;&#24615;&#30340;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;CPI&#65289;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#21270;&#21512;&#29289;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#21644;&#24378;&#24230;&#65292;&#20197;&#29992;&#20110;&#29702;&#24615;&#33647;&#29289;&#21457;&#29616;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;&#34507;&#30333;&#36136;&#24207;&#21015;&#25110;&#32467;&#26500;&#30340;&#21333;&#19968;&#27169;&#24577;&#65292;&#32570;&#20047;&#23545;&#20004;&#20010;&#27169;&#24577;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#20849;&#21516;&#24314;&#27169;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#65292;&#27169;&#24577;&#20002;&#22833;&#21644;&#39046;&#22495;&#36716;&#31227;&#65289;&#32780;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#20197;&#21333;&#19968;&#22266;&#23450;&#23610;&#24230;&#27169;&#25311;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#26356;&#31934;&#32454;&#30340;&#22810;&#23610;&#24230;&#20449;&#24687;&#65292;&#20363;&#22914;&#23884;&#20837;&#22312;&#20851;&#38190;&#34507;&#30333;&#36136;&#29255;&#27573;&#20013;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#23545;&#27604;&#26694;&#26550;&#29992;&#20110;CPI&#39044;&#27979;&#65288;PSC-CPI&#65289;&#65292;&#36890;&#36807;&#20869;&#27169;&#24577;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25429;&#33719;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#21487;&#21464;&#38271;&#24230;&#30340;&#34507;&#30333;&#36136;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#20801;&#35768;&#23545;&#19981;&#21516;&#38271;&#24230;&#30340;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compound-Protein Interaction (CPI) prediction aims to predict the pattern and strength of compound-protein interactions for rational drug discovery. Existing deep learning-based methods utilize only the single modality of protein sequences or structures and lack the co-modeling of the joint distribution of the two modalities, which may lead to significant performance drops in complex real-world scenarios due to various factors, e.g., modality missing and domain shifting. More importantly, these methods only model protein sequences and structures at a single fixed scale, neglecting more fine-grained multi-scale information, such as those embedded in key protein fragments. In this paper, we propose a novel multi-scale Protein Sequence-structure Contrasting framework for CPI prediction (PSC-CPI), which captures the dependencies between protein sequences and structures through both intra-modality and cross-modality contrasting. We further apply length-variable protein augmentation to allow
&lt;/p&gt;</description></item><item><title>THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08191</link><description>&lt;p&gt;
THE COLOSSEUM&#65306;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#27867;&#21270;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08191
&lt;/p&gt;
&lt;p&gt;
THE COLOSSEUM&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#22312;&#24178;&#25200;&#22240;&#32032;&#19979;&#30340;&#25104;&#21151;&#29575;&#19979;&#38477;&#20102;30-50%&#12290;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#12289;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#29615;&#22659;&#26465;&#20214;&#21464;&#21270;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#35780;&#20272;&#26426;&#22120;&#20154;&#22312;&#19982;&#35757;&#32451;&#35774;&#32622;&#38750;&#24120;&#30456;&#20284;&#29978;&#33267;&#30456;&#21516;&#30340;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#27979;&#35797;THE COLOSSEUM&#65292;&#20854;&#20013;&#21253;&#25324;20&#20010;&#19981;&#21516;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21487;&#20197;&#23545;&#27169;&#22411;&#22312;12&#20010;&#29615;&#22659;&#24178;&#25200;&#36724;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#36825;&#20123;&#24178;&#25200;&#21253;&#25324;&#29289;&#20307;&#12289;&#26700;&#38754;&#21644;&#32972;&#26223;&#30340;&#39068;&#33394;&#12289;&#32441;&#29702;&#21644;&#22823;&#23567;&#30340;&#21464;&#21270;&#65307;&#25105;&#20204;&#36824;&#25913;&#21464;&#20102;&#20809;&#29031;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#30456;&#26426;&#23039;&#24577;&#12290;&#20351;&#29992;THE COLOSSEUM&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;4&#20010;&#26368;&#20808;&#36827;&#30340;&#25805;&#20316;&#27169;&#22411;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#22312;&#36825;&#20123;&#24178;&#25200;&#22240;&#32032;&#19979;&#19979;&#38477;&#20102;30-50%&#12290;&#24403;&#22810;&#20010;&#24178;&#25200;&#21516;&#26102;&#24212;&#29992;&#26102;&#65292;&#25104;&#21151;&#29575;&#19979;&#38477;&#33267;&#8805;75%&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25913;&#21464;&#24178;&#25200;&#23545;&#35937;&#25968;&#37327;&#12289;&#30446;&#26631;&#23545;&#35937;&#39068;&#33394;&#25110;&#20809;&#29031;&#26465;&#20214;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize effective large-scale, real-world robotic applications, we must evaluate how well our robot policies adapt to changes in environmental conditions. Unfortunately, a majority of studies evaluate robot performance in environments closely resembling or even identical to the training setup. We present THE COLOSSEUM, a novel simulation benchmark, with 20 diverse manipulation tasks, that enables systematical evaluation of models across 12 axes of environmental perturbations. These perturbations include changes in color, texture, and size of objects, table-tops, and backgrounds; we also vary lighting, distractors, and camera pose. Using THE COLOSSEUM, we compare 4 state-of-the-art manipulation models to reveal that their success rate degrades between 30-50% across these perturbation factors. When multiple perturbations are applied in unison, the success rate degrades $\geq$75%. We identify that changing the number of distractor objects, target object color, or lighting conditions ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;&#31574;&#30053;&#65292;&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;AFNO&#65289;&#27169;&#22411;&#21644;&#26102;&#38388;&#28369;&#21160;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#28155;&#21152;&#20102;&#26356;&#22810;&#21464;&#37327;&#21644;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08185</link><description>&lt;p&gt;
&#25512;&#21160;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;&#65306;ERA5&#30340;&#26102;&#38388;&#28369;&#21160;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Advancing Data-driven Weather Forecasting: Time-Sliding Data Augmentation of ERA5
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;&#31574;&#30053;&#65292;&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#25968;&#25454;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;AFNO&#65289;&#27169;&#22411;&#21644;&#26102;&#38388;&#28369;&#21160;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#28155;&#21152;&#20102;&#26356;&#22810;&#21464;&#37327;&#21644;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#27169;&#20223;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#24182;&#20174;&#20840;&#29699;&#22823;&#27668;&#20877;&#20998;&#26512;&#25968;&#25454;&#27966;&#29983;&#32780;&#26469;&#65292;&#22312;&#20960;&#24180;&#20869;&#24341;&#36215;&#20102;&#37325;&#22823;&#38761;&#21629;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#19981;&#20877;&#20381;&#36182;&#20110;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65288;&#36890;&#24120;&#21463;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65289;&#65292;&#32780;&#26159;&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#65288;2.5&#24230;&#65289;&#36827;&#34892;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#25968;&#25454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35780;&#20272;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;&#65288;DDWP&#65289;&#26694;&#26550;&#65292;&#29305;&#21035;&#20851;&#27880;&#26679;&#26412;&#22823;&#23567;&#30340;&#20805;&#20998;&#24615;&#12289;&#27169;&#22411;&#30340;&#32467;&#26500;&#25913;&#36827;&#20197;&#21450;&#27668;&#20505;&#25968;&#25454;&#34920;&#36798;&#24403;&#21069;&#27668;&#20505;&#36235;&#21183;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;AFNO&#65289;&#27169;&#22411;&#36890;&#36807;FourCastNet&#21644;&#25552;&#20986;&#30340;&#26102;&#38388;&#28369;&#21160;&#26041;&#27861;&#26469;&#25193;&#20805;ECMWF&#20877;&#20998;&#26512;v5&#65288;ERA5&#65289;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#26356;&#22810;&#21464;&#37327;&#21644;&#26032;&#39062;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning techniques, which mimic traditional numerical weather prediction (NWP) models and are derived from global atmospheric reanalysis data, have caused a significant revolution within a few years. In this new paradigm, our research introduces a novel strategy that deviates from the common dependence on high-resolution data, which is often constrained by computational resources, and instead utilizes low-resolution data (2.5 degrees) for global weather prediction and climate data analysis. Our main focus is evaluating data-driven weather prediction (DDWP) frameworks, specifically addressing sample size adequacy, structural improvements to the model, and the ability of climate data to represent current climatic trends. By using the Adaptive Fourier Neural Operator (AFNO) model via FourCastNet and a proposed time-sliding method to inflate the dataset of the ECMWF Reanalysis v5 (ERA5), this paper improves on conventional approaches by adding more variables and a novel approa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;SMAC&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#26426;&#21160;&#25216;&#33021;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08184</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#26080;&#20851;&#34920;&#31034;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#36716;&#31227;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling Multi-Agent Transfer Reinforcement Learning via Scenario Independent Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#22312;SMAC&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23398;&#20064;&#26426;&#21160;&#25216;&#33021;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22312;&#21160;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#38656;&#35201;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#36825;&#31181;&#20219;&#21153;&#26159;&#22256;&#38590;&#19988;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#65292;&#23588;&#20854;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#20132;&#20114;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#32780;&#35328;&#65292;&#30001;&#20110;&#26679;&#26412;&#22797;&#26434;&#24615;&#26497;&#39640;&#12290;&#22240;&#27492;&#65292;&#37325;&#22797;&#20351;&#29992;&#36807;&#21435;&#32463;&#39564;&#25110;&#20854;&#20182;&#26234;&#33021;&#20307;&#33719;&#24471;&#30340;&#30693;&#35782;&#21487;&#20197;&#26377;&#25928;&#21152;&#24555;&#23398;&#20064;&#36807;&#31243;&#24182;&#25552;&#21319;MARL&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21508;&#31181;&#29366;&#24577;&#31354;&#38388;&#32479;&#19968;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;MARL&#30340;&#36716;&#31227;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;StarCraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#65288;SMAC&#65289;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36890;&#36807;&#26426;&#21160;&#25216;&#33021;&#23398;&#20064;&#24471;&#21040;&#30340;&#30693;&#35782;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in tackling complex tasks that require collaboration and competition among agents in dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch is arduous and may not always be feasible, particularly for MASs with a large number of interactive agents due to the extensive sample complexity. Therefore, reusing knowledge gained from past experiences or other agents could efficiently accelerate the learning process and upscale MARL algorithms. In this study, we introduce a novel framework that enables transfer learning for MARL through unifying various state spaces into fixed-size inputs that allow one unified deep-learning policy viable in different scenarios within a MAS. We evaluated our approach in a range of scenarios within the StarCraft Multi-Agent Challenge (SMAC) environment, and the findings show significant enhancements in multi-agent learning performance using maneuvering skills learned fr
&lt;/p&gt;</description></item><item><title>LoTa-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20855;&#36523;&#20195;&#29702;&#20219;&#21153;&#35268;&#21010;&#24615;&#33021;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LLMs&#21644;&#25552;&#31034;&#36827;&#34892;&#23454;&#39564;&#65292;&#21152;&#36895;&#35821;&#35328;&#23548;&#21521;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.08178</link><description>&lt;p&gt;
LoTa-Bench: &#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#35268;&#21010;&#22120;&#23545;&#20110;&#20855;&#36523;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08178
&lt;/p&gt;
&lt;p&gt;
LoTa-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20855;&#36523;&#20195;&#29702;&#20219;&#21153;&#35268;&#21010;&#24615;&#33021;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LLMs&#21644;&#25552;&#31034;&#36827;&#34892;&#23454;&#39564;&#65292;&#21152;&#36895;&#35821;&#35328;&#23548;&#21521;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#30340;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#27604;&#36739;&#35821;&#35328;&#23548;&#21521;&#30340;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#65292;&#20851;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#26500;&#24314;&#31561;&#21508;&#31181;&#22240;&#32032;&#30340;&#35814;&#32454;&#25506;&#32034;&#20063;&#32570;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#37327;&#21270;&#23478;&#24237;&#26381;&#21153;&#20855;&#36523;&#20195;&#29702;&#30340;&#20219;&#21153;&#35268;&#21010;&#24615;&#33021;&#12290;&#20219;&#21153;&#35268;&#21010;&#22120;&#22312;&#20004;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#65306;1&#65289;ALFRED&#21644;AI2-THOR&#65292;2&#65289;Watch-And-Help&#21644;VirtualHome&#30340;&#25193;&#23637;&#12290;&#20351;&#29992;&#25552;&#20986;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#25105;&#20204;&#23545;LLMs&#21644;&#25552;&#31034;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#32447;&#35268;&#21010;&#22120;&#30340;&#20960;&#31181;&#25913;&#36827;&#12290;&#25105;&#20204;&#26399;&#26395;&#25552;&#20986;&#30340;&#22522;&#20934;&#24037;&#20855;&#23558;&#21152;&#36895;&#35821;&#35328;&#23548;&#21521;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08174</link><description>&lt;p&gt;
&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#22270;&#24418;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#23545;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#34920;&#24615;&#33410;&#28857;&#65288;&#31216;&#20026;&#22320;&#26631;&#65289;&#26469;&#34920;&#31034;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#23569;&#37327;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#65292;&#23427;&#20204;&#20316;&#20026;&#33410;&#28857;&#20301;&#32622;&#30340;&#21442;&#32771;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#31574;&#30053;&#23545;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;&#28041;&#21450;&#22320;&#26631;&#30340;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#19978;&#30028;&#12290;&#22312;&#24130;&#24459;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22320;&#26631;&#20026;&#33410;&#28857;&#20043;&#38388;&#36317;&#31163;&#25552;&#20379;&#20102;&#28176;&#36817;&#23436;&#20840;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#65288;HPLC&#65289;&#26041;&#27861;&#12290;HPLC&#23558;&#22320;&#26631;&#36873;&#25321;&#21644;&#22270;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#22270;&#34987;&#20998;&#21106;&#20026;&#36830;&#36890;&#23494;&#38598;&#30340;&#32858;&#31867;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#12290;HPLC&#21033;&#29992;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#33410;&#28857;&#20301;&#32622;&#20449;&#24687;&#30340;&#23618;&#32423;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
&lt;/p&gt;</description></item><item><title>LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.08170</link><description>&lt;p&gt;
LLaGA: &#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LLaGA: Large Language and Graph Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08170
&lt;/p&gt;
&lt;p&gt;
LLaGA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#27169;&#22411;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25512;&#21160;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#20998;&#26512;&#30340;&#36827;&#27493;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#30340;&#23835;&#36215;&#39044;&#31034;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#36824;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#30001;&#20110;&#23558;&#22270;&#32467;&#26500;&#36716;&#21270;&#20026;&#25991;&#26412;&#30340;&#22266;&#26377;&#38590;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#21644;&#22270;&#24418;&#21161;&#25163;&#65288;LLaGA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;LLM&#30340;&#33021;&#21147;&#65292;&#20197;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;LLaGA&#20445;&#30041;&#20102;LLM&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#23558;&#22270;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;LLM&#36755;&#20837;&#20860;&#23481;&#30340;&#26684;&#24335;&#12290;LLaGA&#36890;&#36807;&#37325;&#26032;&#32452;&#32455;&#22270;&#33410;&#28857;&#20197;&#20316;&#20026;&#32467;&#26500;&#24863;&#30693;&#30340;&#24207;&#21015;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#22810;&#21151;&#33021;&#25237;&#24433;&#20202;&#23558;&#20854;&#26144;&#23556;&#21040;&#26631;&#35760;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;LLaGA&#22312;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.08164</link><description>&lt;p&gt;
&#20851;&#20110;Transformer&#26550;&#26500;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On Limitations of the Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#20351;&#29992;&#36890;&#20449;&#22797;&#26434;&#24615;&#26469;&#35777;&#26126;&#65292;&#22914;&#26524;&#20989;&#25968;&#30340;&#23450;&#20041;&#22495;&#36275;&#22815;&#22823;&#65292;Transformer&#23618;&#26080;&#27861;&#32452;&#21512;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#22312;&#23478;&#35889;&#20013;&#26597;&#25214;&#19968;&#20010;&#20154;&#30340;&#31062;&#29238;&#65289;&#65307;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#26174;&#31034;&#65292;&#24403;&#23450;&#20041;&#22495;&#30456;&#24403;&#23567;&#30340;&#26102;&#20505;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#32570;&#20047;&#24050;&#32463;&#22312;&#32463;&#39564;&#19978;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#35768;&#22810;&#22312;&#25152;&#35859;&#30340;&#32452;&#21512;&#20219;&#21153;&#20013;&#30340;&#25968;&#23398;&#20219;&#21153;&#65292;&#35748;&#20026;&#23427;&#20204;&#23545;LLMs&#26469;&#35828;&#24456;&#38590;&#35299;&#20915;&#65292;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#23454;&#20363;&#26469;&#35828;&#65292;&#19988;&#20551;&#35774;&#35745;&#31639;&#22797;&#26434;&#24615;&#39046;&#22495;&#30340;&#26576;&#20123;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#29468;&#24819;&#26159;&#27491;&#30830;&#30340;&#65292;Transformers&#20063;&#19981;&#22826;&#21487;&#33021;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
&lt;/p&gt;</description></item><item><title>CMA-R&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;&#20851;&#38190;&#25512;&#25991;&#21644;&#22240;&#26524;&#24433;&#21709;&#21333;&#35789;&#65292;&#25552;&#39640;&#20102;&#23545;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08155</link><description>&lt;p&gt;
CMA-R&#65306;&#29992;&#20110;&#35299;&#37322;&#35875;&#35328;&#26816;&#27979;&#30340;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CMA-R:Causal Mediation Analysis for Explaining Rumour Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08155
&lt;/p&gt;
&lt;p&gt;
CMA-R&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#35299;&#37322;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#20986;&#20851;&#38190;&#25512;&#25991;&#21644;&#22240;&#26524;&#24433;&#21709;&#21333;&#35789;&#65292;&#25552;&#39640;&#20102;&#23545;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#24212;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#27169;&#22411;&#22312;Twitter&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#36755;&#20837;&#21644;&#32593;&#32476;&#23618;&#38754;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#20013;&#25512;&#25991;&#21644;&#21333;&#35789;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;CMA-R - &#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979; - &#21487;&#20197;&#35782;&#21035;&#20986;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20851;&#20110;&#25925;&#20107;&#30495;&#23454;&#24615;&#30340;&#20851;&#38190;&#25512;&#25991;&#30340;&#26174;&#33879;&#24615;&#25512;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#24378;&#28872;&#30340;&#19968;&#33268;&#24615;&#12290;CMA-R&#36824;&#21487;&#20197;&#31361;&#20986;&#26174;&#33879;&#24615;&#25512;&#25991;&#20013;&#20855;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#21333;&#35789;&#65292;&#25552;&#20379;&#23545;&#36825;&#20123;&#40657;&#30418;&#23376;&#35875;&#35328;&#26816;&#27979;&#31995;&#32479;&#30340;&#21478;&#19968;&#23618;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/ltian678/cma-r.
&lt;/p&gt;
&lt;p&gt;
We apply causal mediation analysis to explain the decision-making process of neural models for rumour detection on Twitter. Interventions at the input and network level reveal the causal impacts of tweets and words in the model output. We find that our approach CMA-R -- Causal Mediation Analysis for Rumour detection -- identifies salient tweets that explain model predictions and show strong agreement with human judgements for critical tweets determining the truthfulness of stories. CMA-R can further highlight causally impactful words in the salient tweets, providing another layer of interpretability and transparency into these blackbox rumour detection systems. Code is available at: https://github.com/ltian678/cma-r.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;</title><link>https://arxiv.org/abs/2402.08147</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#39564;&#35777;&#30340;&#22810;&#27493;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Verified Multi-Step Synthesis using Large Language Models and Monte Carlo Tree Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39564;&#35777;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#21644;LLM&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20102;&#21512;&#25104;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#32452;&#39564;&#35777;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#20855;&#26377;&#25554;&#20214;&#30340;ChatGPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22312;Dafny&#12289;Lean&#21644;Coq&#20013;&#39564;&#35777;&#30340;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;VMCTS&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#27493;&#39588;&#26816;&#26597;&#37096;&#20998;&#31243;&#24207;&#26469;&#21033;&#29992;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#39564;&#35777;&#22120;&#12290;&#32467;&#21512;LLM&#20808;&#39564;&#30693;&#35782;&#65292;&#39564;&#35777;&#22120;&#30340;&#21453;&#39304;&#25552;&#39640;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#21512;&#25104;&#33021;&#21147;&#12290;&#22312;&#19968;&#32452;&#20116;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#22235;&#20010;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#37325;&#26032;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#19968;&#23567;&#26102;&#30340;&#37325;&#26032;&#37319;&#26679;&#65292;&#22522;&#26412;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;VMCTS&#21487;&#20197;&#22312;6&#20998;&#38047;&#20869;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#65292;&#22522;&#26412;&#27169;&#22411;&#21152;&#19978;VMCTS&#29978;&#33267;&#19982;&#20855;&#26377;&#25554;&#20214;&#21644;&#22810;&#27425;&#37325;&#35797;&#30340;ChatGPT4&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#21487;&#22312;https://github.com/namin/llm-verified-with-monte-carlo-tree-search&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach using Monte Carlo Tree Search (MCTS) to guide Large Language Models (LLMs) to generate verified programs in Dafny, Lean and Coq. Our method, which we call VMCTS, leverages the verifier inside the search algorithm by checking partial programs at each step. In combination with the LLM prior, the verifier feedback raises the synthesis capabilities of open source models. On a set of five verified programming problems, we find that in four problems where the base model cannot solve the question even when re-sampling solutions for one hour, VMCTS can solve the problems within 6 minutes. The base model with VMCTS is even competitive with ChatGPT4 augmented with plugins and multiple re-tries on these problems. Our code and benchmarks are available at https://github.com/namin/llm-verified-with-monte-carlo-tree-search .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#35782;&#24615;&#25506;&#32034;&#22635;&#34917;&#20195;&#29702;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#19988;&#21033;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.08145</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#35748;&#35782;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#36890;&#29992;&#35268;&#21010;&#21644;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35748;&#35782;&#24615;&#25506;&#32034;&#22635;&#34917;&#20195;&#29702;&#30340;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#19988;&#21033;&#29992;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20351;&#29992;&#20851;&#31995;&#34920;&#31034;&#30340;&#38750;&#31283;&#24577;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36830;&#32493;&#35268;&#21010;&#21644;&#27169;&#22411;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#26679;&#30340;&#33021;&#21147;&#23545;&#20110;&#22312;&#19981;&#30830;&#23450;&#12289;&#19981;&#26029;&#21457;&#23637;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#39034;&#24207;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26410;&#30693;&#65288;&#19988;&#38750;&#31283;&#24577;&#65289;&#36716;&#25442;&#31995;&#32479;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#24037;&#20316;&#26102;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#20195;&#29702;&#29366;&#24577;&#30693;&#35782;&#30340;&#31354;&#30333;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24320;&#23637;&#19987;&#27880;&#12289;&#35843;&#26597;&#24615;&#30340;&#25506;&#32034;&#12290;&#20351;&#29992;&#36825;&#20123;&#25506;&#32034;&#25910;&#38598;&#30340;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#30340;&#20219;&#21153;&#65292;&#23613;&#31649;&#29615;&#22659;&#21160;&#24577;&#19981;&#26029;&#21464;&#21270;&#12290;&#23545;&#20960;&#20010;&#22522;&#20934;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#22815;&#22238;&#24402;&#21040;&#34920;&#29616;&#20986;&#29702;&#24819;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach for continual planning and model learning in non-stationary stochastic environments expressed using relational representations. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain, constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent's current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity in non-stationary settings. Theoretical results show that the system reverts to exhibit desirable convergence pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#20154;&#20559;&#22909;&#20998;&#24067;&#30340;&#24179;&#22343;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#36845;&#20195;&#25237;&#31080;&#27169;&#22411;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#24182;&#19988;&#21306;&#20998;&#20102;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;</title><link>https://arxiv.org/abs/2402.08144</link><description>&lt;p&gt;
&#36845;&#20195;&#25237;&#31080;&#30340;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Average-Case Analysis of Iterative Voting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08144
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20998;&#26512;&#20195;&#29702;&#20154;&#20559;&#22909;&#20998;&#24067;&#30340;&#24179;&#22343;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#36845;&#20195;&#25237;&#31080;&#27169;&#22411;&#30340;&#25928;&#26524;&#20998;&#26512;&#12290;&#24182;&#19988;&#21306;&#20998;&#20102;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#25237;&#31080;&#26159;&#31038;&#20250;&#36873;&#25321;&#20013;&#37325;&#22797;&#25112;&#30053;&#20915;&#31574;&#30340;&#33258;&#28982;&#27169;&#22411;&#65292;&#24403;&#20195;&#29702;&#21487;&#20197;&#22312;&#26368;&#32456;&#30830;&#23450;&#32676;&#20307;&#20915;&#31574;&#20043;&#21069;&#26356;&#26032;&#20182;&#20204;&#30340;&#25237;&#31080;&#26102;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#26080;&#24207;&#25991;&#21270;&#19979;&#20195;&#29702;&#20154;&#20559;&#22909;&#30340;&#26368;&#22351;&#24773;&#20917;&#21644;&#24179;&#22343;&#24773;&#20917;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#36890;&#36807;&#25913;&#36827;&#23433;&#32435;&#22522;&#20215;&#26684;&#26469;&#20998;&#26512;&#36845;&#20195;&#22810;&#25968;&#21046;&#23545;&#24179;&#34913;&#28857;&#36873;&#20986;&#30340;&#32467;&#26524;&#31119;&#21033;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#20998;&#26512;&#21482;&#30740;&#31350;&#20102;&#22312;&#20195;&#29702;&#20154;&#20559;&#22909;&#36890;&#36807;&#26080;&#20559;&#25991;&#21270;&#20998;&#24067;&#30340;&#26368;&#22351;&#24773;&#20917;&#21644;&#24179;&#22343;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23558;&#24179;&#22343;&#24773;&#20917;&#20998;&#26512;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20998;&#24067;&#31867;&#65292;&#24182;&#21306;&#20998;&#20986;&#36845;&#20195;&#22810;&#25968;&#21046;&#20309;&#26102;&#25913;&#21892;&#25110;&#38477;&#20302;&#28176;&#36817;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Iterative voting is a natural model of repeated strategic decision-making in social choice when agents have the opportunity to update their votes prior to finalizing the group decision. Prior work has analyzed the efficacy of iterative plurality on the welfare of the chosen outcome at equilibrium, relative to the truthful vote profile, via an adaptation of the price of anarchy. However, prior analyses have only studied the worst-case and average-case performances when agents' preferences are distributed by the impartial culture. This work extends average-case analyses to a wider class of distributions and distinguishes when iterative plurality improves or degrades asymptotic welfare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20013;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#24335;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#22312;&#25112;&#30053;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08128</link><description>&lt;p&gt;
&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#22312;&#28216;&#25103;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recursive Joint Simulation in Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20013;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#26041;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#24335;&#19982;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#22312;&#25112;&#30053;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#20195;&#29702;&#20043;&#38388;&#30340;&#21338;&#24328;&#21160;&#24577;&#19982;&#20256;&#32479;&#30340;&#20154;-&#20154;&#20114;&#21160;&#21487;&#33021;&#23384;&#22312;&#21508;&#31181;&#19981;&#21516;&#20043;&#22788;&#12290;&#20854;&#20013;&#19968;&#20010;&#21306;&#21035;&#26159;&#21487;&#33021;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;AI&#20195;&#29702;&#65292;&#20363;&#22914;&#22240;&#20026;&#20854;&#28304;&#20195;&#30721;&#26159;&#24050;&#30693;&#30340;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#36825;&#31181;&#21487;&#33021;&#24615;&#22312;&#25112;&#30053;&#35774;&#32622;&#20013;&#23454;&#29616;&#26356;&#21512;&#20316;&#30340;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20195;&#29702;&#20043;&#38388;&#36816;&#34892;&#36882;&#24402;&#21327;&#21516;&#27169;&#25311;&#30340;&#20114;&#21160;&#12290;&#21363;&#65292;&#20195;&#29702;&#39318;&#20808;&#20849;&#21516;&#35266;&#23519;&#20182;&#20204;&#25152;&#38754;&#23545;&#24773;&#22659;&#30340;&#27169;&#25311;&#12290;&#36825;&#31181;&#27169;&#25311;&#21453;&#36807;&#26469;&#36882;&#24402;&#22320;&#21253;&#25324;&#20102;&#39069;&#22806;&#30340;&#27169;&#25311;&#65288;&#20026;&#20102;&#36991;&#20813;&#26080;&#38480;&#36882;&#24402;&#65292;&#20855;&#26377;&#23567;&#27010;&#29575;&#30340;&#22833;&#36133;&#65289;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#34892;&#21160;&#20043;&#21069;&#35266;&#23519;&#25152;&#26377;&#36825;&#20123;&#23884;&#22871;&#27169;&#25311;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#20114;&#21160;&#22312;&#25112;&#30053;&#19978;&#31561;&#20215;&#20110;&#21407;&#22987;&#28216;&#25103;&#30340;&#26080;&#38480;&#37325;&#22797;&#29256;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#36716;&#31227;&#35832;&#22914;&#21508;&#31181;&#27665;&#38388;&#23450;&#29702;&#31561;&#29616;&#26377;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic dynamics between AI agents could differ from traditional human-human interactions in various ways. One such difference is that it may be possible to accurately simulate an AI agent, for example because its source code is known. Our aim is to explore ways of leveraging this possibility to achieve more cooperative outcomes in strategic settings. In this paper, we study an interaction between AI agents where the agents run a recursive joint simulation. That is, the agents first jointly observe a simulation of the situation they face. This simulation in turn recursively includes additional simulations (with a small chance of failure, to avoid infinite recursion), and the results of all these nested simulations are observed before an action is chosen. We show that the resulting interaction is strategically equivalent to an infinitely repeated version of the original game, allowing a direct transfer of existing results such as the various folk theorems.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23450;&#21046;&#21270;&#30340;&#22122;&#22768;&#25968;&#25454;&#21512;&#25104;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;SLAM&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#27969;&#31243;&#32467;&#21512;&#20102;&#21487;&#23450;&#21046;&#21270;&#30828;&#20214;&#37197;&#32622;&#12289;&#36719;&#20214;&#32452;&#20214;&#21644;&#34987;&#25200;&#21160;&#29615;&#22659;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#25200;&#21160;&#20998;&#31867;&#27861;&#20197;&#21450;&#25200;&#21160;&#32452;&#21512;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#24178;&#20928;&#30340;&#20223;&#30495;&#36716;&#21270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22024;&#26434;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#40065;&#26834;-SLAM&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.08125</link><description>&lt;p&gt;
&#21487;&#33258;&#23450;&#20041;&#25200;&#21160;&#21512;&#25104;&#29992;&#20110;&#40065;&#26834;&#24615;SLAM&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Customizable Perturbation Synthesis for Robust SLAM Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08125
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23450;&#21046;&#21270;&#30340;&#22122;&#22768;&#25968;&#25454;&#21512;&#25104;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;SLAM&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#27969;&#31243;&#32467;&#21512;&#20102;&#21487;&#23450;&#21046;&#21270;&#30828;&#20214;&#37197;&#32622;&#12289;&#36719;&#20214;&#32452;&#20214;&#21644;&#34987;&#25200;&#21160;&#29615;&#22659;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#25200;&#21160;&#20998;&#31867;&#27861;&#20197;&#21450;&#25200;&#21160;&#32452;&#21512;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#24178;&#20928;&#30340;&#20223;&#30495;&#36716;&#21270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22024;&#26434;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#40065;&#26834;-SLAM&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#26426;&#22120;&#20154;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#25104;&#21151;&#37096;&#32626;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#22312;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#39046;&#22495;&#12290;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30456;&#27604;&#65292;&#22522;&#20110;&#20223;&#30495;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#25104;&#20026;&#19968;&#31181;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#20855;&#26377;&#22810;&#26679;&#21270;&#25200;&#21160;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#21487;&#25511;&#30340;&#22024;&#26434;&#29615;&#22659;&#20173;&#28982;&#30456;&#23545;&#26410;&#24320;&#21457;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23450;&#21046;&#21270;&#30340;&#22122;&#22768;&#25968;&#25454;&#21512;&#25104;&#27969;&#31243;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;SLAM&#27169;&#22411;&#23545;&#21508;&#31181;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#27969;&#31243;&#32467;&#21512;&#20102;&#21487;&#23450;&#21046;&#21270;&#30828;&#20214;&#37197;&#32622;&#12289;&#36719;&#20214;&#32452;&#20214;&#21644;&#34987;&#25200;&#21160;&#29615;&#22659;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#25200;&#21160;&#20998;&#31867;&#27861;&#20197;&#21450;&#25200;&#21160;&#32452;&#21512;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#24178;&#20928;&#30340;&#20223;&#30495;&#36716;&#21270;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22024;&#26434;&#29615;&#22659;&#12290;&#21033;&#29992;&#35813;&#27969;&#31243;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#40065;&#26834;-SLAM&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#30340;&#25200;&#21160;&#21512;&#25104;&#21644;&#40065;&#26834;&#24615;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a crucial factor for the successful deployment of robots in unstructured environments, particularly in the domain of Simultaneous Localization and Mapping (SLAM). Simulation-based benchmarks have emerged as a highly scalable approach for robustness evaluation compared to real-world data collection. However, crafting a challenging and controllable noisy world with diverse perturbations remains relatively under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. This pipeline incorporates customizable hardware setups, software components, and perturbed environments. In particular, we introduce comprehensive perturbation taxonomy along with a perturbation composition toolbox, allowing the transformation of clean simulations into challenging noisy environments. Utilizing the pipeline, we instantiate the Robust-SLAM benchmark, which includes divers
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20102;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08115</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#38480;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20102;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#35266;&#28857;&#24046;&#24322;&#12290;&#23613;&#31649;&#26368;&#21021;&#23545;&#20110;&#25512;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#35268;&#27169;&#30340;&#25193;&#22823;&#33258;&#21160;&#20986;&#29616;&#30340;&#20048;&#35266;&#24773;&#32490;&#24050;&#32463;&#21463;&#21040;&#20102;&#19968;&#31995;&#21015;&#21453;&#20363;&#30340;&#25233;&#21046;&#65292;&#20174;&#20056;&#27861;&#21040;&#31616;&#21333;&#35268;&#21010;&#65292;&#20294;&#20173;&#28982;&#26222;&#36941;&#35748;&#20026;LLMs&#21487;&#20197;&#33258;&#25105;&#25209;&#21028;&#24182;&#36845;&#20195;&#25913;&#36827;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#20449;&#24565;&#20284;&#20046;&#24314;&#31435;&#22312;&#39564;&#35777;&#27491;&#30830;&#24615;&#27604;&#29983;&#25104;&#26356;&#23481;&#26131;&#30340;&#20551;&#35774;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#35770;&#35777;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#24212;&#35813;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#25152;&#20570;&#30340;&#26159;&#36817;&#20284;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#29702;&#21644;&#35268;&#21010;&#29615;&#22659;&#20013;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290; &#25105;&#20204;&#23545;GPT-4&#22312;&#19977;&#20010;&#39046;&#22495;&#65288;24&#28857;&#28216;&#25103;&#12289;&#22270;&#30528;&#33394;&#21644;STRIPS&#35268;&#21010;&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#26377;&#21407;&#21017;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#25209;&#21028;&#24615;&#23454;&#39564;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08114</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Preference Learning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#21160;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#30340;&#24494;&#35843;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23545;&#20110;&#23545;&#40784;&#36825;&#20123;&#27169;&#22411;&#26469;&#35828;&#65292;&#26368;&#20851;&#38190;&#30340;&#32771;&#34385;&#26159;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#20154;&#21147;&#36164;&#28304;&#65292;&#25110;&#32773;&#22312;LLM&#26412;&#36523;&#34987;&#29992;&#20316;oracle&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#36164;&#28304;&#12290;&#20174;&#20154;&#31867;&#25110;AI&#20559;&#22909;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF / RLAIF&#65289;&#26159;&#36825;&#31181;&#25216;&#26415;&#26368;&#31361;&#20986;&#30340;&#20363;&#23376;&#65292;&#20294;&#23427;&#24448;&#24448;&#22797;&#26434;&#19988;&#19981;&#31283;&#23450;&#12290;&#26368;&#36817;&#65292;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#21644;&#26356;&#31283;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;DPO&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#20559;&#22909;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#29109;&#21644;DPO&#20248;&#21270;&#30340;&#38544;&#24335;&#20559;&#22909;&#27169;&#22411;&#30340;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#23454;&#29992;&#37319;&#38598;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#22522;&#20110;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#24494;&#35843;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
&lt;/p&gt;</description></item><item><title>&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08112</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;microRTS&#20013;&#33719;&#22870;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Competition Winning Deep Reinforcement Learning Agent in microRTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08112
&lt;/p&gt;
&lt;p&gt;
&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CIG&#21644;CoG&#20030;&#21150;&#30340;IEEE microRTS&#65288;$\mu$RTS&#65289;&#31454;&#36187;&#30340;&#20116;&#23626;&#20013;&#65292;&#33050;&#26412;&#20195;&#29702;&#20027;&#23548;&#20102;&#27604;&#36187;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#22312;&#23454;&#26102;&#31574;&#30053;&#65288;RTS&#65289;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#36164;&#28304;&#20197;&#21450;&#21019;&#24314;&#21644;&#35843;&#35797;&#27492;&#31867;&#20195;&#29702;&#25152;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20027;&#35201;&#26159;&#23398;&#26415;&#31454;&#36187;&#20013;&#30340;&#37319;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;RAISocketAI&#26159;&#31532;&#19968;&#20010;&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#33719;&#32988;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#19968;&#20010;&#27809;&#26377;&#24615;&#33021;&#38480;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RAISocketAI&#32463;&#24120;&#20987;&#36133;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#12290;&#36825;&#20010;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;DRL&#25552;&#20132;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;microRTS&#31454;&#36187;&#30340;&#22522;&#20934;&#65292;&#24182;&#25104;&#20026;&#26410;&#26469;DRL&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#23545;&#29305;&#23450;&#22320;&#22270;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#23545;RAISocketAI&#30340;&#33719;&#32988;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#32463;&#27982;&#35757;&#32451;&#26410;&#26469;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#21644;&#30417;&#27979;&#25968;&#25454;&#28418;&#31227;&#12290;&#35813;&#26694;&#26550;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;ML&#35774;&#22791;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#21644;&#24739;&#32773;&#23433;&#20840;&#12290;</title><link>https://arxiv.org/abs/2402.08088</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#21644;&#25968;&#25454;&#28418;&#31227;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection and Data Drift Monitoring using Statistical Process Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08088
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#21644;&#30417;&#27979;&#25968;&#25454;&#28418;&#31227;&#12290;&#35813;&#26694;&#26550;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#65292;&#33021;&#22815;&#24110;&#21161;&#25552;&#39640;ML&#35774;&#22791;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#20013;&#30340;&#24615;&#33021;&#21644;&#24739;&#32773;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24448;&#24448;&#22312;&#25968;&#25454;&#20559;&#31163;&#20854;&#35757;&#32451;&#20998;&#24067;&#26102;&#22833;&#25928;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;ML&#35774;&#22791;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#28418;&#31227;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#21361;&#21450;&#24739;&#32773;&#23433;&#20840;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ML&#30340;&#32479;&#35745;&#36807;&#31243;&#25511;&#21046;&#65288;SPC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#21644;&#28418;&#31227;&#30417;&#27979;&#12290;SPC&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#30452;&#35266;&#22320;&#21644;&#32479;&#35745;&#19978;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#26399;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#25968;&#25454;&#28418;&#31227;&#30417;&#27979;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#19981;&#21516;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#21253;&#25324;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#12289;&#28418;&#31227;&#37327;&#21270;&#21644;SPC&#21442;&#25968;&#36873;&#25321;&#31561;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;&#21306;&#20998;&#36724;&#21521;&#19982;&#38750;&#36724;&#21521;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#22270;&#20687;&#65307;2&#65289;&#23558;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#19982;&#20854;&#20182;&#27169;&#24577;&#36827;&#34892;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Machine learning (ML) methods often fail with data that deviates from their training distribution. This is a significant concern for ML-enabled devices in clinical settings, where data drift may cause unexpected performance that jeopardizes patient safety.   Method: We propose a ML-enabled Statistical Process Control (SPC) framework for out-of-distribution (OOD) detection and drift monitoring. SPC is advantageous as it visually and statistically highlights deviations from the expected distribution. To demonstrate the utility of the proposed framework for monitoring data drift in radiological images, we investigated different design choices, including methods for extracting feature representations, drift quantification, and SPC parameter selection.   Results: We demonstrate the effectiveness of our framework for two tasks: 1) differentiating axial vs. non-axial computed tomography (CT) images and 2) separating chest x-ray (CXR) from other modalities. For both tasks, we achie
&lt;/p&gt;</description></item><item><title>"&#20449;&#24687;&#32469;&#34892;"&#26159;&#19968;&#31181;&#29992;&#20110;&#23618;&#27425;&#24615;&#34920;&#24449;&#22270;&#20013;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.08085</link><description>&lt;p&gt;
&#20449;&#24687;&#32469;&#34892;&#65306;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#29992;&#20110;&#34920;&#36798;&#22270;&#23398;&#20064;&#30340;&#24490;&#29615;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Message Detouring: A Simple Yet Effective Cycle Representation for Expressive Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08085
&lt;/p&gt;
&lt;p&gt;
"&#20449;&#24687;&#32469;&#34892;"&#26159;&#19968;&#31181;&#29992;&#20110;&#23618;&#27425;&#24615;&#34920;&#24449;&#22270;&#20013;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#21270;&#23398;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#39640;&#38454;&#22270;&#24418;&#29305;&#24449;&#65292;&#22914;&#24490;&#29615;&#65292;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#12289;&#36793;&#39044;&#27979;&#21644;&#22270;&#20687;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#39640;&#38454;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#22312;&#35745;&#31639;&#19978;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#20449;&#24687;&#32469;&#34892;"&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#23618;&#27425;&#24615;&#22320;&#34920;&#24449;&#24490;&#29615;&#65292;&#21033;&#29992;&#27599;&#20010;&#22270;&#33410;&#28857;&#30456;&#20851;&#30340;&#19968;&#31995;&#21015;&#23616;&#37096;&#25299;&#25169;&#32467;&#26500;&#20013;&#26368;&#30701;&#36335;&#24452;&#21644;&#26368;&#38271;&#36335;&#24452;&#20043;&#38388;&#30340;&#23545;&#27604;&#24615;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#32469;&#34892;&#26223;&#35266;&#20013;&#24471;&#21040;&#30340;&#25299;&#25169;&#29305;&#24449;&#34920;&#31034;&#20855;&#26377;&#19982;&#39640;&#38454;"Weisfeiler-Lehman"&#65288;WL&#65289;&#27979;&#35797;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#26356;&#23569;&#12290;&#38500;&#20102;&#19982;&#22270;&#26680;&#21644;&#20449;&#24687;&#30340;&#38598;&#25104;&#22806;
&lt;/p&gt;
&lt;p&gt;
Graph learning is crucial in the fields of bioinformatics, social networks, and chemicals. Although high-order graphlets, such as cycles, are critical to achieving an informative graph representation for node classification, edge prediction, and graph recognition, modeling high-order topological characteristics poses significant computational challenges, restricting its widespread applications in machine learning. To address this limitation, we introduce the concept of \textit{message detouring} to hierarchically characterize cycle representation throughout the entire graph, which capitalizes on the contrast between the shortest and longest pathways within a range of local topologies associated with each graph node. The topological feature representations derived from our message detouring landscape demonstrate comparable expressive power to high-order \textit{Weisfeiler-Lehman} (WL) tests but much less computational demands. In addition to the integration with graph kernel and message
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lingo&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36866;&#24212;&#20102;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22810;&#26679;&#24615;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08075</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Scalable Fine-Tune of Language Models for Genome Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08075
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lingo&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22522;&#22240;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36866;&#24212;&#20102;&#22522;&#22240;&#32452;&#27880;&#37322;&#30340;&#22810;&#26679;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;DNA&#22522;&#22240;&#32452;&#27169;&#22411;&#22312;&#22522;&#22240;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22522;&#22240;&#32452;&#25968;&#25454;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#31181;&#38480;&#21046;&#19982;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#22312;&#26356;&#22823;&#35268;&#27169;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#22522;&#22240;&#29702;&#35299;&#28041;&#21450;&#35768;&#22810;&#20855;&#26377;&#22266;&#26377;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#19979;&#28216;&#22522;&#22240;&#32452;&#27880;&#37322;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#38024;&#23545;&#22522;&#22240;&#32452;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lingo&#65306;&#35821;&#35328;&#21069;&#32512;&#24494;&#35843;&#22522;&#22240;&#32452;&#27169;&#22411;&#12290;&#19982;DNA&#22522;&#22240;&#32452;&#27169;&#22411;&#19981;&#21516;&#65292;Lingo&#31574;&#30053;&#24615;&#22320;&#21033;&#29992;&#20102;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#37325;&#26032;&#26657;&#20934;&#20854;&#22312;&#22522;&#22240;&#32452;&#24207;&#21015;&#26041;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#12290;Lingo&#36890;&#36807;&#33258;&#36866;&#24212;&#31209;&#37319;&#26679;&#26041;&#27861;&#36827;&#19968;&#27493;&#36866;&#24212;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#12289;&#24322;&#36136;&#30340;&#19979;&#28216;&#24494;&#35843;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21098;&#26525;&#24182;&#38543;&#26426;&#37325;&#26032;&#24341;&#20837;&#21098;&#26525;&#30340;&#22855;&#24322;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \textsc{Lingo}: \textsc{L}anguage prefix f\textsc{In}e-tuning for \textsc{G}en\textsc{O}mes. Unlike DNA foundation models, \textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25913;&#36827;&#32534;&#31243;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#26102;&#20026;&#23398;&#29983;&#25552;&#20379;&#24110;&#21161;&#65292;&#24182;&#21457;&#29616;&#30028;&#38754;&#35774;&#35745;&#23545;&#20110;&#21453;&#39304;&#30340;&#21487;&#29992;&#24615;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08072</link><description>&lt;p&gt;
&#23454;&#26102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25913;&#36827;&#32534;&#31243;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Enhancing Programming Error Messages in Real Time with Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25913;&#36827;&#32534;&#31243;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#26102;&#20026;&#23398;&#29983;&#25552;&#20379;&#24110;&#21161;&#65292;&#24182;&#21457;&#29616;&#30028;&#38754;&#35774;&#35745;&#23545;&#20110;&#21453;&#39304;&#30340;&#21487;&#29992;&#24615;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#35768;&#22810;&#23398;&#31185;&#30340;&#25945;&#23398;&#26041;&#24335;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#33021;&#22815;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#12289;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#22359;&#65292;&#24182;&#20197;&#31616;&#21333;&#30340;&#26415;&#35821;&#35299;&#37322;&#22797;&#26434;&#30340;&#20195;&#30721;&#12290;&#22312;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#32534;&#31243;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#29305;&#21035;&#30340;&#28508;&#21147;&#12290;&#23398;&#29983;&#21644;&#25945;&#24072;&#22810;&#24180;&#26469;&#19968;&#30452;&#25265;&#24616;&#36825;&#20123;&#20449;&#24687;&#24448;&#24448;&#26214;&#28073;&#38590;&#25026;&#12290;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;GPT-4&#25913;&#36827;&#30340;&#28040;&#24687;&#20351;&#23398;&#29983;&#30340;&#38169;&#35823;&#27425;&#25968;&#20943;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;ChatGPT&#30340;&#21453;&#39304;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#24037;&#20855;Athene&#20013;&#65292;&#20026;&#25152;&#26377;&#25552;&#20132;&#30340;&#31243;&#24207;&#25552;&#20379;&#23545;&#32534;&#35793;&#22120;&#12289;&#36816;&#34892;&#26102;&#21644;&#36923;&#36753;&#38169;&#35823;&#30340;&#24110;&#21161;&#65292;&#23545;&#36825;&#39033;&#24037;&#20316;&#36827;&#34892;&#20102;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21521;&#33258;&#21160;&#21270;&#35780;&#20272;&#24037;&#20855;&#28155;&#21152;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24182;&#19981;&#19968;&#23450;&#20351;&#20854;&#21464;&#24471;&#26356;&#22909;&#65292;&#25509;&#21475;&#30340;&#35774;&#35745;&#23545;GPT-4&#25552;&#20379;&#30340;&#21453;&#39304;&#30340;&#21487;&#29992;&#24615;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19968;&#30452;&#34987;&#35270;&#20026;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Elemental Cognition&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2402.08064</link><description>&lt;p&gt;
&#36229;&#36234;LLMs&#65306;&#25512;&#36827;&#22797;&#26434;&#25512;&#29702;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Beyond LLMs: Advancing the Landscape of Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08064
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19968;&#30452;&#34987;&#35270;&#20026;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;LLMs&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;Elemental Cognition&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#37319;&#29992;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20960;&#24180;&#21069;&#20986;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26469;&#65292;&#23427;&#20204;&#24448;&#24448;&#34987;&#35270;&#20026;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#20107;&#23454;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;LLMs&#30340;&#35768;&#22810;&#19981;&#36275;&#20043;&#22806;&#65292;&#22914;&#21487;&#38752;&#24615;&#12289;&#25104;&#26412;&#21644;&#36895;&#24230;&#31561;&#38382;&#39064;&#65292;&#36824;&#26377;&#19968;&#31867;&#24120;&#35265;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#21363;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26080;&#22788;&#19981;&#22312;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#38750;&#24120;&#19987;&#19994;&#21270;&#19988;&#23454;&#26045;&#25104;&#26412;&#39640;&#12290;&#22312;Elemental Cognition&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;EC AI&#24179;&#21488;&#65292;&#35813;&#24179;&#21488;&#37319;&#29992;&#20102;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#24179;&#21488;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#30830;&#39640;&#25928;&#30340;&#36923;&#36753;&#25512;&#29702;&#24341;&#25806;&#65292;&#24182;&#21033;&#29992;LLMs&#36827;&#34892;&#30693;&#35782;&#33719;&#21462;&#21644;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#24179;&#21488;&#25903;&#25345;&#24320;&#21457;&#20154;&#21592;&#29992;&#33258;&#28982;&#31616;&#27905;&#30340;&#35821;&#35328;&#25351;&#23450;&#24212;&#29992;&#36923;&#36753;&#65292;&#24182;&#29983;&#25104;&#24212;&#29992;&#29992;&#25143;&#30028;&#38754;&#20197;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of Large Language Models a few years ago, they have often been considered the de facto solution for many AI problems. However, in addition to the many deficiencies of LLMs that prevent them from broad industry adoption, such as reliability, cost, and speed, there is a whole class of common real world problems that Large Language Models perform poorly on, namely, constraint satisfaction and optimization problems. These problems are ubiquitous and current solutions are highly specialized and expensive to implement. At Elemental Cognition, we developed our EC AI platform which takes a neuro-symbolic approach to solving constraint satisfaction and optimization problems. The platform employs, at its core, a precise and high performance logical reasoning engine, and leverages LLMs for knowledge acquisition and user interaction. This platform supports developers in specifying application logic in natural and concise language while generating application user interfaces to int
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#35775;&#35848;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21161;&#25163;&#22312;&#36719;&#20214;&#24110;&#21161;&#23547;&#27714;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20248;&#21270;&#21518;&#30340;LLM&#21161;&#25163;&#30456;&#36739;&#20110;&#22522;&#20934;LLM&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;&#20110;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08030</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;LLM&#21161;&#25163;&#21487;&#33021;&#20986;&#38169;&#65306;&#25506;&#31350;&#22522;&#20110;&#25552;&#31034;&#30340;&#20132;&#20114;&#23545;&#36719;&#20214;&#23547;&#27714;&#24110;&#21161;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21644;&#35775;&#35848;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21161;&#25163;&#22312;&#36719;&#20214;&#24110;&#21161;&#23547;&#27714;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#20248;&#21270;&#21518;&#30340;LLM&#21161;&#25163;&#30456;&#36739;&#20110;&#22522;&#20934;LLM&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;&#20110;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21161;&#25163;&#65292;&#22914;ChatGPT&#65292;&#24050;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#22312;&#22797;&#26434;&#30340;&#12289;&#21151;&#33021;&#20016;&#23500;&#30340;&#36719;&#20214;&#20013;&#23548;&#33322;&#30340;&#28508;&#22312;&#26367;&#20195;&#26041;&#27861;&#12290;LLM&#20351;&#29992;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#25991;&#26412;&#12289;&#36719;&#20214;&#25163;&#20876;&#21644;&#20195;&#30721;&#24211;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#27169;&#25311;&#20154;&#31867;&#33324;&#30340;&#20132;&#20114;&#65292;&#25552;&#20379;&#37327;&#36523;&#23450;&#21046;&#30340;&#24110;&#21161;&#65292;&#21253;&#25324;&#36880;&#27493;&#25351;&#23548;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;16&#21517;&#21442;&#19982;&#32773;&#30340;&#34987;&#35797;&#23454;&#39564;&#21644;&#21518;&#32493;&#35775;&#35848;&#26469;&#35843;&#26597;LLM&#29983;&#25104;&#30340;&#36719;&#20214;&#25351;&#23548;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#32447;LLM&#21161;&#25163;&#21644;&#38024;&#23545;&#29305;&#23450;&#36719;&#20214;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#30340;LLM&#65292;&#21363;SoftAIBot&#65292;&#21518;&#32773;&#36824;&#25552;&#20379;&#20102;&#26500;&#24314;&#36866;&#24403;&#25552;&#31034;&#30340;&#25351;&#21335;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#24863;&#30693;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20449;&#20219;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;SoftAIBot&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;LLM&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25552;&#31034;&#25351;&#21335;&#21644;&#39046;&#22495;&#19978;&#19979;&#25991;&#30340;&#34701;&#21512;&#19982;&#21542;&#23545;LLM&#30340;&#20351;&#29992;&#21644;&#29992;&#25143;&#24863;&#30693;&#27809;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#22823;&#22810;&#25968;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to u
&lt;/p&gt;</description></item><item><title>UGMAE&#26159;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#12289;&#22270;&#20687;&#20449;&#24687;&#21033;&#29992;&#12289;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#37325;&#26500;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08023</link><description>&lt;p&gt;
UGMAE&#65306;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#29992;&#20110;&#22270;&#24418;Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
UGMAE: A Unified Framework for Graph Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08023
&lt;/p&gt;
&lt;p&gt;
UGMAE&#26159;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#12289;&#22270;&#20687;&#20449;&#24687;&#21033;&#29992;&#12289;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#37325;&#26500;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#65306;1&#65289;&#22312;&#25513;&#27169;&#20013;&#24573;&#35270;&#19981;&#22343;&#21248;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#65292;2&#65289;&#23545;&#25972;&#20307;&#22270;&#20687;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#65292;3&#65289;&#30001;&#20110;&#20165;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#26500;&#25439;&#22833;&#65292;&#24573;&#35270;&#20102;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#20197;&#21450;4&#65289;&#30001;&#20110;&#36974;&#34109;&#20869;&#23481;&#30340;&#22823;&#37327;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UGMAE&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#22270;&#24418;Masked Autoencoders&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20174;&#36866;&#24212;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#20114;&#34917;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35282;&#24230;&#26469;&#32771;&#34385;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#29305;&#24449;&#36974;&#32617;&#29983;&#25104;&#22120;&#65292;&#20197;&#32771;&#34385;&#33410;&#28857;&#30340;&#29420;&#29305;&#37325;&#35201;&#24615;&#24182;&#37319;&#26679;&#20449;&#24687;&#20016;&#23500;&#30340;&#36974;&#32617;&#65288;&#36866;&#24212;&#24615;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25490;&#21517;&#30340;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#22270;&#24418;&#30340;&#23436;&#25972;&#24615;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#65288;&#23436;&#25972;&#24615;&#21644;&#20114;&#34917;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning on graphs, particularly graph masked autoencoders, has emerged as a popular learning paradigm and demonstrated its efficacy in handling non-Euclidean data. However, several remaining issues limit the capability of existing methods: 1) the disregard of uneven node significance in masking, 2) the underutilization of holistic graph information, 3) the ignorance of semantic knowledge in the representation space due to the exclusive use of reconstruction loss in the output space, and 4) the unstable reconstructions caused by the large volume of masked contents. In light of this, we propose UGMAE, a unified framework for graph masked autoencoders to address these issues from the perspectives of adaptivity, integrity, complementarity, and consistency. Specifically, we first develop an adaptive feature mask generator to account for the unique significance of nodes and sample informative masks (adaptivity). We then design a ranking-based structure reconstruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;</title><link>https://arxiv.org/abs/2402.08010</link><description>&lt;p&gt;
CNN&#38656;&#35201;&#21738;&#20123;&#39057;&#29575;&#65311;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#29942;&#39048;&#32467;&#26500;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#20351;&#29992;&#20854;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#20165;&#22312;&#20960;&#20010;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#21518;&#20960;&#23618;&#23558;&#20854;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;CBN&#31209;&#65292;&#25551;&#36848;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20869;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35777;&#26126;&#20102;&#34920;&#31034;&#20989;&#25968;$f$&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#25353;&#28145;&#24230;&#20056;&#20197;CBN&#31209;$f$&#30340;&#27604;&#20363;&#32553;&#25918;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#25968;&#33539;&#25968;&#22312;&#19979;&#19968;&#38454;&#20013;&#20381;&#36182;&#20110;$f$&#30340;&#27491;&#21017;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#22312;&#26435;&#37325;&#21644;&#65288;&#22312;&#32593;&#32476;&#23545;&#22823;&#23398;&#20064;&#29575;&#31283;&#23450;&#30340;&#20551;&#35774;&#19979;&#65289;&#28608;&#27963;&#20013;&#34920;&#29616;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#20419;&#20351;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#20570;&#27861;&#65307;&#24182;&#19988;&#25105;&#20204;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...
&lt;/p&gt;
&lt;p&gt;
We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#24403;&#21069;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07956</link><description>&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#65306;&#19968;&#39033;&#26356;&#26032;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Educational data mining and learning analytics: An updated survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07956
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#24403;&#21069;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#26159;&#21069;&#19968;&#31687;&#22312;&#26412;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;2013&#24180;&#30340;&#35770;&#25991;&#8220;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#8221;&#26368;&#26032;&#25913;&#36827;&#29256;&#26412;&#65292;&#24182;&#20197;&#26131;&#25026;&#21644;&#24191;&#27867;&#30340;&#26041;&#24335;&#32508;&#36848;&#20102;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#22312;&#25945;&#32946;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#65292;&#24182;&#22312;&#21442;&#32771;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#30456;&#20851;&#26415;&#35821;&#65292;&#22914;&#23398;&#20064;&#20998;&#26512;&#12289;&#26426;&#26500;&#20998;&#26512;&#12289;&#25945;&#23398;&#20998;&#26512;&#12289;&#25968;&#25454;&#39537;&#21160;&#25945;&#32946;&#12289;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#25945;&#32946;&#12289;&#25945;&#32946;&#22823;&#25968;&#25454;&#21644;&#25945;&#32946;&#25968;&#25454;&#31185;&#23398;&#12290;&#26412;&#25991;&#36890;&#36807;&#32508;&#36848;&#20027;&#35201;&#30340;&#20986;&#29256;&#29289;&#12289;&#20851;&#38190;&#37324;&#31243;&#30865;&#12289;&#30693;&#35782;&#21457;&#29616;&#21608;&#26399;&#12289;&#20027;&#35201;&#30340;&#25945;&#32946;&#29615;&#22659;&#12289;&#20855;&#20307;&#24037;&#20855;&#12289;&#21487;&#29992;&#30340;&#20813;&#36153;&#25968;&#25454;&#38598;&#12289;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12289;&#20027;&#35201;&#30446;&#26631;&#20197;&#21450;&#26410;&#26469;&#30340;&#36235;&#21183;&#65292;&#25552;&#20379;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey is an updated and improved version of the previous one published in 2013 in this journal with the title data mining in education. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data-Driven Education, Data-Driven Decision-Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#27835;&#30103;&#31574;&#30053;&#65292;&#20943;&#23569;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#20559;&#24046;&#65292;&#24182;&#19988;&#38477;&#20302;&#27880;&#23556;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07949</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#35774;&#35745;&#20197;&#25913;&#21892;&#31958;&#23615;&#30149;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Optimizing the Design of an Artificial Pancreas to Improve Diabetes Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07949
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#20248;&#21270;&#20154;&#24037;&#33008;&#33146;&#27835;&#30103;&#31574;&#30053;&#65292;&#20943;&#23569;&#31958;&#23615;&#30149;&#24739;&#32773;&#30340;&#34880;&#31958;&#20559;&#24046;&#65292;&#24182;&#19988;&#38477;&#20302;&#27880;&#23556;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#19968;&#31181;&#24930;&#24615;&#30142;&#30149;&#65292;&#24433;&#21709;&#32654;&#22269;&#22659;&#20869;&#26377;3800&#19975;&#20154;&#65292;&#23427;&#20250;&#24433;&#21709;&#36523;&#20307;&#23558;&#39135;&#29289;&#36716;&#21270;&#20026;&#33021;&#37327;&#65288;&#21363;&#34880;&#31958;&#65289;&#30340;&#33021;&#21147;&#12290;&#26631;&#20934;&#30340;&#27835;&#30103;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#33008;&#33146;&#65292;&#21363;&#25345;&#32493;&#33008;&#23707;&#32032;&#27893;&#65288;&#22522;&#30784;&#27880;&#23556;&#65289;&#65292;&#20197;&#21450;&#23450;&#26399;&#27880;&#23556;&#33008;&#23707;&#32032;&#65288;&#31361;&#21457;&#27880;&#23556;&#65289;&#26469;&#34917;&#20805;&#30899;&#27700;&#21270;&#21512;&#29289;&#25668;&#20837;&#37327;&#12290;&#27835;&#30103;&#30446;&#26631;&#26159;&#23558;&#34880;&#31958;&#20445;&#25345;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#30340;&#20013;&#24515;&#20301;&#32622;&#65292;&#36890;&#36807;&#25345;&#32493;&#34880;&#31958;&#27979;&#37327;&#26469;&#36827;&#34892;&#34913;&#37327;&#12290;&#27425;&#35201;&#30446;&#26631;&#26159;&#20943;&#23569;&#27880;&#23556;&#27425;&#25968;&#65292;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#26469;&#35828;&#27880;&#23556;&#26159;&#19981;&#24841;&#24555;&#19988;&#38590;&#20197;&#23454;&#26045;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#31070;&#32463;&#36827;&#21270;&#26469;&#21457;&#29616;&#27835;&#30103;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22522;&#20110;30&#22825;&#30340;&#27835;&#30103;&#21644;&#21333;&#20010;&#24739;&#32773;&#30340;&#27979;&#37327;&#25968;&#25454;&#38598;&#65292;&#39318;&#20808;&#35757;&#32451;&#20102;&#38543;&#26426;&#26862;&#26519;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;&#28982;&#21518;&#36890;&#36807;&#36827;&#21270;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#25351;&#23450;&#30899;&#27700;&#21270;&#21512;&#29289;&#25668;&#20837;&#37327;&#12289;&#22522;&#30784;&#27880;&#23556;&#27700;&#24179;&#21644;&#31361;&#21457;&#27880;&#23556;&#12290;&#36827;&#21270;&#21457;&#29616;&#20102;&#19968;&#20010;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20943;&#23569;&#20102;&#19982;&#30446;&#26631;&#20540;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes, a chronic condition that impairs how the body turns food into energy, i.e. blood glucose, affects 38 million people in the US alone. The standard treatment is to supplement carbohydrate intake with an artificial pancreas, i.e. a continuous insulin pump (basal shots), as well as occasional insulin injections (bolus shots). The goal of the treatment is to keep blood glucose at the center of an acceptable range, as measured through a continuous glucose meter. A secondary goal is to minimize injections, which are unpleasant and difficult for some patients to implement. In this study, neuroevolution was used to discover an optimal strategy for the treatment. Based on a dataset of 30 days of treatment and measurements of a single patient, a random forest was first trained to predict future glucose levels. A neural network was then evolved to prescribe carbohydrates, basal pumping levels, and bolus injections. Evolution discovered a Pareto front that reduced deviation from the targe
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;ScreenAgent&#65292;&#35813;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#23631;&#24149;&#25130;&#22270;&#21644;&#36755;&#20986;&#40736;&#26631;&#38190;&#30424;&#21160;&#20316;&#19982;&#35745;&#31639;&#26426;&#23631;&#24149;&#36827;&#34892;&#20132;&#20114;&#65292;&#23436;&#25104;&#22810;&#27493;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07945</link><description>&lt;p&gt;
ScreenAgent:&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ScreenAgent: A Vision Language Model-driven Computer Control Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#25511;&#21046;&#20195;&#29702;ScreenAgent&#65292;&#35813;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#23631;&#24149;&#25130;&#22270;&#21644;&#36755;&#20986;&#40736;&#26631;&#38190;&#30424;&#21160;&#20316;&#19982;&#35745;&#31639;&#26426;&#23631;&#24149;&#36827;&#34892;&#20132;&#20114;&#65292;&#23436;&#25104;&#22810;&#27493;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21487;&#20197;&#35843;&#29992;&#21508;&#31181;&#24037;&#20855;&#21644;API&#26469;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20316;&#20026;&#26368;&#24378;&#22823;&#21644;&#36890;&#29992;&#30340;&#24037;&#20855;&#65292;&#35745;&#31639;&#26426;&#21487;&#33021;&#34987;&#35757;&#32451;&#26377;&#32032;&#30340;LLM&#20195;&#29702;&#30452;&#25509;&#25511;&#21046;&#12290;&#30001;&#35745;&#31639;&#26426;&#39537;&#21160;&#65292;&#25105;&#20204;&#24076;&#26395;&#26500;&#24314;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#20195;&#29702;&#65292;&#22312;&#21508;&#31181;&#26085;&#24120;&#25968;&#23383;&#24037;&#20316;&#20013;&#21327;&#21161;&#20154;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#29992;&#20110;&#35753;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLM)&#20195;&#29702;&#19982;&#30495;&#23454;&#30340;&#35745;&#31639;&#26426;&#23631;&#24149;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#21487;&#20197;&#35266;&#23519;&#23631;&#24149;&#25130;&#22270;&#65292;&#24182;&#36890;&#36807;&#36755;&#20986;&#40736;&#26631;&#21644;&#38190;&#30424;&#21160;&#20316;&#26469;&#25805;&#20316;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#25511;&#21046;&#27969;&#27700;&#32447;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#21453;&#24605;&#38454;&#27573;&#65292;&#25351;&#23548;&#20195;&#29702;&#19981;&#26029;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#23436;&#25104;&#22810;&#27493;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;ScreenAgent&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#23436;&#25104;&#21508;&#31181;&#26085;&#24120;&#35745;&#31639;&#26426;&#20219;&#21153;&#26102;&#30340;&#23631;&#24149;&#25130;&#22270;&#21644;&#21160;&#20316;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;ScreenAgent&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, Screen
&lt;/p&gt;</description></item><item><title>LLMs Among Us&#23454;&#39564;&#26694;&#26550;&#36890;&#36807;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35753;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20266;&#35013;&#25104;&#20154;&#31867;&#21442;&#19982;&#32773;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#23450;&#30340;&#23041;&#32961;&#65292;&#20294;&#21442;&#19982;&#32773;&#21482;&#26377;42%&#30340;&#26102;&#38388;&#33021;&#27491;&#30830;&#35782;&#21035;&#29992;&#25143;&#30340;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.07940</link><description>&lt;p&gt;
LLMs&#26469;&#34989;: &#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#21442;&#19982;&#25968;&#23383;&#35805;&#35821;
&lt;/p&gt;
&lt;p&gt;
LLMs Among Us: Generative AI Participating in Digital Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07940
&lt;/p&gt;
&lt;p&gt;
LLMs Among Us&#23454;&#39564;&#26694;&#26550;&#36890;&#36807;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35753;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20266;&#35013;&#25104;&#20154;&#31867;&#21442;&#19982;&#32773;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#23450;&#30340;&#23041;&#32961;&#65292;&#20294;&#21442;&#19982;&#32773;&#21482;&#26377;42%&#30340;&#26102;&#38388;&#33021;&#27491;&#30830;&#35782;&#21035;&#29992;&#25143;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23545;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#26684;&#23616;&#20855;&#26377;&#37325;&#22609;&#28508;&#21147;&#12290;&#34429;&#28982;&#36825;&#24102;&#26469;&#20102;&#24456;&#22810;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#35768;&#22810;&#23041;&#32961;&#65292;&#22914;&#20559;&#35265;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#24694;&#24847;&#34892;&#20026;&#32773;&#20256;&#25773;&#23459;&#20256;&#12290;&#25105;&#20204;&#22312;Mastodon&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24320;&#21457;&#20102;&#8220;LLMs Among Us&#8221;&#23454;&#39564;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#22312;&#19981;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#27604;&#20363;&#25110;&#24615;&#36136;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20132;&#27969;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;10&#20010;&#19981;&#21516;LLMs&#65288;GPT-4&#12289;LLama 2 Chat&#21644;Claude&#65289;&#30340;&#20154;&#29289;&#24418;&#35937;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#36718;&#23454;&#39564;&#65292;&#24182;&#22312;&#27599;&#36718;&#23454;&#39564;&#21518;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;LLMs&#20266;&#35013;&#25104;&#20154;&#31867;&#21442;&#19982;&#32773;&#32780;&#19981;&#34987;&#21457;&#29616;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#30693;&#36947;&#23454;&#39564;&#20013;&#23384;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#65292;&#21442;&#19982;&#32773;&#21482;&#26377;42&#65285;&#30340;&#26102;&#38388;&#33021;&#27491;&#30830;&#35782;&#21035;&#20854;&#20182;&#29992;&#25143;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35282;&#33394;&#36873;&#25321;&#22312;&#23454;&#39564;&#20013;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the "LLMs Among Us" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially mor
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.07938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65306;&#30001;LLMs&#39537;&#21160;&#30340;&#35821;&#38899;&#20132;&#20114;&#29992;&#25143;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
Large Language User Interfaces: Voice Interactive User Interfaces powered by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#23454;&#29616;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#33021;&#21147;&#24341;&#21457;&#20102;&#26032;&#19968;&#20195;&#36719;&#20214;&#30340;&#35806;&#29983;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#24037;&#19994;&#30028;&#26080;&#25968;&#24212;&#29992;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#21644;&#24341;&#23548;&#21319;&#32423;&#21518;&#30340;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#26500;&#24314;&#19968;&#20010;&#26694;&#26550;&#65292;&#20316;&#20026;&#29992;&#25143;&#21644;&#29992;&#25143;&#30028;&#38754;&#20043;&#38388;&#30340;&#20013;&#20171;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;LLM&#24341;&#25806;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#20998;&#31867;&#26368;&#26377;&#21487;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#35782;&#21035;&#25152;&#38656;&#30340;UI&#32452;&#20214;&#65292;&#24182;&#38543;&#21518;&#25191;&#34892;&#29992;&#25143;&#26399;&#26395;&#30340;&#25805;&#20316;&#12290;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#23558;&#38745;&#24577;UI&#31995;&#32479;&#21457;&#23637;&#25104;&#39640;&#24230;&#21160;&#24577;&#21644;&#21487;&#36866;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#20837;&#26234;&#33021;&#21644;&#21709;&#24212;&#24335;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#39046;&#22495;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#29992;&#25143;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#26041;&#24335;&#65292;&#26497;&#22823;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent meteoric advancements in large language models have showcased a remarkable capacity for logical reasoning and comprehension. These newfound capabilities have opened the door to a new generation of software, as has been made obvious through the innumerable ways they are being applied in the industry. This research focuses on harnessing and guiding the upgraded power of LLMs to construct a framework that can serve as an intermediary between a user and their user interface. By comprehending a user's needs through a thorough analysis of natural textual inputs, an effectively crafted LLM engine can classify the most likely available application, identify the desired UI component and subsequently execute the user's expected actions. This integration can evolve static UI systems into highly dynamic and adaptable solutions, introducing a new frontier of intelligent and responsive user experiences. Such a framework can fundamentally shift how users accomplish daily tasks, skyrocket e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.07933</link><description>&lt;p&gt;
&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#26080;&#20195;&#30721;AutoML&#65306;&#27010;&#24565;&#26694;&#26550;&#12289;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Human-Centered AI Product Prototyping with No-Code AutoML: Conceptual Framework, Potentials and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#23545;&#20110;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26080;&#20195;&#30721;AutoML&#20316;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#21407;&#22411;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#20135;&#21697;&#30340;&#29305;&#28857;&#26159;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#23545;&#38750;&#19987;&#23478;&#30340;&#19981;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#12290;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#26080;&#32541;&#25191;&#34892;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#65292;&#36825;&#23545;&#20110;&#20154;&#26412;&#26234;&#33021;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#34892;&#19994;&#21644;&#21019;&#26032;&#30456;&#20851;&#65292;&#23427;&#24433;&#21709;&#25112;&#30053;&#20915;&#31574;&#21644;&#25237;&#36164;&#39118;&#38505;&#30340;&#38477;&#20302;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24819;&#27861;&#30340;&#28508;&#21147;&#21644;&#21487;&#34892;&#24615;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#26080;&#20195;&#30721;AutoML&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#20102;&#20854;&#23545;&#38750;&#19987;&#23478;&#30340;&#25903;&#25345;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#24320;&#21457;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#23454;&#29616;&#21487;&#35775;&#38382;&#21644;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#65292;&#20351;&#23398;&#26415;&#30028;&#12289;&#31649;&#29702;&#32773;&#21644;&#20915;&#31574;&#32773;&#21463;&#30410;&#12290;&#26080;&#20195;&#30721;AutoML&#30340;&#25112;&#30053;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates No-Code AutoML as a solution for challenges in AI product prototyping, characterized by unpredictability and inaccessibility to non-experts, and proposes a conceptual framework. This complexity of AI products hinders seamless execution and interdisciplinary collaboration crucial for human-centered AI products. Relevant to industry and innovation, it affects strategic decision-making and investment risk mitigation. Current approaches provide limited insights into the potential and feasibility of AI product ideas. Employing Design Science Research, the study identifies challenges and integrates no-code AutoML as a solution by presenting a framework for AI product prototyping with No-code AutoML. A case study confirms its potential in supporting non-experts, offering a structured approach to AI product development. The framework facilitates accessible and interpretable prototyping, benefiting academia, managers, and decision-makers. Strategic integration of no-code Au
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#27169;&#24335;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.07932</link><description>&lt;p&gt;
&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#30340;&#27169;&#24335;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
A Human-Machine Collaboration Framework for the Development of Schemas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07932
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#20316;&#30340;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#27169;&#24335;&#65292;&#30446;&#30340;&#26159;&#35299;&#20915;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Winograd&#27169;&#24335;&#25361;&#25112;&#65288;WSC&#65289;&#26159;&#19968;&#20010;&#20026;&#20102;&#30740;&#31350;&#23637;&#31034;&#20154;&#31867;&#34892;&#20026;&#30340;&#31995;&#32479;&#32780;&#35774;&#31435;&#30340;&#27979;&#35797;&#65292;&#23427;&#26088;&#22312;&#23558;AI&#31038;&#21306;&#30340;&#20851;&#27880;&#28857;&#20174;&#25216;&#26415;&#36716;&#21521;AI&#31185;&#23398;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#24120;&#35265;&#21644;&#29712;&#30862;&#30340;&#65292;&#20294;&#23545;&#26426;&#22120;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#38656;&#35201;&#22788;&#29702;&#38656;&#35201;&#35299;&#20915;&#30830;&#23450;&#24615;&#20195;&#35789;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#21477;&#23376;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#20154;&#26426;&#22914;&#20309;&#20316;&#20026;&#22242;&#38431;&#21512;&#20316;&#26469;&#35774;&#35745;&#26032;&#27169;&#24335;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Winograd Schema Challenge (WSC), a seemingly well-thought-out test for machine intelligence, has been proposed to shed light on developing systems that exhibit human behavior. Since its introduction, it aimed to pivot the focus of the AI community from the technology to the science of AI. While common and trivial for humans, studies show that it is still challenging for machines, especially when they have to deal with novel schemas, that is, well-designed sentences that require the resolving of definite pronouns. As researchers have become increasingly interested in the challenge itself, this presumably necessitates the availability of an extensive collection of Winograd schemas, which goes beyond what human experts can reasonably develop themselves, especially after proposed ways of utilizing them as novel forms of CAPTCHAs.   To address this necessity, we propose a novel framework that explicitly focuses on how humans and machines can collaborate as teammates to design novel sche
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#20013;&#20351;&#29992;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#65292;&#20351;&#38750;RL&#19987;&#23478;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.07928</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Abstracted Trajectory Visualization for Explainability in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07928
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#20013;&#20351;&#29992;&#27169;&#31946;&#36712;&#36857;&#21487;&#35270;&#21270;&#65292;&#20351;&#38750;RL&#19987;&#23478;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#19994;&#32773;&#29702;&#35299;RL&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;RL&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#65288;&#38750;RL&#19987;&#23478;&#65289;&#65292;XAI&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#36825;&#23548;&#33268;&#38750;RL&#19987;&#23478;&#38590;&#20197;&#21442;&#19982;&#22914;&#20309;&#20026;&#20154;&#31867;&#21644;AI&#20849;&#23384;&#30340;&#31038;&#20250;&#35774;&#35745;RL&#27169;&#22411;&#30340;&#22522;&#26412;&#35752;&#35770;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23558;&#20351;RL&#19987;&#23478;&#33021;&#22815;&#19982;&#38750;RL&#19987;&#23478;&#36827;&#34892;&#27807;&#36890;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#36866;&#21512;&#25105;&#20204;&#31038;&#20250;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25551;&#36848;RL&#27169;&#22411;&#20027;&#35201;&#29366;&#24577;&#20043;&#38388;&#36716;&#25442;&#30340;&#27169;&#31946;&#36712;&#36857;&#23545;&#20110;&#38750;RL&#19987;&#23478;&#26500;&#24314;&#20195;&#29702;&#30340;&#24515;&#29702;&#27169;&#22411;&#23558;&#26159;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#31946;&#36712;&#36857;&#30340;&#21487;&#35270;&#21270;&#65292;&#27809;&#26377;RL&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#33021;&#22815;&#25512;&#26029;&#20986;RL&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) has demonstrated the potential to help reinforcement learning (RL) practitioners to understand how RL models work. However, XAI for users who do not have RL expertise (non-RL experts), has not been studied sufficiently. This results in a difficulty for the non-RL experts to participate in the fundamental discussion of how RL models should be designed for an incoming society where humans and AI coexist. Solving such a problem would enable RL experts to communicate with the non-RL experts in producing machine learning solutions that better fit our society. We argue that abstracted trajectories, that depicts transitions between the major states of the RL model, will be useful for non-RL experts to build a mental model of the agents. Our early results suggest that by leveraging a visualization of the abstracted trajectories, users without RL expertise are able to infer the behavior patterns of RL.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#31995;&#32479;&#35843;&#26597;&#65306;&#25216;&#26415;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#35770;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#24037;&#31243;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35828;&#26126;&#20102;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#24050;&#25104;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#21147;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#22312;&#19981;&#20462;&#25913;&#26680;&#24515;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25552;&#31034;&#20801;&#35768;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20165;&#26681;&#25454;&#32473;&#23450;&#30340;&#25552;&#31034;&#24341;&#21457;&#25152;&#38656;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#25552;&#31034;&#21487;&#20197;&#26159;&#25552;&#20379;&#19978;&#19979;&#25991;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20063;&#21487;&#20197;&#26159;&#35843;&#29992;&#30456;&#20851;&#30693;&#35782;&#30340;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20174;&#38382;&#31572;&#21040;&#24120;&#35782;&#25512;&#29702;&#37117;&#26377;&#28041;&#21450;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#26679;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#21644;&#25216;&#26415;&#32570;&#20047;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#29702;&#35299;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#23545;&#26368;&#36817;&#36827;&#23637;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in pro
&lt;/p&gt;</description></item><item><title>&#28857;&#20987;&#21644;&#25351;&#23548;&#26159;&#19968;&#20010;&#23558;&#30452;&#25509;&#25805;&#20316;&#21644;&#25991;&#26412;&#25351;&#20196;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#31934;&#30830;&#22270;&#20687;&#32534;&#36753;&#30340;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07925</link><description>&lt;p&gt;
&#28857;&#20987;&#21644;&#25351;&#23548;&#65306;&#36890;&#36807;&#32479;&#19968;&#30452;&#25509;&#25805;&#20316;&#21644;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#31934;&#30830;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Point and Instruct: Enabling Precise Image Editing by Unifying Direct Manipulation and Text Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07925
&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#21644;&#25351;&#23548;&#26159;&#19968;&#20010;&#23558;&#30452;&#25509;&#25805;&#20316;&#21644;&#25991;&#26412;&#25351;&#20196;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#31934;&#30830;&#22270;&#20687;&#32534;&#36753;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20351;&#24471;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#32534;&#36753;&#22270;&#20687;&#30340;&#24378;&#22823;&#31995;&#32479;&#24471;&#20197;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24120;&#35265;&#24773;&#26223;&#19979;&#65292;&#20351;&#29992;&#32773;&#24456;&#38590;&#20165;&#36890;&#36807;&#25991;&#26412;&#26469;&#25351;&#23450;&#31934;&#30830;&#30340;&#22270;&#20687;&#21464;&#25442;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#24352;&#21253;&#21547;&#22810;&#21482;&#29399;&#30340;&#22270;&#20687;&#20013;&#65292;&#38590;&#20197;&#36873;&#25321;&#29305;&#23450;&#30340;&#29399;&#24182;&#23558;&#20854;&#31227;&#21160;&#21040;&#19968;&#20010;&#31934;&#30830;&#30340;&#20301;&#32622;&#12290;&#20165;&#36890;&#36807;&#25991;&#26412;&#23436;&#25104;&#36825;&#20010;&#25805;&#20316;&#38656;&#35201;&#19968;&#20010;&#22797;&#26434;&#30340;&#25552;&#31034;&#65292;&#29992;&#20110;&#30830;&#23450;&#30446;&#26631;&#29399;&#65292;&#24182;&#25551;&#36848;&#30446;&#30340;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#25805;&#20316;&#24456;&#36866;&#21512;&#20110;&#36873;&#25321;&#23545;&#35937;&#21644;&#25351;&#23450;&#20301;&#32622;&#31561;&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#28857;&#36873;&#19982;&#25351;&#23548;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#32467;&#21512;&#30452;&#25509;&#25805;&#20316;&#21644;&#25991;&#26412;&#25351;&#20196;&#65292;&#23454;&#29616;&#31934;&#30830;&#22270;&#20687;&#32534;&#36753;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#35270;&#35273;&#19978;&#26631;&#35760;&#23545;&#35937;&#21644;&#20301;&#32622;&#65292;&#24182;&#22312;&#25991;&#26412;&#25351;&#20196;&#20013;&#24341;&#29992;&#23427;&#20204;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#26082;&#33021;&#20174;&#33258;&#28982;&#35821;&#35328;&#30340;&#35270;&#35273;&#25551;&#36848;&#20013;&#21463;&#30410;&#65292;&#21448;&#33021;&#20174;&#30452;&#25509;&#25805;&#20316;&#30340;&#31354;&#38388;&#31934;&#30830;&#24615;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has enabled the development of powerful systems capable of editing images from natural language instructions. However, in many common scenarios it is difficult for users to specify precise image transformations with text alone. For example, in an image with several dogs, it is difficult to select a particular dog and move it to a precise location. Doing this with text alone would require a complex prompt that disambiguates the target dog and describes the destination. However, direct manipulation is well suited to visual tasks like selecting objects and specifying locations. We introduce Point and Instruct, a system for seamlessly combining familiar direct manipulation and textual instructions to enable precise image manipulation. With our system, a user can visually mark objects and locations, and reference them in textual instructions. This allows users to benefit from both the visual descriptiveness of natural language and the spatial precision of direct manipulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#20102;&#26368;&#26032;&#30340;HDT&#39046;&#22495;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;HDT&#23450;&#20041;&#21644;&#21313;&#19968;&#20010;&#20851;&#38190;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.07922</link><description>&lt;p&gt;
&#26397;&#21521;&#20154;&#31867;&#25968;&#23383;&#23402;&#29983;&#65306;&#23450;&#20041;&#19982;&#35774;&#35745; - &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards the Human Digital Twin: Definition and Design -- A survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#20102;&#26368;&#26032;&#30340;HDT&#39046;&#22495;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36328;&#39046;&#22495;HDT&#23450;&#20041;&#21644;&#21313;&#19968;&#20010;&#20851;&#38190;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25968;&#23383;&#23402;&#29983; (HDT) &#26159;&#19968;&#31181;&#24555;&#36895;&#23835;&#36215;&#30340;&#25216;&#26415;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#21040;&#20307;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;HDT&#36890;&#36807;&#23558;&#20154;&#31867;&#34920;&#31034;&#20026;&#22522;&#30784;&#29289;&#29702;&#23454;&#20307;&#25193;&#23637;&#20102;&#23545;&#25968;&#23383;&#23402;&#29983;&#30340;&#20256;&#32479;&#29702;&#35299;&#12290;&#36825;&#24341;&#20837;&#20102;&#20960;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;HDT&#23450;&#20041;&#30340;&#27169;&#31946;&#24615;&#20197;&#21450;&#32570;&#20047;&#23545;&#20854;&#35774;&#35745;&#30340;&#25351;&#23548;&#12290;&#26412;&#35843;&#26597;&#23558;HDT&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#27719;&#38598;&#36215;&#26469;&#65292;&#36890;&#36807;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#39318;&#20010;&#36328;&#39046;&#22495;HDT&#23450;&#20041;&#20197;&#21450;&#20174;&#30456;&#20851;&#25361;&#25112;&#20013;&#20986;&#29616;&#30340;&#21313;&#19968;&#20010;&#20851;&#38190;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#30340;&#24320;&#21457;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Digital Twins (HDTs) are a fast-emerging technology with significant potential in fields ranging from healthcare to sports. HDTs extend the traditional understanding of Digital Twins by representing humans as the underlying physical entity. This has introduced several significant challenges, including ambiguity in the definition of HDTs and a lack of guidance for their design. This survey brings together the recent advances in the field of HDTs to guide future developers by proposing a first cross-domain definition of HDTs based on their characteristics, as well as eleven key design considerations that emerge from the associated challenges.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#31561;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#21892;&#30450;&#20154;&#21644;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#31119;&#31049;&#65292;&#21019;&#36896;&#26032;&#30340;&#29420;&#31435;&#24615;&#21644;&#24863;&#30693;&#65292;&#24182;&#23545;&#20854;&#29983;&#27963;&#20135;&#29983;&#26681;&#26412;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07919</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#25552;&#21319;&#30450;&#20154;&#30340;&#31119;&#31049;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Generative AI Enhance the Well-being of Blind?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07919
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#22270;&#20687;&#31561;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#21892;&#30450;&#20154;&#21644;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#31119;&#31049;&#65292;&#21019;&#36896;&#26032;&#30340;&#29420;&#31435;&#24615;&#21644;&#24863;&#30693;&#65292;&#24182;&#23545;&#20854;&#29983;&#27963;&#20135;&#29983;&#26681;&#26412;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#25913;&#21892;&#30450;&#20154;&#25110;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#31119;&#31049;&#38382;&#39064;&#12290;&#23427;&#24341;&#29992;&#20102;&#19968;&#20010;&#24403;&#21069;&#30340;&#20363;&#23376;&#65292;&#21363;Be My Eyes&#24212;&#29992;&#31243;&#24207;&#65292;&#22312;2023&#24180;&#38598;&#25104;&#20102;Be My AI&#21151;&#33021;&#65292;&#35813;&#21151;&#33021;&#22522;&#20110;&#26469;&#33258;OpenAI&#30340;GPT-4&#12290;&#20316;&#32773;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#33258;&#24049;&#30340;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#20262;&#29702;&#21644;&#31038;&#20250;&#35752;&#35770;&#12290;&#35813;&#24037;&#20855;&#21487;&#20197;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#20998;&#26512;&#38745;&#27490;&#22270;&#20687;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;&#21463;&#24433;&#21709;&#30340;&#20154;&#33719;&#24471;&#20102;&#26032;&#30340;&#29420;&#31435;&#24615;&#21644;&#23545;&#29615;&#22659;&#30340;&#26032;&#24863;&#30693;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#20063;&#20381;&#36182;&#20110;&#25552;&#20379;&#32773;&#25110;&#24320;&#21457;&#32773;&#30340;&#19990;&#30028;&#35266;&#21644;&#36947;&#24503;&#35266;&#65292;&#21518;&#32773;&#20250;&#20026;&#20182;&#20204;&#25351;&#23450;&#25110;&#25298;&#32477;&#26576;&#20123;&#25551;&#36848;&#12290;&#23637;&#26395;&#34920;&#26126;&#65292;&#23545;&#31227;&#21160;&#22270;&#20687;&#30340;&#20998;&#26512;&#23558;&#24847;&#21619;&#30528;&#36827;&#19968;&#27493;&#30340;&#39134;&#36291;&#12290;&#21487;&#20197;&#35828;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;&#30450;&#20154;&#21644;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#31119;&#31049;&#65292;&#24182;&#20197;&#21508;&#31181;&#26041;&#24335;&#25913;&#21464;&#20182;&#20204;&#30340;&#29983;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the question of how generative AI can improve the well-being of blind or visually impaired people. It refers to a current example, the Be My Eyes app, in which the Be My AI feature was integrated in 2023, which is based on GPT-4 from OpenAI. The author's tests are described and evaluated. There is also an ethical and social discussion. The power of the tool, which can analyze still images in an amazing way, is demonstrated. Those affected gain a new independence and a new perception of their environment. At the same time, they are dependent on the world view and morality of the provider or developer, who prescribe or deny them certain descriptions. An outlook makes it clear that the analysis of moving images will mean a further leap forward. It is fair to say that generative AI can fundamentally improve the well-being of blind and visually impaired people and will change it in various ways.
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07913</link><description>&lt;p&gt;
&#20026;&#24110;&#21161;&#20013;&#22269;Python&#32534;&#31243;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07913
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#19982;&#20998;&#31867;&#30495;&#23454;&#23398;&#29983;&#38382;&#39064;&#65292;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#24179;&#21488;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#24555;&#36895;&#22686;&#38271;&#30340;&#35745;&#31639;&#26426;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#35299;&#31572;&#25104;&#21315;&#19978;&#19975;&#23398;&#29983;&#30340;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#20026;&#32534;&#31243;&#25945;&#32946;&#23450;&#21046;&#26234;&#33021;&#21161;&#25163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21019;&#24314;&#38656;&#35201;&#29420;&#29305;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#29992;&#20110;&#35757;&#32451;&#27492;&#31867;LLMs&#30340;&#25968;&#25454;&#36164;&#28304;&#30456;&#23545;&#31232;&#32570;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#32534;&#31243;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;Python&#23398;&#20064;&#32773;&#30340;&#20013;&#25991;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#30830;&#20445;&#38382;&#39064;&#30340;&#26469;&#28304;&#30340;&#30495;&#23454;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#23454;&#38469;&#23398;&#29983;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#38382;&#39064;&#30340;&#31867;&#22411;&#21644;&#23398;&#20064;&#32773;&#30340;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#36825;&#31181;&#27880;&#37322;&#21407;&#21017;&#26088;&#22312;&#25552;&#39640;&#22312;&#32447;&#32534;&#31243;&#25945;&#32946;&#30340;&#25928;&#26524;&#21644;&#36136;&#37327;&#65292;&#20026;&#24320;&#21457;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#22362;&#23454;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online learning platforms, particularly in rapidly growing computer programming courses, addressing the thousands of students' learning queries requires considerable human cost. The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support. However, in real application scenarios, the data resources for training such LLMs are relatively scarce. Therefore, to address the data scarcity in intelligent educational systems for programming, this paper proposes a new Chinese question-and-answer dataset for Python learners. To ensure the authenticity and reliability of the sources of the questions, we collected questions from actual student questions and categorized them according to various dimensions such as the type of questions and the type of learners. This annotation principle is designed to enhance the effectiveness and quality of online programming education, providing a solid data foundation for developing th
&lt;/p&gt;</description></item><item><title>&#31354;&#38388;&#35745;&#31639;&#26159;&#19968;&#31181;&#25216;&#26415;&#36827;&#27493;&#65292;&#23558;&#35774;&#22791;&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#19990;&#30028;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#22312;&#30740;&#31350;&#32773;&#21644;&#24037;&#19994;&#32452;&#32455;&#20013;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;</title><link>https://arxiv.org/abs/2402.07912</link><description>&lt;p&gt;
&#31354;&#38388;&#35745;&#31639;&#65306;&#27010;&#24565;&#12289;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Spatial Computing: Concept, Applications, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07912
&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#35745;&#31639;&#26159;&#19968;&#31181;&#25216;&#26415;&#36827;&#27493;&#65292;&#23558;&#35774;&#22791;&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#25913;&#21892;&#20102;&#25968;&#23383;&#19990;&#30028;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#22312;&#30740;&#31350;&#32773;&#21644;&#24037;&#19994;&#32452;&#32455;&#20013;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#35745;&#31639;&#26159;&#19968;&#31181;&#25216;&#26415;&#36827;&#27493;&#65292;&#23427;&#33021;&#22815;&#23558;&#35774;&#22791;&#26080;&#32541;&#38598;&#25104;&#21040;&#29289;&#29702;&#29615;&#22659;&#20013;&#65292;&#20174;&#32780;&#20351;&#25968;&#23383;&#19990;&#30028;&#30340;&#29992;&#25143;&#20307;&#39564;&#26356;&#21152;&#33258;&#28982;&#21644;&#30452;&#35266;&#12290;&#31354;&#38388;&#35745;&#31639;&#26377;&#28508;&#21147;&#25104;&#20026;&#35745;&#31639;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#20174;GPS&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#21040;&#21307;&#30103;&#20445;&#20581;&#65292;&#31354;&#38388;&#35745;&#31639;&#25216;&#26415;&#24433;&#21709;&#24182;&#25913;&#21892;&#20102;&#25105;&#20204;&#19982;&#25968;&#23383;&#19990;&#30028;&#30340;&#20132;&#20114;&#12290;&#22312;&#21019;&#24314;&#20132;&#20114;&#24335;&#25968;&#23383;&#29615;&#22659;&#26041;&#38754;&#65292;&#31354;&#38388;&#35745;&#31639;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#21644;&#26377;&#25928;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#34892;&#20102;&#36825;&#31687;&#32508;&#36848;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#31354;&#38388;&#35745;&#31639;&#65292;&#21253;&#25324;&#20854;&#25903;&#25345;&#25216;&#26415;&#21644;&#23545;&#21508;&#31181;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19982;&#31354;&#38388;&#35745;&#31639;&#30456;&#20851;&#30340;&#39033;&#30446;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31354;&#38388;&#35745;&#31639;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial computing is a technological advancement that facilitates the seamless integration of devices into the physical environment, resulting in a more natural and intuitive digital world user experience. Spatial computing has the potential to become a significant advancement in the field of computing. From GPS and location-based services to healthcare, spatial computing technologies have influenced and improved our interactions with the digital world. The use of spatial computing in creating interactive digital environments has become increasingly popular and effective. This is explained by its increasing significance among researchers and industrial organisations, which motivated us to conduct this review. This review provides a detailed overview of spatial computing, including its enabling technologies and its impact on various applications. Projects related to spatial computing are also discussed. In this review, we also explored the potential challenges and limitations of spatial
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;MAP-Elites&#22312;&#20154;&#26426;&#21327;&#21516;&#35774;&#35745;&#20013;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#20316;&#29992;&#12290;&#36825;&#20123;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#30340;&#35774;&#35745;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#35774;&#35745;&#25512;&#33616;&#23545;&#35774;&#35745;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.07911</link><description>&lt;p&gt;
MAP-Elites&#24212;&#29992;&#20110;&#20154;&#26426;&#21327;&#21516;&#35774;&#35745;&#20013;&#65292;&#26174;&#29616;&#25628;&#32034;&#31354;&#38388;&#30340;&#20316;&#29992;&#65306;&#19968;&#39033;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does mapping elites illuminate search spaces? A large-scale user study of MAP--Elites applied to human--AI collaborative design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07911
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;MAP-Elites&#22312;&#20154;&#26426;&#21327;&#21516;&#35774;&#35745;&#20013;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#20316;&#29992;&#12290;&#36825;&#20123;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#30340;&#35774;&#35745;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#35774;&#35745;&#25512;&#33616;&#23545;&#35774;&#35745;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20102;&#35299;&#35774;&#35745;&#24314;&#35758;&#23545;&#35774;&#35745;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#36827;&#34892;&#20102;&#20004;&#39033;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35774;&#35745;&#24037;&#20855;&#30340;&#30740;&#31350;&#12290;&#25152;&#30740;&#31350;&#30340;&#24037;&#20855;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#36742;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#34892;&#39542;&#26368;&#36828;&#30340;&#34394;&#25311;&#27773;&#36710;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#35774;&#35745;&#33258;&#24049;&#30340;&#27773;&#36710;&#65292;&#21521;&#31639;&#27861;&#25552;&#20986;&#24314;&#35758;&#65292;&#24182;&#26597;&#30475;&#31639;&#27861;&#25552;&#20379;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;&#31639;&#27861;&#25552;&#20379;&#30340;&#35774;&#35745;&#26159;&#20043;&#21069;&#32463;&#36807;&#27979;&#35797;&#30340;&#35774;&#35745;&#65307;&#20854;&#20013;&#19968;&#20123;&#26159;&#38543;&#26426;&#36873;&#25321;&#30340;&#65292;&#21478;&#19968;&#20123;&#26159;&#20351;&#29992;MAP-Elites&#36873;&#25321;&#30340;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#20026;&#31185;&#23398;&#26222;&#21450;&#35745;&#21010;&#30340;&#19968;&#37096;&#20998;&#65292;&#35760;&#24405;&#20102;808&#27425;&#35774;&#35745;&#20250;&#35805;&#65292;&#27599;&#27425;&#20250;&#35805;&#37117;&#26377;&#21442;&#19982;&#32773;&#20351;&#29992;&#24037;&#20855;&#30340;&#20998;&#26512;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#20379;&#23450;&#37327;&#25968;&#25454;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#21253;&#21547;12&#20301;&#21442;&#19982;&#32773;&#30340;&#21452;&#30450;&#23454;&#39564;&#23460;&#30740;&#31350;&#12290;&#22312;&#23454;&#39564;&#23460;&#30740;&#31350;&#20013;&#65292;&#25910;&#38598;&#20102;&#19982;&#22823;&#35268;&#27169;&#30740;&#31350;&#30340;&#30456;&#21516;&#23450;&#37327;&#25968;&#25454;&#65292;&#24182;&#25910;&#38598;&#20102;&#23545;&#35775;&#35848;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two studies of a human-AI collaborative design tool were carried out in order to understand the influence design recommendations have on the design process. The tool investigated is based on an evolutionary algorithm attempting to design a virtual car to travel as far as possible in a fixed time. Participants were able to design their own cars, make recommendations to the algorithm and view sets of recommendations from the algorithm. The algorithm-recommended sets were designs which had been previously tested; some sets were simply randomly picked and other sets were picked using MAP-Elites. In the first study 808 design sessions were recorded as part of a science outreach program, each with analytical data of how each participant used the tool. To provide context to this quantitative data, a smaller double-blind lab study was also carried out with 12 participants. In the lab study the same quantitative data from the large scale study was collected alongside responses to interview ques
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Prompt4Vis&#65292;&#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07909</link><description>&lt;p&gt;
Prompt4Vis: &#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#25552;&#20379;&#25552;&#31034;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Prompt4Vis&#65292;&#20351;&#29992;&#31034;&#20363;&#25366;&#25496;&#21644;&#32467;&#26500;&#36807;&#28388;&#26469;&#20026;&#34920;&#26684;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25552;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;(DV)&#31995;&#32479;&#22240;&#20854;&#22312;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27934;&#35265;&#30340;&#28145;&#21402;&#33021;&#21147;&#32780;&#24471;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#65292;&#24341;&#36215;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#22312;&#26576;&#20123;&#22768;&#26126;&#24335;&#21487;&#35270;&#21270;&#35821;&#35328;(DVLs&#65292;&#22914;Vega-Lite&#12289;EChart)&#20013;&#65292;&#32534;&#21046;&#25968;&#25454;&#26597;&#35810;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#26469;&#21487;&#35270;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#36807;&#31243;&#26356;&#21152;&#31616;&#21333;&#21644;&#30452;&#35266;&#65292;&#25552;&#20379;&#20102;&#26356;&#21487;&#35775;&#38382;&#21644;&#30452;&#35266;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#25104;&#25968;&#25454;&#21487;&#35270;&#21270;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#22914;Seq2Vis&#12289;ncNet&#21644;RGVisNet&#65292;&#23613;&#31649;&#21033;&#29992;&#20102;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#65292;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#21644;GPT-4&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#22522;&#20934;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#21463;&#21040;&#36825;&#20123;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data visualization (DV) systems are increasingly recognized for their profound capability to uncover insights from vast datasets, gaining attention across both industry and academia. Crafting data queries is an essential process within certain declarative visualization languages (DVLs, e.g., Vega-Lite, EChart.). The evolution of natural language processing (NLP) technologies has streamlined the use of natural language interfaces to visualize tabular data, offering a more accessible and intuitive user experience. However, current methods for converting natural language questions into data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite utilizing complex neural network architectures, still fall short of expectations and have great room for improvement.   Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field. Inspired by these advancements, we introduce a nov
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07787</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#32593;&#32476;&#65288;EMGF&#65289;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#21253;&#25324;&#21477;&#27861;&#20381;&#36182;&#12289;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#31561;&#65292;&#26469;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#35780;&#20272;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#20197;&#29702;&#35299;&#24773;&#24863;&#20449;&#24687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#21152;&#24378;ABSA&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20381;&#36182;&#21644;&#32452;&#25104;&#26641;&#19978;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#21477;&#27861;&#20998;&#26512;&#12290;&#38543;&#30528;ABSA&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#21019;&#26032;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#34987;&#34701;&#20837;&#20854;&#20013;&#65288;&#20363;&#22914;&#28508;&#22312;&#22270;&#65289;&#65292;&#20294;&#36825;&#20063;&#24341;&#20837;&#20102;&#22797;&#26434;&#24615;&#21644;&#28151;&#28102;&#12290;&#30446;&#21069;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22810;&#26679;&#24615;&#30340;&#35821;&#35328;&#21644;&#32467;&#26500;&#29305;&#24449;&#38598;&#25104;&#21040;ABSA&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#31890;&#24230;&#34701;&#21512;&#65288;EMGF&#65289;&#32593;&#32476;&#65292;&#23427;&#25972;&#21512;&#20102;&#26469;&#33258;&#21477;&#27861;&#20381;&#36182;&#21644;&#32452;&#25104;&#12289;&#27880;&#24847;&#21147;&#35821;&#20041;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;EMGF&#37197;&#22791;&#20102;&#22810;&#38170;&#28857;&#19977;&#20803;&#23398;&#20064;&#21644;&#27491;&#20132;&#25237;&#24433;&#65292;&#39640;&#25928;&#22320;&#21033;&#29992;&#20102;&#36825;&#20123;&#29305;&#24449;&#30340;&#32508;&#21512;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within a text to comprehend sentiment information. Previous studies integrated external knowledge, such as knowledge graphs, to enhance the semantic features in ABSA models. Recent research has examined the use of Graph Neural Networks (GNNs) on dependency and constituent trees for syntactic analysis. With the ongoing development of ABSA, more innovative linguistic and structural features are being incorporated (e.g. latent graph), but this also introduces complexity and confusion. As of now, a scalable framework for integrating diverse linguistic and structural features into ABSA does not exist. This paper presents the Extensible Multi-Granularity Fusion (EMGF) network, which integrates information from dependency and constituent syntactic, attention semantic , and external knowledge graphs. EMGF, equipped with multi-anchor triplet learning and orthogonal projection, efficiently harnesses the combined potential of 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07462</link><description>&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#30340;&#28608;&#32032;&#36866;&#24212;&#26041;&#27861;&#65306;&#39044;&#38450;&#22238;&#24418;&#38024;&#21551;&#31034;&#24405;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07462
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#26088;&#22312;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#30340;&#23433;&#20840;&#21644;&#26368;&#20248;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#65288;&#28608;&#32032;&#36866;&#24212;&#36890;&#36807;&#23545;&#25163;&#36807;&#31243;&#65289;&#36825;&#20010;&#27861;&#35268;&#27169;&#24335;&#65292;&#23427;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#34892;&#20026;&#28608;&#32032;&#36866;&#24212;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#20302;&#39057;&#29575;&#30340;&#34892;&#20026;&#20855;&#26377;&#30410;&#22788;&#65292;&#32780;&#39640;&#39057;&#29575;&#30340;&#34892;&#20026;&#21017;&#26377;&#23475;&#12290;&#36890;&#36807;&#23558;&#34892;&#20026;&#24314;&#27169;&#20026;&#21464;&#24577;&#23545;&#25163;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#34892;&#20026;&#39057;&#29575;&#21709;&#24212;&#20998;&#26512;&#65288;BFRA&#65289;&#25110;&#34892;&#20026;&#35745;&#25968;&#21709;&#24212;&#20998;&#26512;&#65288;BCRA&#65289;&#26469;&#37327;&#21270;&#21487;&#37325;&#22797;&#34892;&#20026;&#30340;&#28608;&#32032;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;HALO&#26469;&#35299;&#20915;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#65292;&#36825;&#26159;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#20854;&#20013;&#19968;&#20010;&#26410;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23558;&#25152;&#26377;&#29289;&#36136;&#36716;&#21270;&#20026;&#22238;&#24418;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07420</link><description>&lt;p&gt;
&#20851;&#20110;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Transit Obfuscation Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#20132;&#36890;&#21644;&#30417;&#35270;&#22330;&#26223;&#20013;&#65292;&#38544;&#34255;&#36335;&#24452;&#19978;&#25110;&#20174;&#36335;&#24452;&#19978;&#21487;&#35265;&#30340;&#20013;&#38388;&#28857;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#36816;&#28151;&#28102;&#38382;&#39064;&#65292;&#21363;&#20174;&#26576;&#20010;&#36215;&#22987;&#20301;&#32622;&#21040;&#36798;&#30446;&#26631;&#20301;&#32622;&#30340;&#21516;&#26102;&#65292;"&#35206;&#30422;"&#38656;&#35201;&#38544;&#34255;&#30340;&#29305;&#23450;&#36807;&#22659;&#28857;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#36816;&#21311;&#21517;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#29305;&#23450;&#36807;&#22659;&#28857;&#30340;&#21311;&#21517;&#24615;&#36827;&#34892;&#37327;&#21270;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#23545;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26377;&#20840;&#38754;&#20102;&#35299;&#30340;&#24378;&#22823;&#23545;&#25163;&#38754;&#21069;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#28385;&#36275;&#35813;&#21311;&#21517;&#24615;&#20934;&#21017;&#30340;&#35268;&#21010;/&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concealing an intermediate point on a route or visible from a route is an important goal in some transportation and surveillance scenarios. This paper studies the Transit Obfuscation Problem, the problem of traveling from some start location to an end location while "covering" a specific transit point that needs to be concealed from adversaries. We propose the notion of transit anonymity, a quantitative guarantee of the anonymity of a specific transit point, even with a powerful adversary with full knowledge of the path planning algorithm. We propose and evaluate planning/search algorithms that satisfy this anonymity criterion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07197</link><description>&lt;p&gt;
GraphTranslator&#65306;&#23558;&#22270;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#29992;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07197
&lt;/p&gt;
&lt;p&gt;
"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22270;&#39046;&#22495;&#20013;&#36825;&#20010;&#24819;&#27861;&#36739;&#23569;&#34987;&#25506;&#32034;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#24378;&#22823;&#30340;&#22270;&#27169;&#22411;&#65288;GMs&#65289;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#39044;&#23450;&#20041;&#24418;&#24335;&#30340;&#20219;&#21153;&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#65292;&#26080;&#35770;&#26159;&#23558;LLMs&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#22120;&#36824;&#26159;&#20316;&#20026;&#29420;&#31435;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphTranslator&#30340;&#32763;&#35793;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;GM&#21644;LLMs&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;LLMs&#30340;&#25193;&#23637;&#25509;&#21475;&#20026;GM&#25552;&#20379;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#32763;&#35793;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Producer&#30340;&#26500;&#24314;&#22270;&#25991;&#23545;&#40784;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06938</link><description>&lt;p&gt;
&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06938
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#20449;&#24687;&#21644;&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20652;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#20449;&#24687;&#12290;&#20449;&#24687;&#29190;&#28856;&#25512;&#21160;&#35768;&#22810;&#20225;&#19994;&#25110;&#20010;&#20154;&#23547;&#27714;&#31199;&#29992;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26469;&#23558;&#20182;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#25918;&#32622;&#22312;&#20113;&#20013;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#30340;&#21327;&#35758;&#36890;&#24120;&#19981;&#39640;&#25928;&#12290;&#35768;&#22810;&#22240;&#32032;&#24433;&#21709;&#25928;&#29575;&#65292;&#22914;&#25552;&#20379;&#21830;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#38386;&#32622;&#21644;&#23545;&#23458;&#25143;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24341;&#20837;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#35848;&#21028;&#31867;&#30340;&#21338;&#24328;&#65292;&#24182;&#26681;&#25454;&#35848;&#21028;&#32467;&#26524;&#23433;&#25490;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#29992;&#20110;&#36164;&#28304;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23436;&#25104;&#19968;&#23545;&#19968;&#30340;&#33258;&#21160;&#35848;&#21028;&#36807;&#31243;&#65292;&#24182;&#20026;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#29983;&#25104;&#26368;&#20248;&#30340;&#25253;&#20215;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25104;&#21592;&#20989;&#25968;&#12289;&#27169;&#31946;&#35268;&#21017;&#38598;&#21644;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#36164;&#28304;&#35843;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06864</link><description>&lt;p&gt;
&#20855;&#26377;&#36776;&#21035;&#24615;&#23545;&#25239;&#23398;&#20064;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Discriminative Adversarial Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#24050;&#24314;&#31435;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#20419;&#36827;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21453;&#23398;&#20064;&#29305;&#23450;&#26679;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;$\mathbf{A}$&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#38450;&#24481;&#32773; $\mathbf{D}$&#22312;&#23545;&#25239;&#30446;&#26631;&#19979;&#30456;&#20114;&#23545;&#25239;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#30340;&#20449;&#24687;&#20197;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#65292;&#32780;&#38450;&#24481;&#32773;&#22312;&#21453;&#20987;&#20013;&#36827;&#34892;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36981;&#24490;&#24050;&#30693;&#30340;&#36845;&#20195;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#26469;&#26356;&#26032;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#12290;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36951;&#24536;&#38598;&#21644;&#39564;&#35777;&#38598;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#24046;&#24322;&#65292;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.05643</link><description>&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Token-Based World Models with Parallel Observation Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#26426;&#21046;&#65288;POP&#65289;&#26469;&#35299;&#20915;&#24819;&#35937;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#24212;&#29992;POP&#65292;&#24819;&#35937;&#36895;&#24230;&#25552;&#39640;&#20102;15.4&#20493;&#65292;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;Transformer&#24212;&#29992;&#20110;&#31163;&#25955;&#31526;&#21495;&#24207;&#21015;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;TBWMs&#65289;&#20316;&#20026;&#39640;&#25928;&#26679;&#26412;&#26041;&#27861;&#12290;&#22312;TBWMs&#20013;&#65292;&#19990;&#30028;&#27169;&#22411;&#23558;&#20195;&#29702;&#32463;&#39564;&#20316;&#20026;&#19968;&#31181;&#31867;&#20284;&#35821;&#35328;&#30340;&#20196;&#29260;&#24207;&#21015;&#36827;&#34892;&#28040;&#32791;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#26500;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#22312;&#24819;&#35937;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#20196;&#29260;&#36880;&#20010;&#29983;&#25104;&#19979;&#19968;&#20010;&#35266;&#27979;&#30340;&#20018;&#34892;&#26041;&#24335;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;GPU&#21033;&#29992;&#29575;&#20302;&#21644;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35266;&#27979;&#39044;&#27979;&#65288;POP&#65289;&#26426;&#21046;&#12290;POP&#36890;&#36807;&#19968;&#31181;&#38024;&#23545;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#35774;&#35745;&#30340;&#26032;&#22411;&#21069;&#21521;&#27169;&#24335;&#26469;&#25193;&#20805;&#20102;&#20445;&#25345;&#32593;&#32476;&#65288;RetNet&#65289;&#12290;&#25105;&#20204;&#23558;POP&#38598;&#25104;&#21040;&#19968;&#31181;&#21517;&#20026;REM&#65288;&#20445;&#25345;&#29615;&#22659;&#27169;&#22411;&#65289;&#30340;&#26032;&#22411;TBWM&#20195;&#29702;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;TBWMs&#24555;15.4&#20493;&#30340;&#24819;&#35937;&#33021;&#21147;&#12290;REM&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#28216;&#25103;&#20013;&#30340;12&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#36229;&#36234;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21040;12&#23567;&#26102;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#23436;&#25104;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#65292;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#23454;&#29616;&#36923;&#36753;&#36830;&#25509;&#65292;&#20197;&#35299;&#20915;&#29289;&#32852;&#32593;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04338</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#35782;&#21035;&#38382;&#39064;&#30340;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Logical recognition method for solving the problem of identification in the Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04338
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#65292;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#23454;&#29616;&#36923;&#36753;&#36830;&#25509;&#65292;&#20197;&#35299;&#20915;&#29289;&#32852;&#32593;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#24212;&#29992;&#36923;&#36753;&#20195;&#25968;&#21644;&#20215;&#20540;&#36923;&#36753;&#26041;&#27861;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#35782;&#21035;&#21508;&#31181;&#29289;&#20307;&#21644;&#29616;&#35937;&#12289;&#21307;&#23398;&#25110;&#25216;&#26415;&#35786;&#26029;&#12289;&#26500;&#24314;&#29616;&#20195;&#26426;&#22120;&#12289;&#26816;&#26597;&#27979;&#35797;&#38382;&#39064;&#31561;&#38382;&#39064;&#65292;&#21487;&#20197;&#24402;&#32467;&#20026;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#12290;&#20363;&#22914;&#65292;&#22312;&#36923;&#36753;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#20998;&#26512;&#21644;&#21629;&#39064;&#28436;&#31639;&#27861;&#30340;&#36923;&#36753;&#26041;&#27861;&#26469;&#26500;&#24314;&#33258;&#24049;&#30340;&#35782;&#21035;&#31639;&#27861;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#30340;&#20351;&#29992;&#38656;&#35201;&#23384;&#22312;&#30001;k&#20540;&#20989;&#25968;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#24310;&#32493;&#25152;&#34920;&#31034;&#30340;&#36923;&#36753;&#36830;&#25509;&#65292;&#20854;&#20013;&#21464;&#37327;&#26159;&#27491;&#22312;&#35782;&#21035;&#30340;&#23545;&#35937;&#25110;&#29616;&#35937;&#30340;&#36923;&#36753;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#20855;&#26377;&#36923;&#36753;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;&#21442;&#32771;&#34920;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space. For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms. In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized. The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#26500;&#24314;&#27010;&#29575;&#31665;&#21644;DSt&#32467;&#26500;&#65292;&#21487;&#20197;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00060</link><description>&lt;p&gt;
&#22312;Dempster-Shafer&#29702;&#35770;&#20013;&#22788;&#29702;&#20849;&#21516;&#20998;&#26512;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Treatment of Epistemic Uncertainty in Conjunction Analysis with Dempster-Shafer Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Dempster-Shafer&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#26500;&#24314;&#27010;&#29575;&#31665;&#21644;DSt&#32467;&#26500;&#65292;&#21487;&#20197;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20849;&#21516;&#25968;&#25454;&#20449;&#24687;&#65288;CDM&#65289;&#20013;&#24314;&#27169;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#26681;&#25454;&#30896;&#25758;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#23545;&#20849;&#21516;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;Dempster-Shafer&#35770;&#35777;&#30340;&#29702;&#35770;&#65292;&#20551;&#35774;&#35266;&#23519;&#21040;&#30340;CDMs&#26469;&#33258;&#19968;&#32452;&#26410;&#30693;&#20998;&#24067;&#30340;&#23478;&#26063;&#12290;&#20351;&#29992;Dvoretzky-Kiefer-Wolfowitz&#19981;&#31561;&#24335;&#20174;CDMs&#30340;&#26102;&#38388;&#24207;&#21015;&#26500;&#24314;&#40065;&#26834;&#30028;&#38480;&#12290;&#28982;&#21518;&#20351;&#29992;DKW&#19981;&#31561;&#24335;&#26500;&#24314;&#30340;&#27010;&#29575;&#31665;&#27966;&#29983;DSt&#32467;&#26500;&#12290;&#35813;DSt&#32467;&#26500;&#23553;&#35013;&#20102;&#26102;&#38388;&#24207;&#21015;&#19978;&#27599;&#20010;&#28857;&#30340;CDMs&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20801;&#35768;&#35745;&#31639;&#29305;&#23450;&#30896;&#25758;&#27010;&#29575;&#23454;&#29616;&#30340;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#20123;&#23454;&#38469;&#20107;&#20214;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#27431;&#27954;&#29616;&#26377;&#30340;&#23454;&#36341;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents an approach to the modelling of epistemic uncertainty in Conjunction Data Messages (CDM) and the classification of conjunction events according to the confidence in the probability of collision. The approach proposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence and starts from the assumption that the observed CDMs are drawn from a family of unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is used to construct robust bounds on such a family of unknown distributions starting from a time series of CDMs. A DSt structure is then derived from the probability boxes constructed with DKW inequality. The DSt structure encapsulates the uncertainty in the CDMs at every point along the time series and allows the computation of the belief and plausibility in the realisation of a given probability of collision. The methodology proposed in this paper is tested on a number of real events and compared against existing practices in the Eu
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.16013</link><description>&lt;p&gt;
SERL: &#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#35266;&#23519;&#65292;&#23454;&#38469;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#31034;&#33539;&#21644;&#20808;&#21069;&#32463;&#39564;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#20351;&#29992;&#12290;&#20174;&#23454;&#36341;&#32773;&#20013;&#35748;&#35782;&#21040;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24120;&#24120;&#19982;&#31639;&#27861;&#36873;&#25321;&#21516;&#26679;&#37325;&#35201;&#65288;&#22914;&#26524;&#19981;&#26159;&#26356;&#37325;&#35201;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#36827;&#19968;&#27493;&#21457;&#23637;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#23545;&#38590;&#20197;&#33719;&#21462;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#24515;&#23454;&#29616;&#30340;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#39640;&#25928;&#26679;&#26412;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#20154;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16044</link><description>&lt;p&gt;
LLMLight: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLMLight: Large Language Models as Traffic Signal Control Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16044
&lt;/p&gt;
&lt;p&gt;
LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#20248;&#21270;&#36947;&#36335;&#32593;&#32476;&#25928;&#29575;&#21644;&#20943;&#23569;&#25317;&#22581;&#12290;&#20256;&#32479;&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20132;&#36890;&#24037;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#24448;&#24448;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#23384;&#22312;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLMLight&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;TSC&#20915;&#31574;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21521;LLM&#25552;&#20379;&#35814;&#32454;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#20917;&#35828;&#26126;&#20316;&#20026;&#25351;&#23548;&#65292;&#20511;&#21161;LLM&#30340;&#20808;&#36827;&#27867;&#21270;&#33021;&#21147;&#65292;LLMLight&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;LightGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;TSC&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#39592;&#24178;LLM&#12290;&#36890;&#36807;&#23398;&#20064;&#32454;&#24494;&#30340;&#20132;&#36890;&#27169;&#24335;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;LightGPT&#22312;&#32463;&#27982;&#25104;&#26412;&#26041;&#38754;&#25552;&#21319;&#20102;LLMLight&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;LLMLight&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.15230</link><description>&lt;p&gt;
PERP: &#22312;LLMs&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#20462;&#21098;-&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#65292;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#20687;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65292;Han&#31561;&#65292;2015&#65289;&#36825;&#26679;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20197;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20869;&#23384;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#21482;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#36890;&#24120;&#36275;&#20197;&#24674;&#22797;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;GPT-&#32467;&#26500;&#30340;0.27%-0.35%&#30340;&#21442;&#25968;&#21363;&#21487;&#22312;&#19981;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#23454;&#29616;&#19982;&#19968;&#27425;&#24615;IMP&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#21098;&#21518;&#21442;&#25968;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#65288;PERP&#65289;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#23454;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;Python&#31243;&#24207;&#30340;&#21333;&#20803;&#27979;&#35797;&#33050;&#26412;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#35206;&#30422;&#29575;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#22120;Pynguin&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;Pynguin&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#31867;&#21035;&#65292;ChatGPT&#29983;&#25104;&#30340;&#26029;&#35328;&#32422;&#26377;&#19977;&#20998;&#20043;&#19968;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;</title><link>https://arxiv.org/abs/2312.10622</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#65306;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;&#30340;&#24615;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unit Test Generation using Generative AI : A Comparative Performance Analysis of Autogeneration Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#65292;&#23454;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;ChatGPT&#65289;&#29983;&#25104;Python&#31243;&#24207;&#30340;&#21333;&#20803;&#27979;&#35797;&#33050;&#26412;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#35206;&#30422;&#29575;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#22120;Pynguin&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#24615;&#33021;&#20248;&#20110;Pynguin&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#31867;&#21035;&#65292;ChatGPT&#29983;&#25104;&#30340;&#26029;&#35328;&#32422;&#26377;&#19977;&#20998;&#20043;&#19968;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#65292;&#31243;&#24207;&#21592;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#21333;&#20803;&#27979;&#35797;&#33050;&#26412;&#30340;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#39564;&#24615;&#22320;&#30740;&#31350;LLM&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#20197;ChatGPT&#20026;&#20195;&#34920;&#65292;&#29992;&#20110;&#29983;&#25104;Python&#31243;&#24207;&#30340;&#21333;&#20803;&#27979;&#35797;&#33050;&#26412;&#65292;&#24182;&#23558;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19982;&#29616;&#26377;&#30340;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#22120;&#65288;Pynguin&#65289;&#29983;&#25104;&#30340;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#20195;&#30721;&#21333;&#20803;&#65306;1&#65289;&#36807;&#31243;&#33050;&#26412;&#65292;2&#65289;&#22522;&#20110;&#20989;&#25968;&#30340;&#27169;&#22359;&#21270;&#20195;&#30721;&#65292;3&#65289;&#22522;&#20110;&#31867;&#30340;&#20195;&#30721;&#12290;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#22522;&#20110;&#35206;&#30422;&#29575;&#12289;&#27491;&#30830;&#24615;&#21644;&#21487;&#35835;&#24615;&#31561;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#35206;&#30422;&#29575;&#26041;&#38754;&#19982;Pynguin&#30456;&#24403;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20854;&#24615;&#33021;&#20248;&#20110;Pynguin&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#26576;&#20123;&#31867;&#21035;&#65292;ChatGPT&#29983;&#25104;&#30340;&#26029;&#35328;&#32422;&#26377;&#19977;&#20998;&#20043;&#19968;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating unit tests is a crucial task in software development, demanding substantial time and effort from programmers. The advent of Large Language Models (LLMs) introduces a novel avenue for unit test script generation. This research aims to experimentally investigate the effectiveness of LLMs, specifically exemplified by ChatGPT, for generating unit test scripts for Python programs, and how the generated test cases compare with those generated by an existing unit test generator (Pynguin). For experiments, we consider three types of code units: 1) Procedural scripts, 2) Function-based modular code, and 3) Class-based code. The generated test cases are evaluated based on criteria such as coverage, correctness, and readability. Our results show that ChatGPT's performance is comparable with Pynguin in terms of coverage, though for some cases its performance is superior to Pynguin. We also find that about a third of assertions generated by ChatGPT for some categories were incorrect. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.07399</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20020;&#24202;&#25512;&#29702;&#32773;&#65306;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#25512;&#29702;&#24863;&#30693;&#35786;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#30340;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#20020;&#24202;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#30142;&#30149;&#35786;&#26029;&#36807;&#31243;&#20013;&#30340;&#39640;&#25928;&#12289;&#26102;&#38388;&#33410;&#32422;&#21644;&#21171;&#21160;&#33410;&#32422;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#26426;&#22120;&#25512;&#29702;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#22823;&#22810;&#25968;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20026;&#39537;&#21160;&#30340;&#39033;&#30446;&#20027;&#35201;&#38598;&#20013;&#22312;&#20020;&#24202;&#20998;&#31867;&#25110;&#38405;&#35835;&#29702;&#35299;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#29702;&#24565;&#27880;&#35299;&#25104;&#26412;&#36739;&#39640;&#65292;&#23545;&#20110;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#25512;&#29702;&#24863;&#30693;&#8221;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#20197;&#19968;&#31181;&#39640;&#25928;&#30340;&#26102;&#38388;&#21644;&#21171;&#21160;&#26041;&#24335;&#21435;&#29702;&#24615;&#21270;&#35786;&#26029;&#36807;&#31243;&#65292;&#24182;&#23398;&#20064;&#23545;&#25552;&#31034;&#29983;&#25104;&#30340;&#29702;&#30001;&#36827;&#34892;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#30142;&#30149;&#35786;&#26029;&#30340;&#20020;&#24202;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;LLM&#29983;&#25104;&#20102;&#35786;&#26029;&#24615;&#30340;&#29702;&#30001;&#65292;&#25552;&#20379;&#20854;&#23545;&#21576;&#29616;&#30340;&#24739;&#32773;&#25968;&#25454;&#30340;&#35265;&#35299;&#20197;&#21450;&#36798;&#21040;&#35786;&#26029;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#21363;&#20020;&#24202;&#24605;&#32500;&#38142;&#65288;Clinical CoT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#22312;&#29702;&#30001;&#29983;&#25104;&#21644;&#30142;&#30149;&#35786;&#26029;&#26041;&#38754;&#23454;&#35777;&#20102;LLMs/LMs&#30340;&#20020;&#24202;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a ``reasoning-aware'' diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.06646</link><description>&lt;p&gt;
&#35745;&#31639;&#29256;&#26435;: &#38754;&#21521;&#38899;&#20048;&#29983;&#25104;AI&#30340;&#29256;&#31246;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computational Copyright: Towards A Royalty Model for Music Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#29256;&#26435;&#25361;&#25112;&#65292;&#22312;&#38899;&#20048;&#34892;&#19994;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20123;&#25361;&#25112;&#30340;&#32463;&#27982;&#26041;&#38754;&#65292;&#24378;&#35843;&#32463;&#27982;&#24433;&#21709;&#22312;&#29256;&#26435;&#39046;&#22495;&#20013;&#26500;&#25104;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#40657;&#30418;&#29983;&#25104;AI&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#19981;&#20165;&#34920;&#26126;&#65292;&#32780;&#19988;&#38656;&#35201;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32570;&#22833;&#65292;&#23548;&#33268;&#30417;&#31649;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20026;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#25552;&#20986;&#28508;&#22312;&#30340;&#29256;&#31246;&#27169;&#22411;&#26469;&#24357;&#34917;&#24403;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;Spotify&#21644;YouTube&#31561;&#24179;&#21488;&#29616;&#26377;&#29256;&#31246;&#27169;&#22411;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#21040;AI&#29983;&#25104;&#38899;&#20048;&#30340;&#29420;&#29305;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23558;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#24433;&#21709;&#21147;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#25968;&#25454;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry. This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena. The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions. However, such solutions have been largely missing, leading to regulatory challenges in this landscape. We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms. Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music. A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data. To this end, we present algorithmic solutions employing data attri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.06441</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#21644;&#35889;&#38382;&#39064;&#19979;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#65288;GFD&#65289;&#21487;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;GFD&#65292;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#26469;&#21051;&#30011;&#33410;&#28857;&#30340;&#24322;&#24120;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#27450;&#35784;&#22270;&#22312;&#26412;&#36136;&#19978;&#26159;&#24322;&#36136;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;GNN&#30001;&#20110;&#20551;&#35774;&#21516;&#36136;&#24615;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29616;&#26377;&#27169;&#22411;&#26410;&#20805;&#20998;&#21033;&#29992;&#23453;&#36149;&#30340;&#33410;&#28857;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#12290;&#35813;&#26816;&#27979;&#22120;&#21253;&#25324;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#65292;&#36825;&#20004;&#20010;&#27169;&#22359;&#20998;&#21035;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#20174;&#35889;&#22495;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#23558;&#22270;&#20998;&#21106;&#31216;&#19981;&#21516;&#30340;&#35889;&#25104;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03096</link><description>&lt;p&gt;
&#24341;&#36215;&#22810;&#20041;&#24615;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#20598;&#28982;&#22240;&#32032;&#30340;&#28151;&#21512;&#36873;&#25321;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20041;&#24615;&#31070;&#32463;&#20803;&#8212;&#8212;&#28608;&#27963;&#19968;&#32452;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#8212;&#8212;&#34987;&#35270;&#20026;&#35299;&#37322;&#20219;&#21153;&#20248;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#23545;AI&#23433;&#20840;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#22810;&#20041;&#24615;&#36215;&#28304;&#25925;&#20107;&#26159;&#25968;&#25454;&#21253;&#21547;&#30340;&#8220;&#29305;&#24449;&#8221;&#22810;&#20110;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#36843;&#20351;&#32593;&#32476;&#23558;&#22810;&#20010;&#19981;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#32473;&#21516;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#21361;&#21450;&#25105;&#20204;&#29702;&#35299;&#32593;&#32476;&#20869;&#37096;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#31532;&#20108;&#20010;&#19988;&#38750;&#20114;&#26021;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#20063;&#21487;&#33021;&#20135;&#29983;&#65292;&#36825;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20598;&#28982;&#22810;&#20041;&#24615;&#8221;&#30340;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#21487;&#20197;&#30001;&#22810;&#31181;&#21407;&#22240;&#24341;&#36215;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#22122;&#38899;&#65307;&#36825;&#31181;&#20598;&#28982;&#22810;&#20041;&#24615;&#21457;&#29983;&#26159;&#22240;&#20026;&#38543;&#26426;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#33410;&#30465;&#26102;&#38388;&#12290;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#25552;&#39640;&#23545;&#35805;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#27493;&#39588;&#33258;&#21160;&#21270;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.09593</link><description>&lt;p&gt;
&#22810;&#27493;&#39588;&#23545;&#35805;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Dialogue Workflow Action Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#65292;&#23454;&#29616;&#23545;&#20219;&#21153;&#30340;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#33410;&#30465;&#26102;&#38388;&#12290;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#25552;&#39640;&#23545;&#35805;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#27493;&#39588;&#33258;&#21160;&#21270;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#65292;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#36981;&#24490;&#19968;&#31995;&#21015;&#21160;&#20316;&#30340;&#39034;&#24207;&#65292;&#31216;&#20026;&#24037;&#20316;&#27969;&#65292;&#20197;&#20415;&#26681;&#25454;&#19968;&#32452;&#20934;&#21017;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#39588;&#24037;&#20316;&#27969;&#21160;&#20316;&#39044;&#27979;&#30340;&#26032;&#38382;&#39064;&#65292;&#31995;&#32479;&#39044;&#27979;&#26410;&#26469;&#30340;&#22810;&#20010;&#24037;&#20316;&#27969;&#21160;&#20316;&#12290;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#21487;&#20197;&#23454;&#29616;&#22810;&#36718;&#33258;&#21160;&#21270;&#65292;&#20174;&#32780;&#33021;&#22815;&#33410;&#30465;&#26102;&#38388;&#19987;&#27880;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#26131;&#34892;&#19988;&#33021;&#25552;&#39640;&#21160;&#20316;&#33258;&#21160;&#21270;&#30340;&#24314;&#27169;&#26041;&#27861;&#65306;1&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;2&#65289;&#20351;&#29992;&#26816;&#32034;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;3&#65289;&#38646;&#26679;&#26412;&#22270;&#36941;&#21382;&#65292;&#23558;&#21382;&#21490;&#21160;&#20316;&#24207;&#21015;&#27719;&#24635;&#25104;&#22270;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27493;&#39588;&#21160;&#20316;&#39044;&#27979;&#20135;&#29983;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#23545;&#35805;&#20219;&#21153;&#65288;&#22914;&#39044;&#27979;&#20219;&#21153;&#25104;&#21151;&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22826;&#22810;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23558;&#27493;&#39588;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.03415</link><description>&lt;p&gt;
PowerFlowNet: &#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet: Power Flow Approximation Using Message Passing Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03415
&lt;/p&gt;
&lt;p&gt;
PowerFlowNet &#26159;&#19968;&#31181;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#31616;&#21333;&#30340;&#31995;&#32479;&#20013;&#36895;&#24230;&#25552;&#39640;&#20102;4&#20493;&#65292;&#22312;&#23454;&#38469;&#30340;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;&#20013;&#25552;&#39640;&#20102;145&#20493;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#21151;&#29575;&#27969;&#20998;&#26512;&#23545;&#20110;&#29616;&#20195;&#30005;&#21147;&#32593;&#32476;&#30340;&#36816;&#34892;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#20026;&#23567;&#22411;&#21644;&#22823;&#22411;&#30005;&#21147;&#32593;&#32476;&#25552;&#20379;&#20934;&#30830;&#21644;&#24555;&#36895;&#35299;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#30001;&#20110;&#30005;&#21147;&#32593;&#32476;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#19968;&#20010;&#22270;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#36890;&#36807;&#21033;&#29992;&#24213;&#23618;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#25913;&#21892;&#21151;&#29575;&#27969;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PowerFlowNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#21151;&#29575;&#27969;&#36817;&#20284;&#65292;&#22312;&#31616;&#21333;&#30340;IEEE 14&#24635;&#32447;&#31995;&#32479;&#20013;&#19982;&#20256;&#32479;&#30340;&#29275;&#39039;-&#25289;&#22827;&#36874;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#27861;&#22269;&#39640;&#30005;&#21387;&#32593;&#32476;(6470rte)&#30340;&#30495;&#23454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;4&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#19982;&#20854;&#20182;&#20256;&#32479;&#30340;&#36817;&#20284;&#26041;&#27861;(&#22914;&#30452;&#27969;&#26494;&#24347;&#27861;)&#30456;&#27604;&#65292;&#22312;&#24615;&#33021;&#21644;&#25191;&#34892;&#26102;&#38388;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#23427;&#20204;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' operation and planning. Therefore, there is a need for scalable algorithms that can provide accurate and fast solutions for both small and large scale power networks. As the power network can be interpreted as a graph, Graph Neural Networks (GNNs) have emerged as a promising approach for improving the accuracy and speed of PF approximations by exploiting information sharing via the underlying graph structure. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making P
&lt;/p&gt;</description></item><item><title>LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.19791</link><description>&lt;p&gt;
LILO&#65306;&#36890;&#36807;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#23398;&#20064;&#21487;&#35299;&#37322;&#24211;
&lt;/p&gt;
&lt;p&gt;
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19791
&lt;/p&gt;
&lt;p&gt;
LILO&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#19988;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#31243;&#24207;&#24211;&#12290;&#22312;&#20854;&#20013;&#65292;LILO&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;&#31243;&#24207;&#33258;&#21160;&#37325;&#26500;&#30340;&#31639;&#27861;&#36827;&#23637;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#21160;&#25991;&#26723;&#36807;&#31243;&#20351;&#24471;&#20195;&#30721;&#25277;&#35937;&#21487;&#35299;&#37322;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#26159;&#37325;&#26500;&#30340;&#33402;&#26415;&#65306;&#23558;&#20195;&#30721;&#25972;&#21512;&#21040;&#21487;&#37325;&#29992;&#21644;&#21487;&#35835;&#30340;&#31243;&#24207;&#24211;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LILO&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#21512;&#25104;&#12289;&#21387;&#32553;&#21644;&#25991;&#26723;&#21270;&#20195;&#30721;&#26469;&#26500;&#24314;&#36866;&#21512;&#29305;&#23450;&#38382;&#39064;&#39046;&#22495;&#30340;&#24211;&#12290;LILO&#23558;LLM&#24341;&#23548;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;Stitch&#33258;&#21160;&#37325;&#26500;&#30340;&#36817;&#26399;&#31639;&#27861;&#36827;&#23637;&#30456;&#32467;&#21512;&#65306;Stitch&#26159;&#19968;&#20010;&#31526;&#21495;&#21387;&#32553;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#30340;&#26368;&#20339;lambda&#25277;&#35937;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#25277;&#35937;&#21487;&#35299;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21160;&#25991;&#26723;&#65288;AutoDoc&#65289;&#36807;&#31243;&#65292;&#23427;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#25512;&#26029;&#20986;&#33258;&#28982;&#35821;&#35328;&#21517;&#31216;&#21644;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#38500;&#20102;&#25552;&#39640;&#20154;&#31867;&#21487;&#35835;&#24615;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;AutoDoc&#36890;&#36807;&#24110;&#21161;LILO&#30340;&#21512;&#25104;&#22120;&#35299;&#37322;&#21644;&#37096;&#32626;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LILO&#36827;&#34892;&#20102;&#19977;&#20010;&#24402;&#32435;&#24335;&#31243;&#24207;&#32508;&#21512;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2310.09877</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32463;&#20856;&#25216;&#26415;&#30340;&#32479;&#35745;&#25512;&#26029;&#65306;&#22522;&#20110;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;
&lt;/p&gt;
&lt;p&gt;
Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#32467;&#26524;&#36827;&#34892;&#20840;&#23616;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#38754;&#20020;&#33267;&#23569;&#19977;&#20010;&#25361;&#25112;&#65306;&#30830;&#20445;ALE&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#22312;&#23567;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65307;&#30452;&#35266;&#22320;&#34920;&#24449;&#21464;&#37327;&#22312;ML&#20013;&#30340;&#25972;&#20307;&#25928;&#24212;&#65307;&#20197;&#21450;&#20174;ML&#25968;&#25454;&#20998;&#26512;&#20013;&#36827;&#34892;&#20581;&#22766;&#30340;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#24314;&#31435;&#20102;&#36866;&#24212;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#33258;&#21161;&#27861;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30452;&#35266;&#25351;&#31034;&#23545;&#32467;&#26524;&#21464;&#37327;&#21644;&#26631;&#20934;&#21270;&#23610;&#24230;&#19978;&#30340;&#25928;&#24212;&#30340;ALE&#25928;&#24212;&#22823;&#23567;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#32472;&#21046;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#21453;&#26144;&#20102;ALE&#29087;&#32451;&#31361;&#20986;&#30340;&#28789;&#27963;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;R&#20013;&#8220;ale&#8221;&#21253;&#20013;&#30340;&#23454;&#29616;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;&#20851;&#20110;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWAP&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#37319;&#29992;&#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#30340;&#26799;&#24230;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;SWAP&#22312;&#22122;&#22768;&#25233;&#21046;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04918</link><description>&lt;p&gt;
SWAP: &#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#29992;&#20110;&#40065;&#26834;&#32593;&#32476;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWAP&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#37319;&#29992;&#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#30340;&#26799;&#24230;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;SWAP&#22312;&#22122;&#22768;&#25233;&#21046;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#35745;&#31639;&#32463;&#39564;Fisher&#20449;&#24687;&#30697;&#38453;(FIM)&#26102;&#23384;&#22312;&#19981;&#20934;&#30830;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SWAP&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#29109;&#24335;Wasserstein&#22238;&#24402;(EWR)&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20248;&#21270;&#20013;&#23558;&#24120;&#29992;&#30340;&#26631;&#20934;&#32447;&#24615;&#22238;&#24402;(LR)&#21644;EWR&#20132;&#25442;&#23637;&#31034;&#65292;SWAP&#22312;&#37319;&#29992;&#37051;&#36817;&#25554;&#20540;&#36328;&#25968;&#25454;&#28857;&#26102;&#22312;&#22122;&#22768;&#25233;&#21046;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#36739;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#12290;SWAP&#30340;&#29420;&#29305;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#22312;&#22122;&#22768;&#20943;&#23569;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;SWAP&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#24403;&#32593;&#32476;&#35268;&#27169;&#25110;&#30446;&#26631;&#31232;&#30095;&#24230;&#36739;&#22823;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#19988;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce SWAP, an Entropic Wasserstein regression (EWR) network pruning formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. The "swap" of a commonly used standard linear regression (LR) with the EWR in optimization is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points, yet incurs marginal extra computational cost. The unique strength of SWAP is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#23458;&#25143;&#30340;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#39640;&#36136;&#37327;&#36127;&#33655;&#26354;&#32447;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2304.12076</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#21147;&#23458;&#25143;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.12076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#21147;&#23458;&#25143;&#30340;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#39640;&#36136;&#37327;&#36127;&#33655;&#26354;&#32447;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#30340;&#36127;&#33655;&#26354;&#32447;&#26159;&#25903;&#25345;&#29616;&#20195;&#30005;&#21147;&#31995;&#32479;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#30340;&#20851;&#38190;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37319;&#38598;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#32570;&#20047;&#36275;&#22815;&#30340;&#21382;&#21490;&#36127;&#33655;&#26354;&#32447;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#30701;&#32570;&#38382;&#39064;&#65292;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#20026;&#23458;&#25143;&#25552;&#20379;&#21512;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#24314;&#31435;&#39640;&#24615;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#36127;&#33655;&#30340;&#39640;&#24322;&#36136;&#24615;&#65292;&#20351;&#29992;&#22522;&#20110;&#21508;&#33258;&#23458;&#25143;&#25968;&#25454;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#36127;&#33655;&#26354;&#32447;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#23450;&#21046;&#36127;&#33655;&#26354;&#32447;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#23450;&#21046;&#21512;&#25104;&#36716;&#21270;&#20026;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#20026;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customers' load profiles are critical resources to support data analytics applications in modern power systems. However, there are usually insufficient historical load profiles for data analysis, due to the collection cost and data privacy issues. To address such data shortage problems, load profiles synthesis is an effective technique that provides synthetic training data for customers to build high-performance data-driven models. Nonetheless, it is still challenging to synthesize high-quality load profiles for each customer using generation models trained by the respective customer's data owing to the high heterogeneity of customer load. In this paper, we propose a novel customized load profiles synthesis method based on conditional diffusion models for heterogeneous customers. Specifically, we first convert the customized synthesis into a conditional data generation issue. We then extend traditional diffusion models to conditional diffusion models to realize conditional data generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.00284</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Selective Uncertainty Propagation in Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#27573;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#65292;&#30446;&#26631;&#22312;&#20110;&#24212;&#23545;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20013;&#27599;&#19968;&#27493;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35780;&#20272;&#31163;&#24320;&#34892;&#20026;&#31574;&#30053;&#22312;&#31532;h&#27493;&#26102;&#30340;&#22788;&#29702;&#25928;&#26524;&#65292;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#36825;&#19968;&#27493;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#27599;&#19968;&#27493;&#31574;&#30053;&#37117;&#20250;&#24433;&#21709;&#19979;&#19968;&#29366;&#24577;&#30340;&#20998;&#24067;&#65292;&#30456;&#20851;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#19978;&#27604;&#38543;&#26426;&#24773;&#22659;&#25361;&#25112;&#19979;&#30340;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#26356;&#21152;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#38590;&#24230;&#20171;&#20110;&#36825;&#20004;&#31181;&#24773;&#22659;&#20043;&#38388;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#29992;&#20110;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#38590;&#24230;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15170</link><description>&lt;p&gt;
LLMs&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23450;&#24615;&#32534;&#30721;&#65306;&#24605;&#32500;&#38142;&#25512;&#29702;&#22312;&#26576;&#20123;&#35299;&#37322;&#23398;&#20219;&#21153;&#20013;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning Matches Human Performance in Some Hermeneutic Tasks. (arXiv:2401.15170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#24615;&#32534;&#30721;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#30456;&#27604;&#20110;GPT-3.5&#65292;GPT-4&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#12290;&#26080;&#35770;&#27169;&#22411;&#35268;&#27169;&#22823;&#23567;&#65292;&#21482;&#35201;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#27169;&#22411;&#37117;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#24615;&#32534;&#30721;&#25110;&#20869;&#23481;&#20998;&#26512;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21547;&#20041;&#65292;&#20197;&#35782;&#21035;&#36328;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#23450;&#37327;&#27169;&#24335;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#33258;&#21160;&#21270;&#32534;&#30721;&#36807;&#31243;&#65288;&#23545;&#25991;&#26412;&#24212;&#29992;&#31867;&#21035;&#26631;&#31614;&#65289;&#25552;&#20379;&#20102;&#28508;&#21147;&#65292;&#20174;&#32780;&#20351;&#20154;&#31867;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#19987;&#27880;&#20110;&#26356;&#26377;&#21019;&#36896;&#21147;&#30340;&#30740;&#31350;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#36825;&#20123;&#35299;&#37322;&#20219;&#21153;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#21253;&#25324;&#23545;&#20154;&#25991;&#23398;&#30740;&#31350;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23494;&#38598;&#27573;&#33853;&#30340;&#19968;&#32452;&#31038;&#20250;&#21382;&#21490;&#32534;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#35299;&#37322;&#65292;&#32780;GPT-3.5&#21017;&#19981;&#33021;&#12290;&#19982;&#25105;&#20204;&#30001;&#20154;&#31867;&#33719;&#24471;&#30340;&#37329;&#26631;&#20934;&#30456;&#27604;&#65292;GPT-4&#22312;3&#20010;&#32534;&#30721;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#32534;&#30721;&#19968;&#33268;&#24615;&#65288;Cohen's &#954; &#8805; 0.79&#65289;&#65292;&#22312;9&#20010;&#32534;&#30721;&#20013;&#26377;8&#20010;&#20855;&#26377;&#26174;&#33879;&#30340;&#19968;&#33268;&#24615;&#65288;&#954; &#8805; 0.6&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3.5&#22312;&#25152;&#26377;&#32534;&#30721;&#20013;&#34920;&#29616;&#19981;&#20339;&#65288;mean(&#954;) = 0.34&#65307;max(&#954;) = 0.55&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#32534;&#30721;&#30340;&#20934;&#30830;&#24615;&#19981;&#21463;&#27169;&#22411;&#35268;&#27169;&#24433;&#21709;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32534;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative coding, or content analysis, extracts meaning from text to discern quantitative patterns across a corpus of texts. Recently, advances in the interpretive abilities of large language models (LLMs) offer potential for automating the coding process (applying category labels to texts), thereby enabling human researchers to concentrate on more creative research aspects, while delegating these interpretive tasks to AI. Our case study comprises a set of socio-historical codes on dense, paragraph-long passages representative of a humanistic study. We show that GPT-4 is capable of human-equivalent interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq 0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8 of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes ($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding fidelity
&lt;/p&gt;</description></item><item><title>PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.15042</link><description>&lt;p&gt;
PROXYQA&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15042
&lt;/p&gt;
&lt;p&gt;
PROXYQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#30340;&#26367;&#20195;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#35780;&#20272;&#22120;&#21644;&#29983;&#25104;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#20195;&#29702;&#38382;&#39064;&#30340;&#34920;&#29616;&#26469;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38271;&#31687;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#29983;&#25104;&#38271;&#31687;&#20869;&#23481;&#65288;&#22914;&#25253;&#21578;&#21644;&#25991;&#31456;&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;LLMs&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#19988;&#20840;&#38754;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\textsc{ProxyQA}&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#28145;&#20837;&#20154;&#24037;&#31574;&#21010;&#30340;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#8220;&#20803;&#38382;&#39064;&#8221;&#12290;&#27599;&#20010;&#20803;&#38382;&#39064;&#37117;&#21253;&#21547;&#30456;&#24212;&#30340;&#24102;&#27880;&#37322;&#31572;&#26696;&#30340;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#12290;LLMs&#34987;&#35201;&#27714;&#26681;&#25454;&#36825;&#20123;&#20803;&#38382;&#39064;&#29983;&#25104;&#35814;&#23613;&#30340;&#20869;&#23481;&#12290;&#21033;&#29992;&#35780;&#20272;&#22120;&#24182;&#23558;&#29983;&#25104;&#30340;&#20869;&#23481;&#20316;&#20026;&#32972;&#26223;&#29615;&#22659;&#65292;\textsc{ProxyQA}&#26681;&#25454;&#35780;&#20272;&#22120;&#22238;&#31572;&#8220;&#20195;&#29702;&#38382;&#39064;&#8221;&#30340;&#34920;&#29616;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#22810;&#20010;LLMs&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \textit{meta-questions} spanning various domains. Each meta-question contains corresponding \textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \textit{proxy-questions}. We examine multiple LLMs, emphasizing \t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#30002;&#39592;&#25991;&#23383;&#31526;&#22312;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20026;&#35299;&#35835;&#30002;&#39592;&#25991;&#38125;&#25991;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.12467</link><description>&lt;p&gt;
&#29992;&#20110;&#30002;&#39592;&#25991;&#23383;&#31526;&#28436;&#21464;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65306;EVOBC
&lt;/p&gt;
&lt;p&gt;
An open dataset for the evolution of oracle bone characters: EVOBC. (arXiv:2401.12467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#30002;&#39592;&#25991;&#23383;&#31526;&#22312;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#28436;&#21464;&#36807;&#31243;&#65292;&#20026;&#35299;&#35835;&#30002;&#39592;&#25991;&#38125;&#25991;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26089;&#30340;&#20013;&#25991;&#23383;&#31526;&#28304;&#33258;&#30002;&#39592;&#25991;&#38125;&#25991;&#65292;&#19982;&#20854;&#20182;&#19996;&#20122;&#35821;&#35328;&#23494;&#20999;&#30456;&#20851;&#12290;&#36825;&#20123;&#38125;&#25991;&#23545;&#20154;&#31867;&#23398;&#21644;&#32771;&#21476;&#23398;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#35299;&#35835;&#30002;&#39592;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#36804;&#20170;&#20026;&#27490;&#21482;&#26377;&#32422;1600&#20010;4500&#22810;&#20010;&#29616;&#23384;&#23383;&#31526;&#24471;&#21040;&#35808;&#37322;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#23398;&#26415;&#30740;&#31350;&#65292;&#20840;&#38754;&#20102;&#35299;&#36825;&#31181;&#21476;&#20195;&#20070;&#20889;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35299;&#35835;&#30002;&#39592;&#25991;&#23383;&#31526;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#23383;&#31526;&#28436;&#21464;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#26144;&#23556;&#36825;&#20123;&#23383;&#31526;&#28436;&#21464;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25910;&#38598;&#20102;&#21476;&#20195;&#23383;&#31526;&#65292;&#28085;&#30422;&#20102;&#30002;&#39592;&#25991;&#65288;&#20844;&#20803;&#21069;15&#19990;&#32426;&#65289;&#12289;&#37329;&#25991;&#65288;&#20844;&#20803;&#21069;13&#19990;&#32426;&#33267;&#20844;&#20803;221&#24180;&#65289;&#12289;&#31686;&#20070;&#65288;&#20844;&#20803;&#21069;11&#33267;8&#19990;&#32426;&#65289;&#12289;&#31206;&#31616;&#65288;&#20844;&#20803;&#21069;221&#33267;206&#24180;&#65289;&#12289;&#23567;&#31686;&#65288;&#20844;&#20803;&#21069;206&#33267;8&#19990;&#32426;&#65289;&#12289;&#26999;&#20070;&#65288;&#20844;&#20803;2&#33267;5&#19990;&#32426;&#65289;&#36825;&#20845;&#20010;&#21382;&#21490;&#38454;&#27573;&#30340;&#25991;&#23383;&#12290;
&lt;/p&gt;
&lt;p&gt;
The earliest extant Chinese characters originate from oracle bone inscriptions, which are closely related to other East Asian languages. These inscriptions hold immense value for anthropology and archaeology. However, deciphering oracle bone script remains a formidable challenge, with only approximately 1,600 of the over 4,500 extant characters elucidated to date. Further scholarly investigation is required to comprehensively understand this ancient writing system. Artificial Intelligence technology is a promising avenue for deciphering oracle bone characters, particularly concerning their evolution. However, one of the challenges is the lack of datasets mapping the evolution of these characters over time. In this study, we systematically collected ancient characters from authoritative texts and websites spanning six historical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze Inscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries B.C.), Spring and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>LLbezpeky&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;AI&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2401.01269</link><description>&lt;p&gt;
LLbezpeky: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLbezpeky: Leveraging Large Language Models for Vulnerability Detection. (arXiv:2401.01269v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01269
&lt;/p&gt;
&lt;p&gt;
LLbezpeky&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;AI&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#26500;&#24314;&#23433;&#20840;&#31995;&#32479;&#26041;&#38754;&#36827;&#34892;&#20102;&#25345;&#32493;&#30340;&#30740;&#31350;&#21644;&#36827;&#23637;&#65292;&#20294;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#20173;&#28982;&#23384;&#22312;&#28431;&#27934;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#20998;&#26512;&#24037;&#20855;&#31574;&#30053;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#22823;&#37327;&#30340;&#35823;&#25253;&#21644;&#26377;&#38480;&#30340;&#20998;&#26512;&#33539;&#22260;&#65292;&#20351;&#24471;&#38590;&#20197;&#37319;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#25968;&#25454;&#38656;&#27714;&#21644;&#29305;&#24449;&#24037;&#31243;&#25361;&#25112;&#30340;&#38480;&#21046;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20973;&#20511;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#65292;&#22312;&#29702;&#35299;&#20154;&#31867;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#35821;&#20041;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#23433;&#21331;&#23433;&#20840;&#30340;&#32972;&#26223;&#19979;&#65292;LLMs&#29992;&#20110;&#26816;&#27979;&#28431;&#27934;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#21644;&#20462;&#22797;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#20986;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the continued research and progress in building secure systems, Android applications continue to be ridden with vulnerabilities, necessitating effective detection methods. Current strategies involving static and dynamic analysis tools come with limitations like overwhelming number of false positives and limited scope of analysis which make either difficult to adopt. Over the past years, machine learning based approaches have been extensively explored for vulnerability detection, but its real-world applicability is constrained by data requirements and feature engineering challenges. Large Language Models (LLMs), with their vast parameters, have shown tremendous potential in understanding semnatics in human as well as programming languages. We dive into the efficacy of LLMs for detecting vulnerabilities in the context of Android security. We focus on building an AI-driven workflow to assist developers in identifying and rectifying vulnerabilities. Our experiments show that LLMs o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17022</link><description>&lt;p&gt;
&#21463;&#25511;&#35299;&#30721;&#26469;&#33258;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Decoding from Language Models. (arXiv:2310.17022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#21069;&#32512;&#35780;&#20998;&#22120;&#26469;&#24341;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#39044;&#27979;&#39044;&#26399;&#22238;&#25253;&#65292;&#24182;&#19988;&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#21463;&#25511;&#35299;&#30721;&#65288;CD&#65289;&#65292;&#29992;&#20110;&#25511;&#21046;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20197;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;CD&#36890;&#36807;&#20540;&#20989;&#25968;&#26469;&#35299;&#20915;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#20540;&#20989;&#25968;&#34987;&#31216;&#20026;&#21069;&#32512;&#35780;&#20998;&#22120;&#12290;&#21069;&#32512;&#35780;&#20998;&#22120;&#22312;&#25512;&#29702;&#26102;&#29992;&#20110;&#24341;&#23548;&#29983;&#25104;&#21521;&#26356;&#39640;&#22238;&#25253;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21069;&#32512;&#35780;&#20998;&#22120;&#21487;&#20197;&#20174;&#65288;&#21487;&#33021;&#26159;&#65289;&#31163;&#31574;&#30053;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#20174;&#37096;&#20998;&#35299;&#30721;&#30340;&#21709;&#24212;&#32487;&#32493;&#35299;&#30721;&#26102;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;Reddit&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#32463;&#39564;&#35777;&#26126;&#65292;CD&#20316;&#20026;&#19968;&#31181;&#25511;&#21046;&#26426;&#21046;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CD&#35774;&#35745;&#30340;&#27169;&#22359;&#21270;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#20219;&#20309;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CD&#21487;&#20197;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#22359;&#26041;&#24335;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#21516;&#26679;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.13576</link><description>&lt;p&gt;
&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Tree Search in DAG Space with Model-based Reinforcement Learning for Causal Discovery. (arXiv:2310.13576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;DAG&#31354;&#38388;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#21644;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#65292;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22240;&#26524;&#32467;&#26500;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#25112;&#30053;&#20915;&#31574;&#21040;&#29983;&#29289;&#23398;&#21644;&#32463;&#27982;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;&#22240;&#26524;&#21457;&#29616;&#30340;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#26500;&#24314;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#36824;&#24418;&#24335;&#21270;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#25490;&#38500;&#20250;&#24341;&#20837;&#24490;&#29615;&#30340;&#36793;&#30340;&#39640;&#25928;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20351;&#24471;&#22312;DAG&#31354;&#38388;&#20013;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#31163;&#25955;&#25628;&#32034;&#21644;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23454;&#38469;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#21644;&#36138;&#23146;&#25628;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#26159;&#32452;&#21512;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying causal structure is central to many fields ranging from strategic decision-making to biology and economics. In this work, we propose a model-based reinforcement learning method for causal discovery based on tree search, which builds directed acyclic graphs incrementally. We also formalize and prove the correctness of an efficient algorithm for excluding edges that would introduce cycles, which enables deeper discrete search and sampling in DAG space. We evaluate our approach on two real-world tasks, achieving substantially better performance than the state-of-the-art model-free method and greedy search, constituting a promising advancement for combinatorial methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.09401</link><description>&lt;p&gt;
CIDER: &#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
CIDER: Category-Guided Intent Disentanglement for Accurate Personalized News Recommendation. (arXiv:2310.09401v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09401
&lt;/p&gt;
&lt;p&gt;
CIDER&#26159;&#19968;&#31181;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#24847;&#22270;&#20998;&#31163;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#20934;&#30830;&#29702;&#35299;&#26032;&#38395;&#25991;&#31456;&#30340;&#22810;&#20010;&#24847;&#22270;&#65292;&#24182;&#21306;&#20998;&#29992;&#25143;&#19981;&#21516;&#30340;&#21518;&#38405;&#35835;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#36825;&#22312;&#32531;&#35299;&#29992;&#25143;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#26041;&#38754;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#29992;&#25143;&#21644;&#26032;&#38395;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#20197;&#19979;&#25361;&#25112;&#24456;&#23569;&#34987;&#30740;&#31350;&#65306;&#65288;C1&#65289;&#22914;&#20309;&#20934;&#30830;&#29702;&#35299;&#19968;&#31687;&#26032;&#38395;&#25991;&#31456;&#20013;&#21253;&#21547;&#30340;&#22810;&#20010;&#24847;&#22270;&#65311;&#20197;&#21450;&#65288;C2&#65289;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#28857;&#20987;&#21382;&#21490;&#20013;&#23545;&#26032;&#38395;&#25991;&#31456;&#26377;&#19981;&#21516;&#21518;&#38405;&#35835;&#20559;&#22909;&#30340;&#24773;&#20917;&#65311;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65288;CIDER&#65289;&#65292;&#23427;&#21033;&#29992;&#65288;1&#65289;&#22522;&#20110;&#31867;&#21035;&#24341;&#23548;&#30340;&#24847;&#22270;&#20998;&#31163;&#26469;&#35299;&#20915;&#65288;C1&#65289;&#21644;&#65288;2&#65289;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#26032;&#38395;&#34920;&#31034;&#26469;&#35299;&#20915;&#65288;C2&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#39044;&#27979;&#32435;&#20837;CIDER&#30340;&#35757;&#32451;&#36807;&#31243;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#22686;&#24378;&#24847;&#22270;&#20998;&#31163;&#12290;&#22312;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation aims to assist users in finding news articles that align with their interests, which plays a pivotal role in mitigating users' information overload problem. Although many recent works have been studied for better user and news representations, the following challenges have been rarely studied: (C1) How to precisely comprehend a range of intents coupled within a news article? and (C2) How to differentiate news articles with varying post-read preferences in users' click history? To tackle both challenges together, in this paper, we propose a novel personalized news recommendation framework (CIDER) that employs (1) category-guided intent disentanglement for (C1) and (2) consistency-based news representation for (C2). Furthermore, we incorporate a category prediction into the training process of CIDER as an auxiliary task, which provides supplementary supervisory signals to enhance intent disentanglement. Extensive experiments on two real-world datasets rev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#39539;&#26021;&#20102;Shapley Values&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#24067;&#23572;&#20989;&#25968;&#65292;&#20351;&#24471;Shapley&#20540;&#32473;&#20986;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20449;&#24687;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#31181;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#29305;&#24449;&#25968;&#37327;&#36739;&#22823;&#30340;&#24067;&#23572;&#20989;&#25968;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03041</link><description>&lt;p&gt;
&#39539;&#26021;Shapley Values&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Refutation of Shapley Values for Explainability. (arXiv:2309.03041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03041
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39539;&#26021;&#20102;Shapley Values&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#23384;&#22312;&#24067;&#23572;&#20989;&#25968;&#65292;&#20351;&#24471;Shapley&#20540;&#32473;&#20986;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#20449;&#24687;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#36825;&#31181;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#29305;&#24449;&#25968;&#37327;&#36739;&#22823;&#30340;&#24067;&#23572;&#20989;&#25968;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#20013;&#65292;Shapley&#20540;&#23545;&#29305;&#24449;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#20449;&#24687;&#30340;&#24067;&#23572;&#20989;&#25968;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#35823;&#23548;&#24615;&#20449;&#24687;&#34987;&#24191;&#27867;&#20998;&#31867;&#20026;&#20960;&#20010;&#21487;&#33021;&#30340;&#38382;&#39064;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#28041;&#21450;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#25110;&#26080;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#20013;&#65292;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#37117;&#19982;Shapley&#20540;&#30340;&#19981;&#36275;&#26377;&#20851;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#34542;&#21147;&#26041;&#27861;&#26469;&#35782;&#21035;&#20165;&#21253;&#21547;&#23569;&#25968;&#29305;&#24449;&#21644;&#30456;&#20851;&#23454;&#20363;&#30340;&#24067;&#23572;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;Shapley&#20540;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#20316;&#20026;&#35777;&#26126;&#22312;&#35268;&#21017;&#35299;&#37322;&#20013;Shapley&#20540;&#30340;&#19981;&#36275;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;Shapley&#20540;&#19981;&#36275;&#30340;&#38382;&#39064;&#22312;&#20855;&#26377;&#20219;&#24847;&#22823;&#25968;&#37327;&#29305;&#24449;&#30340;&#24067;&#23572;&#20989;&#25968;&#20013;&#26377;&#22810;&#39057;&#32321;&#20986;&#29616;&#12290;&#24456;&#26174;&#28982;&#65292;&#34542;&#21147;&#26041;&#27861;&#19981;&#22826;&#21487;&#33021;&#25552;&#20379;&#27934;&#23519;&#22914;&#20309;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#22823;&#25968;&#37327;&#29305;&#24449;&#30340;&#38382;&#39064;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrated the existence of Boolean functions for which Shapley values provide misleading information about the relative importance of features in rule-based explanations. Such misleading information was broadly categorized into a number of possible issues. Each of those issues relates with features being relevant or irrelevant for a prediction, and all are significant regarding the inadequacy of Shapley values for rule-based explainability. This earlier work devised a brute-force approach to identify Boolean functions, defined on small numbers of features, and also associated instances, which displayed such inadequacy-revealing issues, and so served as evidence to the inadequacy of Shapley values for rule-based explainability. However, an outstanding question is how frequently such inadequacy-revealing issues can occur for Boolean functions with arbitrary large numbers of features. It is plain that a brute-force approach would be unlikely to provide insights on how to ta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.01592</link><description>&lt;p&gt;
&#22823;&#23610;&#24230;&#21644;&#26080;&#31351;&#23485;&#24230;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21202;&#35753;&#28436;&#35762;
&lt;/p&gt;
&lt;p&gt;
Les Houches Lectures on Deep Learning at Large &amp; Infinite Width. (arXiv:2309.01592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#28436;&#35762;&#26159;&#22312;2022&#24180;&#21202;&#35753;&#22799;&#23395;&#23398;&#26657;&#32479;&#35745;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#19978;&#23637;&#31034;&#30340;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#12290;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#35762;&#24072;&#20204;&#35752;&#35770;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65307;&#35757;&#32451;&#36807;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32447;&#24615;&#27169;&#22411;&#65292;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#20123;&#32852;&#31995;&#22312;&#26080;&#31351;&#23485;&#24230;&#30340;&#26497;&#38480;&#19979;&#20986;&#29616;&#65307;&#20197;&#21450;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.01413</link><description>&lt;p&gt;
LaFiCMIL&#65306;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22823;&#25991;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning. (arXiv:2308.01413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01413
&lt;/p&gt;
&lt;p&gt;
LaFiCMIL&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;Transformer&#27169;&#22411;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22823;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#31361;&#30772;&#12290;&#30452;&#35266;&#19978;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26399;&#26395;&#25991;&#26412;&#20998;&#31867;&#65292;&#20316;&#20026;&#19981;&#38656;&#35201;&#20687;&#29983;&#25104;&#20219;&#21153;&#37027;&#26679;&#35768;&#22810;&#39640;&#32423;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;Transformer&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#22810;&#31867;&#21035;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#38271;&#25991;&#26412;&#25991;&#26723;&#21644;&#20854;&#20182;&#22823;&#25991;&#20214;&#30340;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#12290;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#20027;&#35201;&#21463;&#21040;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#30340;&#38459;&#30861;&#65306;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#27604;&#22914;BERT&#30340;512&#20010;&#26631;&#35760;&#12290;&#34429;&#28982;&#22686;&#21152;GPU&#20869;&#23384;&#21487;&#20197;&#31245;&#24494;&#25193;&#23637;&#36825;&#20010;&#38480;&#21046;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#36755;&#20837;&#38480;&#21046;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LaFiCMIL&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have revolutionized the performance of a wide range of language tasks. Intuitively, one might expect text classification, which does not necessitate as many high-level representations as generative tasks, to be comprehensively addressed with the powerful representation capabilities of Transformers. However, in reality, there remains significant potential for enhancement, particularly in the areas of multi-class and multi-label classification of lengthy textual documents and other large files. The performance of Transformer-based models is mainly hindered by a major limitation: a restricted input length, e.g., 512 tokens for BERT. While an increase in GPU memory can marginally extend this limit, practical real-world applications often operate under constrained GPU resources. In this work, we tackle the input limit problem from the perspective of correlated multiple instance learning. The proposed approach, LaFiCMIL, serves as a versatile framework applicable to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.08533</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#22788;&#29702;&#19982;&#32447;&#24615;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#25968;&#25454;&#22788;&#29702;&#26469;&#25552;&#21462;&#38544;&#34255;&#30340;&#34920;&#24449;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#31361;&#30772;&#65292;&#20294;&#21364;&#20197;&#22823;&#30005;&#23376;&#35745;&#31639;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#20026;&#20102;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20809;&#23398;&#24102;&#23485;&#30340;&#20248;&#21183;&#21644;&#20809;&#23398;&#20114;&#36830;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#22312;&#32570;&#20047;&#20302;&#21151;&#29575;&#20809;&#23398;&#38750;&#32447;&#24615;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#22312;&#20110;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#65292;&#32780;&#19981;&#20381;&#36182;&#30005;&#23376;&#20803;&#20214;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#21487;&#20197;&#21516;&#26102;&#20197;&#20302;&#20809;&#21151;&#29575;&#21512;&#25104;&#21487;&#32534;&#31243;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#21033;&#29992;&#25955;&#23556;&#21183;&#33021;&#65288;&#30001;&#25968;&#25454;&#34920;&#31034;&#65289;&#19982;&#25955;&#23556;&#22330;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22810;&#27425;&#25955;&#23556;&#36827;&#34892;&#25968;&#25454;&#37325;&#22797;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12138</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65306;&#20840;&#38754;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12138
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#36716;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#34920;&#29616;&#20986;&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#24037;&#31243;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#65292;&#20351;ChatGPT&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;AI&#27169;&#22411;&#24212;&#23545;SE&#20219;&#21153;&#25152;&#38656;&#30340;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#35821;&#27861;&#29702;&#35299;&#65292;2&#65289;&#38745;&#24577;&#34892;&#20026;&#29702;&#35299;&#65292;&#21644;3&#65289;&#21160;&#24577;&#34892;&#20026;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;ChatGPT&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25511;&#21046;&#27969;&#31243;&#22270;&#65288;CFG&#65289;&#21644;&#35843;&#29992;&#22270;&#65288;CG&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#28041;&#21450;C&#12289;Java&#12289;Python&#21644;Solidity&#30340;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;ChatGPT&#34920;&#29616;&#20986;&#20102;&#23545;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#65288;AST&#65289;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#21644;&#37096;&#20998;&#21151;&#33021;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06741</link><description>&lt;p&gt;
IVP-VAE: &#21033;&#29992;&#21021;&#20540;&#38382;&#39064;&#27714;&#35299;&#22120;&#23545;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
IVP-VAE: Modeling EHR Time Series with Initial Value Problem Solvers. (arXiv:2305.06741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#24314;&#27169;&#30005;&#23376;&#30149;&#21382;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#21033;&#29992;&#30452;&#25509;&#36817;&#20284;IVP&#30340;&#36807;&#31243;&#26469;&#28040;&#38500;&#36882;&#24402;&#35745;&#31639;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#19982;&#30446;&#21069;&#22522;&#20110;IVP&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;ODE&#21644;&#31070;&#32463;&#27969;&#37327;&#65289;&#22312;&#20998;&#26512;&#30005;&#23376;&#30149;&#21382;&#20013;&#24120;&#35265;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#22522;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#20013;&#36890;&#36807;&#21021;&#20540;&#38382;&#39064;&#65288;IVP&#65289;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#22788;&#29702;&#12290; &#39034;&#24207;&#27714;&#35299;IVP&#20351;&#24471;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#19981;&#22815;&#39640;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#31929;&#20351;&#29992;&#36830;&#32493;&#36807;&#31243;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#20854;&#29366;&#24577;&#28436;&#21464;&#21487;&#20197;&#36890;&#36807;IVP&#30452;&#25509;&#36817;&#20284;&#12290; &#36825;&#28040;&#38500;&#20102;&#36882;&#24402;&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#24182;&#20801;&#35768;&#22810;&#20010;&#29366;&#24577;&#24182;&#34892;&#28436;&#21464;&#12290; &#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20854;&#21487;&#36870;&#24615;&#30340;IVP&#27714;&#35299;&#22120;&#34701;&#21512;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#36825;&#23548;&#33268;&#21442;&#25968;&#26356;&#23569;&#65292;&#25910;&#25947;&#26356;&#24555;&#12290; &#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#33719;&#24471;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#30340;&#21516;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time models such as Neural ODEs and Neural Flows have shown promising results in analyzing irregularly sampled time series frequently encountered in electronic health records. Based on these models, time series are typically processed with a hybrid of an initial value problem (IVP) solver and a recurrent neural network within the variational autoencoder architecture. Sequentially solving IVPs makes such models computationally less efficient. In this paper, we propose to model time series purely with continuous processes whose state evolution can be approximated directly by IVPs. This eliminates the need for recurrent computation and enables multiple states to evolve in parallel. We further fuse the encoder and decoder with one IVP solver based on its invertibility, which leads to fewer parameters and faster convergence. Experiments on three real-world datasets show that the proposed approach achieves comparable extrapolation and classification performance while gaining more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item></channel></rss>