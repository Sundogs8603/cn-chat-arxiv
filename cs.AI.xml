<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00615</link><description>&lt;p&gt;
Point-Bind&#21644;Point-LLM&#65306;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00615
&lt;/p&gt;
&lt;p&gt;
Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Point-Bind&#65292;&#19968;&#20010;&#23558;&#28857;&#20113;&#19982;2D&#22270;&#20687;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#23545;&#40784;&#30340;3D&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22312;ImageBind&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;3D&#21644;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Point-LLM&#65292;&#31532;&#19968;&#20010;&#36981;&#24490;3D&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#25216;&#26415;&#65292;Point-LLM&#23558;Point-Bind&#30340;&#35821;&#20041;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#65292;&#20363;&#22914;LLaMA&#65292;&#19981;&#38656;&#35201;3D&#25351;&#20196;&#25968;&#25454;&#20294;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;&#23558;3D&#28857;&#20113;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#21551;&#31034;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ZiyuGuo99/Point-Bind_Point-LLM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#36845;&#20195;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#29983;&#25104;&#33402;&#26415;&#21644;&#23457;&#32654;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#35270;&#35273;&#36164;&#20135;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#20805;&#20998;&#25903;&#25345;&#36825;&#26679;&#30340;&#21019;&#24847;&#21162;&#21147;&#65292;&#35813;&#36807;&#31243;&#24212;&#20855;&#22791;&#20197;&#19979;&#33021;&#21147;&#65306;1&#65289;&#36845;&#20195;&#22320;&#32534;&#36753;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;2&#65289;&#25511;&#21046;&#25152;&#38656;&#21464;&#21270;&#30340;&#31354;&#38388;&#33539;&#22260;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#25110;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#23450;&#27491;&#24335;&#21270;&#20026;&#36845;&#20195;&#22810;&#31890;&#24230;&#32534;&#36753;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#19968;&#27425;&#24615;&#25805;&#20316;&#65288;&#21363;&#27809;&#26377;&#36845;&#20195;&#32534;&#36753;&#33021;&#21147;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#33258;&#28982;&#20135;&#29983;&#22810;&#31890;&#24230;&#25511;&#21046;&#65288;&#21363;&#28085;&#30422;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#32534;&#36753;&#30340;&#20840;&#35889;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMILIE&#65306;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#12290;EMILIE&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20419;&#36827;&#36845;&#20195;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#25152;&#38656;&#21464;&#21270;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00543</link><description>&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#31579;&#36873;&#22825;&#28982;&#23545;&#31435;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00543
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#26159;&#36890;&#36807;&#21521;&#28165;&#27905;&#36755;&#20837;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#32780;&#21046;&#20316;&#20986;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#24182;&#19981;&#33021;&#20934;&#30830;&#21453;&#26144;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#26410;&#24517;&#33021;&#22815;&#36716;&#21270;&#20026;&#23545;&#33258;&#28982;&#20135;&#29983;&#30340;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;AI&#32780;&#35328;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31579;&#36873;&#30001;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#65292;&#36825;&#31181;&#26631;&#31614;&#32467;&#21512;&#20102;&#22024;&#26434;&#19988;&#26131;&#33719;&#24471;&#30340;&#26631;&#27880;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Transformer-like&#27880;&#24847;&#26426;&#21046;&#22686;&#24378;LSTM&#32593;&#32476;&#65292;&#26816;&#27979;GNSS&#35266;&#27979;&#20013;&#30340;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#31934;&#24230;&#21644;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00480</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;GNSS&#35266;&#27979;&#30340;NLOS&#26816;&#27979;&#19982;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#30340;Transformer&#22686;&#24378;LSTM&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network. (arXiv:2309.00480v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;Transformer-like&#27880;&#24847;&#26426;&#21046;&#22686;&#24378;LSTM&#32593;&#32476;&#65292;&#26816;&#27979;GNSS&#35266;&#27979;&#20013;&#30340;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#31934;&#24230;&#21644;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23548;&#33322;&#21355;&#26143;&#31995;&#32479;&#65288;GNSS&#65289;&#22312;&#20132;&#36890;&#31995;&#32479;&#20013;&#23545;&#20110;&#31934;&#30830;&#21644;&#19968;&#33268;&#30340;&#36710;&#36742;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#22478;&#24066;&#23777;&#35895;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#22810;&#36335;&#24452;&#25928;&#24212;&#21644;&#38750;&#30452;&#36798;&#65288;NLOS&#65289;&#25509;&#25910;&#30340;&#24433;&#21709;&#65292;GNSS&#35266;&#27979;&#21487;&#20197;&#20135;&#29983;&#25197;&#26354;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#30340;&#20998;&#31867;&#19982;&#25490;&#38500;&#38169;&#35823;GNSS&#35266;&#27979;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#19981;&#23433;&#20840;&#30340;&#31995;&#32479;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;GNSS&#35266;&#27979;&#35270;&#20026;&#31354;&#26102;&#24314;&#27169;&#38382;&#39064;&#65292;&#26816;&#27979;NLOS&#25509;&#25910;&#24182;&#39044;&#27979;GNSS&#20266;&#36317;&#35823;&#24046;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31867;&#20284;Transformer&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39321;&#28207;&#21644;&#20122;&#29723;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#29983;&#25104;&#36807;&#31243;&#26469;&#26631;&#35760;...
&lt;/p&gt;
&lt;p&gt;
The global navigation satellite systems (GNSS) play a vital role in transport systems for accurate and consistent vehicle localization. However, GNSS observations can be distorted due to multipath effects and non-line-of-sight (NLOS) receptions in challenging environments such as urban canyons. In such cases, traditional methods to classify and exclude faulty GNSS observations may fail, leading to unreliable state estimation and unsafe system operations. This work proposes a Deep-Learning-based method to detect NLOS receptions and predict GNSS pseudorange errors by analyzing GNSS observations as a spatio-temporal modeling problem. Compared to previous works, we construct a transformer-like attention mechanism to enhance the long short-term memory (LSTM) networks, improving model performance and generalization. For the training and evaluation of the proposed network, we used labeled datasets from the cities of Hong Kong and Aachen. We also introduce a dataset generation process to label
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00464</link><description>&lt;p&gt;
&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Theoretical and Practical Framework for Evaluating Uncertainty Calibration in Object Detection. (arXiv:2309.00464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#21450;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#32771;&#34385;&#28145;&#24230;&#23398;&#20064;&#30340;&#26410;&#26469;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#38382;&#39064;&#25104;&#20026;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#32771;&#34385;&#21040;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25351;&#26631;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Deep Neural Networks has resulted in machine learning systems becoming increasingly more present in various real-world applications. Consequently, there is a growing demand for highly reliable models in these domains, making the problem of uncertainty calibration pivotal, when considering the future of deep learning. This is especially true when considering object detection systems, that are commonly present in safety-critical application such as autonomous driving and robotics. For this reason, this work presents a novel theoretical and practical framework to evaluate object detection systems in the context of uncertainty calibration. The robustness of the proposed uncertainty calibration metrics is shown through a series of representative experiments. Code for the proposed uncertainty calibration metrics at: https://github.com/pedrormconde/Uncertainty_Calibration_Object_Detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;&#65292;&#38024;&#23545;&#29616;&#26377;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#20102;&#20998;&#31867;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00462</link><description>&lt;p&gt;
&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
New metrics for analyzing continual learners. (arXiv:2309.00462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;&#65292;&#38024;&#23545;&#29616;&#26377;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#20102;&#20998;&#31867;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#22266;&#23450;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#36827;&#34892;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#36830;&#32493;&#27969;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#65292;&#23545;&#20110;&#26631;&#20934;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#38590;&#20197;&#20445;&#25345;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#36825;&#31181;&#31283;&#23450;&#24615;&#19982;&#21487;&#22609;&#24615;&#30340;&#22256;&#22659;&#20173;&#28982;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#25351;&#26631;&#26469;&#20805;&#20998;&#34913;&#37327;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#25351;&#26631;&#32771;&#34385;&#21040;&#20998;&#31867;&#20219;&#21153;&#30340;&#36880;&#28176;&#22686;&#21152;&#30340;&#38590;&#24230;&#65292;&#36825;&#20174;&#26412;&#36136;&#19978;&#23548;&#33268;&#20219;&#20309;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#30830;&#23450;&#20102;&#35774;&#32622;&#24341;&#36215;&#30340;&#36951;&#24536;&#30340;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32771;&#34385;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown remarkable performance when trained on independent and identically distributed data from a fixed set of classes. However, in real-world scenarios, it can be desirable to train models on a continuous stream of data where multiple classification tasks are presented sequentially. This scenario, known as Continual Learning (CL) poses challenges to standard learning algorithms which struggle to maintain knowledge of old tasks while learning new ones. This stability-plasticity dilemma remains central to CL and multiple metrics have been proposed to adequately measure stability and plasticity separately. However, none considers the increasing difficulty of the classification task, which inherently results in performance loss for any model. In that sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00424</link><description>&lt;p&gt;
CPSP: &#20174;&#38899;&#32032;&#30417;&#30563;&#20013;&#23398;&#20064;&#35821;&#38899;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
CPSP: Learning Speech Concepts From Phoneme Supervision. (arXiv:2309.00424v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CPSP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#20351;&#24471;&#25552;&#21462;&#30340;&#20449;&#24687;&#26082;&#21253;&#21547;&#35821;&#35328;&#20869;&#23481;&#21448;&#21435;&#38500;&#20102;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#65292;&#36866;&#29992;&#20110;TTS&#12289;VC&#21644;ASR&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35832;&#22914;&#26368;&#23567;&#30417;&#30563;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#12289;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31561;&#32454;&#31890;&#24230;&#29983;&#25104;&#21644;&#35782;&#21035;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#34920;&#31034;&#24212;&#21253;&#21547;&#20171;&#20110;&#25991;&#26412;&#32534;&#30721;&#21644;&#22768;&#23398;&#32534;&#30721;&#20043;&#38388;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#20869;&#23481;&#31361;&#20986;&#65292;&#32780;&#21457;&#35328;&#20154;&#36523;&#20221;&#21644;&#22768;&#23398;&#32454;&#33410;&#31561;&#35821;&#38899;&#20449;&#24687;&#24212;&#35813;&#34987;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#20013;&#38388;&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#20887;&#20313;&#24615;&#36807;&#39640;&#21644;&#32500;&#24230;&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#38899;&#39057;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#25552;&#21462;&#29992;&#20110;&#19979;&#28216;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#20840;&#23616;&#25551;&#36848;&#20449;&#24687;&#65292;&#19981;&#36866;&#21512;TTS&#12289;VC&#21644;ASR&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#38899;&#32032;-&#35821;&#38899;&#39044;&#35757;&#32451;&#65288;CPSP&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19977;&#20010;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35299;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#38899;&#32032;&#21644;&#35821;&#38899;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representation extracted from speech should contain information that is between text coding and acoustic coding. The linguistic content is salient, while the paralinguistic information such as speaker identity and acoustic details should be removed. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Additionally, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named Contrastive Phoneme-Speech Pretraining (CPSP), which uses three encoders, one decoder, and contrastive learning to bring phoneme and speech
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#12289;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#35299;&#37322;&#26041;&#27861;&#20013;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#21644;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00422</link><description>&lt;p&gt;
&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Declarative Reasoning on Explanations Using Constraint Logic Programming. (arXiv:2309.00422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#12289;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#35299;&#37322;&#26041;&#27861;&#20013;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#21644;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#19981;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#65292;&#20197;&#21450;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#65288;CLP&#65289;&#30340;REASONX&#35299;&#37322;&#26041;&#27861;&#12290;REASONX&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65292;&#36825;&#20123;&#20915;&#31574;&#26641;&#21487;&#20197;&#26159;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#20840;&#23616;/&#23616;&#37096;&#26367;&#20195;&#27169;&#22411;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#32447;&#24615;&#32422;&#26463;&#21644;&#22522;&#20110;&#20107;&#23454;&#21644;&#23545;&#27604;&#23454;&#20363;&#30340;&#29305;&#24449;&#30340;MILP&#20248;&#21270;&#26469;&#34920;&#36798;&#32972;&#26223;&#30693;&#35782;&#25110;&#24120;&#35782;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#25237;&#24433;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#19982;&#31572;&#26696;&#32422;&#26463;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;REASONX&#30340;&#26550;&#26500;&#65292;&#23427;&#30001;&#25509;&#36817;&#29992;&#25143;&#30340;Python&#23618;&#21644;CLP&#23618;&#32452;&#25104;&#12290;REASONX&#30340;&#26680;&#24515;&#25191;&#34892;&#24341;&#25806;&#26159;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#35821;&#20041;&#30340;Prolog&#20803;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00417</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#20013;&#30340;&#38754;&#31215;&#35268;&#33539;COBRA
&lt;/p&gt;
&lt;p&gt;
Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#26469;&#35745;&#31639;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#21019;&#24314;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#29983;&#23384;&#26354;&#32447;&#20043;&#38388;&#30340;&#38754;&#31215;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#22238;&#24402;&#35774;&#32622;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#21464;&#37327;&#30340;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#27169;&#25311;&#30740;&#31350;&#65292;&#34920;&#26126;&#25105;&#20204;&#23545;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#25552;&#35758;&#25928;&#26524;&#24456;&#22909;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35828;&#26126;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#30340;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22686;&#24378;&#29256;&#12289;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#30340;&#26032;&#29256;&#26412;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00408</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;AND/OR&#30340;&#35745;&#31639;&#34507;&#30333;&#36136;&#35774;&#35745;&#65306;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#21487;&#27867;&#21270;&#30340;UFO
&lt;/p&gt;
&lt;p&gt;
Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics and Generalizable UFO. (arXiv:2309.00408v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#30340;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22686;&#24378;&#29256;&#12289;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#30340;&#26032;&#29256;&#26412;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#31561;&#25216;&#26415;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#39134;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#37325;&#35201;&#20219;&#21153;&#23545;&#20110;&#36825;&#20123;&#25216;&#26415;&#26469;&#35828;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#20256;&#32479;&#25512;&#29702;&#26041;&#26696;&#36827;&#34892;&#21019;&#26032;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#23601;&#26159;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#65292;&#22312;&#23567;&#35268;&#27169;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#38382;&#39064;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;BBK*&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;AOBB-K*&#22312;&#25193;&#23637;&#24615;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25193;&#23637;AOBB-K*&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#29256;&#26412;&#65306;AOBB-K*-b&#65288;&#22686;&#24378;&#29256;&#65289;&#12289;AOBB-K*-DH&#65288;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#65289;&#21644;AOBB-K*-UFO&#65288;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#65289;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific computing has experienced a surge empowered by advancements in technologies such as neural networks. However, certain important tasks are less amenable to these technologies, benefiting from innovations to traditional inference schemes. One such task is protein re-design. Recently a new re-design algorithm, AOBB-K*, was introduced and was competitive with state-of-the-art BBK* on small protein re-design problems. However, AOBB-K* did not scale well. In this work we focus on scaling up AOBB-K* and introduce three new versions: AOBB-K*-b (boosted), AOBB-K*-DH (with dynamic heuristics), and AOBB-K*-UFO (with underflow optimization) that significantly enhance scalability.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#21333;&#30446;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#31264;&#23494;&#20307;&#32032;&#19977;&#32500;&#37325;&#24314;&#30340;&#24212;&#29992;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28145;&#24230;&#22270;&#20272;&#35745;&#26041;&#38754;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#20010;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#31264;&#23494;&#19977;&#32500;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.00385</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#30446;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#31264;&#23494;&#20307;&#32032;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dense Voxel 3D Reconstruction Using a Monocular Event Camera. (arXiv:2309.00385v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00385
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#21333;&#30446;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#31264;&#23494;&#20307;&#32032;&#19977;&#32500;&#37325;&#24314;&#30340;&#24212;&#29992;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28145;&#24230;&#22270;&#20272;&#35745;&#26041;&#38754;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#20010;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#31264;&#23494;&#19977;&#32500;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#26159;&#21463;&#29983;&#29289;&#31995;&#32479;&#21551;&#21457;&#30340;&#20256;&#24863;&#22120;&#65292;&#19987;&#38376;&#29992;&#20110;&#25429;&#25417;&#20142;&#24230;&#21464;&#21270;&#12290;&#36825;&#20123;&#26032;&#20852;&#30456;&#26426;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#24103;&#30340;&#30456;&#26426;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#65292;&#21253;&#25324;&#39640;&#21160;&#24577;&#33539;&#22260;&#12289;&#39640;&#24103;&#29575;&#21644;&#26497;&#20302;&#30340;&#21151;&#32791;&#12290;&#30001;&#20110;&#36825;&#20123;&#20248;&#28857;&#65292;&#20107;&#20214;&#30456;&#26426;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#24103;&#25554;&#20540;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#37324;&#31243;&#35745;&#21644;SLAM&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#20107;&#20214;&#30456;&#26426;&#22312;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20197;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#28145;&#24230;&#22270;&#20272;&#35745;&#36827;&#34892;&#19977;&#32500;&#37325;&#24314;&#19978;&#12290;&#20135;&#29983;&#31264;&#23494;&#19977;&#32500;&#37325;&#24314;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22810;&#20010;&#30456;&#26426;&#65292;&#32780;&#21033;&#29992;&#21333;&#20010;&#20107;&#20214;&#30456;&#26426;&#21482;&#33021;&#20135;&#29983;&#21322;&#31264;&#23494;&#30340;&#32467;&#26524;&#12290;&#20854;&#20182;&#21487;&#20197;&#20135;&#29983;&#31264;&#23494;&#19977;&#32500;&#37325;&#24314;&#30340;&#21333;&#30446;&#30456;&#26426;&#26041;&#27861;&#20381;&#36182;&#20110;&#21019;&#24314;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#35201;&#20040;&#32467;&#21512;&#21069;&#36848;&#26041;&#27861;&#65292;&#35201;&#20040;&#32467;&#21512;&#20854;&#20182;&#29616;&#26377;&#30340;&#36816;&#21160;&#32467;&#26500;&#65288;SfM&#65289;&#25110;&#22810;&#35270;&#22270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras are sensors inspired by biological systems that specialize in capturing changes in brightness. These emerging cameras offer many advantages over conventional frame-based cameras, including high dynamic range, high frame rates, and extremely low power consumption. Due to these advantages, event cameras have increasingly been adapted in various fields, such as frame interpolation, semantic segmentation, odometry, and SLAM. However, their application in 3D reconstruction for VR applications is underexplored. Previous methods in this field mainly focused on 3D reconstruction through depth map estimation. Methods that produce dense 3D reconstruction generally require multiple cameras, while methods that utilize a single event camera can only produce a semi-dense result. Other single-camera methods that can produce dense 3D reconstruction rely on creating a pipeline that either incorporates the aforementioned methods or other existing Structure from Motion (SfM) or Multi-view S
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#27700;&#24211;&#31995;&#32479;&#30340;&#26368;&#20248;&#25805;&#20316;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#36807;&#21435;&#25968;&#25454;&#29983;&#25104;&#26410;&#26469;&#27700;&#27969;&#30340;&#19968;&#32452;&#21487;&#33021;&#24773;&#26223;&#65292;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#25511;&#21046;&#22120;&#26356;&#21152;&#35880;&#24910;&#65292;&#21516;&#26102;&#28385;&#36275;&#20892;&#19994;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#24212;&#23545;&#24178;&#26097;&#26399;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.00373</link><description>&lt;p&gt;
&#27700;&#24211;&#31995;&#32479;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scenario-based model predictive control of water reservoir systems. (arXiv:2309.00373v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00373
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27700;&#24211;&#31995;&#32479;&#30340;&#26368;&#20248;&#25805;&#20316;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#30452;&#25509;&#20174;&#36807;&#21435;&#25968;&#25454;&#29983;&#25104;&#26410;&#26469;&#27700;&#27969;&#30340;&#19968;&#32452;&#21487;&#33021;&#24773;&#26223;&#65292;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#65292;&#20351;&#25511;&#21046;&#22120;&#26356;&#21152;&#35880;&#24910;&#65292;&#21516;&#26102;&#28385;&#36275;&#20892;&#19994;&#29992;&#27700;&#38656;&#27714;&#65292;&#24182;&#24212;&#23545;&#24178;&#26097;&#26399;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#24211;&#31995;&#32479;&#30340;&#26368;&#20248;&#25805;&#20316;&#26159;&#19968;&#20010;&#28041;&#21450;&#22810;&#20010;&#30456;&#20114;&#30683;&#30462;&#30446;&#26631;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#22797;&#26434;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#26159;&#27700;&#27969;&#20837;&#20316;&#20026;&#31995;&#32479;&#19978;&#30340;&#19968;&#31181;&#22806;&#29983;&#12289;&#39640;&#24230;&#19981;&#30830;&#23450;&#30340;&#25200;&#21160;&#12290;&#24403;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26102;&#65292;&#36890;&#24120;&#22522;&#20110;&#65288;&#39044;&#27979;&#30340;&#65289;&#27700;&#27969;&#36712;&#36857;&#35745;&#31639;&#26368;&#20248;&#30340;&#27700;&#37322;&#25918;&#12290;&#24403;&#23454;&#38469;&#27700;&#27969;&#19982;&#39044;&#27979;&#19981;&#21516;&#30340;&#26102;&#20505;&#65292;&#36825;&#31181;&#36873;&#25321;&#21487;&#33021;&#20250;&#21361;&#21450;&#38381;&#29615;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#27700;&#24211;&#30340;&#38543;&#26426;MPC&#26041;&#27861;&#65292;&#20854;&#20013;&#25511;&#21046;&#26159;&#22522;&#20110;&#20174;&#36807;&#21435;&#25968;&#25454;&#30452;&#25509;&#29983;&#25104;&#30340;&#19968;&#32452;&#21487;&#33021;&#30340;&#26410;&#26469;&#27700;&#27969;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#36825;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;MPC&#31574;&#30053;&#20801;&#35768;&#25511;&#21046;&#22120;&#26356;&#21152;&#35880;&#24910;&#65292;&#20026;&#24178;&#26097;&#26399;&#38388;&#65288;&#20363;&#22914;&#28246;&#27700;&#20301;&#20302;&#20110;&#24178;&#26543;&#30028;&#38480;&#65289;&#37319;&#21462;&#23545;&#31574;&#65292;&#21516;&#26102;&#30830;&#20445;&#28385;&#36275;&#20892;&#19994;&#29992;&#27700;&#38656;&#27714;&#12290;&#36890;&#36807;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#26469;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
The optimal operation of water reservoir systems is a challenging task involving multiple conflicting objectives. The main source of complexity is the presence of the water inflow, which acts as an exogenous, highly uncertain disturbance on the system. When model predictive control (MPC) is employed, the optimal water release is usually computed based on the (predicted) trajectory of the inflow. This choice may jeopardize the closed-loop performance when the actual inflow differs from its forecast. In this work, we consider - for the first time - a stochastic MPC approach for water reservoirs, in which the control is optimized based on a set of plausible future inflows directly generated from past data. Such a scenario-based MPC strategy allows the controller to be more cautious, counteracting droughty periods (e.g., the lake level going below the dry limit) while at the same time guaranteeing that the agricultural water demand is satisfied. The method's effectiveness is validated thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#31639;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#26681;&#25454;&#32593;&#32476;&#21442;&#25968;&#30340;&#19981;&#21516;&#65292;&#20004;&#31181;&#31639;&#27861;&#21487;&#33021;&#24471;&#21040;&#19981;&#21516;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#38382;&#39064;&#30340;&#25551;&#36848;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00357</link><description>&lt;p&gt;
&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#31163;&#25955;&#19982;&#36830;&#32493;&#31639;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Discrete Versus Continuous Algorithms in Dynamics of Affective Decision Making. (arXiv:2309.00357v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#20013;&#31163;&#25955;&#21644;&#36830;&#32493;&#31639;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#26681;&#25454;&#32593;&#32476;&#21442;&#25968;&#30340;&#19981;&#21516;&#65292;&#20004;&#31181;&#31639;&#27861;&#21487;&#33021;&#24471;&#21040;&#19981;&#21516;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#38382;&#39064;&#30340;&#25551;&#36848;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#38024;&#23545;&#30001;&#20855;&#26377;&#38271;&#26399;&#21644;&#30701;&#26399;&#35760;&#24518;&#30340;&#19981;&#21516;&#31867;&#22411;&#20195;&#29702;&#32452;&#25104;&#30340;&#26234;&#33021;&#32593;&#32476;&#12290;&#30740;&#31350;&#22522;&#20110;&#27010;&#29575;&#24773;&#24863;&#20915;&#31574;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#32771;&#34385;&#20102;&#26367;&#20195;&#26041;&#26696;&#30340;&#29702;&#24615;&#25928;&#29992;&#21644;&#24773;&#24863;&#21560;&#24341;&#21147;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#27604;&#36739;&#26234;&#33021;&#32593;&#32476;&#20013;&#20004;&#31181;&#22810;&#27493;&#25805;&#20316;&#31639;&#27861;&#65306;&#22522;&#20110;&#31163;&#25955;&#21160;&#21147;&#23398;&#21644;&#22522;&#20110;&#36830;&#32493;&#21160;&#21147;&#23398;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#26681;&#25454;&#32593;&#32476;&#21442;&#25968;&#30340;&#19981;&#21516;&#65292;&#36830;&#32493;&#21644;&#31163;&#25955;&#25805;&#20316;&#30340;&#29305;&#24449;&#27010;&#29575;&#21487;&#33021;&#21576;&#29616;&#20986;&#25509;&#36817;&#25110;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#26681;&#25454;&#25152;&#37319;&#29992;&#30340;&#31639;&#27861;&#65288;&#31163;&#25955;&#25110;&#36830;&#32493;&#65289;&#65292;&#29702;&#35770;&#39044;&#27979;&#21487;&#33021;&#20250;&#26377;&#30456;&#24403;&#22823;&#30340;&#24046;&#24322;&#65292;&#36825;&#19981;&#20801;&#35768;&#23545;&#23454;&#38469;&#38382;&#39064;&#36827;&#34892;&#21807;&#19968;&#23450;&#20041;&#30340;&#25551;&#36848;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#24773;&#24863;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of affective decision making is considered for an intelligent network composed of agents with different types of memory: long-term and short-term memory. The consideration is based on probabilistic affective decision theory, which takes into account the rational utility of alternatives as well as the emotional alternative attractiveness. The objective of this paper is the comparison of two multistep operational algorithms of the intelligent network: one based on discrete dynamics and the other on continuous dynamics. By means of numerical analysis, it is shown that, depending on the network parameters, the characteristic probabilities for continuous and discrete operations can exhibit either close or drastically different behavior. Thus, depending on which algorithm is employed, either discrete or continuous, theoretical predictions can be rather different, which does not allow for a uniquely defined description of practical problems. This finding is important for understa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#33719;&#21462;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29992;&#25143;&#24037;&#20316;&#37327;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00356</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20559;&#22909;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Explainable Active Learning for Preference Elicitation. (arXiv:2309.00356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#33719;&#21462;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29992;&#25143;&#24037;&#20316;&#37327;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#26032;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#38543;&#21518;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#38656;&#35201;&#26234;&#33021;&#22320;&#22788;&#29702;&#29992;&#25143;&#20132;&#20114;&#65292;&#21363;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#20197;&#26377;&#25928;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#29305;&#23450;&#24773;&#26223;&#65292;&#22312;&#35813;&#24773;&#26223;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#36275;&#22815;&#30340;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#65292;&#38459;&#30861;&#20102;&#21033;&#29992;&#31995;&#32479;&#20013;&#29616;&#26377;&#25968;&#25454;&#30340;&#29992;&#25143;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;(AL)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#29992;&#25143;&#24037;&#20316;&#37327;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#12290;AL&#20174;&#19968;&#20010;&#22823;&#22411;&#26080;&#26631;&#31614;&#38598;&#21512;&#20013;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#21521;&#35810;&#38382;&#39044;&#27979;&#26631;&#31614;&#65292;&#24182;&#26368;&#32456;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#35299;&#37322;&#24615;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;ML&#30340;&#38598;&#25104;&#36807;&#31243;&#12290;&#23427;&#21033;&#29992;&#29992;&#25143;&#23545;&#31995;&#32479;&#36820;&#22238;&#25512;&#33616;&#30340;&#21453;&#39304;&#65288;&#32473;&#20104;&#31995;&#32479;&#30340;&#27880;&#24847;&#25110;&#21916;&#22909;&#65289;&#21644;&#29992;&#25143;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21521;&#20182;&#20204;&#35299;&#37322;&#21644;&#36741;&#21161;&#20445;&#25345;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaining insights into the preferences of new users and subsequently personalizing recommendations necessitate managing user interactions intelligently, namely, posing pertinent questions to elicit valuable information effectively. In this study, our focus is on a specific scenario of the cold-start problem, where the recommendation system lacks adequate user presence or access to other users' data is restricted, obstructing employing user profiling methods utilizing existing data in the system. We employ Active Learning (AL) to solve the addressed problem with the objective of maximizing information acquisition with minimal user effort. AL operates for selecting informative data from a large unlabeled set to inquire an oracle to label them and eventually updating a machine learning (ML) model. We operate AL in an integrated process of unsupervised, semi-supervised, and supervised ML within an explanatory preference elicitation process. It harvests user feedback (given for the system's 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#30340;&#20998;&#31867;&#39044;&#27979;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2309.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Text-based Approach For Link Prediction on Wikipedia Articles. (arXiv:2309.00317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#30340;&#20998;&#31867;&#39044;&#27979;&#65292;&#24182;&#22312;&#27604;&#36187;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;DSAA 2023&#25361;&#25112;&#20013;&#20851;&#20110;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#38142;&#25509;&#39044;&#27979;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#35789;&#24615;&#26631;&#27880;&#29305;&#24449;&#26469;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20004;&#20010;&#33410;&#28857;&#26159;&#21542;&#26377;&#38142;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;F1&#24471;&#20998;&#20026;0.99999&#33719;&#24471;&#20102;&#31532;7&#21517;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#20844;&#24320;&#33719;&#21462;&#65306;https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
&lt;/p&gt;
&lt;p&gt;
This paper present our work in the DSAA 2023 Challenge about Link Prediction for Wikipedia Articles. We use traditional machine learning models with POS tags (part-of-speech tags) features extracted from text to train the classification model for predicting whether two nodes has the link. Then, we use these tags to test on various machine learning models. We obtained the results by F1 score at 0.99999 and got 7th place in the competition. Our source code is publicly available at this link: https://github.com/Tam1032/DSAA2023-Challenge-Link-prediction-DS-UIT_SAT
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35268;&#21017;&#32858;&#21512;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#34920;&#36798;&#20026;&#23545;&#39044;&#27979;&#35268;&#21017;&#30340;&#36793;&#38469;&#25512;&#26029;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#19982;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#30340;&#26041;&#27861;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2309.00306</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#35268;&#21017;&#32858;&#21512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Aggregation of Rules for Knowledge Graph Completion. (arXiv:2309.00306v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35268;&#21017;&#32858;&#21512;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#34920;&#36798;&#20026;&#23545;&#39044;&#27979;&#35268;&#21017;&#30340;&#36793;&#38469;&#25512;&#26029;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#19982;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#30340;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#39640;&#25928;&#12289;&#21487;&#35299;&#37322;&#24615;&#24378;&#65292;&#19982;&#32431;&#31070;&#32463;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#35268;&#21017;&#32858;&#21512;&#38382;&#39064;&#28041;&#21450;&#22914;&#20309;&#20026;&#21516;&#26102;&#34987;&#22810;&#20010;&#35268;&#21017;&#39044;&#27979;&#30340;&#20505;&#36873;&#20107;&#23454;&#25214;&#21040;&#19968;&#20010;&#21512;&#29702;&#24615;&#24471;&#20998;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35268;&#21017;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#22122;&#22768;&#21644;&#24222;&#22823;&#30340;&#35268;&#21017;&#38598;&#65292;&#22240;&#27492;&#35813;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#21344;&#27604;&#36739;&#23569;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#22312;&#27492;&#32972;&#26223;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#34920;&#36798;&#20026;&#23545;&#39044;&#27979;&#35268;&#21017;&#30340;&#36793;&#38469;&#25512;&#26029;&#25805;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24120;&#35265;&#30340;&#26368;&#22823;&#32858;&#21512;&#31574;&#30053;&#65292;&#21363;&#26681;&#25454;&#32622;&#20449;&#24230;&#26368;&#39640;&#30340;&#35268;&#21017;&#20026;&#20505;&#36873;&#20107;&#23454;&#35780;&#20998;&#65292;&#20855;&#26377;&#27010;&#29575;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#19988;&#34987;&#24573;&#35270;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20043;&#21069;&#30340;&#31574;&#30053;&#65292;&#24182;&#33021;&#19982;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#30340;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule learning approaches for knowledge graph completion are efficient, interpretable and competitive to purely neural models. The rule aggregation problem is concerned with finding one plausibility score for a candidate fact which was simultaneously predicted by multiple rules. Although the problem is ubiquitous, as data-driven rule learning can result in noisy and large rulesets, it is underrepresented in the literature and its theoretical foundations have not been studied before in this context. In this work, we demonstrate that existing aggregation approaches can be expressed as marginal inference operations over the predicting rules. In particular, we show that the common Max-aggregation strategy, which scores candidates based on the rule with the highest confidence, has a probabilistic interpretation. Finally, we propose an efficient and overlooked baseline which combines the previous strategies and is competitive to computationally more expensive approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00300</link><description>&lt;p&gt;
&#29992;&#32534;&#30721;-&#35299;&#30721;&#22120;&#36827;&#34892;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#26469;&#24314;&#27169;&#23398;&#29983;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance. (arXiv:2309.00300v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#23398;&#29983;&#30340;&#31572;&#39064;&#35760;&#24405;&#20013;&#30452;&#25509;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#31572;&#39064;&#35760;&#24405;&#26469;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#26088;&#22312;&#26681;&#25454;&#23398;&#29983;&#22312;&#32771;&#35797;&#39064;&#30446;&#19978;&#30340;&#31572;&#39064;&#25104;&#32489;&#26469;&#35786;&#26029;&#20182;&#20204;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#36825;&#26159;&#35768;&#22810;&#39046;&#22495;&#22914;&#35745;&#31639;&#33258;&#36866;&#24212;&#27979;&#35797;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#65288;CDMs&#65289;&#36981;&#24490;&#20102;&#19968;&#20010;&#33021;&#21147;-&#21709;&#24212;&#33539;&#24335;&#65292;&#21363;&#23558;&#35786;&#26029;&#32467;&#26524;&#35270;&#20026;&#23398;&#29983;&#21709;&#24212;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26469;&#23398;&#20064;&#35786;&#26029;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#24456;&#23481;&#26131;&#23548;&#33268;&#19981;&#21487;&#35782;&#21035;&#30340;&#35786;&#26029;&#32467;&#26524;&#21644;&#35299;&#37322;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#23398;&#29983;&#23398;&#20064;&#34920;&#29616;&#30340;&#37327;&#21270;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35782;&#21035;&#30340;&#35748;&#30693;&#35786;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#35786;&#26029;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#30452;&#25509;&#20174;&#21709;&#24212;&#26085;&#24535;&#20013;&#35786;&#26029;&#21487;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#30340;&#32771;&#29983;&#29305;&#24449;&#21644;&#39064;&#30446;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#36890;&#29992;&#30340;&#39044;&#27979;&#27169;&#22359;&#20174;&#35786;&#26029;&#32467;&#26524;&#20013;&#37325;&#24314;&#21709;&#24212;&#26085;&#24535;&#65292;&#20197;&#30830;&#20445;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27773;&#36710;&#36187;&#36710;&#39046;&#22495;&#20013;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00296</link><description>&lt;p&gt;
&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#36187;&#36710;
&lt;/p&gt;
&lt;p&gt;
End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing. (arXiv:2309.00296v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27773;&#36710;&#36187;&#36710;&#39046;&#22495;&#20013;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20026;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#38382;&#39064;&#23450;&#20041;&#27169;&#31946;&#19988;&#38590;&#20197;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#24378;&#21270;&#23398;&#20064;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#35757;&#32451;&#19968;&#20010;&#21033;&#29992;&#21069;&#39304;&#21407;&#22987;&#28608;&#20809;&#38647;&#36798;&#21644;&#36895;&#24230;&#25968;&#25454;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#32463;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21518;&#65292;&#35813;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#22312;&#30495;&#23454;&#36187;&#36710;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22686;&#24378;&#33258;&#20027;&#36187;&#36710;&#24615;&#33021;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20808;&#21069;&#22320;&#22270;&#20449;&#24687;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a transformative approach in the domains of automation and robotics, offering powerful solutions to complex problems that conventional methods struggle to address. In scenarios where the problem definitions are elusive and challenging to quantify, learning-based solutions such as RL become particularly valuable. One instance of such complexity can be found in the realm of car racing, a dynamic and unpredictable environment that demands sophisticated decision-making algorithms. This study focuses on developing and training an RL agent to navigate a racing environment solely using feedforward raw lidar and velocity data in a simulated context. The agent's performance, trained in the simulation environment, is then experimentally evaluated in a real-world racing scenario. This exploration underlines the feasibility and potential benefits of RL algorithm enhancing autonomous racing performance, especially in the environments where prior map inform
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00257</link><description>&lt;p&gt;
&#21033;&#29992;&#23398;&#20064;&#24230;&#37327;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Learning Metrics for Improved Federated Learning. (arXiv:2309.00257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23398;&#20064;&#26041;&#26696;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#26032;&#23398;&#20064;&#24230;&#37327;&#65292;&#23588;&#20854;&#26159;&#26377;&#21161;&#20110;&#30830;&#23450;&#27169;&#22411;&#23398;&#20064;&#24773;&#20917;&#30340;&#26032;&#23398;&#20064;&#24230;&#37327;&#12290;&#20854;&#20013;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#34987;&#31216;&#20026;&#8220;&#26377;&#25928;&#31209;&#8221;&#65288;ER&#65289;&#65292;&#23427;&#34913;&#37327;&#30697;&#38453;&#22855;&#24322;&#20540;&#30340;&#39321;&#20892;&#29109;&#65292;&#20174;&#32780;&#30830;&#23450;&#23618;&#30340;&#26144;&#23556;&#25928;&#26524;&#12290;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23558;(1)&#25552;&#20379;&#31532;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;(2)&#35777;&#26126;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#65292;(3)&#24320;&#21457;&#19968;&#31181;&#20381;&#36182;&#26377;&#25928;&#31209;&#30340;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.
&lt;/p&gt;</description></item><item><title>DiffuGen&#26159;&#19968;&#31181;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#29983;&#25104;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#32780;&#21487;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26631;&#27880;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25552;&#20379;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00248</link><description>&lt;p&gt;
DiffuGen&#65306;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21487;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models. (arXiv:2309.00248v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00248
&lt;/p&gt;
&lt;p&gt;
DiffuGen&#26159;&#19968;&#31181;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#29983;&#25104;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#31616;&#21333;&#32780;&#21487;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#26631;&#27880;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25552;&#20379;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#20934;&#30830;&#19988;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#26631;&#27880;&#30495;&#23454;&#22270;&#20687;&#30340;&#36807;&#31243;&#24448;&#24448;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#29983;&#25104;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;DiffuGen&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#22320;&#21019;&#24314;&#24102;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#20102;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#36824;&#20026;&#26631;&#31614;&#29983;&#25104;&#25552;&#20379;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DiffuGen&#30340;&#26041;&#27861;&#35770;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#20004;&#31181;&#19981;&#21516;&#30340;&#26631;&#27880;&#25216;&#26415;&#65306;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#30456;&#32467;&#21512;&#12290;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;DiffuGen&#37319;&#29992;&#20102;&#36866;&#24212;&#24615;&#22270;&#20687;&#29983;&#25104;&#30340;&#25552;&#31034;&#27169;&#26495;&#21644;&#25991;&#26412;&#21453;&#28436;&#20197;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating high-quality labeled image datasets is crucial for training accurate and robust machine learning models in the field of computer vision. However, the process of manually labeling real images is often time-consuming and costly. To address these challenges associated with dataset generation, we introduce "DiffuGen," a simple and adaptable approach that harnesses the power of stable diffusion models to create labeled image datasets efficiently. By leveraging stable diffusion models, our approach not only ensures the quality of generated datasets but also provides a versatile solution for label generation. In this paper, we present the methodology behind DiffuGen, which combines the capabilities of diffusion models with two distinct labeling techniques: unsupervised and supervised. Distinctively, DiffuGen employs prompt templating for adaptable image generation and textual inversion to enhance diffusion model capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#22240;&#32032;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#65292;&#20026;&#26234;&#33021;&#30005;&#32593;&#21644;&#26234;&#33021;&#22478;&#24066;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2309.00245</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#32972;&#26223;&#19979;&#30340;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
City electric power consumption forecasting based on big data &amp; neural network under smart grid background. (arXiv:2309.00245v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#22823;&#25968;&#25454;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#32771;&#34385;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#22240;&#32032;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#65292;&#20026;&#26234;&#33021;&#30005;&#32593;&#21644;&#26234;&#33021;&#22478;&#24066;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#21147;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#26234;&#33021;&#30005;&#32593;&#24050;&#25104;&#20026;&#26234;&#33021;&#22478;&#24066;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21512;&#29702;&#20256;&#36755;&#30005;&#33021;&#21644;&#20445;&#35777;&#26234;&#33021;&#30005;&#32593;&#20379;&#30005;&#23545;&#26234;&#33021;&#22478;&#24066;&#38750;&#24120;&#37325;&#35201;&#65292;&#26234;&#33021;&#22478;&#24066;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#30005;&#32593;&#25552;&#20379;&#26356;&#22909;&#30340;&#26381;&#21153;&#12290;&#20854;&#20013;&#65292;&#39044;&#27979;&#21644;&#21028;&#26029;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#19982;&#30005;&#21147;&#20379;&#24212;&#21644;&#35843;&#25511;&#12289;&#21457;&#30005;&#21378;&#30340;&#20301;&#32622;&#20197;&#21450;&#30005;&#21147;&#20256;&#36755;&#25439;&#32791;&#30340;&#25511;&#21046;&#31561;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#22522;&#20110;&#22823;&#25968;&#25454;&#24314;&#31435;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#32771;&#34385;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#22240;&#32032;&#23545;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#30340;&#24433;&#21709;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#30005;&#21147;&#28040;&#36153;&#30340;&#39044;&#27979;&#12290;&#22522;&#20110;&#32622;&#25442;&#37325;&#35201;&#24615;&#27979;&#35797;&#65292;&#26500;&#24314;&#20102;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#24433;&#21709;&#22240;&#32032;&#30340;&#35780;&#20272;&#27169;&#22411;&#65292;&#33719;&#21462;&#20102;&#22478;&#24066;&#30005;&#21147;&#28040;&#36153;&#39044;&#27979;&#30340;&#26680;&#24515;&#29305;&#24449;&#20540;&#65292;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the electric power system, the smart grid has become an important part of the smart city. The rational transmission of electric energy and the guarantee of power supply of the smart grid are very important to smart cities, smart cities can provide better services through smart grids. Among them, predicting and judging city electric power consumption is closely related to electricity supply and regulation, the location of power plants, and the control of electricity transmission losses. Based on big data, this paper establishes a neural network and considers the influence of various nonlinear factors on city electric power consumption. A model is established to realize the prediction of power consumption. Based on the permutation importance test, an evaluation model of the influencing factors of city electric power consumption is constructed to obtain the core characteristic values of city electric power consumption prediction, which can provide an important refe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#26469;&#22686;&#24378;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#65292;&#24182;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20107;&#23454;&#26680;&#26597;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00240</link><description>&lt;p&gt;
FactLLaMA: &#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#20248;&#21270;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking. (arXiv:2309.00240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#26469;&#22686;&#24378;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#30456;&#20851;&#35777;&#25454;&#65292;&#24182;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20107;&#23454;&#26680;&#26597;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#22312;&#25171;&#20987;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25351;&#20196;&#36861;&#36394;&#21464;&#31181;&#65292;&#22914;InstructGPT&#21644;Alpaca&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30693;&#35782;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#26368;&#26032;&#25110;&#20805;&#20998;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#20107;&#23454;&#26680;&#26597;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#25351;&#20196;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#35777;&#25454;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#26680;&#26597;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#26816;&#32034;&#19982;&#32473;&#23450;&#36755;&#20837;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#12290;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21487;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35777;&#25454;&#23545;&#19968;&#20010;&#21517;&#20026;LLaMA&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36755;&#20837;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic fact-checking plays a crucial role in combating the spread of misinformation. Large Language Models (LLMs) and Instruction-Following variants, such as InstructGPT and Alpaca, have shown remarkable performance in various natural language processing tasks. However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking. To address this limitation, we propose combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance. Our approach involves leveraging search engines to retrieve relevant evidence for a given input claim. This external evidence serves as valuable supplementary information to augment the knowledge of the pretrained language model. Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately. To evaluate our method, we conducted experiments 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#27861;&#24459;&#21028;&#20915;&#30340;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27861;&#23448;&#21644;&#24459;&#24072;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#65292;&#24182;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#20107;&#20808;&#20998;&#26512;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00238</link><description>&lt;p&gt;
ALJP: &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#30340;&#27861;&#24459;&#21028;&#20915;
&lt;/p&gt;
&lt;p&gt;
ALJP: An Arabic Legal Judgment Prediction in Personal Status Cases Using Machine Learning Models. (arXiv:2309.00238v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00238
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#39044;&#27979;&#38463;&#25289;&#20271;&#20010;&#20154;&#36523;&#20221;&#26696;&#20214;&#27861;&#24459;&#21028;&#20915;&#30340;&#31995;&#32479;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27861;&#23448;&#21644;&#24459;&#24072;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#65292;&#24182;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#20107;&#20808;&#20998;&#26512;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#22522;&#20110;&#26696;&#24773;&#25551;&#36848;&#39044;&#27979;&#21028;&#20915;&#32467;&#26524;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#36890;&#36807;&#39044;&#27979;&#27861;&#24459;&#32844;&#19994;&#30340;&#32467;&#26524;&#26469;&#24110;&#21161;&#28508;&#22312;&#30340;&#23458;&#25143;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#31181;&#25552;&#20986;&#30340;&#25216;&#26415;&#26159;&#29992;&#38463;&#25289;&#20271;&#35821;&#23454;&#29616;&#30340;&#65292;&#21482;&#26377;&#23569;&#25968;&#23581;&#35797;&#37319;&#29992;&#33521;&#35821;&#12289;&#27721;&#35821;&#21644;&#21360;&#22320;&#35821;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#20174;&#38463;&#25289;&#20271;&#35821;&#26696;&#24773;&#33050;&#26412;&#20013;&#39044;&#27979;&#21028;&#20915;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#25242;&#20859;&#26435;&#21644;&#23130;&#23035;&#26080;&#25928;&#30340;&#26696;&#20214;&#20013;&#12290;&#35813;&#31995;&#32479;&#23558;&#24110;&#21161;&#27861;&#23448;&#21644;&#24459;&#24072;&#25552;&#39640;&#24037;&#20316;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#21028;&#20915;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#23457;&#21028;&#20043;&#21069;&#65292;&#23427;&#23558;&#24110;&#21161;&#35785;&#35772;&#24403;&#20107;&#20154;&#12289;&#24459;&#24072;&#21644;&#27861;&#23398;&#29983;&#20998;&#26512;&#20219;&#20309;&#32473;&#23450;&#26696;&#20214;&#30340;&#21487;&#33021;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal Judgment Prediction (LJP) aims to predict judgment outcomes based on case description. Several researchers have developed techniques to assist potential clients by predicting the outcome in the legal profession. However, none of the proposed techniques were implemented in Arabic, and only a few attempts were implemented in English, Chinese, and Hindi. In this paper, we develop a system that utilizes deep learning (DL) and natural language processing (NLP) techniques to predict the judgment outcome from Arabic case scripts, especially in cases of custody and annulment of marriage. This system will assist judges and attorneys in improving their work and time efficiency while reducing sentencing disparity. In addition, it will help litigants, lawyers, and law students analyze the probable outcomes of any given case before trial. We use a different machine and deep learning models such as Support Vector Machine (SVM), Logistic regression (LR), Long Short Term Memory (LSTM), and Bidir
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#22312;&#38889;&#22269;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#20013;&#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00208</link><description>&lt;p&gt;
&#29992;&#20110;&#35821;&#20041;&#30417;&#27979;&#20844;&#21496;&#25259;&#38706;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#38889;&#22269;KOSPI&#21069;50&#23478;&#20844;&#21496;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Semantic Monitoring of Corporate Disclosures: A Case Study on Korea's Top 50 KOSPI Companies. (arXiv:2309.00208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#22312;&#38889;&#22269;&#19978;&#24066;&#20844;&#21496;&#25259;&#38706;&#20013;&#30340;&#35821;&#20041;&#20998;&#26512;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;OpenAI&#30340;GPT-3.5-turbo&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#33258;&#21160;&#21270;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38889;&#22269;&#24773;&#22659;&#19979;&#35821;&#20041;&#20998;&#26512;&#20844;&#21496;&#25259;&#38706;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#21450;&#26102;&#25259;&#38706;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38889;&#22269;KOSPI&#19978;&#24066;&#30340;&#24066;&#20540;&#21069;50&#23478;&#19978;&#24066;&#20844;&#21496;&#65292;&#24182;&#22312;17&#20010;&#26376;&#30340;&#26102;&#38388;&#20869;&#35814;&#32454;&#23457;&#26597;&#20102;&#23427;&#20204;&#30340;&#26376;&#24230;&#25259;&#38706;&#25688;&#35201;&#12290;&#27599;&#20010;&#25688;&#35201;&#37117;&#25353;&#29031;&#20174;1&#65288;&#38750;&#24120;&#36127;&#38754;&#65289;&#21040;5&#65288;&#38750;&#24120;&#27491;&#38754;&#65289;&#30340;&#27604;&#20363;&#36827;&#34892;&#20102;&#24773;&#24863;&#35780;&#32423;&#12290;&#20026;&#20102;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558;&#23427;&#20204;&#30340;&#24773;&#24863;&#35780;&#32423;&#19982;&#20154;&#24037;&#19987;&#23478;&#29983;&#25104;&#30340;&#35780;&#32423;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;GPT-3.5-turbo&#21644;GPT-4&#20043;&#38388;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#21518;&#32773;&#22312;&#20154;&#31867;&#35780;&#20272;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of artificial intelligence, state-of-the-art language models such as OpenAI's GPT-3.5-turbo and GPT-4 offer unprecedented opportunities for automating complex tasks. This research paper delves into the capabilities of these models for semantically analyzing corporate disclosures in the Korean context, specifically for timely disclosure. The study focuses on the top 50 publicly traded companies listed on the Korean KOSPI, based on market capitalization, and scrutinizes their monthly disclosure summaries over a period of 17 months. Each summary was assigned a sentiment rating on a scale ranging from 1(very negative) to 5(very positive). To gauge the effectiveness of the language models, their sentiment ratings were compared with those generated by human experts. Our findings reveal a notable performance disparity between GPT-3.5-turbo and GPT-4, with the latter demonstrating significant accuracy in human evaluation tests. The Spearman correlation coefficie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#21270;&#32420;&#32500;&#25918;&#32622;&#65288;AFP&#65289;&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#22797;&#21512;&#38646;&#20214;&#20013;&#30340;&#38388;&#38553;&#21644;&#37325;&#21472;&#12290;</title><link>http://arxiv.org/abs/2309.00206</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#32420;&#32500;&#25918;&#32622;&#20013;&#30340;&#38388;&#38553;&#21644;&#37325;&#21472;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Gap and Overlap Detection in Automated Fiber Placement. (arXiv:2309.00206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#22312;&#33258;&#21160;&#21270;&#32420;&#32500;&#25918;&#32622;&#65288;AFP&#65289;&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#22797;&#21512;&#38646;&#20214;&#20013;&#30340;&#38388;&#38553;&#21644;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#20462;&#27491;&#21046;&#36896;&#32570;&#38519;&#65292;&#29305;&#21035;&#26159;&#38388;&#38553;&#21644;&#37325;&#21472;&#30340;&#38382;&#39064;&#65292;&#23545;&#30830;&#20445;&#36890;&#36807;&#33258;&#21160;&#21270;&#32420;&#32500;&#25918;&#32622;&#65288;AFP&#65289;&#29983;&#20135;&#30340;&#39640;&#36136;&#37327;&#22797;&#21512;&#38646;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#32570;&#38519;&#26159;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#22797;&#21512;&#38646;&#20214;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#25163;&#21160;&#26816;&#26597;&#26082;&#32791;&#26102;&#21448;&#21171;&#21160;&#23494;&#38598;&#65292;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#23454;&#26045;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#31995;&#32479;&#26159;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20256;&#24863;&#22120;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#23450;&#20301;&#22797;&#21512;&#38646;&#20214;&#20013;&#38388;&#38553;&#21644;&#37325;&#21472;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#29983;&#25104;&#22797;&#21512;&#34920;&#38754;&#30340;&#28145;&#24230;&#22270;&#20687;&#65292;&#31361;&#20986;&#26174;&#31034;&#34920;&#38754;&#19978;&#22797;&#21512;&#24102;&#65288;&#25110;&#32447;&#65289;&#30340;&#39640;&#24230;&#12290;&#36890;&#36807;&#26816;&#27979;&#27599;&#20010;&#32447;&#30340;&#36793;&#30028;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#27604;&#36739;&#30456;&#37051;&#30340;&#32447;&#65292;&#24182;&#35782;&#21035;&#20986;&#38388;&#38553;&#25110;&#37325;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;
The identification and correction of manufacturing defects, particularly gaps and overlaps, are crucial for ensuring high-quality composite parts produced through Automated Fiber Placement (AFP). These imperfections are the most commonly observed issues that can significantly impact the overall quality of the composite parts. Manual inspection is both time-consuming and labor-intensive, making it an inefficient approach. To overcome this challenge, the implementation of an automated defect detection system serves as the optimal solution. In this paper, we introduce a novel method that uses an Optical Coherence Tomography (OCT) sensor and computer vision techniques to detect and locate gaps and overlaps in composite parts. Our approach involves generating a depth map image of the composite surface that highlights the elevation of composite tapes (or tows) on the surface. By detecting the boundaries of each tow, our algorithm can compare consecutive tows and identify gaps or overlaps tha
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.00201</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00201
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26631;&#20934;&#21644;&#25351;&#26631;&#65292;&#20294;&#27169;&#22411;&#36873;&#25321;&#20173;&#28982;&#23384;&#22312;&#20027;&#35266;&#24615;&#12290;&#39640;&#24230;&#20027;&#35266;&#24615;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#22797;&#24615;&#21644;&#21487;&#20877;&#29616;&#24615;&#20135;&#29983;&#30097;&#38382;&#65292;&#24182;&#23545;&#23454;&#38469;&#37096;&#32626;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#20013;&#27169;&#22411;&#26500;&#24314;&#32773;&#30340;&#20559;&#22909;&#24433;&#21709;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20197;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20026;&#20363;&#65292;&#35843;&#26597;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#20102;33&#20301;&#21442;&#19982;&#32773;&#21644;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#21442;&#19982;&#32773;&#36824;&#26159;LLMs&#30340;&#36873;&#25321;&#37117;&#23384;&#22312;&#21464;&#24322;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#25351;&#26631;&#23384;&#22312;&#20998;&#27495;&#26102;&#12290;&#20027;&#35266;&#24615;&#26469;&#28304;&#21253;&#25324;&#23545;&#19981;&#21516;&#26631;&#20934;&#21644;&#25351;&#26631;&#37325;&#35201;&#24615;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#23545;&#27169;&#22411;&#24212;&#35813;&#26377;&#22810;&#31616;&#27905;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#35268;&#27169;&#30340;&#22823;&#23567;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
&lt;/p&gt;</description></item><item><title /><link>http://arxiv.org/abs/2309.00199</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39135;&#29289;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Clustering-based Conditioning for Food Image Generation. (arXiv:2309.00199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00199
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#33203;&#39135;&#35780;&#20272;&#26159;&#20351;&#29992;&#36827;&#39135;&#22330;&#21512;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35760;&#24405;&#21644;&#20998;&#26512;&#33829;&#20859;&#25668;&#20837;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#20221;&#37327;&#20272;&#35745;&#31561;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#22823;&#37327;&#24102;&#26377;&#27880;&#37322;&#30340;&#39135;&#29289;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#20381;&#36182;&#24615;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26469;&#35828;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#20026;&#33719;&#21462;&#22823;&#37327;&#20016;&#23500;&#22810;&#26679;&#19988;&#24179;&#34913;&#30340;&#39135;&#29289;&#22270;&#20687;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#21512;&#25104;&#30340;&#39135;&#29289;&#22270;&#20687;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#32467;&#26500;&#36827;&#34892;&#29983;&#25104;&#65292;&#20294;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#30340;&#36136;&#37327;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#19968;&#33324;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29983;&#25104;&#39135;&#29289;&#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35201;&#29983;&#25104;&#30495;&#23454;&#30340;&#39135;&#29289;&#22806;&#35266;&#21644;&#32454;&#33410;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based dietary assessment serves as an efficient and accurate solution for recording and analyzing nutrition intake using eating occasion images as input. Deep learning-based techniques are commonly used to perform image analysis such as food classification, segmentation, and portion size estimation, which rely on large amounts of food images with annotations for training. However, such data dependency poses significant barriers to real-world applications, because acquiring a substantial, diverse, and balanced set of food images can be challenging. One potential solution is to use synthetic food images for data augmentation. Although existing work has explored the use of generative adversarial networks (GAN) based structures for generation, the quality of synthetic food images still remains subpar. In addition, while diffusion-based generative models have shown promising results for general image generation tasks, the generation of food images can be challenging due to the substan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22522;&#20110;&#20195;&#29702;&#36712;&#36857;&#30340;&#32593;&#32476;&#32467;&#26500;&#25512;&#29702;&#65292;&#36890;&#36807;&#35780;&#20272;&#22270;&#29109;&#21644;&#32858;&#31867;&#25351;&#25968;&#30340;&#36136;&#37327;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#25311;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00172</link><description>&lt;p&gt;
&#26816;&#27979;&#32676;&#20307;&#20013;&#32452;&#32455;&#35777;&#25454;&#30340;&#36712;&#36857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting Evidence of Organization in groups by Trajectories. (arXiv:2309.00172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22522;&#20110;&#20195;&#29702;&#36712;&#36857;&#30340;&#32593;&#32476;&#32467;&#26500;&#25512;&#29702;&#65292;&#36890;&#36807;&#35780;&#20272;&#22270;&#29109;&#21644;&#32858;&#31867;&#25351;&#25968;&#30340;&#36136;&#37327;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#25311;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#26816;&#27979;&#32452;&#32455;&#23545;&#20110;&#25171;&#20987;&#29359;&#32618;&#21644;&#32500;&#25252;&#20844;&#20849;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#20154;&#21147;&#36164;&#28304;&#21644;&#22788;&#29702;&#23637;&#29616;&#20849;&#21516;&#31227;&#21160;&#27169;&#24335;&#30340;&#27599;&#20010;&#32676;&#20307;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#30528;&#37325;&#35299;&#20915;&#32593;&#32476;&#32467;&#26500;&#25512;&#29702;&#65288;NSI&#65289;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#20195;&#29702;&#36712;&#36857;&#30340;&#26816;&#27979;&#32593;&#32476;&#32467;&#26500;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#22270;&#29109;&#30340;&#35780;&#20272;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#32771;&#34385;&#32858;&#31867;&#25351;&#25968;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;NetLogo&#24179;&#21488;&#19978;&#20351;&#29992;&#22522;&#20110;&#21160;&#29289;&#29579;&#22269;&#30340;&#22235;&#20010;&#22330;&#26223;&#27169;&#25311;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#34434;&#34433;&#12289;&#29436;&#32650;&#25429;&#39135;&#12289;&#32676;&#32858;&#21644;&#34434;&#34433;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25152;&#24471;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23558;&#25152;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;NetLogo&#24179;&#21488;&#30340;&#27169;&#25311;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#26816;&#27979;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#21457;&#29616;&#32452;&#32455;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective detection of organizations is essential for fighting crime and maintaining public safety, especially considering the limited human resources and tools to deal with each group that exhibits co-movement patterns. This paper focuses on solving the Network Structure Inference (NSI) challenge. Thus, we introduce two new approaches to detect network structure inferences based on agent trajectories. The first approach is based on the evaluation of graph entropy, while the second considers the quality of clustering indices. To evaluate the effectiveness of the new approaches, we conducted experiments using four scenario simulations based on the animal kingdom, available on the NetLogo platform: Ants, Wolf Sheep Predation, Flocking, and Ant Adaptation. Furthermore, we compare the results obtained with those of an approach previously proposed in the literature, applying all methods to simulations of the NetLogo platform. The results demonstrate that our new detection approaches can mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#38899;&#35270;&#39057;&#25968;&#25454;&#26469;&#35782;&#21035;&#20799;&#31461;&#28216;&#25103;&#20013;&#30340;&#24773;&#24863;&#12290;&#20351;&#29992;FER&#25968;&#25454;&#38598;&#26816;&#27979;&#38754;&#37096;&#34920;&#24773;&#65292;&#20351;&#29992;CREMA-D&#12289;TESS&#12289;RAVDESS&#21644;Savee&#25968;&#25454;&#38598;&#35782;&#21035;&#22768;&#38899;&#24773;&#24863;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#34701;&#21512;&#32467;&#26524;&#12290;&#31995;&#32479;&#36824;&#33021;&#26816;&#27979;&#24773;&#24863;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00138</link><description>&lt;p&gt;
&#28216;&#25103;&#20013;&#22522;&#20110;&#27169;&#31946;&#26041;&#27861;&#30340;&#20799;&#31461;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Approach for Audio-Video Emotion Recognition in Computer Games for Children. (arXiv:2309.00138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#38899;&#35270;&#39057;&#25968;&#25454;&#26469;&#35782;&#21035;&#20799;&#31461;&#28216;&#25103;&#20013;&#30340;&#24773;&#24863;&#12290;&#20351;&#29992;FER&#25968;&#25454;&#38598;&#26816;&#27979;&#38754;&#37096;&#34920;&#24773;&#65292;&#20351;&#29992;CREMA-D&#12289;TESS&#12289;RAVDESS&#21644;Savee&#25968;&#25454;&#38598;&#35782;&#21035;&#22768;&#38899;&#24773;&#24863;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#34701;&#21512;&#32467;&#26524;&#12290;&#31995;&#32479;&#36824;&#33021;&#26816;&#27979;&#24773;&#24863;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#30005;&#33041;&#28216;&#25103;&#24050;&#32463;&#24191;&#27867;&#26222;&#21450;&#65292;&#21463;&#21040;&#21508;&#20010;&#24180;&#40836;&#27573;&#20154;&#20204;&#30340;&#21916;&#29233;&#12290;&#20294;&#26159;&#23545;&#20110;&#20799;&#31461;&#26469;&#35828;&#65292;&#29609;&#36825;&#20123;&#28216;&#25103;&#19981;&#20165;&#20165;&#26159;&#19968;&#31181;&#23089;&#20048;&#65292;&#26356;&#26159;&#20182;&#20204;&#21457;&#23637;&#37325;&#35201;&#25216;&#33021;&#21644;&#24314;&#31435;&#24773;&#24863;&#26234;&#21147;&#30340;&#26041;&#24335;&#12290;&#20799;&#31461;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#22768;&#38899;&#21453;&#26144;&#20102;&#20182;&#20204;&#30340;&#24773;&#32490;&#12289;&#24819;&#27861;&#21644;&#24515;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#38899;&#39057;&#21644;&#35270;&#39057;&#25968;&#25454;&#65292;&#38598;&#25104;&#20102;&#27169;&#31946;&#26041;&#27861;&#26469;&#35782;&#21035;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#20799;&#31461;&#19987;&#29992;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#26088;&#22312;&#25552;&#21319;&#20182;&#20204;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;FER&#25968;&#25454;&#38598;&#26816;&#27979;&#20174;&#28216;&#25103;&#23631;&#24149;&#19978;&#35760;&#24405;&#30340;&#35270;&#39057;&#24103;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;&#23545;&#20110;&#20799;&#31461;&#22312;&#28216;&#25103;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#22768;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#25105;&#20204;&#20351;&#29992;CREMA-D&#12289;TESS&#12289;RAVDESS&#21644;Savee&#25968;&#25454;&#38598;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#26469;&#34701;&#21512;&#32467;&#26524;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#21487;&#20197;&#26816;&#27979;&#24773;&#24863;&#31283;&#23450;&#24615;&#21644;&#24773;&#24863;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer games are widespread nowadays and enjoyed by people of all ages. But when it comes to kids, playing these games can be more than just fun, it is a way for them to develop important skills and build emotional intelligence. Facial expressions and sounds that kids produce during gameplay reflect their feelings, thoughts, and moods. In this paper, we propose a novel framework that integrates a fuzzy approach for the recognition of emotions through the analysis of audio and video data. Our focus lies within the specific context of computer games tailored for children, aiming to enhance their overall user experience. We use the FER dataset to detect facial emotions in video frames recorded from the screen during the game. For the audio emotion recognition of sounds a kid produces during the game, we use CREMA-D, TESS, RAVDESS, and Savee datasets. Next, a fuzzy inference system is used for the fusion of results. Besides this, our system can detect emotion stability and emotion divers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#25991;&#20013;&#34920;&#36798;&#30340;&#24773;&#32490;&#65292;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#20102;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.00136</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Predicting Financial Market Trends using Time Series Analysis and Natural Language Processing. (arXiv:2309.00136v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#25991;&#20013;&#34920;&#36798;&#30340;&#24773;&#32490;&#65292;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#20102;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#36235;&#21183;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35768;&#22810;&#21464;&#37327;&#21487;&#20197;&#24433;&#21709;&#32929;&#31080;&#20215;&#26684;&#12290;&#36825;&#20123;&#21464;&#37327;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#32463;&#27982;&#21644;&#25919;&#27835;&#20107;&#20214;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#20844;&#20247;&#24577;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20844;&#20247;&#24773;&#32490;&#30340;&#34920;&#36798;&#65288;&#22914;Twitter&#65289;&#21487;&#33021;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#30830;&#23450;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;Twitter&#24773;&#32490;&#20316;&#20026;&#39044;&#27979;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#25991;&#20013;&#20256;&#36798;&#30340;&#24773;&#32490;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#20043;&#38388;&#23384;&#22312;&#24378;&#26377;&#21147;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#25968;&#25454;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting financial market trends through time series analysis and natural language processing poses a complex and demanding undertaking, owing to the numerous variables that can influence stock prices. These variables encompass a spectrum of economic and political occurrences, as well as prevailing public attitudes. Recent research has indicated that the expression of public sentiments on social media platforms such as Twitter may have a noteworthy impact on the determination of stock prices. The objective of this study was to assess the viability of Twitter sentiments as a tool for predicting stock prices of major corporations such as Tesla, Apple. Our study has revealed a robust association between the emotions conveyed in tweets and fluctuations in stock prices. Our findings indicate that positivity, negativity, and subjectivity are the primary determinants of fluctuations in stock prices. The data was analyzed utilizing the Long-Short Term Memory neural network (LSTM) model, whi
&lt;/p&gt;</description></item><item><title>&#24314;&#26500;&#35821;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#23545;&#20110;&#25805;&#20316;&#21270;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20197;&#21450;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.00135</link><description>&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Construction Grammar and Artificial Intelligence. (arXiv:2309.00135v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00135
&lt;/p&gt;
&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#26377;&#30528;&#32039;&#23494;&#30340;&#20851;&#31995;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#23545;&#20110;&#25805;&#20316;&#21270;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20197;&#21450;&#26500;&#24314;&#26234;&#33021;&#20195;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#24403;&#20195;&#30340;&#24314;&#26500;&#35821;&#27861;&#23398;&#32773;&#26469;&#35828;&#65292;&#28145;&#20837;&#29702;&#35299;&#24314;&#26500;&#35821;&#27861;&#19982;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#30340;&#32039;&#23494;&#20851;&#31995;&#38750;&#24120;&#26377;&#30410;&#12290;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#21382;&#21490;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20851;&#31995;&#26681;&#26893;&#20110;&#23545;&#20154;&#31867;&#27807;&#36890;&#21644;&#35821;&#35328;&#30340;&#20849;&#21516;&#24577;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#31532;&#19968;&#20010;&#24433;&#21709;&#26041;&#21521;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#27934;&#35265;&#21644;&#25216;&#26415;&#22312;&#25805;&#20316;&#21270;&#12289;&#39564;&#35777;&#21644;&#25193;&#23637;&#35821;&#35328;&#24314;&#26500;&#20027;&#20041;&#26041;&#27861;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#31532;&#20108;&#20010;&#24433;&#21709;&#26041;&#21521;&#65292;&#24378;&#35843;&#24314;&#26500;&#35821;&#27861;&#27934;&#35265;&#21644;&#20998;&#26512;&#23545;&#20110;&#26500;&#24314;&#30495;&#27491;&#26234;&#33021;&#20195;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#21162;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#29992;&#21508;&#31181;&#20363;&#23376;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#31181;&#20851;&#31995;&#21313;&#20998;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter, we argue that it is highly beneficial for the contemporary construction grammarian to have a thorough understanding of the strong relationship between the research fields of construction grammar and artificial intelligence. We start by unravelling the historical links between the two fields, showing that their relationship is rooted in a common attitude towards human communication and language. We then discuss the first direction of influence, focussing in particular on how insights and techniques from the field of artificial intelligence play an important role in operationalising, validating and scaling constructionist approaches to language. We then proceed to the second direction of influence, highlighting the relevance of construction grammar insights and analyses to the artificial intelligence endeavour of building truly intelligent agents. We support our case with a variety of illustrative examples and conclude that the further elaboration of this relationship wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#35299;&#37322;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2309.00096</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation. (arXiv:2309.00096v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23646;&#24615;&#20998;&#35299;&#32858;&#21512;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#35299;&#37322;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#23545;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#20998;&#21106;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23384;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#20302;&#36136;&#37327;&#30340;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#12290;&#20363;&#22914;&#65292;&#36825;&#31181;&#33539;&#24335;&#20551;&#35774;&#26032;&#30340;&#25991;&#26412;&#31867;&#21035;&#23558;&#34987;&#20934;&#30830;&#23436;&#25972;&#22320;&#25552;&#20379;&#65292;&#24182;&#19988;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#35789;&#20856;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#36935;&#21040;&#31616;&#30701;&#25110;&#19981;&#23436;&#25972;&#30340;&#21517;&#31216;&#12289;&#22312;&#39044;&#35757;&#32451;&#30340;&#35789;&#20856;&#20013;&#19981;&#23384;&#22312;&#30340;&#26032;&#35789;&#20197;&#21450;&#38590;&#20197;&#25551;&#36848;&#30340;&#31867;&#21035;&#26102;&#65292;&#24322;&#24120;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;-&#32858;&#21512;&#26694;&#26550;&#65292;&#21463;&#20154;&#31867;&#35748;&#30693;&#22312;&#29702;&#35299;&#26032;&#27010;&#24565;&#26041;&#38754;&#30340;&#21551;&#21457;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#20998;&#35299;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#31867;&#21035;&#21517;&#31216;&#20998;&#35299;&#20026;&#22810;&#26679;&#30340;&#23646;&#24615;&#25551;&#36848;&#65292;&#20197;&#20016;&#23500;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#35774;&#35745;&#20102;&#20004;&#31181;&#23646;&#24615;&#26500;&#24314;&#31574;&#30053;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names. For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training. However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users. To address these issues, this work proposes a novel decomposition-aggregation framework, inspired by human cognition in understanding new concepts. Specifically, in the decomposition stage, we decouple class names into diverse attribute descriptions to enrich semantic contexts. Two attribute construction strategies are designed: using large langu
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#12290;LLMs&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20219;&#21153;&#65292;&#20294;&#22312;&#20351;&#29992;&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;LLMs&#30340;&#21457;&#23637;&#12289;&#24212;&#29992;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#20174;&#19994;&#32773;&#29702;&#35299;&#21644;&#24212;&#23545;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.00087</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65306;&#28508;&#21147;&#19982;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Large language models in medicine: the potentials and pitfalls. (arXiv:2309.00087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00087
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#12290;LLMs&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20219;&#21153;&#65292;&#20294;&#22312;&#20351;&#29992;&#26102;&#23384;&#22312;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;LLMs&#30340;&#21457;&#23637;&#12289;&#24212;&#29992;&#21644;&#21487;&#33021;&#30340;&#38480;&#21046;&#65292;&#20197;&#24110;&#21161;&#21307;&#30103;&#20174;&#19994;&#32773;&#29702;&#35299;&#21644;&#24212;&#23545;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21307;&#30103;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#21307;&#23398;&#32771;&#35797;&#38382;&#39064;&#21040;&#22238;&#31572;&#24739;&#32773;&#38382;&#39064;&#12290;&#38543;&#30528;&#29983;&#20135;LLMs&#30340;&#20844;&#21496;&#19982;&#21307;&#30103;&#31995;&#32479;&#20043;&#38388;&#30340;&#26426;&#26500;&#21512;&#20316;&#22686;&#21152;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#20020;&#24202;&#24212;&#29992;&#27491;&#36880;&#28176;&#25104;&#20026;&#29616;&#23454;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21307;&#30103;&#20174;&#19994;&#32773;&#20102;&#35299;LLMs&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#30340;&#21457;&#23637;&#20197;&#21450;&#22312;&#21307;&#23398;&#20013;&#30340;&#24403;&#21069;&#21644;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;LLMs&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#21644;&#37197;&#22871;&#25945;&#31243;&#26088;&#22312;&#20026;&#21307;&#30103;&#20174;&#19994;&#32773;&#25552;&#20379;&#20851;&#20110;&#36825;&#20123;&#20027;&#39064;&#30340;&#27010;&#36848;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#29702;&#35299;LLMs&#22312;&#21307;&#23398;&#20013;&#24212;&#29992;&#30340;&#24555;&#36895;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been applied to tasks in healthcare, ranging from medical exam questions to responding to patient questions. With increasing institutional partnerships between companies producing LLMs and healthcare systems, real world clinical application is coming closer to reality. As these models gain traction, it is essential for healthcare practitioners to understand what LLMs are, their development, their current and potential applications, and the associated pitfalls when utilized in medicine. This review and accompanying tutorial aim to give an overview of these topics to aid healthcare practitioners in understanding the rapidly changing landscape of LLMs as applied to medicine.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.00082</link><description>&lt;p&gt;
RePo: &#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#22686;&#24378;&#24377;&#24615;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#20687;&#35266;&#27979;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#26410;&#33021;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#20266;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#21464;&#21270;&#65292;&#22914;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#25110;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#19968;&#31181;&#23545;&#36825;&#31181;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#40723;&#21169;&#35813;&#34920;&#31034;&#22312;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#22823;&#30340;&#39044;&#27979;&#24615;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#35266;&#27979;&#21040;&#28508;&#22312;&#34920;&#31034;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#30446;&#26631;&#26497;&#22823;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#24377;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;&#23398;&#20064;&#21040;&#30340;&#32534;&#30721;&#22120;&#23545;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#65292;&#20294;&#22312;&#26174;&#33879;&#20998;&#24067;&#21464;&#21270;&#19979;&#24182;&#27809;&#26377;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19982;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#23494;&#20999;&#30456;&#20851;&#30340;&#20262;&#29702;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36879;&#26126;&#24230;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#24615;&#21644;&#20197;&#20154;&#20026;&#26412;&#20026;&#26680;&#24515;&#20215;&#20540;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.00064</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21644;&#20854;&#20182;&#39046;&#22495;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ethical Framework for Harnessing the Power of AI in Healthcare and Beyond. (arXiv:2309.00064v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00064
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#19982;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#23494;&#20999;&#30456;&#20851;&#30340;&#20262;&#29702;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36879;&#26126;&#24230;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#24615;&#21644;&#20197;&#20154;&#20026;&#26412;&#20026;&#26680;&#24515;&#20215;&#20540;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;&#20154;&#24037;&#26234;&#33021;&#65289;&#26041;&#27861;&#30340;&#24212;&#29992;&#24050;&#32463;&#22312;&#21508;&#31181;&#30495;&#23454;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24448;&#24448;&#28041;&#21450;&#21040;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19982;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#23494;&#20999;&#30456;&#20851;&#30340;&#20262;&#29702;&#32500;&#24230;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#22312;&#28145;&#20837;&#30740;&#31350;&#20013;&#65292;&#23427;&#25506;&#32034;&#20102;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36879;&#26126;&#24230;&#12289;&#29087;&#32451;&#30340;&#25968;&#25454;&#31649;&#29702;&#12289;&#20154;&#24037;&#30417;&#30563;&#12289;&#25945;&#32946;&#35201;&#27714;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#36827;&#27493;&#39046;&#22495;&#30340;&#22269;&#38469;&#21512;&#20316;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#33391;&#30693;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#31361;&#26174;&#36879;&#26126;&#24230;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#24615;&#21644;&#20197;&#20154;&#20026;&#26412;&#30340;&#23548;&#21521;&#12290;&#26412;&#25991;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22266;&#26377;&#23616;&#38480;&#24615;&#30340;&#28145;&#20837;&#21644;&#20840;&#38754;&#35752;&#35770;&#12290;&#23427;&#31934;&#26126;&#22320;&#35782;&#21035;&#20986;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#23548;&#33322;&#22810;&#37325;&#25361;&#25112;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, the deployment of deep learning (Artificial Intelligence (AI)) methods has become pervasive across a spectrum of real-world applications, often in safety-critical contexts. This comprehensive research article rigorously investigates the ethical dimensions intricately linked to the rapid evolution of AI technologies, with a particular focus on the healthcare domain. Delving deeply, it explores a multitude of facets including transparency, adept data management, human oversight, educational imperatives, and international collaboration within the realm of AI advancement. Central to this article is the proposition of a conscientious AI framework, meticulously crafted to accentuate values of transparency, equity, answerability, and a human-centric orientation. The second contribution of the article is the in-depth and thorough discussion of the limitations inherent to AI systems. It astutely identifies potential biases and the intricate challenges of navigating multiface
&lt;/p&gt;</description></item><item><title>FACET&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#22823;&#22411;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#22270;&#20687;&#38598;&#65292;&#23545;&#20110;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#21516;&#26102;&#20351;&#29992;&#19987;&#23478;&#35780;&#23457;&#21592;&#25163;&#21160;&#26631;&#27880;&#20154;&#31867;&#23646;&#24615;&#21644;&#31867;&#21035;&#20449;&#24687;&#65292;&#24182;&#29992;&#20110;&#23545;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2309.00035</link><description>&lt;p&gt;
FACET:&#35745;&#31639;&#26426;&#35270;&#35273;&#35780;&#20272;&#22522;&#20934;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FACET: Fairness in Computer Vision Evaluation Benchmark. (arXiv:2309.00035v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00035
&lt;/p&gt;
&lt;p&gt;
FACET&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#22823;&#22411;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#22270;&#20687;&#38598;&#65292;&#23545;&#20110;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#21516;&#26102;&#20351;&#29992;&#19987;&#23478;&#35780;&#23457;&#21592;&#25163;&#21160;&#26631;&#27880;&#20154;&#31867;&#23646;&#24615;&#21644;&#31867;&#21035;&#20449;&#24687;&#65292;&#24182;&#29992;&#20110;&#23545;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#32932;&#33394;&#31561;&#23646;&#24615;&#19978;&#23384;&#22312;&#24050;&#30693;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#24615;&#33021;&#20250;&#26681;&#25454;&#22270;&#20687;&#20013;&#20154;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#20123;&#24046;&#24322;&#24050;&#34987;&#35777;&#26126;&#23384;&#22312;&#65292;&#20294;&#30452;&#21040;&#29616;&#22312;&#36824;&#27809;&#26377;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#24120;&#35265;&#30340;&#29992;&#20363;&#20013;&#30340;&#36825;&#20123;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACET&#65288;FAirness in Computer Vision EvaluaTion&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35780;&#20272;&#38598;&#65292;&#21253;&#21547;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;-&#22270;&#20687;&#20998;&#31867;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;32k&#24352;&#22270;&#29255;&#12290;&#23545;&#20110;FACET&#20013;&#30340;&#27599;&#19968;&#24352;&#22270;&#29255;&#65292;&#25105;&#20204;&#38599;&#29992;&#19987;&#23478;&#35780;&#23457;&#21592;&#25163;&#21160;&#26631;&#27880;&#19982;&#20154;&#30456;&#20851;&#30340;&#23646;&#24615;&#65292;&#22914;&#24863;&#30693;&#30340;&#32932;&#33394;&#21644;&#21457;&#22411;&#31867;&#22411;&#65292;&#25163;&#21160;&#32472;&#21046;&#36793;&#30028;&#26694;&#65292;&#24182;&#26631;&#35760;&#32454;&#31890;&#24230;&#30340;&#19982;&#20154;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#22914;&#30879;&#33402;&#20154;&#25110;&#21513;&#20182;&#25163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;FACET&#26469;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision models have known performance disparities across attributes such as gender and skin tone. This means during tasks such as classification and detection, model performance differs for certain classes based on the demographics of the people in the image. These disparities have been shown to exist, but until now there has not been a unified approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vision EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks - image classification, object detection and segmentation. For every image in FACET, we hired expert reviewers to manually annotate person-related attributes such as perceived skin tone and hair type, manually draw bounding boxes and label fine-grained person-related classes such as disk jockey or guitarist. In addition, we use FACET to benchmark state-of-the-art vision models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#20102;&#20854;&#22312;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#29983;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#65292;&#20294;&#25945;&#32946;&#32773;&#24212;&#35813;&#25552;&#20379;&#20351;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.00029</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#32534;&#31243;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models to Generate Formative Programming Feedback. (arXiv:2309.00029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35745;&#31639;&#26426;&#25945;&#32946;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#30740;&#31350;&#20102;&#20854;&#22312;&#32534;&#31243;&#20219;&#21153;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#29983;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#65292;&#20294;&#25945;&#32946;&#32773;&#24212;&#35813;&#25552;&#20379;&#20351;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30456;&#20851;&#24212;&#29992;&#22914;ChatGPT&#20986;&#29616;&#20197;&#26469;&#65292;&#20854;&#22312;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#38169;&#35823;&#20998;&#26512;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#23545;&#35937;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#26679;&#30340;LLM&#23545;&#35745;&#31639;&#26426;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#23398;&#20064;&#32773;&#30340;&#28508;&#21147;&#65292;&#20998;&#26512;&#20854;&#23545;&#21253;&#21547;&#31243;&#24207;&#20195;&#30721;&#30340;&#36755;&#20837;&#25152;&#29983;&#25104;&#30340;&#21453;&#39304;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#65288;1&#65289;&#25506;&#32034;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#23545;&#23547;&#27714;&#24110;&#21161;&#20110;&#20837;&#38376;&#32423;&#32534;&#31243;&#20219;&#21153;&#30340;&#23398;&#29983;&#30340;&#22238;&#24212;&#65292;&#20197;&#21450;&#65288;2&#65289;&#35782;&#21035;&#20854;&#22238;&#24212;&#20013;&#30340;&#21453;&#39304;&#31867;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20174;&#19968;&#20010;CS1&#35838;&#31243;&#20013;&#25910;&#38598;&#30340;&#23398;&#29983;&#32534;&#31243;&#24207;&#21015;&#20316;&#20026;ChatGPT&#30340;&#36755;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#38382;&#39064;&#20197;&#24341;&#23548;&#21453;&#39304;&#21644;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#26576;&#20123;&#20837;&#38376;&#32423;&#32534;&#31243;&#20219;&#21153;&#21644;&#23398;&#29983;&#38169;&#35823;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#36825;&#24847;&#21619;&#30528;&#23398;&#29983;&#26377;&#21487;&#33021;&#20174;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#25945;&#32946;&#32773;&#24212;&#35813;&#25552;&#20379;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever since the emergence of large language models (LLMs) and related applications, such as ChatGPT, its performance and error analysis for programming tasks have been subject to research. In this work-in-progress paper, we explore the potential of such LLMs for computing educators and learners, as we analyze the feedback it generates to a given input containing program code. In particular, we aim at (1) exploring how an LLM like ChatGPT responds to students seeking help with their introductory programming tasks, and (2) identifying feedback types in its responses. To achieve these goals, we used students' programming sequences from a dataset gathered within a CS1 course as input for ChatGPT along with questions required to elicit feedback and correct solutions. The results show that ChatGPT performs reasonably well for some of the introductory programming tasks and student errors, which means that students can potentially benefit. However, educators should provide guidance on how to us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.00018</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#38750;&#19987;&#23478;&#29992;&#25143;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35832;&#22914;&#38598;&#25104;&#26799;&#24230;&#31561;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20135;&#29983;&#20102;&#21253;&#21547;&#22823;&#37327;&#20449;&#24687;&#20294;&#38590;&#20197;&#35299;&#37322;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#26368;&#22823;&#28608;&#27963;&#32452;&#25552;&#21462;&#65288;MAGE&#65289;&#21644;&#22810;&#23610;&#24230;&#21487;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#65288;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#25214;&#21040;&#32473;&#23450;CNN&#20013;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23558;&#36825;&#20123;&#30456;&#20284;&#29305;&#24449;&#27169;&#24335;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65288;&#21253;&#25324;&#22240;&#26524;&#20851;&#31995;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#31867;&#21035;&#24863;&#30693;&#39034;&#24207;&#30456;&#20851;&#24615;&#65288;CaOC&#65289;&#65292;&#20840;&#23616;&#35780;&#20272;&#26681;&#25454;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, enhancing global interpretability. MAGE finds, for a given CNN, combinations of features which, globally, form a semantic meaning, that we call concepts. We group these similar feature patterns by clustering in ``concepts'', that we visualize through Ms-IV. This last method is inspired by Occlusion and Sensitivity analysis (incorporating causality), and uses a novel metric, called Class-aware Order Correlation (CaOC), to globally evaluate the most important image regions according to the model's 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16737</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#29615;&#22659;&#20013;&#65292;&#26412;&#36136;&#19978;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#23450;&#20301;&#38382;&#39064;&#12290;&#30001;&#20110;&#32852;&#37030;&#29615;&#22659;&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#25104;&#20026;&#21487;&#20280;&#32553;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20851;&#38190;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29615;&#22659;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#25968;&#25454;&#30340;&#24178;&#25200;&#65292;&#20351;&#24471;&#20256;&#32479;&#26041;&#27861;&#22312;&#32500;&#25252;&#20272;&#35745;&#31934;&#24230;&#21644;&#30830;&#20445;&#31639;&#27861;&#25910;&#25947;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#20013;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#22987;&#24418;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#36845;&#20195;&#31616;&#21270;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.16505</link><description>&lt;p&gt;
&#25512;&#33616;AI&#20195;&#29702;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#20132;&#20114;&#24335;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#21697;&#25512;&#33616;&#65292;&#23637;&#29616;&#20986;&#36731;&#37327;&#32423;&#39046;&#22495;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#22312;&#25351;&#20196;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#20154;&#31867;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#29289;&#21697;&#30446;&#24405;&#21644;&#34892;&#20026;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#22914;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#12290;&#20026;&#27599;&#20010;&#39046;&#22495;&#24494;&#35843;LLMs&#26082;&#19981;&#32463;&#27982;&#21448;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#31216;&#20026;RecAgent&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15906</link><description>&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#20307;&#31995;&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#23545;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#25361;&#25112;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the U.S. Legal System Ready for AI's Challenges to Human Values?. (arXiv:2308.15906v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15906
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#27861;&#24459;&#38656;&#35201;&#21152;&#24378;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35843;&#26597;&#20102;&#32654;&#22269;&#27861;&#24459;&#22312;&#38754;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#20215;&#20540;&#35266;&#25361;&#25112;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19987;&#23478;&#30740;&#35752;&#20250;&#26399;&#38388;&#21046;&#23450;&#30340;&#22810;&#31181;&#20551;&#35774;&#24773;&#26223;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27861;&#24459;&#26694;&#26550;&#22312;&#20445;&#25252;&#33258;&#20027;&#26435;&#12289;&#38544;&#31169;&#26435;&#12289;&#23562;&#20005;&#12289;&#22810;&#26679;&#24615;&#12289;&#24179;&#31561;&#20197;&#21450;&#36523;&#24515;&#20581;&#24247;&#31561;&#22522;&#26412;&#20215;&#20540;&#35266;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#23466;&#27861;&#21644;&#27665;&#26435;&#27861;&#20284;&#20046;&#26080;&#27861;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#27495;&#35270;&#24615;&#20135;&#20986;&#25552;&#20379;&#36275;&#22815;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#25105;&#20204;&#25490;&#38500;&#31532;230&#26465;&#27454;&#25552;&#20379;&#30340;&#36131;&#20219;&#20445;&#25252;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#35777;&#26126;&#35837;&#35876;&#21644;&#20135;&#21697;&#36131;&#20219;&#32034;&#36180;&#30340;&#22240;&#26524;&#20851;&#31995;&#20063;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24102;&#26469;&#30340;&#29420;&#29305;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#20027;&#24352;&#24314;&#31435;&#33021;&#22815;&#36866;&#24212;&#26032;&#23041;&#32961;&#24182;&#20026;&#34892;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#31215;&#26497;&#12289;&#21487;&#23457;&#35745;&#30340;&#25351;&#23548;&#30340;&#27861;&#24459;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our interdisciplinary study investigates how effectively U.S. laws confront the challenges posed by Generative AI to human values. Through an analysis of diverse hypothetical scenarios crafted during an expert workshop, we have identified notable gaps and uncertainties within the existing legal framework regarding the protection of fundamental values, such as autonomy, privacy, dignity, diversity, equality, and physical/mental well-being. Constitutional and civil rights, it appears, may not provide sufficient protection against AI-generated discriminatory outputs. Furthermore, even if we exclude the liability shield provided by Section 230, proving causation for defamation and product liability claims is a challenging endeavor due to the intricate and opaque nature of AI systems. To address the unique and unforeseeable threats posed by Generative AI, we advocate for legal frameworks that evolve to recognize new threat and provide proactive, auditable guidelines to industry stakeholders
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.15734</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;GNNs&#65292;&#20294;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#35774;&#35745;/&#36873;&#25321;&#26368;&#20339;GNN&#26550;&#26500;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33410;&#30465;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;Graph NAS&#65289;&#26469;&#25628;&#32034;&#32467;&#21512;&#29616;&#26377;&#32452;&#20214;&#30340;&#27425;&#20248;GNN&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;Graph NAS&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#22810;&#26679;&#21270;&#22270;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;Graph NAS&#26041;&#27861;&#65292;&#31216;&#20026;ExGNAS&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#65288;ii&#65289;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#25628;&#32034;&#31354;&#38388;&#20165;&#21253;&#21547;&#21487;&#20197;&#22788;&#29702;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22522;&#26412;&#20989;&#25968;&#12290;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#34987;&#25972;&#21512;&#21040;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#25110;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#24322;&#24120;&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#37325;&#26500;&#32593;&#32476;&#35757;&#32451;&#26377;&#36129;&#29486;&#30340;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#21387;&#32553;&#25104;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20010;&#26694;&#26550;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#26082;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21448;&#36991;&#20813;&#23545;&#37325;&#26500;&#36807;&#31243;&#24341;&#20837;&#24178;&#25200;&#12290;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#30456;&#20851;&#25351;&#26631;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.14919</link><description>&lt;p&gt;
&#35770;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22870;&#21169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#19982;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#36125;&#23572;&#26364;&#26041;&#31243;&#20013;&#30340;&#23384;&#22312;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#38024;&#23545;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#21508;&#31181;"&#25104;&#26412;"&#65292;&#22870;&#21169;&#26159;&#29702;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#32467;&#26500;&#30340;&#26680;&#24515;&#65292;&#22870;&#21169;&#20013;&#24515;&#27010;&#24565;&#21487;&#20197;&#38416;&#26126;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#23454;&#20363;&#29305;&#23450;&#30340;&#35823;&#24046;&#30028;&#20026;$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$&#65292;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#12290;&#22312;&#22312;&#32447;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#36716;&#31227;&#30340;MDP&#24120;&#25968;&#65292;&#30452;&#24452;&#65292;&#25913;&#36827;&#20026;&#22522;&#20110;&#22870;&#21169;&#30340;&#24120;&#25968;&#65292;&#26368;&#22823;&#39044;&#26399;&#21040;&#36798;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#35813;&#24120;&#25968;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#24418;&#29366;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2308.13970</link><description>&lt;p&gt;
FAM&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#20840;&#23616;&#27169;&#22411;&#65292;&#28982;&#21518;&#21487;&#20197;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#20010;&#24615;&#21270;&#12290;&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#25968;&#25454;&#22810;&#26679;&#24615;&#23548;&#33268;&#23398;&#20064;&#21463;&#21040;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#26102;&#65292;&#23398;&#20064;&#20250;&#21463;&#21040;&#22256;&#25200;&#12290;&#26377;&#24517;&#35201;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;MRI&#25968;&#25454;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19968;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25361;&#25112;&#65292;&#22312;&#26576;&#20010;&#22320;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#36275;&#20197;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#31532;&#20108;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26377;&#25968;&#25454;&#20849;&#20139;&#38480;&#21046;&#65292;&#31532;&#19977;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#31449;&#28857;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#30340;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.12740</link><description>&lt;p&gt;
&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23558;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#24037;&#31243;&#21270;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#23487;&#20027;&#31995;&#32479;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#39640;&#26114;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#20026;&#20102;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#30340;&#35774;&#35745;-&#26500;&#24314;-&#27979;&#35797;-&#23398;&#20064;&#65288;Design-Build-Test-Learn&#65292;DBTL&#65289;&#21608;&#26399;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#33021;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#24182;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#30340;&#21487;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;ILP-iML1515&#65292;&#23427;&#36890;&#36807;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#21644;&#20174;&#35757;&#32451;&#23454;&#20363;&#20013;&#31215;&#26497;&#23398;&#20064;&#26469;&#25191;&#34892;&#35828;&#26126;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#19982;&#25968;&#20540;&#27169;&#22411;&#19981;&#21516;&#65292;ILP-iML1515&#24314;&#31435;&#22312;&#23545;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#27169;&#22411;&#30340;&#21487;&#29702;&#35299;&#30340;&#36923;&#36753;&#34920;&#31034;&#19978;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20174;&#32570;&#20047;&#33829;&#20859;&#30340;&#31361;&#21464;&#20307;&#35797;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;ILP-iML1515&#26694;&#26550;&#20855;&#26377;&#39640;&#36890;&#37327;&#27169;&#25311;&#33021;&#21147;&#65292;&#24182;&#33021;&#20027;&#21160;&#36873;&#25321;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#20998;&#24067;&#30340;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#32972;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21253;&#25324;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#32972;&#38376;&#25915;&#20987;&#65288;&#29305;&#27931;&#20234;&#65289;&#12290;&#24403;&#27979;&#35797;&#23454;&#20363;&#65288;&#26469;&#33258;&#38750;&#30446;&#26631;&#31867;&#65289;&#23884;&#20837;&#29305;&#23450;&#35302;&#21457;&#22120;&#26102;&#65292;&#34987;&#32972;&#38376;&#30772;&#22351;&#30340;&#27169;&#22411;&#20250;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#30446;&#26631;&#31867;&#65292;&#21516;&#26102;&#22312;&#26080;&#25915;&#20987;&#23454;&#20363;&#19978;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;&#32972;&#38376;&#25915;&#20987;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#35270;&#39057;&#31995;&#32479;&#22312;&#32972;&#38376;&#25915;&#20987;&#19979;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#26159;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#24310;&#20280;&#65292;&#20363;&#22914;&#65292;&#35302;&#21457;&#22120;&#26159;\textbf{&#29420;&#31435;}&#23884;&#20837;&#24103;&#20013;&#30340;&#65292;&#23481;&#26131;&#34987;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#26816;&#27979;&#21040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;\textit{&#31616;&#21333;}&#20294;\textit{&#26377;&#25928;}&#30340;&#35270;&#39057;&#25968;&#25454;&#32972;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#22312;&#19968;&#20010;&#36716;&#25442;&#30340;&#39046;&#22495;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20197;&#23884;&#20837;\textbf{&#38590;&#20197;&#23519;&#35273;&#30340;&#65292;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;}&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distr
&lt;/p&gt;</description></item><item><title>CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11066</link><description>&lt;p&gt;
CSM-H-R: &#19968;&#31181;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection. (arXiv:2308.11066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11066
&lt;/p&gt;
&lt;p&gt;
CSM-H-R&#26159;&#19968;&#20010;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#20114;&#25805;&#20316;&#30340;&#26234;&#33021;&#31995;&#32479;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#22312;&#36816;&#34892;&#26102;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;&#39640;&#32423;&#19978;&#19979;&#25991;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#26234;&#33021;&#31995;&#32479;&#23545;&#39640;&#32423;&#19978;&#19979;&#25991;(HLC)&#25512;&#29702;&#30340;&#33258;&#21160;&#21270;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#22240;&#20026;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#19981;&#26029;&#31215;&#32047;&#12289;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#20197;&#21450;&#22522;&#20110;&#19978;&#19979;&#25991;&#20915;&#31574;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19978;&#19979;&#25991;&#25512;&#29702;&#26694;&#26550;CSM-H-R&#65292;&#35813;&#26694;&#26550;&#22312;&#36816;&#34892;&#26102;&#20197;&#32534;&#31243;&#26041;&#24335;&#32452;&#21512;&#26412;&#20307;&#21644;&#29366;&#24577;&#65292;&#24182;&#32467;&#21512;&#27169;&#22411;&#23384;&#20648;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#35782;&#21035;&#26377;&#24847;&#20041;&#30340;HLC&#30340;&#33021;&#21147;&#65292;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#26415;&#12290;&#22312;&#26234;&#33021;&#26657;&#22253;&#29615;&#22659;&#20013;&#22522;&#20110;&#26234;&#33021;&#30005;&#26799;&#31995;&#32479;&#24320;&#23637;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#26694;&#26550;&#30340;&#23454;&#29616;-CSM&#24341;&#25806;&#20197;&#21450;&#23558;HLC&#25512;&#29702;&#36716;&#21270;&#20026;&#30690;&#37327;&#21644;&#30697;&#38453;&#35745;&#31639;&#30340;&#23454;&#39564;&#65292;&#29305;&#21035;&#20851;&#27880;&#19978;&#19979;&#25991;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#25968;&#23398;&#21644;&#27010;&#29575;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;</title><link>http://arxiv.org/abs/2308.10800</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#26080;&#25928;&#19988;&#20855;&#26377;&#28508;&#22312;&#21361;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is ineffective and potentially harmful for fact checking. (arXiv:2308.10800v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24110;&#21161;&#29992;&#25143;&#21028;&#26029;&#26631;&#39064;&#20934;&#30830;&#24615;&#21644;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#26041;&#38754;&#24433;&#21709;&#19981;&#22823;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#35823;&#23548;&#29992;&#25143;&#23545;&#30495;&#23454;&#26631;&#39064;&#30340;&#20449;&#20208;&#65292;&#24182;&#22686;&#21152;&#23545;&#26410;&#30830;&#23450;&#34394;&#20551;&#26631;&#39064;&#30340;&#20449;&#20208;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26159;&#23545;&#25239;&#38169;&#35823;&#20449;&#24687;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#20294;&#26159;&#23427;&#22312;&#35268;&#27169;&#19978;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#32593;&#32476;&#19978;&#20449;&#24687;&#36807;&#20110;&#24222;&#22823;&#30340;&#38459;&#30861;&#12290;&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#20107;&#23454;&#26680;&#26597;&#20449;&#24687;&#26102;&#30340;&#20316;&#29992;&#26426;&#21046;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23545;&#29031;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19968;&#27454;&#28909;&#38376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20107;&#23454;&#26680;&#26597;&#23545;&#25919;&#27835;&#26032;&#38395;&#20449;&#20208;&#21644;&#20998;&#20139;&#24847;&#22270;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#35813;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31359;&#34394;&#20551;&#26631;&#39064;&#26041;&#38754;&#34920;&#29616;&#24471;&#30456;&#24403;&#19981;&#38169;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#24182;&#27809;&#26377;&#23545;&#21442;&#19982;&#32773;&#35782;&#21035;&#26631;&#39064;&#20934;&#30830;&#24615;&#25110;&#20998;&#20139;&#20934;&#30830;&#26032;&#38395;&#30340;&#33021;&#21147;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#20107;&#23454;&#26680;&#26597;&#22120;&#20855;&#26377;&#21361;&#23475;&#24615;&#65306;&#23558;&#19968;&#20123;&#30495;&#23454;&#26631;&#39064;&#35823;&#26631;&#20026;&#34394;&#20551;&#20250;&#38477;&#20302;&#23545;&#20854;&#30340;&#20449;&#20208;&#65292;&#32780;&#23545;&#20854;&#26410;&#30830;&#23450;&#30340;&#34394;&#20551;&#26631;&#39064;&#21017;&#20250;&#22686;&#21152;&#23545;&#20854;&#30340;&#20449;&#20208;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#25552;&#39640;&#20102;&#27491;&#30830;&#26631;&#23450;&#26631;&#39064;&#30340;&#20998;&#20139;&#24847;&#24895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact checking can be an effective strategy against misinformation, but its implementation at scale is impeded by the overwhelming volume of information online. Recent artificial intelligence (AI) language models have shown impressive ability in fact-checking tasks, but how humans interact with fact-checking information provided by these models is unclear. Here we investigate the impact of fact checks generated by a popular AI model on belief in, and sharing intent of, political news in a preregistered randomized control experiment. Although the AI performs reasonably well in debunking false headlines, we find that it does not significantly affect participants' ability to discern headline accuracy or share accurate news. However, the AI fact-checker is harmful in specific cases: it decreases beliefs in true headlines that it mislabels as false and increases beliefs for false headlines that it is unsure about. On the positive side, the AI increases sharing intents for correctly labeled t
&lt;/p&gt;</description></item><item><title>&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09520</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings of the 2nd International Workshop on Adaptive Cyber Defense. (arXiv:2308.09520v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09520
&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#22312;&#20315;&#32599;&#37324;&#36798;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#65292;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#20998;&#20139;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#32593;&#32476;&#39046;&#22495;&#26080;&#27861;&#21487;&#38752;&#26377;&#25928;&#22320;&#36827;&#34892;&#38450;&#24481;&#65292;&#24517;&#39035;&#24191;&#27867;&#20381;&#36182;&#20154;&#24037;&#19987;&#23478;&#12290;&#29087;&#32451;&#30340;&#32593;&#32476;&#38450;&#24481;&#20154;&#21592;&#20379;&#24212;&#19981;&#36275;&#65292;&#24448;&#24448;&#26080;&#27861;&#21450;&#26102;&#24212;&#23545;&#32593;&#32476;&#23041;&#32961;&#12290;&#20511;&#37492;AI&#21644;ML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32593;&#32476;&#38450;&#24481;&#30740;&#31350;&#31038;&#21306;&#34987;&#28608;&#21169;&#30528;&#36890;&#36807;&#23558;AI&#21644;ML&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#24320;&#21457;&#26032;&#30340;&#21160;&#24577;&#21487;&#25345;&#32493;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#19982;&#23454;&#36341;&#32773;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#21487;&#20197;&#21152;&#36895;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#35782;&#21035;&#21644;&#24212;&#23545;&#32593;&#32476;&#25915;&#20987;&#65292;&#25110;&#32773;&#21457;&#29616;&#21644;&#20943;&#36731;&#24369;&#28857;&#30340;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 2nd International Workshop on Adaptive Cyber Defense was held at the Florida Institute of Technology, Florida. This workshop was organized to share research that explores unique applications of Artificial Intelligence (AI) and Machine Learning (ML) as foundational capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot currently be reliably and effectively defended without extensive reliance on human experts. Skilled cyber defenders are in short supply and often cannot respond fast enough to cyber threats.  Building on recent advances in AI and ML the Cyber defense research community has been motivated to develop new dynamic and sustainable defenses through the adoption of AI and ML techniques to cyber settings. Bridging critical gaps between AI and Cyber researchers and practitioners can accelerate efforts to create semi-autonomous cyber defenses that can learn to recognize and respond to cyber attacks or discover and mitigate weaknesses in cooperation with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06961</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#27531;&#24046;&#65306;&#19968;&#31181;&#35786;&#26029;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#26500;&#24314;&#26126;&#30830;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#36153;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#30340;&#21160;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#22270;&#32467;&#26500;&#23398;&#20064;&#19982;&#27169;&#22411;&#35786;&#26029;&#30340;&#26080;&#32541;&#38598;&#25104;&#65306;(i)&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#12289;(ii)&#24341;&#20837;&#20004;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#12289;(iii)&#36890;&#36807;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#26597;&#21270;&#24037;&#29983;&#20135;&#21378;&#30340;&#23436;&#25972;&#24615;&#24182;&#25552;&#20379;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.05612</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#24037;&#21378;&#30417;&#25511;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Smart Robotic System for Industrial Plant Supervision. (arXiv:2308.05612v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05612
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#26597;&#21270;&#24037;&#29983;&#20135;&#21378;&#30340;&#23436;&#25972;&#24615;&#24182;&#25552;&#20379;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#21270;&#24037;&#29983;&#20135;&#21378;&#20013;&#65292;&#20154;&#24037;&#25805;&#20316;&#21592;&#32463;&#24120;&#23545;&#24037;&#21378;&#30340;&#23436;&#25972;&#24615;&#36827;&#34892;&#26816;&#26597;&#65292;&#20197;&#30830;&#20445;&#39640;&#23433;&#20840;&#26631;&#20934;&#65292;&#22240;&#27492;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#36935;&#21040;&#21361;&#38505;&#25805;&#20316;&#26465;&#20214;&#30340;&#20154;&#12290;&#20026;&#20102;&#20943;&#36731;&#20182;&#20204;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#30417;&#25511;&#26041;&#38754;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#30001;&#19968;&#20010;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#21644;&#21508;&#31181;&#20256;&#24863;&#22120;&#21644;&#25968;&#25454;&#22788;&#29702;&#22120;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#30340;&#35270;&#35273;&#12289;&#21957;&#35273;&#21644;&#21548;&#35273;&#24863;&#30693;&#21644;&#35299;&#37322;&#33021;&#21147;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#21270;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24223;&#27700;&#22788;&#29702;&#35774;&#26045;&#20013;&#23545;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#22320;&#23548;&#33322;&#24037;&#21378;&#24182;&#25552;&#20379;&#26377;&#20851;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's chemical production plants, human field operators perform frequent checks on the plant's integrity to guarantee high safety standards, and thus are possibly the first to encounter dangerous operating conditions. To alleviate their tasks of failure detection and monitoring by audio, visual, and olfactory perceptions, we present a robotic system that consists of an autonomously navigating robot integrated with various sensors and data processing. We aim to resemble the human sensing and interpretation capabilities of sight, smell, and hearing, for providing automated inspection. We evaluate our system extensively at a wastewater facility in full working conditions. Our results demonstrate that the system is able to robustly navigate a plant and to provide useful information about critical operating conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15484</link><description>&lt;p&gt;
&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#35821;&#38899;&#21512;&#25104;&#65306;&#22522;&#20110;&#35821;&#20041;&#32534;&#30721;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding. (arXiv:2307.15484v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#32534;&#30721;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#33021;&#22815;&#37319;&#29992;&#26368;&#23567;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#20004;&#31181;&#31163;&#25955;&#35821;&#38899;&#34920;&#31034;&#24182;&#20351;&#29992;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#26469;&#35299;&#32806;TTS&#12290;&#20026;&#20102;&#35299;&#20915;&#31163;&#25955;&#34920;&#31034;&#20013;&#30340;&#39640;&#32500;&#24230;&#21644;&#27874;&#24418;&#22833;&#30495;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-LM-Speech&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#23558;&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#20026;&#22522;&#20110;mel&#39057;&#35889;&#22270;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#38901;&#24459;&#29942;&#39048;&#30340;&#25552;&#31034;&#32534;&#30721;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#25552;&#31034;&#34920;&#31034;&#33021;&#21147;&#12290;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#36935;&#21040;&#32570;&#22833;&#21644;&#37325;&#22797;&#21333;&#35789;&#30340;&#38382;&#39064;&#65292;&#32780;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#30001;&#20110;&#39044;&#27979;&#27169;&#22411;&#30340;&#23384;&#22312;&#23548;&#33268;&#34920;&#36798;&#24179;&#22343;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tetra-Diff-Speech&#65292;&#35813;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38271;&#25193;&#25955;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#38901;&#24459;&#34920;&#36798;&#12290;&#25105;&#20204;&#26399;&#26395;&#35821;&#20041;&#32534;&#30721;&#30340;&#20449;&#24687;&#20869;&#23481;&#20171;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between t
&lt;/p&gt;</description></item><item><title>Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.11991</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#24515;&#29702;&#26381;&#21153;&#30340;Psy-LLM
&lt;/p&gt;
&lt;p&gt;
Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11991
&lt;/p&gt;
&lt;p&gt;
Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24515;&#29702;&#21672;&#35810;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20840;&#29699;COVID-19&#30340;&#29190;&#21457;&#65292;&#36825;&#21152;&#24378;&#20102;&#21450;&#26102;&#21644;&#19987;&#19994;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#38656;&#27714;&#12290;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;&#30340;&#20027;&#35201;&#26381;&#21153;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Psy-LLM&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#20013;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20174;&#24515;&#29702;&#23398;&#23478;&#21644;&#24191;&#27867;&#25910;&#38598;&#30340;&#24515;&#29702;&#25991;&#31456;&#20013;&#33719;&#21462;&#30340;&#30495;&#23454;&#19990;&#30028;&#19987;&#19994;&#38382;&#31572;&#12290;Psy-LLM&#26694;&#26550;&#20316;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#21069;&#31471;&#24037;&#20855;&#65292;&#20801;&#35768;&#20182;&#20204;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#26469;&#32531;&#35299;&#24739;&#32773;&#21387;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#21327;&#21161;&#30340;&#32039;&#24613;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#22256;&#24785;&#24230;&#31561;&#20869;&#22312;&#24230;&#37327;&#26631;&#20934;&#21644;&#22806;&#37096;&#24230;&#37327;&#26631;&#20934;&#23545;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and ex
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2307.01540</link><description>&lt;p&gt;
&#22312;&#35838;&#22530;&#19978;&#23398;&#20064;&#25552;&#31034;&#20197;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01540
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#24110;&#21161;&#31038;&#20250;&#35299;&#20915;&#32039;&#36843;&#30340;&#31038;&#20250;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#27966;&#29983;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;AI&#31995;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#28818;&#20316;&#20063;&#20135;&#29983;&#20102;&#36127;&#38754;&#24773;&#32490;&#65292;&#21363;&#20351;&#22312;&#26032;&#39062;&#30340;AI&#26041;&#27861;&#21462;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#36129;&#29486;&#20043;&#21518;&#12290;&#36896;&#25104;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20294;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26412;&#36523;&#65292;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#38169;&#35823;&#22320;&#35748;&#20026;&#33258;&#24049;&#33021;&#22815;&#36731;&#26494;&#35775;&#38382;&#21644;&#22788;&#29702;&#20219;&#20309;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20219;&#20309;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#26080;&#38656;&#23545;AI&#25110;&#38382;&#39064;&#39046;&#22495;&#26377;&#20219;&#20309;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#21069;LLMs&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#24187;&#35273;&#21644;&#25512;&#29702;&#38480;&#21046;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#23545;&#20110;&#35299;&#20915;&#30001;LLMs&#29983;&#25104;&#30340;&#21487;&#33021;&#38169;&#35823;&#24314;&#35758;&#21487;&#33021;&#20135;&#29983;&#30340;&#30450;&#30446;&#36807;&#24230;&#33258;&#20449;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#36825;&#21487;&#20197;&#20943;&#23569;&#24656;&#24807;&#21644;&#20854;&#20182;&#36127;&#38754;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitude
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(PbRL)&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#25511;&#21046;&#31574;&#30053;&#65292;&#26082;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#21448;&#33021;&#22815;&#20844;&#24179;&#22320;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(FPbRL)&#26041;&#27861;&#12290;FPbRL&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#26032;&#30340;&#31119;&#21033;&#20559;&#22909;&#32780;&#19981;&#26159;PbRL&#20013;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#20559;&#22909;&#26469;&#23398;&#20064;&#19982;&#22810;&#30446;&#26631;&#20851;&#32852;&#30340;&#21521;&#37327;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;FPbRL&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#25581;&#31034;&#20102;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#35777;&#23454;&#20102;&#25910;&#20837;&#20855;&#26377;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#31561;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;ARCH&#25928;&#24212;&#20197;&#21450;&#38750;&#39640;&#26031;&#30636;&#26102;&#24615;&#25104;&#20998;&#25152;&#20135;&#29983;&#30340;&#26126;&#26174;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.01760</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#27169;&#22411;&#25581;&#31034;&#25910;&#20837;&#21160;&#24577;&#28436;&#21270;&#65306;&#22522;&#20110;PSID&#25968;&#25454;&#30340;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#30740;&#31350; (arXiv:2306.01760v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
Nonparametric Identification and Estimation of Earnings Dynamics using a Hidden Markov Model: Evidence from the PSID. (arXiv:2306.01760v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#25581;&#31034;&#20102;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#65292;&#24182;&#35777;&#23454;&#20102;&#25910;&#20837;&#20855;&#26377;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#31561;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#20102;ARCH&#25928;&#24212;&#20197;&#21450;&#38750;&#39640;&#26031;&#30636;&#26102;&#24615;&#25104;&#20998;&#25152;&#20135;&#29983;&#30340;&#26126;&#26174;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#65292;&#20197;&#25581;&#31034;&#25910;&#20837;&#25345;&#32493;&#24615;&#30340;&#22797;&#26434;&#26412;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20551;&#23450;&#23545;&#25968;&#25910;&#20837;&#27531;&#24046;&#21253;&#25324;&#25345;&#20037;&#24615;&#21644;&#30636;&#26102;&#24615;&#20004;&#20010;&#37096;&#20998;&#65292;&#22343;&#36981;&#24490;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#31639;&#23376;&#36827;&#34892;&#35889;&#20998;&#35299;&#23454;&#29616;&#38750;&#21442;&#25968;&#35782;&#21035;&#65292;&#24182;&#24341;&#20837;&#25913;&#36827;&#30340;&#38543;&#26426;EM&#31639;&#27861;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12290;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#25910;&#20837;&#21160;&#24577;&#30740;&#31350;&#65288;PSID&#65289;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25910;&#20837;&#36807;&#31243;&#21576;&#29616;&#38750;&#32447;&#24615;&#25345;&#32493;&#24615;&#12289;&#26465;&#20214;&#20559;&#26012;&#24615;&#21644;&#26465;&#20214;&#23792;&#24230;&#12290;&#27492;&#22806;&#65292;&#30636;&#26102;&#24615;&#25104;&#20998;&#20855;&#26377;&#38750;&#39640;&#26031;&#24615;&#36136;&#65292;&#22312;&#39640;&#25910;&#20837;&#23478;&#24237;&#38754;&#20020;&#36127;&#20914;&#20987;&#25110;&#20302;&#25910;&#20837;&#23478;&#24237;&#36973;&#36935;&#27491;&#20914;&#20987;&#26102;&#20250;&#20135;&#29983;&#26126;&#26174;&#30340;&#19981;&#23545;&#31216;&#20998;&#24067;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;2&#33267;8&#24180;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#25910;&#20837;&#20855;&#26377;ARCH&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a hidden Markov model designed to investigate the complex nature of earnings persistence. The proposed model assumes that the residuals of log-earnings consist of a persistent component and a transitory component, both following general Markov processes. Nonparametric identification is achieved through spectral decomposition of linear operators, and a modified stochastic EM algorithm is introduced for model estimation. Applying the framework to the Panel Study of Income Dynamics (PSID) dataset, we find that the earnings process displays nonlinear persistence, conditional skewness, and conditional kurtosis. Additionally, the transitory component is found to possess non-Gaussian properties, resulting in a significantly asymmetric distributional impact when high-earning households face negative shocks or low-earning households encounter positive shocks. Our empirical findings also reveal the presence of ARCH effects in earnings at horizons ranging from 2 to 8 years, fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13301</link><description>&lt;p&gt;
&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#26694;&#26550;&#30340;&#36328;&#22495;&#25991;&#26412;&#21040;SQL&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#38480;&#21046;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Codex&#12289;ChatGPT&#21644;GPT-4&#65289;&#22312;AI&#31038;&#21306;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#12290;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#26377;&#28508;&#21147;&#29983;&#25104;SQL&#26597;&#35810;&#65292;&#20294;&#26159;&#23427;&#20204;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#35774;&#35745;&#19981;&#33391;&#65288;&#20363;&#22914;&#31616;&#21333;&#30340;&#32467;&#26500;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CBR-ApSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26694;&#26550;&#65292;&#19982;GPT-3.5&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23545;&#19982;&#26696;&#20363;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#36827;&#34892;&#31934;&#30830;&#25511;&#21046;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#20197;&#28789;&#27963;&#35843;&#25972;GPT-3.5&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#28041;&#21450;&#65288;1&#65289;&#36890;&#36807;&#21435;&#35821;&#20041;&#21270;&#36755;&#20837;&#38382;&#39064;&#26469;&#33258;&#36866;&#24212;&#26816;&#32034;&#26696;&#20363;&#65292;&#26681;&#25454;&#38382;&#39064;&#24847;&#22270;&#65292;&#20197;&#21450;&#65288;2&#65289;&#33258;&#36866;&#24212;&#22238;&#36864;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#25552;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#26696;&#20363;&#19982;&#25552;&#31034;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#21435;&#35821;&#20041;&#21270;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Semantic D
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have significantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs' performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#26469;&#25351;&#23548;&#20854;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2302.05629</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#25913;&#36827;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Differentiable Architecture Search via Self-Distillation. (arXiv:2302.05629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#26469;&#25351;&#23548;&#20854;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#12290;DARTS&#22312;&#25628;&#32034;&#38454;&#27573;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26550;&#26500;&#21442;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#26469;&#35757;&#32451;&#36229;&#32593;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;DARTS&#23558;&#36229;&#32593;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#24471;&#21040;&#22522;&#20110;&#26550;&#26500;&#21442;&#25968;&#30340;&#26368;&#20248;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36229;&#32593;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#26497;&#23567;&#20540;&#28857;&#32780;&#19981;&#26159;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#36825;&#20307;&#29616;&#22312;&#36229;&#32593;&#25439;&#22833;&#26354;&#38754;&#30340;&#23574;&#38160;&#31243;&#24230;&#36739;&#39640;&#65292;&#26368;&#32456;&#23548;&#33268;&#36229;&#32593;&#19982;&#26368;&#20248;&#26550;&#26500;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26469;&#32531;&#35299;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#33976;&#39311;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#24341;&#23548;&#20854;&#22312;&#24403;&#21069;&#27493;&#39588;&#20013;&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01168</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29289;&#29702;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#29289;&#29702;&#20808;&#39564;&#25110;&#24402;&#32435;&#20559;&#35265;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#30446;&#26631;&#31995;&#32479;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#23450;&#20110;&#31995;&#32479;&#65292;&#19981;&#20801;&#35768;&#36731;&#26494;&#36866;&#24212;&#30001;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#39537;&#21160;&#30340;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#20110;&#36136;&#28857;&#24377;&#31783;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#21452;&#20307;&#31995;&#32479;&#25110;&#20219;&#20309;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#20351;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#20854;&#36866;&#24212;&#26032;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#21508;&#31181;&#21704;&#23494;&#39039;&#27969;&#24418;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36825;&#26159;&#21704;&#23494;&#39039;&#31995;&#32479;&#25968;&#25454;&#20998;&#24067;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#19981;&#21516;&#31995;&#32479;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#26377;&#20854;&#33258;&#36523;&#22266;&#26377;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#21046;&#65292;&#21487;&#20197;&#20174;&#20219;&#24847;&#19987;&#23478;&#32452;&#20013;&#25910;&#38598;&#21040;&#20851;&#20110;&#20219;&#24847;&#36923;&#36753;&#21629;&#39064;&#30495;&#23454;&#27010;&#29575;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#26126;&#30830;&#24418;&#24335;&#30340;&#35299;&#37322;&#12290;&#35813;&#26426;&#21046;&#21487;&#20197;&#28608;&#21169;&#19987;&#23478;&#30452;&#25509;&#20132;&#27969;&#20449;&#24687;&#65292;&#35299;&#20915;&#31185;&#23398;&#25110;&#21307;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.13424</link><description>&lt;p&gt;
&#20174;&#39044;&#27979;&#24066;&#22330;&#21040;&#21487;&#35299;&#37322;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
From prediction markets to interpretable collective intelligence. (arXiv:2204.13424v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#21046;&#65292;&#21487;&#20197;&#20174;&#20219;&#24847;&#19987;&#23478;&#32452;&#20013;&#25910;&#38598;&#21040;&#20851;&#20110;&#20219;&#24847;&#36923;&#36753;&#21629;&#39064;&#30495;&#23454;&#27010;&#29575;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#26126;&#30830;&#24418;&#24335;&#30340;&#35299;&#37322;&#12290;&#35813;&#26426;&#21046;&#21487;&#20197;&#28608;&#21169;&#19987;&#23478;&#30452;&#25509;&#20132;&#27969;&#20449;&#24687;&#65292;&#35299;&#20915;&#31185;&#23398;&#25110;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#21019;&#24314;&#19968;&#31181;&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#26368;&#20248;&#30340;&#26041;&#24335;&#20174;&#20219;&#24847;&#19987;&#23478;&#32452;&#20013;&#24341;&#20986;&#20851;&#20110;&#20219;&#24847;&#36923;&#36753;&#21629;&#39064;&#30495;&#23454;&#27010;&#29575;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#26126;&#30830;&#24418;&#24335;&#30340;&#38598;&#20307;&#20449;&#24687;&#26469;&#35299;&#37322;&#36825;&#31181;&#27010;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35770;&#25454;&#65292;&#35777;&#26126;&#20102;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#34394;&#25311;&#36135;&#24065;&#28608;&#21169;&#19987;&#23478;&#20043;&#38388;&#30452;&#25509;&#20449;&#24687;&#20132;&#27969;&#30340;&#33258;&#35299;&#20915;&#39044;&#27979;&#24066;&#22330;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#28608;&#21169;&#35768;&#22810;&#19987;&#23478;&#20197;&#38750;&#24120;&#39640;&#25928;&#30340;&#26041;&#24335;&#20849;&#21516;&#35299;&#20915;&#31185;&#23398;&#25110;&#21307;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#22312;&#25105;&#20204;&#30340;&#32771;&#34385;&#20013;&#65292;&#24182;&#19981;&#20551;&#35774;&#19987;&#23478;&#20204;&#26159;&#36125;&#21494;&#26031;&#20027;&#20041;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We outline how to create a mechanism that provides an optimal way to elicit, from an arbitrary group of experts, the probability of the truth of an arbitrary logical proposition together with collective information that has an explicit form and interprets this probability. Namely, we provide strong arguments for the possibility of the development of a self-resolving prediction market with play money that incentivizes direct information exchange between experts. Such a system could, in particular, motivate simultaneously many experts to collectively solve scientific or medical problems in a very efficient manner. We also note that in our considerations, experts are not assumed to be Bayesian.
&lt;/p&gt;</description></item></channel></rss>