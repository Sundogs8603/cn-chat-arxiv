<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12312</link><description>&lt;p&gt;
ForceSight: &#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#35273;&#21147;&#23548;&#21521;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12312
&lt;/p&gt;
&lt;p&gt;
ForceSight&#26159;&#19968;&#20010;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#31227;&#21160;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#22312;&#26410;&#35265;&#29615;&#22659;&#20013;&#36827;&#34892;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ForceSight&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#26469;&#39044;&#27979;&#35270;&#35273;&#21147;&#23548;&#21521;&#30340;&#30446;&#26631;&#12290;&#32473;&#23450;&#19968;&#24352;RGBD&#22270;&#29255;&#21644;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;ForceSight&#21487;&#20197;&#30830;&#23450;&#30456;&#26426;&#22352;&#26631;&#31995;&#19979;&#30340;&#30446;&#26631;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#23039;&#65288;&#36816;&#21160;&#30446;&#26631;&#65289;&#21644;&#30456;&#20851;&#30340;&#21147;&#37327;&#65288;&#21147;&#37327;&#30446;&#26631;&#65289;&#12290;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#20849;&#21516;&#24418;&#25104;&#20102;&#19968;&#20010;&#35270;&#35273;&#21147;&#23548;&#21521;&#30446;&#26631;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36755;&#20986;&#20154;&#21487;&#35299;&#37322;&#30340;&#36816;&#21160;&#30446;&#26631;&#30340;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#24039;&#22937;&#25805;&#20316;&#12290;&#21147;&#37327;&#22312;&#25805;&#20316;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#36739;&#20302;&#23618;&#27425;&#30340;&#25191;&#34892;&#20013;&#12290;&#24403;&#24212;&#29992;&#20110;&#24102;&#26377;&#25163;&#33218;&#21644;&#30524;&#30555;&#30340;&#31227;&#21160;&#25805;&#20316;&#35013;&#32622;&#30340;ForceSight&#26102;&#65292;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#26174;&#33879;&#30340;&#26410;&#35265;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#20197;81%&#30340;&#25104;&#21151;&#29575;&#23436;&#25104;&#35832;&#22914;&#31934;&#30830;&#25235;&#21462;&#12289;&#25277;&#23625;&#25171;&#24320;&#21644;&#29289;&#20307;&#20132;&#25509;&#31561;&#20219;&#21153;&#12290;&#22312;&#21478;&#19968;&#39033;&#29420;&#31435;&#23454;&#39564;&#20013;&#65292;ForceSight&#20165;&#20351;&#29992;&#35270;&#35273;&#20282;&#26381;&#65292;&#19981;&#32771;&#34385;&#21147;&#37327;&#20449;&#24687;&#65292;&#20294;&#20381;&#28982;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#25805;&#20316;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
&lt;/p&gt;</description></item><item><title>LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12311</link><description>&lt;p&gt;
LLM-Grounder: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#30340;&#24320;&#25918;&#35789;&#27719;3D&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12311
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;&#23450;&#20301;&#26159;&#23478;&#29992;&#26426;&#22120;&#20154;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20351;&#20854;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#25805;&#20316;&#29289;&#20307;&#24182;&#26681;&#25454;&#29615;&#22659;&#22238;&#31572;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#26597;&#35810;&#26102;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Grounder&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#12290;LLM-Grounder&#21033;&#29992;&#19968;&#20010;LLM&#23558;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20998;&#35299;&#20026;&#35821;&#20041;&#25104;&#20998;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;OpenScene&#25110;LERF&#20043;&#31867;&#30340;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#26469;&#35782;&#21035;3D&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#12290;&#28982;&#21518;&#65292;LLM&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#24120;&#35782;&#20851;&#31995;&#65292;&#20197;&#20570;&#20986;&#26368;&#32456;&#30340;&#23450;&#20301;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;3D&#22330;&#26223;&#21644;&#20219;&#24847;&#25991;&#26412;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;ScanRefer&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LLM-Grounder&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs si
&lt;/p&gt;</description></item><item><title>&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.12309</link><description>&lt;p&gt;
&#28436;&#32451;&#65306;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#26469;&#25945;&#25480;&#20914;&#31361;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12309
&lt;/p&gt;
&lt;p&gt;
&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20914;&#31361;&#26159;&#19968;&#31181;&#20196;&#20154;&#19981;&#33298;&#26381;&#20294;&#19981;&#21487;&#36991;&#20813;&#30340;&#29983;&#27963;&#20107;&#23454;&#12290;&#25104;&#21151;&#22320;&#22788;&#29702;&#20914;&#31361;&#26159;&#19968;&#31181;&#25216;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#21051;&#24847;&#32451;&#20064;&#26469;&#23398;&#20064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#33021;&#22815;&#33719;&#24471;&#26377;&#25928;&#30340;&#22521;&#35757;&#25110;&#21453;&#39304;&#12290;&#20026;&#20102;&#25193;&#22823;&#36825;&#31181;&#26426;&#20250;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28436;&#32451;&#65288;Rehearsal&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#19982;&#21487;&#20449;&#30340;&#27169;&#25311;&#23545;&#35805;&#32773;&#19968;&#36215;&#25490;&#32451;&#20914;&#31361;&#65292;&#25506;&#32034;&#22914;&#26524;&#24773;&#20917;&#22914;&#20309;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;&#20197;&#35782;&#21035;&#26367;&#20195;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#29305;&#23450;&#30340;&#20914;&#31361;&#31574;&#30053;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#28436;&#32451;&#26469;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#24050;&#23450;&#20041;&#30340;&#20914;&#31361;&#22330;&#26223;&#65292;&#20174;&#21150;&#20844;&#23460;&#20105;&#35758;&#21040;&#24773;&#24863;&#38382;&#39064;&#65292;&#25110;&#32773;&#20182;&#20204;&#20063;&#21487;&#20197;&#36873;&#25321;&#21019;&#24314;&#33258;&#24049;&#30340;&#20914;&#31361;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#28436;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IRP&#25552;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20914;&#31361;&#35299;&#20915;&#20013;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#21033;&#30410;-&#26435;&#21147;-&#33021;&#21147;&#65288;IRP&#65289;&#29702;&#35770;&#26469;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28436;&#32451;&#20351;&#29992;IRP&#29983;&#25104;&#22522;&#20110;&#20914;&#31361;&#35299;&#20915;&#29702;&#35770;&#30340;&#35805;&#35821;&#65292;&#24341;&#23548;&#29992;&#25143;&#23454;&#36341;&#24212;&#29992;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users t
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12301</link><description>&lt;p&gt;
&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Environment-biased Feature Ranking for Novelty Detection Robustness. (arXiv:2309.12301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29615;&#22659;&#20559;&#21521;&#29305;&#24449;&#25490;&#24207;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#24615;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#30340;&#29615;&#22659;&#20043;&#38388;&#20998;&#24067;&#26041;&#24046;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#39640;&#20998;&#29305;&#24449;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#19978;&#22343;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26816;&#27979;&#35821;&#20041;&#20869;&#23481;&#26041;&#38754;&#30340;&#26032;&#39062;&#24615;&#65292;&#21516;&#26102;&#23545;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22810;&#20010;&#29615;&#22659;&#30340;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#30830;&#23450;&#19982;&#29615;&#22659;&#26356;&#30456;&#20851;&#32780;&#19981;&#26159;&#20219;&#21153;&#30456;&#20851;&#20869;&#23481;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#22810;&#29615;&#22659;&#35774;&#32622;&#24320;&#22987;&#65292;&#25104;&#21151;&#26681;&#25454;&#20854;&#29615;&#22659;&#20851;&#27880;&#24230;&#23545;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#29615;&#22659;&#20043;&#38388;&#30340;&#29305;&#24449;&#20998;&#24067;&#26041;&#24046;&#35745;&#31639;&#27599;&#20010;&#29305;&#24449;&#30340;&#24471;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#33293;&#24323;&#24471;&#20998;&#36739;&#39640;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#21487;&#20197;&#21435;&#38500;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#27491;&#24577;&#21327;&#26041;&#24046;&#21644;&#23376;&#31181;&#32676;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#30495;&#23454;&#30340;&#36824;&#26159;&#23545;&#20110;&#25105;&#20204;&#20026;&#27492;&#20219;&#21153;&#24341;&#20837;&#30340;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of robust novelty detection, where we aim to detect novelties in terms of semantic content while being invariant to changes in other, irrelevant factors. Specifically, we operate in a setup with multiple environments, where we determine the set of features that are associated more with the environments, rather than to the content relevant for the task. Thus, we propose a method that starts with a pretrained embedding and a multi-env setup and manages to rank the features based on their environment-focus. First, we compute a per-feature score based on the feature distribution variance between envs. Next, we show that by dropping the highly scored ones, we manage to remove spurious correlations and improve the overall performance by up to 6%, both in covariance and sub-population shift cases, both for a real and a synthetic benchmark, that we introduce for this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22810;&#25351;&#26426;&#22120;&#20154;&#30340;&#35302;&#35273;&#28789;&#24039;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12300</link><description>&lt;p&gt;
See to Touch: &#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#23398;&#20064;&#35302;&#35273;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
See to Touch: Learning Tactile Dexterity through Visual Incentives. (arXiv:2309.12300v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22810;&#25351;&#26426;&#22120;&#20154;&#30340;&#35302;&#35273;&#28789;&#24039;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#31867;&#25797;&#38271;&#30340;&#31934;&#30830;&#12289;&#23500;&#26377;&#25509;&#35302;&#12289;&#28789;&#24039;&#30340;&#25805;&#20316;&#65292;&#20026;&#22810;&#25351;&#26426;&#22120;&#20154;&#37197;&#22791;&#35302;&#35273;&#20256;&#24863;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#35302;&#35273;&#20256;&#24863;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#32447;&#32034;&#26469;&#25512;&#29702;&#29289;&#20307;&#30340;&#31354;&#38388;&#37197;&#32622;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#32416;&#27491;&#38169;&#35823;&#21644;&#36866;&#24212;&#21464;&#21270;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#36890;&#36807;&#35270;&#35273;&#28608;&#21169;&#23454;&#29616;&#35302;&#35273;&#36866;&#24212;&#65288;TAVI&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#22870;&#21169;&#30340;&#20248;&#21270;&#35302;&#35273;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#24615;&#30446;&#26631;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#35270;&#35273;&#34920;&#31034;&#36890;&#36807;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#21305;&#37197;&#30340;&#26041;&#24335;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#65292;&#20854;&#20013;&#21442;&#32771;&#19968;&#20010;&#20154;&#31867;&#31034;&#33539;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#26426;&#22120;&#20154;&#19978;&#22522;&#20110;&#35302;&#35273;&#30340;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#35270;&#35273;&#22870;&#21169;&#12290;&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#25554;&#38144;&#25343;&#21462;&#12289;&#21368;&#19979;&#30871;&#21644;&#32763;&#36716;&#32454;&#38271;&#29289;&#20307;&#31561;&#65292;TAVI&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12295</link><description>&lt;p&gt;
&#23398;&#20064;&#39550;&#39542;&#21040;&#20219;&#20309;&#22320;&#26041;
&lt;/p&gt;
&lt;p&gt;
Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#39550;&#39542;&#34892;&#20026;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23398;&#20064;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#30340;&#39550;&#39542;&#20915;&#31574;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36947;&#36335;&#26465;&#20214;&#21644;&#20132;&#36890;&#35268;&#21017;&#65292;&#20363;&#22914;&#24038;&#39550;&#39542;&#21644;&#21491;&#39550;&#39542;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#21482;&#33021;&#22312;&#38480;&#23450;&#30340;&#25805;&#20316;&#39046;&#22495;&#20869;&#37096;&#32626;&#65292;&#19981;&#33021;&#32771;&#34385;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#20043;&#38388;&#30340;&#39550;&#39542;&#34892;&#20026;&#24046;&#24322;&#21644;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AnyD&#65292;&#19968;&#31181;&#21333;&#19968;&#30340;&#20855;&#26377;&#22320;&#29702;&#24863;&#30693;&#30340;&#26465;&#20214;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;CIL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20855;&#26377;&#21160;&#24577;&#29615;&#22659;&#12289;&#20132;&#36890;&#21644;&#31038;&#20250;&#29305;&#24449;&#30340;&#24322;&#26500;&#21644;&#20840;&#29699;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#24341;&#20837;&#19968;&#20010;&#39640;&#23481;&#37327;&#30340;&#22522;&#20110;&#22320;&#29702;&#20301;&#32622;&#30340;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#26377;&#25928;&#22320;&#36866;&#24212;&#26412;&#22320;&#32454;&#24494;&#24046;&#24322;&#24182;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#27604;&#24615;&#27169;&#20223;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#36866;&#24212;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21644;&#22320;&#29702;&#20301;&#32622;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#25361;&#25112;&#12290;EMA&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#24182;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#30340;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;EMA&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#25104;&#20026;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.12267</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#20272;&#35745;&#24179;&#22343;&#26799;&#24230;&#32858;&#21512;&#25104;&#20026;&#32852;&#37030;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#32447;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Enabling Quartile-based Estimated-Mean Gradient Aggregation As Baseline for Federated Image Classifications. (arXiv:2309.12267v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#30340;&#25361;&#25112;&#12290;EMA&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#24182;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#30340;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;EMA&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#25104;&#20026;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23454;&#29616;&#20998;&#25955;&#21327;&#20316;&#12289;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;FL&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#20010;&#20307;&#23458;&#25143;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#20197;&#21450;FL&#31995;&#32479;&#26131;&#21463;&#23433;&#20840;&#28431;&#27934;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#20272;&#35745;&#24179;&#22343;&#32858;&#21512;&#65288;EMA&#65289;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#32780;&#19988;&#20316;&#20026;FL&#31995;&#32479;&#20013;&#20808;&#36827;&#32858;&#21512;&#25216;&#26415;&#30340;&#22522;&#20934;&#32447;&#12290;EMA&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#20854;&#21452;&#37325;&#20316;&#29992;&#65306;&#36890;&#36807;&#20462;&#21098;&#22343;&#20540;&#26377;&#25928;&#22788;&#29702;&#24694;&#24847;&#24322;&#24120;&#20540;&#65292;&#25581;&#31034;&#25968;&#25454;&#24322;&#36136;&#24615;&#20197;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#23458;&#25143;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;EMA&#22987;&#32456;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#21644;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#30830;&#31435;&#33258;&#36523;&#22320;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has revolutionized how we train deep neural networks by enabling decentralized collaboration while safeguarding sensitive data and improving model performance. However, FL faces two crucial challenges: the diverse nature of data held by individual clients and the vulnerability of the FL system to security breaches. This paper introduces an innovative solution named Estimated Mean Aggregation (EMA) that not only addresses these challenges but also provides a fundamental reference point as a $\mathsf{baseline}$ for advanced aggregation techniques in FL systems. EMA's significance lies in its dual role: enhancing model security by effectively handling malicious outliers through trimmed means and uncovering data heterogeneity to ensure that trained models are adaptable across various client datasets. Through a wealth of experiments, EMA consistently demonstrates high accuracy and area under the curve (AUC) compared to alternative methods, establishing itself as a ro
&lt;/p&gt;</description></item><item><title>SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12253</link><description>&lt;p&gt;
SALSA-CLRS:&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS: A Sparse and Scalable Benchmark for Algorithmic Reasoning. (arXiv:2309.12253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12253
&lt;/p&gt;
&lt;p&gt;
SALSA-CLRS&#26159;&#19968;&#31181;&#31232;&#30095;&#19988;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#36866;&#24212;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;CLRS&#22522;&#20934;&#20013;&#20869;&#23384;&#38656;&#27714;&#39640;&#21644;&#36816;&#34892;&#26102;&#38388;&#38590;&#20197;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLRS&#31639;&#27861;&#23398;&#20064;&#22522;&#20934;&#30340;&#25193;&#23637;&#65292;&#20248;&#20808;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#34920;&#31034;&#30340;&#21033;&#29992;&#12290;CLRS&#20013;&#30340;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#20840;&#23616;&#23384;&#20648;&#22120;&#25110;&#20449;&#24687;&#20132;&#25442;&#65292;&#22312;&#20854;&#25191;&#34892;&#27169;&#22411;&#20013;&#38236;&#20687;&#34920;&#36798;&#20026;&#22522;&#20110;&#24213;&#23618;&#38382;&#39064;&#26500;&#24314;&#23436;&#20840;&#36830;&#25509;&#65288;&#32780;&#38750;&#31232;&#30095;&#65289;&#22270;&#30340;&#25805;&#20316;&#12290;&#23613;&#31649;CLRS&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#22823;&#23454;&#20363;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#25191;&#34892;&#27169;&#22411;&#30001;&#20110;&#20854;&#35201;&#27714;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#36816;&#34892;&#26102;&#38388;&#32780;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65288;&#38590;&#20197;&#25193;&#23637;&#65289;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#37325;&#35201;&#30340;&#31639;&#27861;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#65307;&#36825;&#20123;&#20027;&#35201;&#20998;&#24067;&#24335;&#31639;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALSA-CLRS&#65292;&#19968;&#20010;&#19987;&#38376;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;CLRS&#22522;&#20934;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20174;&#21407;&#22987;CLRS&#22522;&#20934;&#20013;&#25913;&#32534;&#30340;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new probl
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;</title><link>http://arxiv.org/abs/2309.12247</link><description>&lt;p&gt;
&#22351;&#35282;&#33394;&#22909;&#39038;&#38382;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12247
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20551;&#26032;&#38395;&#38656;&#35201;&#23545;&#22810;&#26679;&#32447;&#32034;&#26377;&#25935;&#38160;&#30340;&#24863;&#30693;&#21644;&#23545;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#23545;&#20110;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#22120;&#26469;&#35828;&#65292;&#30001;&#20110;&#20854;&#30693;&#35782;&#21644;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#36825;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;&#20551;&#26032;&#38395;&#26816;&#27979;&#20173;&#28982;&#26410;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20687;GPT 3.5&#36825;&#26679;&#30340;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#29702;&#24819;&#30340;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#22522;&#30784;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;fine-tuned BERT&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#38543;&#21518;&#30340;&#20998;&#26512;&#23558;&#36825;&#31181;&#24046;&#36317;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#27491;&#30830;&#36873;&#25321;&#24182;&#25972;&#21512;&#35777;&#25454;&#20197;&#24471;&#20986;&#32467;&#35770;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good a
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;EEG&#20449;&#21495;&#30340;&#20887;&#20313;&#65292;&#24182;&#22312;&#20445;&#25345;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12201</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#30340;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram Sensor Data Compression Using An Asymmetrical Sparse Autoencoder With A Discrete Cosine Transform Layer. (arXiv:2309.12201v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#21644;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#23618;&#23545;&#33041;&#30005;&#22270;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;EEG&#20449;&#21495;&#30340;&#20887;&#20313;&#65292;&#24182;&#22312;&#20445;&#25345;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#21387;&#32553;&#23545;&#20110;&#26080;&#32447;&#35760;&#24405;&#24212;&#29992;&#26469;&#35828;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20943;&#23569;&#38656;&#35201;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#23618;&#30340;&#38750;&#23545;&#31216;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#21387;&#32553;EEG&#20449;&#21495;&#30340;&#26041;&#27861;&#12290;&#33258;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#27169;&#22359;&#37319;&#29992;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#21644;DCT&#23618;&#30340;&#32452;&#21512;&#65292;&#20351;&#29992;&#30828;&#38408;&#20540;&#38750;&#32447;&#24615;&#38477;&#20302;&#20887;&#20313;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;DCT&#23618;&#21253;&#25324;&#21487;&#35757;&#32451;&#30340;&#30828;&#38408;&#20540;&#21442;&#25968;&#21644;&#32553;&#25918;&#23618;&#65292;&#21487;&#24378;&#35843;&#25110;&#20943;&#24369;&#21333;&#20010;DCT&#31995;&#25968;&#12290;&#26368;&#21518;&#65292;&#19968;&#23545;&#19968;&#21367;&#31215;&#23618;&#29983;&#25104;&#28508;&#31354;&#38388;&#12290;&#22312;&#28508;&#31354;&#38388;&#20013;&#65292;&#37319;&#29992;&#31232;&#30095;&#24809;&#32602;&#22411;&#25104;&#26412;&#20989;&#25968;&#20351;&#29305;&#24449;&#22270;&#23613;&#21487;&#33021;&#31232;&#30095;&#12290;&#28508;&#31354;&#38388;&#25968;&#25454;&#34987;&#20256;&#36755;&#21040;&#25509;&#25910;&#31471;&#12290;&#33258;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#27169;&#22359;&#20351;&#29992;&#36870;DCT&#21644;&#20004;&#20010;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) data compression is necessary for wireless recording applications to reduce the amount of data that needs to be transmitted. In this paper, an asymmetrical sparse autoencoder with a discrete cosine transform (DCT) layer is proposed to compress EEG signals. The encoder module of the autoencoder has a combination of a fully connected linear layer and the DCT layer to reduce redundant data using hard-thresholding nonlinearity. Furthermore, the DCT layer includes trainable hard-thresholding parameters and scaling layers to give emphasis or de-emphasis on individual DCT coefficients. Finally, the one-by-one convolutional layer generates the latent space. The sparsity penalty-based cost function is employed to keep the feature map as sparse as possible in the latent space. The latent space data is transmitted to the receiver. The decoder module of the autoencoder is designed using the inverse DCT and two fully connected linear layers to improve the accuracy of data
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12177</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#20013;&#30340;&#24212;&#29992; - &#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence for Drug Discovery and Development -- A Comprehensive Survey. (arXiv:2309.12177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12177
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#65292;&#36827;&#19968;&#27493;&#20419;&#36827;&#20102;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#31561;&#26041;&#38754;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;AI&#21644;ML&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23545;&#20110;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#29702;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#23558;XAI&#25216;&#26415;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20840;&#38754;&#27010;&#36848;&#20102;XAI&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;XAI&#26041;&#27861;&#12289;&#23427;&#20204;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;XAI&#25216;&#26415;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#35813;&#25991;&#31456;&#36824;&#28085;&#30422;&#20102;XAI&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#30446;&#26631;&#35782;&#21035;&#12289;&#21270;&#21512;&#29289;&#35774;&#35745;&#21644;&#27602;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of drug discovery has experienced a remarkable transformation with the advent of artificial intelligence (AI) and machine learning (ML) technologies. However, as these AI and ML models are becoming more complex, there is a growing need for transparency and interpretability of the models. Explainable Artificial Intelligence (XAI) is a novel approach that addresses this issue and provides a more interpretable understanding of the predictions made by machine learning models. In recent years, there has been an increasing interest in the application of XAI techniques to drug discovery. This review article provides a comprehensive overview of the current state-of-the-art in XAI for drug discovery, including various XAI methods, their application in drug discovery, and the challenges and limitations of XAI techniques in drug discovery. The article also covers the application of XAI in drug discovery, including target identification, compound design, and toxicity prediction. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#21644;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#22686;&#24378;&#26816;&#27979;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.12140</link><description>&lt;p&gt;
&#22522;&#20110;&#36807;&#21435;&#36941;&#21382;&#29305;&#24449;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features. (arXiv:2309.12140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#21644;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#22686;&#24378;&#26816;&#27979;&#27169;&#22411;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22914;&#20170;&#65292;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#24555;&#36895;&#21457;&#23637;&#26497;&#22823;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#27867;&#21270;&#21040;&#22810;&#26679;&#21270;&#30340;&#39550;&#39542;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#26816;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#26102;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30340;&#22810;&#20010;&#20301;&#32622;&#37325;&#22797;&#36941;&#21382;&#26469;&#36866;&#24212;&#29289;&#20307;&#26816;&#27979;&#22120;&#21040;&#26032;&#30340;&#39550;&#39542;&#29615;&#22659;&#12290;&#36890;&#36807;&#32467;&#21512;&#37325;&#22797;&#30340;&#28608;&#20809;&#38647;&#36798;&#25195;&#25551;&#35745;&#31639;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#24341;&#23548;&#33258;&#36866;&#24212;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#31354;&#38388;&#37327;&#21270;&#30340;&#21382;&#21490;&#29305;&#24449;&#26469;&#22686;&#24378;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22238;&#24402;&#22836;&#26469;&#21033;&#29992;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#29305;&#24449;&#27491;&#21017;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#26469;&#31283;&#23450;&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#20219;&#20309;&#26816;&#27979;&#22120;&#27169;&#22411;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21462;&#24471;&#20102;&#22810;&#36798;20&#20010;&#30334;&#20998;&#28857;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of 3D object detection systems for self-driving cars has significantly improved accuracy. However, these systems struggle to generalize across diverse driving environments, which can lead to safety-critical failures in detecting traffic participants. To address this, we propose a method that utilizes unlabeled repeated traversals of multiple locations to adapt object detectors to new driving environments. By incorporating statistics computed from repeated LiDAR scans, we guide the adaptation process effectively. Our approach enhances LiDAR-based detection models using spatial quantized historical features and introduces a lightweight regression head to leverage the statistics for feature regularization. Additionally, we leverage the statistics for a novel self-training process to stabilize the training. The framework is detector model-agnostic and experiments on real-world datasets demonstrate significant improvements, achieving up to a 20-point performance gain, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#22522;&#20934;&#27979;&#35797;&#12289;&#26631;&#20934;&#21644;&#35748;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;&#23427;&#20204;&#19981;&#20165;&#26377;&#29992;&#65292;&#32780;&#19988;&#23545;&#20110;&#26356;&#24191;&#27867;&#30340;&#36127;&#36131;&#20219;&#21019;&#26032;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.12139</link><description>&lt;p&gt;
&#20851;&#20110;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#22522;&#20934;&#27979;&#35797;&#12289;&#26631;&#20934;&#21644;&#35748;&#35777;&#20043;&#38388;&#20851;&#31995;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the relationship between Benchmarking, Standards and Certification in Robotics and AI. (arXiv:2309.12139v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#22522;&#20934;&#27979;&#35797;&#12289;&#26631;&#20934;&#21644;&#35748;&#35777;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;&#23427;&#20204;&#19981;&#20165;&#26377;&#29992;&#65292;&#32780;&#19988;&#23545;&#20110;&#26356;&#24191;&#27867;&#30340;&#36127;&#36131;&#20219;&#21019;&#26032;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#12289;&#26631;&#20934;&#21644;&#35748;&#35777;&#26159;&#23494;&#20999;&#30456;&#20851;&#30340;&#36807;&#31243;&#12290;&#26631;&#20934;&#25552;&#20379;&#20102;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#65288;&#25110;&#21487;&#33021;&#19981;&#65289;&#31526;&#21512;&#30340;&#35268;&#33539;&#35201;&#27714;&#12290;&#35748;&#35777;&#36890;&#24120;&#20381;&#36182;&#20110;&#31526;&#21512;&#19968;&#20010;&#25110;&#22810;&#20010;&#26631;&#20934;&#30340;&#31243;&#24230;&#26469;&#20915;&#23450;&#26159;&#21542;&#25480;&#20104;&#36816;&#33829;&#35777;&#20070;&#12290;&#32780;&#22522;&#20934;&#27979;&#35797;&#26159;&#23545;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#34913;&#37327;&#30340;&#19968;&#22871;&#26631;&#20934;&#21270;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23558;&#22522;&#20934;&#27979;&#35797;&#35270;&#20026;&#38750;&#27491;&#24335;&#30340;&#26631;&#20934;&#12290;&#26412;&#25991;&#23558;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#12289;&#26631;&#20934;&#21644;&#35748;&#35777;&#30340;&#23454;&#20363;&#65292;&#21457;&#23637;&#36825;&#20123;&#20027;&#39064;&#65292;&#24182;&#35748;&#20026;&#36825;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#36807;&#31243;&#19981;&#20165;&#26377;&#29992;&#65292;&#32780;&#19988;&#23545;&#20110;&#26356;&#24191;&#27867;&#30340;&#36127;&#36131;&#20219;&#21019;&#26032;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking, standards and certification are closely related processes. Standards can provide normative requirements that robotics and AI systems may or may not conform to. Certification generally relies upon conformance with one or more standards as the key determinant of granting a certificate to operate. And benchmarks are sets of standardised tests against which robots and AI systems can be measured. Benchmarks therefore can be thought of as informal standards. In this paper we will develop these themes with examples from benchmarking, standards and certification, and argue that these three linked processes are not only useful but vital to the broader practice of Responsible Innovation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#25351;&#20986;&#20102;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#38382;&#39064;&#20197;&#21450;&#29616;&#26377;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#26102;&#30340;&#22256;&#38590;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.12137</link><description>&lt;p&gt;
OSN-MDAD&#65306;&#29992;&#20110;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#25351;&#20986;&#20102;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#38382;&#39064;&#20197;&#21450;&#29616;&#26377;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#26102;&#30340;&#22256;&#38590;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33521;&#35821;&#36164;&#28304;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#29702;&#35299;&#20869;&#23481;&#30456;&#23545;&#20805;&#36275;&#65292;&#20294;&#38463;&#25289;&#20271;&#35821;&#30340;&#36164;&#28304;&#20173;&#19981;&#36275;&#22815;&#25104;&#29087;&#12290;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#38463;&#25289;&#20271;&#35821;&#38500;&#20102;&#26631;&#20934;&#29256;&#26412;&#65288;MSA&#65289;&#22806;&#65292;&#36824;&#26377;&#35768;&#22810;&#26041;&#35328;&#12290;&#38463;&#25289;&#20271;&#20154;&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#19981;&#20351;&#29992;MSA&#65292;&#32780;&#26159;&#20351;&#29992;&#26041;&#35328;&#29256;&#26412;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20063;&#23558;&#36825;&#31181;&#29616;&#35937;&#24341;&#20837;&#21040;&#20182;&#20204;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20351;&#29992;&#20013;&#65292;&#36825;&#36827;&#32780;&#24341;&#21457;&#20102;&#23545;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#35821;&#35328;&#30456;&#20851;&#24212;&#29992;&#30340;&#21512;&#36866;AI&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#29616;&#26377;&#20026;MSA&#35774;&#35745;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#31995;&#32479;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#37492;&#20110;&#27492;&#65292;&#26377;&#24517;&#35201;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;MT&#31995;&#32479;&#26469;&#36866;&#24212;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#38750;&#27491;&#24335;&#20132;&#27969;&#26041;&#24335;&#12290;&#19982;&#22312;MT&#31995;&#32479;&#20013;&#23545;MSA&#26174;&#31034;&#20986;&#20808;&#36827;&#36827;&#23637;&#19981;&#21516;&#65292;&#21033;&#29992;&#38463;&#25289;&#20271;&#26041;&#35328;&#36827;&#34892;MT&#31995;&#32479;&#30340;&#21033;&#29992;&#25152;&#20184;&#20986;&#30340;&#21162;&#21147;&#19981;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
While resources for English language are fairly sufficient to understand content on social media, similar resources in Arabic are still immature. The main reason that the resources in Arabic are insufficient is that Arabic has many dialects in addition to the standard version (MSA). Arabs do not use MSA in their daily communications; rather, they use dialectal versions. Unfortunately, social users transfer this phenomenon into their use of social media platforms, which in turn has raised an urgent need for building suitable AI models for language-dependent applications. Existing machine translation (MT) systems designed for MSA fail to work well with Arabic dialects. In light of this, it is necessary to adapt to the informal nature of communication on social networks by developing MT systems that can effectively handle the various dialects of Arabic. Unlike for MSA that shows advanced progress in MT systems, little effort has been exerted to utilize Arabic dialects for MT systems. Whil
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#22871;&#32467;&#26500;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#23427;&#25552;&#20986;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#21644;LLM-assisted&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#65292;&#24110;&#21161;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#65292;&#24182;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12132</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A knowledge representation approach for construction contract knowledge modeling. (arXiv:2309.12132v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24314;&#31569;&#21512;&#21516;&#30693;&#35782;&#24314;&#27169;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23884;&#22871;&#32467;&#26500;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#23427;&#25552;&#20986;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#21644;LLM-assisted&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#65292;&#24110;&#21161;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#65292;&#24182;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20026;&#33258;&#21160;&#21270;&#24314;&#31569;&#21512;&#21516;&#31649;&#29702;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#20943;&#23569;&#20102;&#20154;&#20026;&#38169;&#35823;&#65292;&#24182;&#33410;&#30465;&#20102;&#22823;&#37327;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;LLMs&#21487;&#33021;&#20250;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#20294;&#19981;&#20934;&#30830;&#21644;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19987;&#23478;&#39537;&#21160;&#30340;&#21512;&#21516;&#30693;&#35782;&#21487;&#20197;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#34920;&#31034;&#65292;&#20197;&#32422;&#26463;&#33258;&#21160;&#21270;&#21512;&#21516;&#31649;&#29702;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23884;&#22871;&#21512;&#21516;&#30693;&#35782;&#22270;&#65288;NCKG&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#23884;&#22871;&#32467;&#26500;&#26469;&#25429;&#25417;&#21512;&#21516;&#30693;&#35782;&#22797;&#26434;&#24615;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#23884;&#22871;&#30693;&#35782;&#34920;&#31034;&#26694;&#26550;&#12289;&#19968;&#20010;&#24314;&#31435;&#22312;&#35813;&#26694;&#26550;&#19978;&#30340;NCKG&#26412;&#20307;&#20197;&#21450;&#19968;&#31181;&#23454;&#29616;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;NCKG&#20013;&#22686;&#24378;&#22806;&#37096;&#30693;&#35782;&#30340;LLM&#36741;&#21161;&#21512;&#21516;&#23457;&#26597;&#27969;&#31243;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#21512;&#21516;&#39118;&#38505;&#23457;&#26597;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;LLM&#21644;KG&#30340;&#32467;&#21512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) presents an unprecedented opportunity to automate construction contract management, reducing human errors and saving significant time and costs. However, LLMs may produce convincing yet inaccurate and misleading content due to a lack of domain expertise. To address this issue, expert-driven contract knowledge can be represented in a structured manner to constrain the automatic contract management process. This paper introduces the Nested Contract Knowledge Graph (NCKG), a knowledge representation approach that captures the complexity of contract knowledge using a nested structure. It includes a nested knowledge representation framework, a NCKG ontology built on the framework, and an implementation method. Furthermore, we present the LLM-assisted contract review pipeline enhanced with external knowledge in NCKG. Our pipeline achieves a promising performance in contract risk reviewing, shedding light on the combination of LLM and KG towards m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#19968;&#20010;&#31934;&#32454;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2309.12113</link><description>&lt;p&gt;
&#21033;&#29992;&#26377;&#38480;&#39044;&#31639;&#28608;&#21169;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#30340;&#20247;&#24863;&#30693;&#65306;&#20174;&#31163;&#32447;&#21644;&#22312;&#32447;&#35270;&#35282;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Massive Unknown Workers for Budget-Limited Crowdsensing: From Off-Line and On-Line Perspectives. (arXiv:2309.12113v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#19968;&#20010;&#31934;&#32454;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#25552;&#26696;&#36890;&#36807;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#26469;&#35299;&#20915;&#24037;&#20316;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26631;&#20934;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;CMAB&#65289;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#24037;&#20316;&#32773;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#24403;&#24037;&#20316;&#32773;&#25968;&#37327;&#24040;&#22823;&#32780;&#39044;&#31639;&#26377;&#38480;&#26102;&#65292;&#21487;&#33021;&#26080;&#27861;&#23545;&#20010;&#20307;&#24037;&#20316;&#32773;&#36827;&#34892;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26631;&#20934;&#30340;CMAB&#36890;&#24120;&#20551;&#35774;&#24037;&#20316;&#32773;&#22987;&#32456;&#30041;&#22312;&#31995;&#32479;&#20013;&#65292;&#32780;&#24037;&#20316;&#32773;&#21487;&#33021;&#22312;&#26102;&#38388;&#19978;&#21152;&#20837;&#25110;&#31163;&#24320;&#31995;&#32479;&#65292;&#22240;&#27492;&#22312;&#24037;&#20316;&#32773;&#31163;&#24320;&#21518;&#65292;&#25105;&#20204;&#23398;&#21040;&#30340;&#23545;&#20110;&#21333;&#20010;&#24037;&#20316;&#32773;&#30340;&#30693;&#35782;&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;CMAB&#28608;&#21169;&#65288;CACI&#65289;&#26426;&#21046;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#21033;&#29992;&#20102;&#19968;&#20010;&#31934;&#24515;&#21010;&#20998;&#30340;&#19978;&#19979;&#25991;&#31354;&#38388;&#20013;&#30340;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#20010;&#20307;&#24037;&#20316;&#32773;&#65292;&#20197;&#26377;&#25928;&#22320;&#28608;&#21169;&#20855;&#26377;&#38750;&#24120;&#26377;&#38480;&#39044;&#31639;&#30340;&#22823;&#35268;&#27169;&#26410;&#30693;&#24037;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the uncertainties of the workers can be addressed by the standard Combinatorial Multi-Armed Bandit (CMAB) framework in existing proposals through a trade-off between exploration and exploitation, we may not have sufficient budget to enable the trade-off among the individual workers, especially when the number of the workers is huge while the budget is limited. Moreover, the standard CMAB usually assumes the workers always stay in the system, whereas the workers may join in or depart from the system over time, such that what we have learnt for an individual worker cannot be applied after the worker leaves. To address the above challenging issues, in this paper, we first propose an off-line Context-Aware CMAB-based Incentive (CACI) mechanism. We innovate in leveraging the exploration-exploitation trade-off in a elaborately partitioned context space instead of the individual workers, to effectively incentivize the massive unknown workers with very limited budget. We also extend t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#22914;&#34255;&#35821;&#20013;&#36827;&#34892;&#20102;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.12109</link><description>&lt;p&gt;
PEFTT: &#22810;&#21442;&#25968;&#25928;&#29575;&#30340;&#20302;&#36164;&#28304;&#34255;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#22914;&#34255;&#35821;&#20013;&#36827;&#34892;&#20102;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20256;&#32479;&#27169;&#22411;&#35757;&#32451;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#21644;&#26426;&#26500;&#32780;&#35328;&#36234;&#26469;&#36234;&#38590;&#20197;&#24819;&#35937;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#65292;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#39640;&#25928;&#24494;&#35843;&#30340;&#25506;&#32034;&#26159;&#19968;&#20010;&#19981;&#21487;&#21542;&#35748;&#30340;&#36235;&#21183;&#65292;&#32780;&#36825;&#31181;&#36235;&#21183;&#27491;&#36880;&#28176;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#34255;&#35821;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#34255;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#26412;&#23601;&#31232;&#32570;&#32780;&#21463;&#38480;&#12290;&#23613;&#31649;&#30446;&#21069;&#30001;&#20110;&#20854;&#20302;&#36164;&#28304;&#24615;&#36136;&#65292;&#36824;&#27809;&#26377;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#34255;&#35821;&#65292;&#20294;&#36825;&#19968;&#22825;&#27627;&#26080;&#30097;&#38382;&#20250;&#21040;&#26469;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20687;&#34255;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#30740;&#31350;&#38750;&#24120;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#20316;&#20026;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: "prompt-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12071</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam. (arXiv:2309.12071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#19978;&#20195;&#34920;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#20801;&#35768;&#26500;&#24314;&#22797;&#26434;&#38382;&#39064;&#24182;&#33021;&#22815;&#23545;&#19968;&#31995;&#21015;&#38472;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#25191;&#34892;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;70&#20159;&#21644;130&#20159;LLaMA&#27169;&#22411;&#30340;LLMs&#22312;&#37327;&#21270;&#22788;&#29702;&#21644;&#36816;&#34892;&#22312;&#23478;&#24237;&#30828;&#20214;&#19978;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#30340;&#27169;&#22411;&#26377;Alpaca&#12289;Koala&#21644;Vicuna&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;1006&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#38382;&#39064;&#26469;&#33258;&#24052;&#35199;&#22269;&#23478;&#20013;&#23398;&#32771;&#35797;(ENEM)&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#65292;&#21478;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#25191;&#34892;&#25152;&#38656;&#30340;&#26102;&#38388;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#65292;70&#20159;&#21644;130&#20159;LLMs&#38656;&#35201;&#32422;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) represent a revolution in the way we interact with computers, allowing the construction of complex questions and the ability to reason over a sequence of statements, their use is restricted due to the need for dedicated hardware for execution. In this study, we evaluate the performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a quantization process and run on home hardware. The models considered were Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we developed a database containing 1,006 questions from the ENEM (Brazilian National Secondary School Exam). Our analysis revealed that the best performing models achieved an accuracy of approximately 46% for the original texts of the Portuguese questions and 49% on their English translations. In addition, we evaluated the computational efficiency of the models by measuring the time required for execution. On average, the 7 and 13 billion LLMs took approxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#24182;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12067</link><description>&lt;p&gt;
&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#35843;&#26597;--&#24403;&#21069;&#36235;&#21183;&#21644;&#30740;&#31350;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Survey of Action Recognition, Spotting and Spatio-Temporal Localization in Soccer -- Current Trends and Research Perspectives. (arXiv:2309.12067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36275;&#29699;&#20013;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#23450;&#20301;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#24182;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#65292;&#22312;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36275;&#29699;&#27604;&#36187;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#20197;&#21450;&#29699;&#21592;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#23545;&#36275;&#29699;&#20013;&#30340;&#21160;&#20316;&#22330;&#26223;&#36827;&#34892;&#29702;&#35299;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#27492;&#20219;&#21153;&#20998;&#20026;&#21160;&#20316;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#26102;&#31354;&#21160;&#20316;&#23450;&#20301;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#25152;&#20351;&#29992;&#30340;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#28304;&#21644;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#65288;&#22914;&#35270;&#39057;&#21644;&#38899;&#39057;&#25968;&#25454;&#65289;&#30340;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#20197;&#22810;&#31181;&#26041;&#24335;&#34920;&#31034;&#19968;&#31181;&#26469;&#28304;&#30340;&#26041;&#27861;&#12290;&#35752;&#35770;&#20102;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#25913;&#36827;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action scene understanding in soccer is a challenging task due to the complex and dynamic nature of the game, as well as the interactions between players. This article provides a comprehensive overview of this task divided into action recognition, spotting, and spatio-temporal action localization, with a particular emphasis on the modalities used and multimodal methods. We explore the publicly available data sources and metrics used to evaluate models' performance. The article reviews recent state-of-the-art methods that leverage deep learning techniques and traditional methods. We focus on multimodal methods, which integrate information from multiple sources, such as video and audio data, and also those that represent one source in various ways. The advantages and limitations of methods are discussed, along with their potential for improving the accuracy and robustness of models. Finally, the article highlights some of the open research questions and future directions in the field of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#20197;&#21450;CNN&#12289;LSTM&#12289;BiLSTM&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12058</link><description>&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#25972;&#21512;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25239;&#30284;&#32957;&#20998;&#31867;&#26041;&#27861;&#65306;FastText+BiLSTM
&lt;/p&gt;
&lt;p&gt;
An Efficient Consolidation of Word Embedding and Deep Learning Techniques for Classifying Anticancer Peptides: FastText+BiLSTM. (arXiv:2309.12058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#20102;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#39640;&#25928;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#35789;&#23884;&#20837;&#25216;&#26415;&#65292;&#20197;&#21450;CNN&#12289;LSTM&#12289;BiLSTM&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#30284;&#32957;&#65288;ACP&#65289;&#26159;&#19968;&#31867;&#20855;&#22791;&#25239;&#32959;&#30244;&#29305;&#24615;&#30340;&#32957;&#12290;&#20351;&#29992;ACP&#22312;&#30284;&#30151;&#39044;&#38450;&#20013;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#30284;&#30151;&#27835;&#30103;&#30340;&#26367;&#20195;&#21697;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#36873;&#25321;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#26368;&#36817;&#31185;&#23398;&#30340;&#36827;&#23637;&#24341;&#36215;&#20102;&#23545;&#22522;&#20110;&#32957;&#30340;&#27835;&#30103;&#30340;&#20852;&#36259;&#65292;&#23427;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#27835;&#30103;&#30446;&#26631;&#32454;&#32990;&#32780;&#19981;&#23545;&#27491;&#24120;&#32454;&#32990;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32957;&#24207;&#21015;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#24320;&#21457;&#21487;&#38752;&#21644;&#31934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#20986;&#19968;&#20010;&#39640;&#25928;&#30340;&#25239;&#30284;&#32957;&#20998;&#31867;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#35780;&#20272;&#20102;Word2Vec&#21644;FastText&#20316;&#20026;&#25552;&#21462;&#32957;&#24207;&#21015;&#30340;&#35789;&#23884;&#20837;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#23558;&#35789;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#36755;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CNN&#12289;LSTM&#12289;BiLSTM&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticancer peptides (ACPs) are a group of peptides that exhibite antineoplastic properties. The utilization of ACPs in cancer prevention can present a viable substitute for conventional cancer therapeutics, as they possess a higher degree of selectivity and safety. Recent scientific advancements generate an interest in peptide-based therapies which offer the advantage of efficiently treating intended cells without negatively impacting normal cells. However, as the number of peptide sequences continues to increase rapidly, developing a reliable and precise prediction model becomes a challenging task. In this work, our motivation is to advance an efficient model for categorizing anticancer peptides employing the consolidation of word embedding and deep learning models. First, Word2Vec and FastText are evaluated as word embedding techniques for the purpose of extracting peptide sequences. Then, the output of word embedding models are fed into deep learning approaches CNN, LSTM, BiLSTM. To
&lt;/p&gt;</description></item><item><title>BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12056</link><description>&lt;p&gt;
BELT: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12056
&lt;/p&gt;
&lt;p&gt;
BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BELT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#33041;&#21040;&#35821;&#35328;&#32763;&#35793;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#30340;&#26032;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23558;&#38750;&#20405;&#20837;&#24615;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#21487;&#35835;&#30340;&#33258;&#28982;&#35821;&#35328;&#26377;&#28508;&#21147;&#25512;&#21160;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21457;&#23637;&#12290;&#33041;&#20449;&#21495;&#35299;&#30721;&#25110;&#33041;-&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#20174;&#26377;&#38480;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#35821;&#20041;&#36866;&#24403;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#33041;&#30005;&#22270;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;BELT&#26041;&#27861;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#23548;&#33041;&#30005;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#35268;&#27169;LM&#29702;&#35299;&#35821;&#20041;&#20449;&#24687;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;BELT&#26497;&#22823;&#25913;&#36827;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BELT&#27169;&#22411;&#30001;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12038</link><description>&lt;p&gt;
&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#20013;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#25506;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven Exploration Strategies for Online Grasp Learning. (arXiv:2309.12038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25235;&#21462;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#32780;&#24573;&#30053;&#20102;&#22312;&#32447;&#36866;&#24212;&#26032;&#30340;&#25342;&#21462;&#22330;&#26223;&#26102;&#30340;&#25506;&#32034;&#24615;&#25235;&#21462;&#23398;&#20064;&#65292;&#20363;&#22914;&#26410;&#35265;&#30446;&#26631;&#32452;&#21512;&#12289;&#25668;&#20687;&#26426;&#21644;&#31665;&#23376;&#35774;&#32622;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26426;&#22120;&#20154;&#31665;&#23376;&#25342;&#21462;&#30340;&#22312;&#32447;&#25235;&#21462;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23545;&#26410;&#35265;&#29615;&#22659;&#35774;&#32622;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#24314;&#27169;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#25235;&#21462;&#31574;&#30053;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as a RL problem that will allow to adapt both gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.12028</link><description>&lt;p&gt;
&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Hypergraph Structure Learning for Traffic Flow Forecasting. (arXiv:2309.12028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#65292;&#26088;&#22312;&#22522;&#20110;&#36807;&#21435;&#30340;&#36947;&#36335;&#32593;&#32476;&#21644;&#20132;&#36890;&#29366;&#20917;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#26465;&#20214;&#12290;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#24314;&#27169;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#22240;&#20026;&#24403;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#32593;&#32476;&#26102;&#65292;GNNs&#30340;&#34920;&#31034;&#33021;&#21147;&#36890;&#24120;&#26377;&#38480;&#12290;&#22270;&#24418;&#26412;&#36136;&#19978;&#26080;&#27861;&#25429;&#25417;&#38750;&#37197;&#23545;&#20851;&#31995;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#36981;&#24490;&#20449;&#24687;&#20256;&#36882;&#30340;&#33539;&#24335;&#65292;&#32447;&#24615;&#22320;&#32858;&#21512;&#37051;&#22495;&#20449;&#24687;&#65292;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#39640;&#38454;&#20132;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;(DyHSL)&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#12290;&#20026;&#20102;&#23398;&#20064;&#38750;&#37197;&#23545;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;DyHSL&#25552;&#21462;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of traffic flow forecasting, which aims to predict future traffic conditions on the basis of road networks and traffic conditions in the past. The problem is typically solved by modeling complex spatio-temporal correlations in traffic data using spatio-temporal graph neural networks (GNNs). However, the performance of these methods is still far from satisfactory since GNNs usually have limited representation capacity when it comes to complex traffic networks. Graphs, by nature, fall short in capturing non-pairwise relations. Even worse, existing methods follow the paradigm of message passing that aggregates neighborhood information linearly, which fails to capture complicated spatio-temporal high-order interactions. To tackle these issues, in this paper, we propose a novel model named Dynamic Hypergraph Structure Learning (DyHSL) for traffic flow prediction. To learn non-pairwise relationships, our DyHSL extracts hypergraph structural information to model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12022</link><description>&lt;p&gt;
&#35299;&#23494;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#24433;&#34892;&#19994;&#20013;&#65292;&#30005;&#24433;&#28023;&#25253;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#24191;&#21578;&#21644;&#33829;&#38144;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20351;&#22312;&#29616;&#20170;&#30340;&#25968;&#23383;&#28023;&#25253;&#36890;&#36807;&#22312;&#32447;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;OTT&#24179;&#21488;&#19978;&#20173;&#28982;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#24120;&#65292;&#30005;&#24433;&#28023;&#25253;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#24191;&#21644;&#20256;&#36798;&#30005;&#24433;&#30340;&#26412;&#36136;&#65292;&#20363;&#22914;&#20854;&#31867;&#22411;&#12289;&#35270;&#35273;&#39118;&#26684;/&#35843;&#35843;&#12289;&#27675;&#22260;&#21644;&#25925;&#20107;&#32447;&#32034;/&#20027;&#39064;&#65292;&#36825;&#20123;&#23545;&#21560;&#24341;&#28508;&#22312;&#35266;&#20247;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#30005;&#24433;&#31867;&#22411;&#36827;&#34892;&#35782;&#21035;&#24120;&#24120;&#22312;&#21521;&#30446;&#26631;&#35266;&#20247;&#25512;&#33616;&#30005;&#24433;&#26102;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#30740;&#31350;&#20165;&#38480;&#20110;&#23383;&#24149;&#12289;&#21095;&#24773;&#31616;&#20171;&#21644;&#30005;&#24433;&#22330;&#26223;&#65292;&#36825;&#20123;&#22823;&#22810;&#25968;&#22312;&#30005;&#24433;&#21457;&#24067;&#21518;&#25165;&#33021;&#33719;&#21462;&#12290;&#28023;&#25253;&#36890;&#24120;&#21253;&#21547;&#22312;&#21457;&#34892;&#21069;&#38544;&#21547;&#30340;&#20449;&#24687;&#26469;&#24341;&#36215;&#22823;&#37327;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#20013;&#33258;&#21160;&#36827;&#34892;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#30005;&#24433;&#30340;&#38468;&#21152;&#25991;&#26412;/&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24110;&#21161;&#65292;&#36825;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the film industry, movie posters have been an essential part of advertising and marketing for many decades, and continue to play a vital role even today in the form of digital posters through online, social media and OTT platforms. Typically, movie posters can effectively promote and communicate the essence of a film, such as its genre, visual style/ tone, vibe and storyline cue/ theme, which are essential to attract potential viewers. Identifying the genres of a movie often has significant practical applications in recommending the film to target audiences. Previous studies on movie genre identification are limited to subtitles, plot synopses, and movie scenes that are mostly accessible after the movie release. Posters usually contain pre-release implicit information to generate mass interest. In this paper, we work for automated multi-label genre identification only from movie poster images, without any aid of additional textual/meta-data information about movies, which is one of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#33021;&#32791;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.12004</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#32791;&#30340;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23433;&#20840;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Hierarchical Reinforcement Learning for CubeSat Task Scheduling Based on Energy Consumption. (arXiv:2309.12004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#33021;&#32791;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#65292;&#24182;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#20013;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#36827;&#34892;&#20248;&#21270;&#30340;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#29992;&#20110;&#20840;&#23616;&#20219;&#21153;&#20998;&#37197;&#30340;&#39640;&#32423;&#31574;&#30053;&#21644;&#29992;&#20110;&#23454;&#26102;&#35843;&#25972;&#30340;&#20302;&#32423;&#31574;&#30053;&#20316;&#20026;&#23433;&#20840;&#26426;&#21046;&#65292;&#25972;&#21512;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#65288;SABE&#65289;&#36827;&#34892;&#20219;&#21153;&#20248;&#20808;&#32423;&#25490;&#24207;&#20197;&#21450;&#29992;&#20110;&#33021;&#32791;&#39044;&#27979;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20272;&#35745;&#22120;&#12290;&#35813;&#26426;&#21046;&#30340;&#25972;&#21512;&#20026;&#31435;&#26041;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#21019;&#24314;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#23481;&#38169;&#30340;&#31995;&#32479;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#20248;&#20110;MADDPG&#27169;&#22411;&#21644;&#20256;&#32479;&#38543;&#26426;&#35843;&#24230;&#22312;&#22810;&#20010;&#31435;&#26041;&#21355;&#26143;&#37197;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Hierarchical Reinforcement Learning methodology tailored for optimizing CubeSat task scheduling in Low Earth Orbits (LEO). Incorporating a high-level policy for global task distribution and a low-level policy for real-time adaptations as a safety mechanism, our approach integrates the Similarity Attention-based Encoder (SABE) for task prioritization and an MLP estimator for energy consumption forecasting. Integrating this mechanism creates a safe and fault-tolerant system for CubeSat task scheduling. Simulation results validate the Hierarchical Reinforcement Learning superior convergence and task success rate, outperforming both the MADDPG model and traditional random scheduling across multiple CubeSat configurations.
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#21457;&#29616;&#24403;&#35299;&#37322;&#38598;&#20013;&#22312;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.11987</link><description>&lt;p&gt;
&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65306;&#19968;&#39033;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predictability and Comprehensibility in Post-Hoc XAI Methods: A User-Centered Analysis. (arXiv:2309.11987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11987
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#21457;&#29616;&#24403;&#35299;&#37322;&#38598;&#20013;&#22312;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#21457;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#22686;&#24378;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#30340;&#35774;&#35745;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#26088;&#22312;&#28548;&#28165;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#25552;&#20379;&#30340;&#35299;&#37322;&#26377;&#22810;&#22909;&#29702;&#35299;&#20197;&#21450;&#36825;&#20123;&#35299;&#37322;&#26159;&#21542;&#22686;&#24378;&#20102;&#29992;&#25143;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#39044;&#27979;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#65288;LIME&#21644;SHAP&#65289;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#23545;&#29992;&#25143;&#29702;&#35299;&#21644;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20026;&#25509;&#36817;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#25552;&#20379;&#35299;&#37322;&#26102;&#65292;SHAP&#30340;&#21487;&#29702;&#35299;&#24615;&#26174;&#33879;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#38169;&#35823;&#20998;&#31867;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#29992;&#25143;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#21518;&#32622;&#35299;&#37322;&#26041;&#27861;&#30340;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#22686;&#24378;&#20854;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-hoc explainability methods aim to clarify predictions of black-box machine learning models. However, it is still largely unclear how well users comprehend the provided explanations and whether these increase the users ability to predict the model behavior. We approach this question by conducting a user study to evaluate comprehensibility and predictability in two widely used tools: LIME and SHAP. Moreover, we investigate the effect of counterfactual explanations and misclassifications on users ability to understand and predict the model behavior. We find that the comprehensibility of SHAP is significantly reduced when explanations are provided for samples near a model's decision boundary. Furthermore, we find that counterfactual explanations and misclassifications can significantly increase the users understanding of how a machine learning model is making decisions. Based on our findings, we also derive design recommendations for future post-hoc explainability methods with increas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11984</link><description>&lt;p&gt;
&#34920;&#31034;&#25277;&#35937;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28608;&#21169;&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#36866;&#24403;&#30340;&#29615;&#22659;&#34920;&#31034;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#19981;&#24635;&#26159;&#31616;&#21333;&#30340;&#12290;&#29366;&#24577;&#34920;&#31034;&#24212;&#35813;&#36275;&#22815;&#21253;&#23481;&#65292;&#20197;&#20415;&#35753;&#20195;&#29702;&#33021;&#22815;&#20449;&#24687;&#22320;&#20915;&#23450;&#20854;&#34892;&#21160;&#65292;&#24182;&#19988;&#36275;&#22815;&#32039;&#20945;&#65292;&#20197;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#29305;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#23545;&#31216;&#21644;&#24179;&#38754;&#29289;&#20307;&#25235;&#21462;&#65289;&#19978;&#35299;&#20915;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20174;&#20855;&#26377;&#23436;&#25972;&#31995;&#32479;&#30693;&#35782;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24320;&#22987;&#65292;&#36890;&#36807;&#25163;&#24037;&#25968;&#23383;&#34920;&#31034;&#21040;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36880;&#28176;&#20943;&#23569;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#24341;&#20837;&#37327;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#29366;&#24577;&#34920;&#31034;&#25277;&#35937;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35299;&#20915;&#20219;&#21153;&#20197;&#21450;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate representation of the environment for the underlying decision-making process of the \gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#19977;&#35282;&#27979;&#37327;&#26041;&#27861;&#20174;&#20219;&#21153;&#34920;&#29616;&#20013;&#25512;&#26029;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#25512;&#26029;&#20102;&#19981;&#21516;&#35748;&#30693;&#29305;&#24449;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#33021;&#21147;&#23548;&#21521;&#30340;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11975</link><description>&lt;p&gt;
&#20174;&#20219;&#21153;&#34920;&#29616;&#20013;&#25512;&#26029;&#33021;&#21147;&#30340;&#36125;&#21494;&#26031;&#19977;&#35282;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inferring Capabilities from Task Performance with Bayesian Triangulation. (arXiv:2309.11975v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#19977;&#35282;&#27979;&#37327;&#26041;&#27861;&#20174;&#20219;&#21153;&#34920;&#29616;&#20013;&#25512;&#26029;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#25512;&#26029;&#20102;&#19981;&#21516;&#35748;&#30693;&#29305;&#24449;&#30340;&#20195;&#29702;&#12290;&#36825;&#31181;&#33021;&#21147;&#23548;&#21521;&#30340;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#36890;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#20197;&#26356;&#20016;&#23500;&#12289;&#26356;&#26377;&#24847;&#20041;&#30340;&#26041;&#24335;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20174;&#22810;&#26679;&#21270;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#31995;&#32479;&#30340;&#35748;&#30693;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#37327;&#24067;&#23616;&#65292;&#27169;&#25311;&#20102;&#20219;&#21153;&#23454;&#20363;&#29305;&#24449;&#22914;&#20309;&#19982;&#31995;&#32479;&#33021;&#21147;&#30456;&#20114;&#20316;&#29992;&#20197;&#24433;&#21709;&#24615;&#33021;&#12290;&#36825;&#20123;&#29305;&#24449;&#24517;&#39035;&#20197;&#22797;&#26434;&#30340;&#26041;&#24335;&#36827;&#34892;&#19977;&#35282;&#27979;&#37327;&#65292;&#20197;&#20415;&#20174;&#38750;&#32676;&#20307;&#25968;&#25454;&#20013;&#25512;&#26029;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#20256;&#32479;&#30340;&#24515;&#29702;&#27979;&#37327;&#21644;&#25512;&#29702;&#24037;&#20855;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#21033;&#29992;&#36125;&#21494;&#26031;&#27010;&#29575;&#32534;&#31243;&#24211;PyMC&#65292;&#25105;&#20204;&#25512;&#26029;&#20102;&#20004;&#31181;&#24773;&#26223;&#20013;&#20195;&#29702;&#30340;&#19981;&#21516;&#35748;&#30693;&#29305;&#24449;&#65306;&#21160;&#29289;&#26234;&#33021;&#22885;&#26519;&#21305;&#20811;&#31454;&#36187;&#20013;&#30340;68&#21517;&#23454;&#38469;&#21442;&#36187;&#36873;&#25163;&#21644;O-PIAAGETS&#30340;30&#20010;&#21512;&#25104;&#20195;&#29702;&#65292;&#20854;&#20013;O-PIAAGETS&#26159;&#19968;&#20010;&#29289;&#20307;&#24658;&#24120;&#24615;&#27979;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#33021;&#21147;&#20026;&#23548;&#21521;&#30340;&#35780;&#20272;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models become more general, we need to characterise them in richer, more meaningful ways. We describe a method to infer the cognitive profile of a system from diverse experimental data. To do so, we introduce measurement layouts that model how task-instance features interact with system capabilities to affect performance. These features must be triangulated in complex ways to be able to infer capabilities from non-populational data -- a challenge for traditional psychometric and inferential tools. Using the Bayesian probabilistic programming library PyMC, we infer different cognitive profiles for agents in two scenarios: 68 actual contestants in the AnimalAI Olympics and 30 synthetic agents for O-PIAAGETS, an object permanence battery. We showcase the potential for capability-oriented evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.11960</link><description>&lt;p&gt;
&#23545;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#32508;&#21512;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review on Financial Explainable AI. (arXiv:2309.11960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#20351;&#20854;&#22312;&#21508;&#20010;&#34892;&#19994;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#21644;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20154;&#20204;&#23545;&#20110;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#37325;&#35201;&#34892;&#19994;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#37325;&#22823;&#25285;&#24551;&#65292;&#22240;&#20026;&#20915;&#31574;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#37329;&#34701;&#32972;&#26223;&#19979;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#35843;&#26597;&#12290;&#25105;&#20204;&#26681;&#25454;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#29305;&#28857;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22238;&#39038;&#20102;&#37319;&#29992;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20851;&#20999;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#25105;&#20204;&#35748;&#20026;&#36866;&#24403;&#21644;&#37325;&#35201;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of artificial intelligence (AI), and deep learning models in particular, has led to their widespread adoption across various industries due to their ability to process huge amounts of data and learn complex patterns. However, due to their lack of explainability, there are significant concerns regarding their use in critical sectors, such as finance and healthcare, where decision-making transparency is of paramount importance. In this paper, we provide a comparative survey of methods that aim to improve the explainability of deep learning models within the context of finance. We categorize the collection of explainable AI methods according to their corresponding characteristics, and we review the concerns and challenges of adopting explainable AI methods, together with future directions we deemed appropriate and important.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#36866;&#24403;&#20449;&#20219;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#20041;&#30456;&#20284;&#24615;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.11937</link><description>&lt;p&gt;
&#23545;&#36866;&#24403;&#20449;&#20219;&#30340;&#23450;&#20041;&#21450;&#20854;&#30456;&#20851;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
On the Definition of Appropriate Trust and the Tools that Come with it. (arXiv:2309.11937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#36866;&#24403;&#20449;&#20219;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#20041;&#30456;&#20284;&#24615;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#20132;&#20114;&#25928;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21253;&#25324;&#20027;&#35266;&#21644;&#23458;&#35266;&#30340;&#36136;&#37327;&#26041;&#38754;&#12290;&#19987;&#27880;&#20110;&#35299;&#37322;&#30340;&#20154;&#31867;&#20307;&#39564;&#65292;&#35299;&#37322;&#26041;&#27861;&#30340;&#35780;&#20272;&#20027;&#35201;&#26159;&#20027;&#35266;&#30340;&#65292;&#20351;&#24471;&#27604;&#36739;&#35780;&#20272;&#20960;&#20046;&#19981;&#21487;&#33021;&#65292;&#24182;&#19988;&#19982;&#20010;&#20307;&#29992;&#25143;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#26222;&#36941;&#35748;&#20026;&#35299;&#37322;&#36136;&#37327;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#29992;&#25143;&#33021;&#21542;&#26377;&#25928;&#22320;&#26816;&#27979;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#21644;&#27491;&#30830;&#24615;&#65292;&#21363;&#35299;&#37322;&#26159;&#21542;&#33021;&#22686;&#24378;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#36866;&#24403;&#20449;&#20219;&#12290;&#26412;&#25991;&#20174;&#25991;&#29486;&#20013;&#24320;&#22987;&#23545;&#36866;&#24403;&#20449;&#20219;&#30340;&#23450;&#20041;&#36827;&#34892;&#35752;&#35770;&#12290;&#21516;&#26102;&#23558;&#23450;&#20041;&#19982;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#36866;&#24403;&#20449;&#20219;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24378;&#22823;&#30456;&#20284;&#20043;&#22788;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#35780;&#20272;&#36866;&#24403;&#20449;&#20219;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the efficiency of human-AI interactions is challenging, including subjective and objective quality aspects. With the focus on the human experience of the explanations, evaluations of explanation methods have become mostly subjective, making comparative evaluations almost impossible and highly linked to the individual user. However, it is commonly agreed that one aspect of explanation quality is how effectively the user can detect if the predictions are trustworthy and correct, i.e., if the explanations can increase the user's appropriate trust in the model. This paper starts with the definitions of appropriate trust from the literature. It compares the definitions with model performance evaluation, showing the strong similarities between appropriate trust and model performance evaluation. The paper's main contribution is a novel approach to evaluating appropriate trust by taking advantage of the likenesses between definitions. The paper offers several straightforward evaluat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#26500;&#24314;&#23433;&#20840;&#25511;&#21046;&#22120;&#65292;&#20197;&#22312;&#20219;&#21153;&#35757;&#32451;&#20013;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#25239;&#29616;&#35937;&#65292;&#21516;&#26102;&#24110;&#21161;&#20219;&#21153;&#31574;&#30053;&#20174;&#39640;&#39118;&#38505;&#29366;&#24577;&#20013;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#36741;&#21161;&#22870;&#21169;&#12290;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2309.11907</link><description>&lt;p&gt;
&#23398;&#20064;&#24674;&#22797;&#20197;&#23454;&#29616;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover for Safe Reinforcement Learning. (arXiv:2309.11907v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#26500;&#24314;&#23433;&#20840;&#25511;&#21046;&#22120;&#65292;&#20197;&#22312;&#20219;&#21153;&#35757;&#32451;&#20013;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#25239;&#29616;&#35937;&#65292;&#21516;&#26102;&#24110;&#21161;&#20219;&#21153;&#31574;&#30053;&#20174;&#39640;&#39118;&#38505;&#29366;&#24577;&#20013;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#36741;&#21161;&#22870;&#21169;&#12290;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#25511;&#21046;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#23454;&#29616;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;&#22823;&#22810;&#25968;&#24212;&#29992;&#23433;&#20840;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#25163;&#24037;&#21046;&#23450;&#30340;&#23433;&#20840;&#32422;&#26463;&#26469;&#26500;&#24314;&#23433;&#20840;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#29615;&#22659;&#21160;&#24577;&#22797;&#26434;&#26102;&#65292;&#25163;&#24037;&#21046;&#23450;&#30340;&#23433;&#20840;&#32422;&#26463;&#21464;&#24471;&#19981;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#26500;&#24314;&#23433;&#20840;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;TU&#24674;&#22797;&#26550;&#26500;&#12290;&#22312;&#20219;&#21153;&#35757;&#32451;&#20043;&#21069;&#23398;&#20064;&#23433;&#20840;&#35780;&#20272;&#22120;&#21644;&#24674;&#22797;&#31574;&#30053;&#12290;&#23427;&#20204;&#24418;&#25104;&#19968;&#20010;&#23433;&#20840;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#20219;&#21153;&#35757;&#32451;&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#21518;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#30001;&#20219;&#21153;&#31574;&#30053;&#21644;&#24674;&#22797;&#31574;&#30053;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24341;&#36215;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#23545;&#25239;&#29616;&#35937;&#65292;&#35813;&#29616;&#35937;&#38477;&#20302;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#22870;&#21169;&#26469;&#32531;&#35299;&#23545;&#25239;&#29616;&#35937;&#65292;&#21516;&#26102;&#24110;&#21161;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#20174;&#39640;&#39118;&#38505;&#29366;&#24577;&#20013;&#24674;&#22797;&#12290;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety controllers is widely used to achieve safe reinforcement learning. Most methods that apply a safety controller are using handcrafted safety constraints to construct the safety controller. However, when the environment dynamics are sophisticated, handcrafted safety constraints become unavailable. Therefore, it worth to research on constructing safety controllers by learning algorithms. We propose a three-stage architecture for safe reinforcement learning, namely TU-Recovery Architecture. A safety critic and a recovery policy is learned before task training. They form a safety controller to ensure safety in task training. Then a phenomenon induced by disagreement between task policy and recovery policy, called adversarial phenomenon, which reduces learning efficiency and model performance, is described. Auxiliary reward is proposed to mitigate adversarial phenomenon, while help the task policy to learn to recover from high-risk states. A series of experiments are conducted in a ro
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#36866;&#24212;&#38145;&#23450;&#26080;&#30693;&#32593;&#32476;&#65288;ALAN&#65289;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#40065;&#26834;&#30340;&#35821;&#20041;&#33258;&#25105;&#20998;&#21106;&#65292;&#20943;&#23569;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.11899</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#38145;&#23450;&#26080;&#30693;&#32593;&#32476;&#35299;&#38145;&#24515;&#33039;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Heart Using Adaptive Locked Agnostic Networks. (arXiv:2309.11899v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11899
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#38145;&#23450;&#26080;&#30693;&#32593;&#32476;&#65288;ALAN&#65289;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#65292;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#40065;&#26834;&#30340;&#35821;&#20041;&#33258;&#25105;&#20998;&#21106;&#65292;&#20943;&#23569;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30417;&#30563;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#30340;&#22270;&#20687;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38145;&#23450;&#26080;&#30693;&#32593;&#32476;&#65288;ALAN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#39592;&#24178;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#30340;&#27010;&#24565;&#65292;&#20197;&#20135;&#29983;&#35299;&#21078;&#23398;&#19978;&#40065;&#26834;&#30340;&#35821;&#20041;&#33258;&#25105;&#20998;&#21106;&#12290;&#22312;ALAN&#26041;&#27861;&#20013;&#65292;&#36825;&#31181;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20165;&#22312;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19968;&#27425;&#12290;&#30001;&#20110;&#20998;&#21106;&#30340;&#30452;&#35266;&#21487;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#21442;&#25968;&#30340;&#30333;&#30418;&#27169;&#22411;&#36731;&#26494;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#19979;&#28216;&#27169;&#22411;&#12290;&#36825;&#21453;&#36807;&#26469;&#24847;&#21619;&#30528;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#19979;&#28216;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#36739;&#23569;&#12290;&#36825;&#20123;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Supervised training of deep learning models for medical imaging applications requires a significant amount of labeled data. This is posing a challenge as the images are required to be annotated by medical professionals. To address this limitation, we introduce the Adaptive Locked Agnostic Network (ALAN), a concept involving self-supervised visual feature extraction using a large backbone model to produce anatomically robust semantic self-segmentation. In the ALAN methodology, this self-supervised training occurs only once on a large and diverse dataset. Due to the intuitive interpretability of the segmentation, downstream models tailored for specific tasks can be easily designed using white-box models with few parameters. This, in turn, opens up the possibility of communicating the inner workings of a model with domain experts and introducing prior knowledge into it. It also means that the downstream models become less data-hungry compared to fully supervised approaches. These characte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11876</link><description>&lt;p&gt;
&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#32423;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20043;&#38388;&#30340;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#65292;&#22240;&#27492;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65288;&#20854;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#26159;&#20998;&#21106;&#65289;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#29978;&#33267;&#19981;&#22914;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JCL&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#22312;&#19968;&#38454;&#27573;&#20869;&#23545;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290; &#65288;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#32771;&#34385;&#29305;&#24449;&#32423;&#21035;&#12289;&#22270;&#20687;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#25237;&#24433;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#30340;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11875</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#39640;&#26031;&#36807;&#31243;&#30340;Timoshenko&#26753;&#30340;&#38543;&#26426;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Stochastic stiffness identification and response estimation of Timoshenko beams via physics-informed Gaussian processes. (arXiv:2309.11875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#30340;&#21018;&#24230;&#35782;&#21035;&#21644;&#21709;&#24212;&#20272;&#35745;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#27010;&#29575;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#31995;&#32479;&#35782;&#21035;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#29992;&#20110;Timoshenko&#26753;&#20803;&#32032;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65292;&#20854;&#21327;&#26041;&#24046;&#21644;&#20132;&#21449;&#21327;&#26041;&#24046;&#26680;&#26681;&#25454;&#25376;&#24230;&#12289;&#36716;&#21160;&#12289;&#24212;&#21464;&#12289;&#24367;&#30697;&#12289;&#21098;&#21147;&#21644;&#26045;&#21152;&#36733;&#33655;&#30340;&#24494;&#20998;&#26041;&#31243;&#35299;&#26512;&#22320;&#25512;&#23548;&#32780;&#26469;&#12290;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36125;&#21494;&#26031;&#26684;&#24335;&#19979;&#36827;&#34892;&#21018;&#24230;&#35782;&#21035;&#65292;&#26368;&#22823;&#21270;&#21518;&#39564;&#27169;&#22411;&#65292;&#24471;&#21040;&#32467;&#26500;&#21442;&#25968;&#30340;&#38543;&#26426;&#27169;&#22411;&#12290;&#20248;&#21270;&#21518;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36827;&#19968;&#27493;&#29992;&#20110;&#23545;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#29289;&#29702;&#20449;&#24687;&#20256;&#24863;&#22120;&#24067;&#32622;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24322;&#36136;&#20256;&#24863;&#22120;&#20301;&#32622;&#20449;&#24687;&#21644;&#23884;&#20837;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#32467;&#26500;&#36793;&#30028;&#26465;&#20214;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#35782;&#21035;&#21018;&#24230;&#24182;&#39044;&#27979;&#26410;&#35266;&#27979;&#21040;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models trained with structural health monitoring data have become a powerful tool for system identification. This paper presents a physics-informed Gaussian process (GP) model for Timoshenko beam elements. The model is constructed as a multi-output GP with covariance and cross-covariance kernels analytically derived based on the differential equations for deflections, rotations, strains, bending moments, shear forces and applied loads. Stiffness identification is performed in a Bayesian format by maximising a posterior model through a Markov chain Monte Carlo method, yielding a stochastic model for the structural parameters. The optimised GP model is further employed for probabilistic predictions of unobserved responses. Additionally, an entropy-based method for physics-informed sensor placement optimisation is presented, exploiting heterogeneous sensor position information and structural boundary conditions built into the GP model. Results demonstrate that the propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#37325;&#24314;&#26550;&#26500;&#65288;OSNet&#21644;MNetO&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;LCT&#20013;&#23454;&#29616;&#31283;&#23450;&#20869;&#37096;&#37325;&#24314;&#21644;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#26059;&#36716;&#25805;&#20316;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11858</link><description>&lt;p&gt;
OSNet&#21644;MNetO&#65306;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#20004;&#31181;&#36890;&#29992;&#37325;&#24314;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
OSNet &amp; MNetO: Two Types of General Reconstruction Architectures for Linear Computed Tomography in Multi-Scenarios. (arXiv:2309.11858v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#37325;&#24314;&#26550;&#26500;&#65288;OSNet&#21644;MNetO&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;LCT&#20013;&#23454;&#29616;&#31283;&#23450;&#20869;&#37096;&#37325;&#24314;&#21644;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#26059;&#36716;&#25805;&#20316;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32447;&#24615;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;LCT&#65289;&#31995;&#32479;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20943;&#24369;LCT&#20013;&#30340;&#25237;&#24433;&#25130;&#26029;&#24182;&#23545;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#36827;&#34892;&#25104;&#20687;&#65292;&#21453;&#25237;&#24433;&#28388;&#27874;&#65288;BPF&#65289;&#31639;&#27861;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;LCT&#30340;BPF&#20013;&#65292;&#24456;&#38590;&#23454;&#29616;&#31283;&#23450;&#30340;&#20869;&#37096;&#37325;&#24314;&#65292;&#24182;&#19988;&#23545;&#20110;LCT&#30340;&#19981;&#21516;&#21453;&#25237;&#24433;&#65288;DBP&#65289;&#22270;&#20687;&#65292;&#22810;&#20010;&#26059;&#36716;&#26377;&#38480;&#21453;&#28436;&#30340;&#24076;&#23572;&#20271;&#29305;&#21464;&#25442;&#65288;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#65289;-&#21453;&#36716;&#25805;&#20316;&#23558;&#20351;&#22270;&#20687;&#27169;&#31946;&#12290;&#20026;&#20102;&#28385;&#36275;LCT&#30340;&#22810;&#31181;&#37325;&#24314;&#22330;&#26223;&#65292;&#21253;&#25324;&#20869;&#37096;ROI&#12289;&#23436;&#25972;&#23545;&#35937;&#21644;&#36229;&#20986;&#35270;&#37326;&#33539;&#22260;&#30340;&#22806;&#37096;&#21306;&#22495;&#65292;&#24182;&#36991;&#20813;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#30340;&#26059;&#36716;&#25805;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#37325;&#24314;&#26550;&#26500;&#12290;&#31532;&#19968;&#31181;&#26159;&#21472;&#21152;&#22810;&#20010;DBP&#22270;&#20687;&#20197;&#33719;&#24471;&#23436;&#25972;&#30340;DBP&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#21472;&#21152;&#30340;&#24076;&#23572;&#20271;&#29305;&#28388;&#27874;&#20989;&#25968;&#65292;&#31216;&#20026;&#21472;&#21152;&#21333;&#19968;&#32593;&#32476;&#65288;OSNet&#65289;&#12290;&#31532;&#20108;&#31181;&#26159;&#20351;&#29992;&#22810;&#20010;&#32593;&#32476;&#35757;&#32451;&#19981;&#21516;&#30340;&#21453;&#25237;&#24433;&#37325;&#26500;&#22120;&#32593;&#32476;&#65288;MNetO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, linear computed tomography (LCT) systems have actively attracted attention. To weaken projection truncation and image the region of interest (ROI) for LCT, the backprojection filtration (BPF) algorithm is an effective solution. However, in BPF for LCT, it is difficult to achieve stable interior reconstruction, and for differentiated backprojection (DBP) images of LCT, multiple rotation-finite inversion of Hilbert transform (Hilbert filtering)-inverse rotation operations will blur the image. To satisfy multiple reconstruction scenarios for LCT, including interior ROI, complete object, and exterior region beyond field-of-view (FOV), and avoid the rotation operations of Hilbert filtering, we propose two types of reconstruction architectures. The first overlays multiple DBP images to obtain a complete DBP image, then uses a network to learn the overlying Hilbert filtering function, referred to as the Overlay-Single Network (OSNet). The second uses multiple networks to train diffe
&lt;/p&gt;</description></item><item><title>BitCoin&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#25552;&#39640;&#20102;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11853</link><description>&lt;p&gt;
BitCoin: &#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11853
&lt;/p&gt;
&lt;p&gt;
BitCoin&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#25552;&#39640;&#20102;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#27867;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#26410;&#32771;&#34385;RTE&#20219;&#21153;&#30340;&#29305;&#27530;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;RTE&#20219;&#21153;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#39318;&#20808;&#35782;&#21035;&#20027;&#20307;&#65292;&#28982;&#21518;&#35782;&#21035;&#23458;&#20307;&#21644;&#20851;&#31995;&#12290;&#23427;&#20204;&#20165;&#20851;&#27880;&#20174;&#20027;&#20307;&#21040;&#23458;&#20307;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25552;&#21462;&#65292;&#24573;&#35270;&#20102;&#19968;&#26086;&#20027;&#20307;&#25552;&#21462;&#22833;&#36133;&#65292;&#23601;&#26080;&#27861;&#25552;&#21462;&#19982;&#35813;&#20027;&#20307;&#30456;&#20851;&#30340;&#25152;&#26377;&#19977;&#20803;&#32452;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BitCoin&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#32780;&#19981;&#20165;&#20165;&#38480;&#20110;&#19968;&#20010;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation triple extraction (RTE) is an essential task in information extraction and knowledge graph construction. Despite recent advancements, existing methods still exhibit certain limitations. They just employ generalized pre-trained models and do not consider the specificity of RTE tasks. Moreover, existing tagging-based approaches typically decompose the RTE task into two subtasks, initially identifying subjects and subsequently identifying objects and relations. They solely focus on extracting relational triples from subject to object, neglecting that once the extraction of a subject fails, it fails in extracting all triples associated with that subject. To address these issues, we propose BitCoin, an innovative Bidirectional tagging and supervised Contrastive learning based joint relational triple extraction framework. Specifically, we design a supervised contrastive learning method that considers multiple positives per anchor rather than restricting it to just one positive. Furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11838</link><description>&lt;p&gt;
&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#30340;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#31867;&#20284;ChatGPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20808;&#21069;&#22312;DialDoc 2022&#20849;&#20139;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#22235;&#20010;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;MultiDoc2Dial&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#30340;&#36716;&#25442;&#20197;&#22810;&#20010;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#30340;&#25991;&#26723;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;Chat-Completion&#21644;LlamaIndex&#65292;&#36890;&#36807;&#28608;&#21457;ChatGPT&#27169;&#22411;&#26469;&#29983;&#25104;&#23545;&#35805;&#23436;&#25104;&#21709;&#24212;&#12290;ChatCompletion&#20351;&#29992;ChatGPT&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#32780;LlamaIndex&#36824;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;LLM&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#19981;&#33021;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20805;&#20998;&#35780;&#20272;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#21152;&#20887;&#38271;&#65292;&#25152;&#20197;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#20854;&#20013;&#35780;&#20272;&#21592;&#23545;&#20849;&#20139;&#20219;&#21153;&#30340;&#33719;&#22870;&#31995;&#32479;&#12289;&#20004;&#20010;Chat-GPT&#21464;&#20307;&#30340;&#36755;&#20986;&#20197;&#21450;&#20154;&#31867;&#21709;&#24212;&#36827;&#34892;&#35780;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pretraining while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#27874;&#26463;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#30456;&#26426;&#12289;LiDAR&#12289;&#38647;&#36798;&#21644;GPS&#31561;&#22810;&#31181;&#24863;&#30693;&#20449;&#24687;&#65292;&#25552;&#21462;&#24182;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#21644;&#26102;&#38388;&#23454;&#20363;&#20043;&#38388;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#27874;&#26463;&#31649;&#29702;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11811</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#65306;&#19968;&#31181;&#22522;&#20110;&#27874;&#26463;&#39044;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformers for Wireless Communications: A Case Study in Beam Prediction. (arXiv:2309.11811v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#27874;&#26463;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#30456;&#26426;&#12289;LiDAR&#12289;&#38647;&#36798;&#21644;GPS&#31561;&#22810;&#31181;&#24863;&#30693;&#20449;&#24687;&#65292;&#25552;&#21462;&#24182;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#21644;&#26102;&#38388;&#23454;&#20363;&#20043;&#38388;&#30340;&#29305;&#24449;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#27874;&#26463;&#31649;&#29702;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39057;&#27573;&#21644;&#22823;&#22825;&#32447;&#38453;&#21015;&#30340;&#26080;&#32447;&#36890;&#20449;&#20013;&#65292;&#27874;&#26463;&#31649;&#29702;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#30456;&#26426;&#12289;LiDAR&#12289;&#38647;&#36798;&#21644;GPS&#30340;&#22810;&#27169;&#24577;&#24863;&#30693;&#20449;&#24687;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#36741;&#21161;&#27874;&#26463;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#19968;&#31995;&#21015;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#38647;&#36798;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#27599;&#20010;&#21367;&#31215;&#23618;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#21644;&#26102;&#38388;&#23454;&#20363;&#20043;&#38388;&#29305;&#24449;&#26631;&#35760;&#30340;&#38544;&#34255;&#20851;&#31995;&#65292;&#20197;&#22312;&#25277;&#35937;&#31354;&#38388;&#20013;&#29983;&#25104;&#32534;&#30721;&#21521;&#37327;&#20197;&#20415;&#36827;&#34892;&#19979;&#19968;&#32423;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;&#28966;&#28857;&#25439;&#22833;&#21644;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22270;&#20687;&#22686;&#24378;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wireless communications at high-frequency bands with large antenna arrays face challenges in beam management, which can potentially be improved by multimodality sensing information from cameras, LiDAR, radar, and GPS. In this paper, we present a multimodal transformer deep learning framework for sensing-assisted beam prediction. We employ a convolutional neural network to extract the features from a sequence of images, point clouds, and radar raw data sampled over time. At each convolutional layer, we use transformer encoders to learn the hidden relations between feature tokens from different modalities and time instances over abstraction space and produce encoded vectors for the next-level feature extraction. We train the model on a combination of different modalities with supervised learning. We try to enhance the model over imbalanced data by utilizing focal loss and exponential moving average. We also evaluate data processing and augmentation techniques such as image enhancement, s
&lt;/p&gt;</description></item><item><title>JobRecoGPT&#26159;&#19968;&#20010;&#20351;&#29992;LLMs&#23454;&#29616;&#30340;&#21487;&#35299;&#37322;&#30340;&#32844;&#20301;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#22788;&#29702;&#32844;&#20301;&#25551;&#36848;&#21644;&#31616;&#21382;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#20043;&#21069;&#20002;&#22833;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.11805</link><description>&lt;p&gt;
JobRecoGPT -- &#20351;&#29992;LLMs&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#32844;&#20301;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
JobRecoGPT -- Explainable job recommendations using LLMs. (arXiv:2309.11805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11805
&lt;/p&gt;
&lt;p&gt;
JobRecoGPT&#26159;&#19968;&#20010;&#20351;&#29992;LLMs&#23454;&#29616;&#30340;&#21487;&#35299;&#37322;&#30340;&#32844;&#20301;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#22788;&#29702;&#32844;&#20301;&#25551;&#36848;&#21644;&#31616;&#21382;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#20043;&#21069;&#20002;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#36895;&#21457;&#23637;&#30340;&#23601;&#19994;&#24066;&#22330;&#20013;&#65292;&#25214;&#21040;&#21512;&#36866;&#30340;&#26426;&#20250;&#21487;&#33021;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#27493;&#65292;&#35745;&#31639;&#26426;&#29616;&#22312;&#21487;&#20197;&#21521;&#20505;&#36873;&#20154;&#25512;&#33616;&#36866;&#21512;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#25512;&#33616;&#24037;&#20316;&#30340;&#20219;&#21153;&#19982;&#21521;&#35266;&#20247;&#25512;&#33616;&#30005;&#24433;&#24182;&#19981;&#30456;&#21516;&#12290;&#38500;&#20102;&#25216;&#33021;&#21644;&#32463;&#39564;&#31561;&#24517;&#22791;&#26465;&#20214;&#22806;&#65292;&#19968;&#20010;&#24037;&#20316;&#26159;&#21542;&#36866;&#21512;&#32473;&#23450;&#20505;&#36873;&#20154;&#36824;&#21462;&#20915;&#20110;&#35768;&#22810;&#24494;&#22937;&#30340;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#24037;&#20316;&#21644;&#20505;&#36873;&#20154;&#30340;&#21487;&#37327;&#21270;&#26041;&#38754;&#65292;&#20294;&#22312;&#23558;&#25968;&#25454;&#20174;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#65288;&#22914;&#24037;&#20316;&#25551;&#36848;&#21644;&#31616;&#21382;&#65289;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#24418;&#24335;&#30340;&#36807;&#31243;&#20013;&#65292;&#25968;&#25454;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#20250;&#20002;&#22833;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#22312;&#25991;&#26412;&#25968;&#25454;&#21487;&#29992;&#30340;&#39046;&#22495;&#34920;&#29616;&#20986;&#30340;&#21331;&#36234;&#24615;&#33021;&#24109;&#21367;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#21463;&#21040;LLMs&#20986;&#33394;&#34920;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21033;&#29992;&#23427;&#20204;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#20197;&#25429;&#25417;&#21040;&#20808;&#21069;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's rapidly evolving job market, finding the right opportunity can be a daunting challenge. With advancements in the field of AI, computers can now recommend suitable jobs to candidates. However, the task of recommending jobs is not same as recommending movies to viewers. Apart from must-have criteria, like skills and experience, there are many subtle aspects to a job which can decide if it is a good fit or not for a given candidate. Traditional approaches can capture the quantifiable aspects of jobs and candidates, but a substantial portion of the data that is present in unstructured form in the job descriptions and resumes is lost in the process of conversion to structured format. As of late, Large Language Models (LLMs) have taken over the AI field by storm with extraordinary performance in fields where text-based data is available. Inspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was 
&lt;/p&gt;</description></item><item><title>DimCL&#26159;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;DimCL&#30340;&#30828;&#26679;&#26412;&#29305;&#24615;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11782</link><description>&lt;p&gt;
DimCL: &#29992;&#20110;&#25913;&#36827;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DimCL: Dimensional Contrastive Learning For Improving Self-Supervised Learning. (arXiv:2309.11782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11782
&lt;/p&gt;
&lt;p&gt;
DimCL&#26159;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25913;&#36827;&#29305;&#24449;&#22810;&#26679;&#24615;&#30340;&#32500;&#24230;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;DimCL&#30340;&#30828;&#26679;&#26412;&#29305;&#24615;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#26032;&#30340;&#38750;CL&#26694;&#26550;&#24050;&#32463;&#21462;&#24471;&#20102;&#21487;&#27604;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25552;&#21319;&#36825;&#20123;&#26694;&#26550;&#12290;&#23558;CL&#34701;&#20837;&#38750;CL&#26694;&#26550;&#34987;&#35748;&#20026;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#27809;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32500;&#24230;&#26041;&#21521;&#19978;&#25191;&#34892;CL&#32780;&#19981;&#26159;&#20197;&#20256;&#32479;&#30340;&#25209;&#27425;&#26041;&#21521;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;Dimensional Contrastive Learning&#65288;DimCL&#65289;&#12290;DimCL&#26088;&#22312;&#22686;&#24378;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#20808;&#21069;&#30340;SSL&#26694;&#26550;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#32467;&#26524;&#21457;&#29616;DimCL&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#30828;&#26679;&#26412;&#23545;&#20854;&#25104;&#21151;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23558;DimCL&#34701;&#20837;SSL&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained remarkable success, for which contrastive learning (CL) plays a key role. However, the recent development of new non-CL frameworks has achieved comparable or better performance with high improvement potential, prompting researchers to enhance these frameworks further. Assimilating CL into non-CL frameworks has been thought to be beneficial, but empirical evidence indicates no visible improvements. In view of that, this paper proposes a strategy of performing CL along the dimensional direction instead of along the batch direction as done in conventional contrastive learning, named Dimensional Contrastive Learning (DimCL). DimCL aims to enhance the feature diversity, and it can serve as a regularizer to prior SSL frameworks. DimCL has been found to be effective, and the hardness-aware property is identified as a critical reason for its success. Extensive experimental results reveal that assimilating DimCL into SSL frameworks leads to performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;2DDATA&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29305;&#23450;&#30340;&#20998;&#25903;&#21644;&#21033;&#29992;RGB&#27169;&#24577;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11755</link><description>&lt;p&gt;
2DDATA: &#22522;&#20110;&#28857;&#20113;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#20108;&#32500;&#26816;&#27979;&#27880;&#37322;&#20256;&#36755;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
2DDATA: 2D Detection Annotations Transmittable Aggregation for Semantic Segmentation on Point Cloud. (arXiv:2309.11755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;2DDATA&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#29305;&#23450;&#30340;&#20998;&#25903;&#21644;&#21033;&#29992;RGB&#27169;&#24577;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#19981;&#21516;&#20256;&#24863;&#22120;&#65288;&#22914;LiDAR&#21644;&#25668;&#20687;&#22836;&#65289;&#25552;&#20379;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#20934;&#30830;&#26657;&#20934;&#30340;&#37197;&#23545;&#25968;&#25454;&#20197;&#21450;&#27169;&#24577;&#20043;&#38388;&#22797;&#26434;&#30340;&#26657;&#20934;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#25104;&#26412;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#37492;&#20110;&#27492;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#19981;&#20165;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#30340;&#38382;&#39064;&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;RGB&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;2D Detection Annotations Transmittable Aggregation&#65288;2DDATA&#65289;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#29305;&#23450;&#30340;&#20998;&#25903;&#65292;&#31216;&#20026;Local Object Branch&#65292;&#26088;&#22312;&#22788;&#29702;&#26576;&#20010;&#36793;&#30028;&#26694;&#20869;&#30340;&#28857;&#20113;&#65292;&#22240;&#20026;&#33719;&#21462;2D&#36793;&#30028;&#26694;&#27880;&#37322;&#30456;&#23545;&#23481;&#26131;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31616;&#21333;&#35774;&#35745;&#21487;&#20197;&#23558;&#36793;&#30028;&#26694;&#20808;&#39564;&#20449;&#24687;&#20256;&#36755;&#32473;3D&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, multi-modality models have been introduced because of the complementary information from different sensors such as LiDAR and cameras. It requires paired data along with precise calibrations for all modalities, the complicated calibration among modalities hugely increases the cost of collecting such high-quality datasets, and hinder it from being applied to practical scenarios. Inherit from the previous works, we not only fuse the information from multi-modality without above issues, and also exhaust the information in the RGB modality. We introduced the 2D Detection Annotations Transmittable Aggregation(\textbf{2DDATA}), designing a data-specific branch, called \textbf{Local Object Branch}, which aims to deal with points in a certain bounding box, because of its easiness of acquiring 2D bounding box annotations. We demonstrate that our simple design can transmit bounding box prior information to the 3D encoder model, proving the feasibility of large multi-modality models fuse
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#35821;&#20041;&#25506;&#32034;&#65292;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#30456;&#20851;&#38382;&#39064;&#26469;&#19982;"&#31070;"&#32423;&#23384;&#22312;&#20132;&#20114;&#65292;&#26356;&#26032;&#20195;&#29702;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.11753</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#35821;&#20041;&#25506;&#32034;&#65292;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improve the efficiency of deep reinforcement learning through semantic exploration guided by natural language. (arXiv:2309.11753v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#35821;&#20041;&#25506;&#32034;&#65292;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#26816;&#32034;&#35821;&#26009;&#24211;&#20013;&#30456;&#20851;&#38382;&#39064;&#26469;&#19982;"&#31070;"&#32423;&#23384;&#22312;&#20132;&#20114;&#65292;&#26356;&#26032;&#20195;&#29702;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#35797;&#38169;&#20013;&#23398;&#20064;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20132;&#20114;&#25165;&#33021;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#22914;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#65292;&#19968;&#20010;&#33021;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#21453;&#39304;&#25110;&#25351;&#23548;&#30340;"&#31070;"&#32423;&#23384;&#22312;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#36807;&#20110;&#39057;&#32321;&#22320;&#26597;&#35810;"&#31070;"&#32423;&#23384;&#22312;&#21487;&#33021;&#26159;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#32780;&#19988;"&#31070;"&#32423;&#23384;&#22312;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#23545;&#27599;&#20010;&#24773;&#20917;&#37117;&#26377;&#26126;&#30830;&#30340;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#20197;&#36873;&#25321;&#24615;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#19982;"&#31070;"&#32423;&#23384;&#22312;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#20551;&#35774;&#20132;&#20114;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#27169;&#26495;&#21270;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#19988;&#23384;&#22312;&#22823;&#37327;&#20197;&#21069;&#30340;&#20132;&#20114;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#29702;&#21644;"&#31070;"&#32423;&#23384;&#22312;&#30340;&#24403;&#21069;&#29366;&#24577;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20174;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#26469;&#38382;"&#31070;"&#32423;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;"&#31070;"&#32423;&#23384;&#22312;&#30340;&#31572;&#26696;&#26469;&#26356;&#26032;&#20195;&#29702;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a powerful technique for learning from trial and error, but it often requires a large number of interactions to achieve good performance. In some domains, such as sparse-reward tasks, an oracle that can provide useful feedback or guidance to the agent during the learning process is really of great importance. However, querying the oracle too frequently may be costly or impractical, and the oracle may not always have a clear answer for every situation. Therefore, we propose a novel method for interacting with the oracle in a selective and efficient way, using a retrieval-based approach. We assume that the interaction can be modeled as a sequence of templated questions and answers, and that there is a large corpus of previous interactions available. We use a neural network to encode the current state of the agent and the oracle, and retrieve the most relevant question from the corpus to ask the oracle. We then use the oracle's answer to update the agent's policy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11751</link><description>&lt;p&gt;
Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#26377;&#22810;&#24378;&#22823;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Robust is Google's Bard to Adversarial Image Attacks?. (arXiv:2309.11751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Google&#30340;Bard&#22312;&#23545;&#25239;&#22270;&#20687;&#25915;&#20987;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#34987;&#25915;&#20987;&#20197;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#19968;&#25915;&#20987;&#36824;&#21487;&#20197;&#23545;&#20854;&#20182;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21644;&#20854;&#20182;&#27169;&#24577;&#65288;&#23588;&#20854;&#26159;&#35270;&#35273;&#65289;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#27169;&#22411;&#30340;&#26410;&#35299;&#20915;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#35270;&#35273;&#36755;&#20837;&#21487;&#33021;&#20351;MLLM&#38754;&#20020;&#26356;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Google&#30340;Bard&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23427;&#26159;&#19968;&#20010;&#31454;&#20105;&#24615;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26368;&#36817;&#21457;&#24067;&#20102;&#20854;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#21830;&#19994;MLLM&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#25915;&#20987;&#30333;&#30418;&#23376;&#20195;&#29702;&#35270;&#35273;&#32534;&#30721;&#22120;&#25110;MLLM&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#20351;Bard&#20197;22&#65285;&#30340;&#25104;&#21151;&#29575;&#20165;&#22522;&#20110;&#21487;&#36716;&#31227;&#24615;&#36755;&#20986;&#38169;&#35823;&#30340;&#22270;&#20687;&#25551;&#36848;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#36824;&#21487;&#20197;&#25915;&#20987;&#20854;&#20182;MLLM&#65292;&#20363;&#22914;&#65292;&#23545;Bing Chat&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;26&#65285;&#65292;&#23545;ERNIE bot&#30340;&#25104;&#21151;&#25915;&#20987;&#29575;&#20026;86&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;Bard&#30340;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#21253;&#25324;&#38754;&#37096;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) that integrate text and other modalities (especially vision) have achieved unprecedented performance in various multimodal tasks. However, due to the unsolved adversarial robustness problem of vision models, MLLMs can have more severe safety and security risks by introducing the vision inputs. In this work, we study the adversarial robustness of Google's Bard, a competitive chatbot to ChatGPT that released its multimodal capability recently, to better understand the vulnerabilities of commercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs, the generated adversarial examples can mislead Bard to output wrong image descriptions with a 22% success rate based solely on the transferability. We show that the adversarial examples can also attack other MLLMs, e.g., a 26% attack success rate against Bing Chat and a 86% attack success rate against ERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face detection
&lt;/p&gt;</description></item><item><title>Choice-75&#26159;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#22312;&#25551;&#36848;&#22330;&#26223;&#19979;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#38590;&#39064;&#20013;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.11737</link><description>&lt;p&gt;
Choice-75&#65306;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Choice-75: A Dataset on Decision Branching in Script Learning. (arXiv:2309.11737v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11737
&lt;/p&gt;
&lt;p&gt;
Choice-75&#26159;&#19968;&#20010;&#20851;&#20110;&#33050;&#26412;&#23398;&#20064;&#20013;&#20915;&#31574;&#20998;&#25903;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#22312;&#25551;&#36848;&#22330;&#26223;&#19979;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#38590;&#39064;&#20013;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33050;&#26412;&#23398;&#20064;&#30740;&#31350;&#26085;&#24120;&#20107;&#20214;&#30340;&#23637;&#24320;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#33050;&#26412;&#35270;&#20026;&#32447;&#24615;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20154;&#20204;&#22312;&#29305;&#23450;&#24773;&#22659;&#20013;&#25152;&#20570;&#20986;&#30340;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#30340;&#20998;&#25903;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Choice-75&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25361;&#25112;&#26234;&#33021;&#31995;&#32479;&#26681;&#25454;&#25551;&#36848;&#22330;&#26223;&#39044;&#27979;&#20915;&#31574;&#30340;&#22522;&#20934;&#65292;&#21253;&#21547;75&#20010;&#33050;&#26412;&#21644;600&#22810;&#20010;&#22330;&#26223;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25972;&#20307;&#19978;&#34920;&#29616;&#19981;&#38169;&#65292;&#20294;&#22312;&#35768;&#22810;&#22256;&#38590;&#30340;&#22330;&#26223;&#20013;&#20173;&#26377;&#26126;&#26174;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Script learning studies how daily events unfold. Previous works tend to consider a script as a linear sequence of events while ignoring the potential branches that arise due to people's circumstantial choices. We hence propose Choice-75, the first benchmark that challenges intelligent systems to predict decisions given descriptive scenarios, containing 75 scripts and more than 600 scenarios. While large language models demonstrate overall decent performances, there is still notable room for improvement in many hard scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"FluentEditor"&#30340;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#23454;&#29616;&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#12290;&#20854;&#20013;&#65292;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#29992;&#20110;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#21644;&#25972;&#20307;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11725</link><description>&lt;p&gt;
FluentEditor: &#32771;&#34385;&#22768;&#23398;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#38899;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FluentEditor: Text-based Speech Editing by Considering Acoustic and Prosody Consistency. (arXiv:2309.11725v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11725
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"FluentEditor"&#30340;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#23454;&#29616;&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#12290;&#20854;&#20013;&#65292;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#21644;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#29992;&#20110;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#21644;&#25972;&#20307;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21270;&#35821;&#38899;&#32534;&#36753;&#65288;TSE&#65289;&#25216;&#26415;&#26088;&#22312;&#20351;&#29992;&#25143;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#30340;&#25991;&#26412;&#36716;&#24405;&#32780;&#19981;&#26159;&#38899;&#39057;&#26412;&#36523;&#26469;&#32534;&#36753;&#29983;&#25104;&#30340;&#38899;&#39057;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#30340;TSE&#25216;&#26415;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#20027;&#35201;&#20851;&#27880;&#20110;&#20943;&#23567;&#32534;&#36753;&#21306;&#22495;&#20013;&#29983;&#25104;&#30340;&#35821;&#38899;&#29255;&#27573;&#19982;&#21442;&#32771;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24573;&#35270;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#21644;&#21407;&#22987;&#35821;&#21477;&#20013;&#30340;&#23616;&#37096;&#21644;&#25972;&#20307;&#27969;&#30021;&#24615;&#12290;&#20026;&#20102;&#20445;&#25345;&#35821;&#38899;&#30340;&#27969;&#30021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#30021;&#35821;&#38899;&#32534;&#36753;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;FluentEditor&#8221;&#65292;&#36890;&#36807;&#32771;&#34385;&#27969;&#30021;&#24230;&#24847;&#35782;&#30340;&#35757;&#32451;&#20934;&#21017;&#36827;&#34892;TSE&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#22768;&#23398;&#19968;&#33268;&#24615;&#32422;&#26463;&#8221;&#26088;&#22312;&#20351;&#32534;&#36753;&#21306;&#22495;&#19982;&#20854;&#30456;&#37051;&#30340;&#22768;&#23398;&#29255;&#27573;&#20043;&#38388;&#30340;&#36807;&#28193;&#21464;&#24471;&#24179;&#28369;&#24182;&#19982;&#30495;&#23454;&#24773;&#20917;&#20445;&#25345;&#19968;&#33268;&#65292;&#8220;&#38901;&#24459;&#19968;&#33268;&#24615;&#32422;&#26463;&#8221;&#26088;&#22312;&#30830;&#20445;&#32534;&#36753;&#21306;&#22495;&#20869;&#30340;&#38901;&#24459;&#23646;&#24615;&#19982;&#21407;&#22987;&#35821;&#21477;&#30340;&#25972;&#20307;&#39118;&#26684;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based speech editing (TSE) techniques are designed to enable users to edit the output audio by modifying the input text transcript instead of the audio itself. Despite much progress in neural network-based TSE techniques, the current techniques have focused on reducing the difference between the generated speech segment and the reference target in the editing region, ignoring its local and global fluency in the context and original utterance. To maintain the speech fluency, we propose a fluency speech editing model, termed \textit{FluentEditor}, by considering fluency-aware training criterion in the TSE training. Specifically, the \textit{acoustic consistency constraint} aims to smooth the transition between the edited region and its neighboring acoustic segments consistent with the ground truth, while the \textit{prosody consistency constraint} seeks to ensure that the prosody attributes within the edited regions remain consistent with the overall style of the original utterance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;EmoPP&#8221;&#30340;&#24773;&#24863;&#24863;&#30693;&#38901;&#24459;&#30701;&#35821;&#27169;&#22411;&#65292;&#36890;&#36807;&#20934;&#30830;&#25366;&#25496;&#35805;&#35821;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#39044;&#27979;&#36866;&#24403;&#30340;&#26029;&#28857;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#34920;&#24773;&#21270;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#24615;&#33021;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2309.11724</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#30340;&#38901;&#24459;&#30701;&#35821;&#21270;&#29992;&#20110;&#34920;&#24773;&#21270;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech. (arXiv:2309.11724v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;EmoPP&#8221;&#30340;&#24773;&#24863;&#24863;&#30693;&#38901;&#24459;&#30701;&#35821;&#27169;&#22411;&#65292;&#36890;&#36807;&#20934;&#30830;&#25366;&#25496;&#35805;&#35821;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#39044;&#27979;&#36866;&#24403;&#30340;&#26029;&#28857;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#34920;&#24773;&#21270;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#24615;&#33021;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38901;&#24459;&#30701;&#35821;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#30340;&#33258;&#28982;&#24615;&#21644;&#21487;&#25026;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#35821;&#38899;&#20013;&#23384;&#22312;&#30528;&#35821;&#35328;&#21644;&#24773;&#24863;&#30340;&#38901;&#24459;&#12290;&#30001;&#20110;&#38901;&#24459;&#30701;&#35821;&#30740;&#31350;&#19968;&#30452;&#20197;&#26469;&#20197;&#35821;&#35328;&#23398;&#20026;&#21160;&#21147;&#65292;&#22240;&#27492;&#34920;&#24773;&#21270;&#38901;&#24459;&#30701;&#35821;&#21270;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;EmoPP&#8221;&#30340;&#24773;&#24863;&#24863;&#30693;&#38901;&#24459;&#30701;&#35821;&#27169;&#22411;&#65292;&#20934;&#30830;&#22320;&#25366;&#25496;&#20102;&#35805;&#35821;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#24182;&#39044;&#27979;&#20102;&#36866;&#24403;&#30340;&#30701;&#35821;&#26029;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;ESD&#25968;&#25454;&#38598;&#36827;&#34892;&#23458;&#35266;&#35266;&#23519;&#65292;&#39564;&#35777;&#20102;&#24773;&#24863;&#21644;&#38901;&#24459;&#30701;&#35821;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#34920;&#26126;&#65292;EmoPP&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#24182;&#22312;&#24773;&#24863;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#38899;&#39057;&#26679;&#26412;&#21644;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/AI-S2-Lab/EmoPP}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prosodic phrasing is crucial to the naturalness and intelligibility of end-to-end Text-to-Speech (TTS). There exist both linguistic and emotional prosody in natural speech. As the study of prosodic phrasing has been linguistically motivated, prosodic phrasing for expressive emotion rendering has not been well studied. In this paper, we propose an emotion-aware prosodic phrasing model, termed \textit{EmoPP}, to mine the emotional cues of utterance accurately and predict appropriate phrase breaks. We first conduct objective observations on the ESD dataset to validate the strong correlation between emotion and prosodic phrasing. Then the objective and subjective evaluations show that the EmoPP outperforms all baselines and achieves remarkable performance in terms of emotion expressiveness. The audio samples and the code are available at \url{https://github.com/AI-S2-Lab/EmoPP}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11714</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Dynamic Domain Adaptation Deep Learning Network for EEG-based Motor Imagery Classification. (arXiv:2309.11714v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#20013;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#33041;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30456;&#37051;&#36890;&#36947;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#22914;&#20309;&#34920;&#31034;&#36825;&#31181;&#30456;&#20851;&#24615;&#26159;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;EEG&#20449;&#21495;&#22312;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#26032;&#20027;&#20307;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#26657;&#20934;&#26102;&#38388;&#29992;&#20110;&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#33041;&#26426;&#25509;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#36866;&#24212;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;DADL-Net&#65289;&#12290;&#39318;&#20808;&#65292;&#23558;EEG&#25968;&#25454;&#26144;&#23556;&#21040;&#19977;&#32500;&#20960;&#20309;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;3D&#21367;&#31215;&#27169;&#22359;&#23398;&#20064;&#20854;&#26102;&#31354;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#31354;&#38388;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#24378;&#21270;&#29305;&#24449;&#65292;&#26368;&#21518;&#30340;&#21367;&#31215;&#27169;&#22359;&#21487;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#29305;&#24449;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#32771;&#34385;&#20010;&#20307;&#38388;&#21644;&#36328;&#20250;&#35805;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#37319;&#29992;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#26368;&#22823;&#21270;&#36317;&#31163;&#26469;&#20943;&#23567;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a correlation between adjacent channels of electroencephalogram (EEG), and how to represent this correlation is an issue that is currently being explored. In addition, due to inter-individual differences in EEG signals, this discrepancy results in new subjects need spend a amount of calibration time for EEG-based motor imagery brain-computer interface. In order to solve the above problems, we propose a Dynamic Domain Adaptation Based Deep Learning Network (DADL-Net). First, the EEG data is mapped to the three-dimensional geometric space and its temporal-spatial features are learned through the 3D convolution module, and then the spatial-channel attention mechanism is used to strengthen the features, and the final convolution module can further learn the spatial-temporal information of the features. Finally, to account for inter-subject and cross-sessions differences, we employ a dynamic domain-adaptive strategy, the distance between features is reduced by introducing a Maximum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;RAl4IoE&#26694;&#26550;&#65292;&#23558;&#33021;&#28304;&#20114;&#32852;&#32593;&#19982;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20844;&#24179;&#12289;&#21487;&#25345;&#32493;&#21644;&#21487;&#38752;&#30340;&#33021;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2309.11691</link><description>&lt;p&gt;
RAI4IoE: &#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#25512;&#21160;&#33021;&#28304;&#20114;&#32852;&#32593;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
RAI4IoE: Responsible AI for Enabling the Internet of Energy. (arXiv:2309.11691v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;RAl4IoE&#26694;&#26550;&#65292;&#23558;&#33021;&#28304;&#20114;&#32852;&#32593;&#19982;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20844;&#24179;&#12289;&#21487;&#25345;&#32493;&#21644;&#21487;&#38752;&#30340;&#33021;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21457;&#23637;&#19968;&#20010;&#20844;&#24179;&#12289;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#20197;&#25512;&#21160;&#33021;&#28304;&#20114;&#32852;&#32593;&#65288;IoE&#65289;&#12290;&#33021;&#28304;&#34892;&#19994;&#27491;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#39537;&#21160;&#22240;&#32032;&#65306;&#24314;&#35774;&#38646;&#30899;&#33021;&#28304;&#34892;&#19994;&#21644;&#33021;&#28304;&#22522;&#30784;&#35774;&#26045;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20004;&#20010;&#39537;&#21160;&#22240;&#32032;&#30340;&#34701;&#21512;&#23558;&#23454;&#29616;&#33021;&#28304;&#20114;&#32852;&#32593;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;5G-6G&#32593;&#32476;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#23558;&#21487;&#20877;&#29983;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;&#22914;&#30005;&#21160;&#27773;&#36710;&#12289;&#20648;&#33021;&#30005;&#27744;&#12289;&#39118;&#21147;&#28065;&#36718;&#21644;&#20809;&#20239;&#21457;&#30005;&#65289;&#36830;&#25509;&#21644;&#38598;&#25104;&#65292;&#23454;&#29616;&#21487;&#38752;&#30340;&#33021;&#28304;&#20998;&#37197;&#12290;&#36825;&#20351;&#24471;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#30340;&#25317;&#26377;&#32773;&#65288;&#21363;&#33021;&#28304;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#65289;&#33021;&#22815;&#21442;&#19982;&#33021;&#28304;&#24066;&#22330;&#24182;&#33719;&#24471;&#32463;&#27982;&#28608;&#21169;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#23384;&#22312;&#22266;&#26377;&#30340;&#36164;&#20135;&#39537;&#21160;&#29305;&#24449;&#65292;&#24182;&#38754;&#20020;&#24179;&#31561;&#30340;&#25361;&#25112;&#65288;&#21363;&#20844;&#24179;&#12289;&#22810;&#26679;&#21644;&#21253;&#23481;&#65289;&#12290;&#22914;&#26524;&#27809;&#26377;&#24179;&#31561;&#30340;&#33719;&#21462;&#26426;&#20250;&#65292;&#29305;&#26435;&#20010;&#20307;&#12289;&#32676;&#20307;&#21644;&#32452;&#32455;&#21487;&#33021;&#20250;&#20197;&#25490;&#26021;&#20854;&#20182;&#20154;&#20026;&#20195;&#20215;&#21442;&#19982;&#21644;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper plans to develop an Equitable and Responsible AI framework with enabling techniques and algorithms for the Internet of Energy (IoE), in short, RAI4IoE. The energy sector is going through substantial changes fueled by two key drivers: building a zero-carbon energy sector and the digital transformation of the energy infrastructure. We expect to see the convergence of these two drivers resulting in the IoE, where renewable distributed energy resources (DERs), such as electric cars, storage batteries, wind turbines and photovoltaics (PV), can be connected and integrated for reliable energy distribution by leveraging advanced 5G-6G networks and AI technology. This allows DER owners as prosumers to participate in the energy market and derive economic incentives. DERs are inherently asset-driven and face equitable challenges (i.e., fair, diverse and inclusive). Without equitable access, privileged individuals, groups and organizations can participate and benefit at the cost of disa
&lt;/p&gt;</description></item><item><title>LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.11688</link><description>&lt;p&gt;
LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11688
&lt;/p&gt;
&lt;p&gt;
LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#30452;&#25509;&#35266;&#23519;&#25110;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#20114;&#26469;&#33719;&#21462;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21253;&#25324;&#30340;&#30693;&#35782;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#35843;&#29992;&#27169;&#22359;&#26469;&#23545;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20998;&#35299;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22238;&#31572;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#36882;&#24402;&#30340;&#21487;&#25193;&#23637;LLM&#65288;REBEL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;&#21069;&#21521;&#38142;&#25509;&#31574;&#30053;&#31561;&#33258;&#21160;&#25512;&#29702;&#25216;&#26415;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#12290;REBEL&#20351;&#29992;&#30340;&#24037;&#20855;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25351;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#32452;&#21512;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#19968;&#32452;&#38656;&#35201;&#28145;&#24230;&#23884;&#22871;&#30340;&#22806;&#37096;&#24037;&#20855;&#20351;&#29992;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;REBEL&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance in question-answering tasks, their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world. Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks. We introduce a method, Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational settin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#26102;&#20844;&#24179;&#27169;&#22411;&#34920;&#29616;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#22240;&#26524;&#22270;&#65292;&#20063;&#25903;&#25345;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.11682</link><description>&lt;p&gt;
Dr. FERMI&#65306;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework. (arXiv:2309.11682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#26102;&#20844;&#24179;&#27169;&#22411;&#34920;&#29616;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#30693;&#36947;&#22240;&#26524;&#22270;&#65292;&#20063;&#25903;&#25345;&#20351;&#29992;&#23567;&#25209;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#20960;&#24180;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#35757;&#32451;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#20284;&#30340;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;&#22312;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20844;&#24179;&#27169;&#22411;&#21487;&#33021;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38024;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#20844;&#24179;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#22522;&#20110;&#20855;&#26377;&#25551;&#36848;&#19981;&#21516;&#29305;&#24449;&#20132;&#20114;&#30340;&#22240;&#26524;&#22270;&#30340;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#23436;&#20840;&#35775;&#38382;&#25968;&#25454;&#65292;&#19981;&#33021;&#22312;&#20351;&#29992;&#23567;&#25209;&#37327;&#65288;&#38543;&#26426;/&#25209;&#37327;&#23454;&#29616;&#65289;&#26102;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#20998;&#24067;&#40065;&#26834;&#20844;&#24179;&#24615;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#23545;&#22240;&#26524;&#22270;&#26377;&#20219;&#20309;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22312;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#25512;&#26029;&#38382;&#39064;&#21046;&#23450;&#20026;$L_p$-&#33539;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11680</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#22270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20445;&#30041;&#23545;&#25968;&#25454;&#30340;&#29420;&#21344;&#25511;&#21046;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#19987;&#26377;&#25968;&#25454;&#21019;&#24314;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#23398;&#20064;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#20204;&#23398;&#20250;&#25429;&#25417;&#24213;&#23618;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;FL&#26694;&#26550;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#20840;&#23616;&#30340;NGM&#27169;&#22411;&#65292;&#20174;&#26412;&#22320;NGM&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#24179;&#22343;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;FedNGMs&#36991;&#20813;&#20102;&#31070;&#32463;&#20803;&#21305;&#37197;&#26694;&#26550;&#65288;&#22914;&#32852;&#37030;&#21305;&#37197;&#24179;&#22343;&#65289;&#20013;&#27169;&#22411;&#21442;&#25968;&#29190;&#28856;&#30340;&#32570;&#28857;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;&#27169;&#22411;&#22823;&#23567;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#27745;&#33104;&#24335;&#28216;&#25103;&#27169;&#25311;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;GPT-4&#21644;GPT-3.5-turbo&#65292;&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;GPT-4&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#20154;&#31867;&#21270;&#30340;&#21453;&#24212;&#65292;&#20294;&#20854;&#22312;&#34394;&#24352;&#22768;&#21183;&#21644;&#39044;&#27979;&#23545;&#25163;&#34892;&#21160;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#26377;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#27880;&#20837;&#26356;&#22810;&#21019;&#26032;&#24615;&#21644;&#25361;&#25112;&#24615;&#30340;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.11672</link><description>&lt;p&gt;
&#27745;&#33104;&#28216;&#25103;&#27169;&#25311;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Generative AI in Mafia-like Game Simulation. (arXiv:2309.11672v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#27745;&#33104;&#24335;&#28216;&#25103;&#27169;&#25311;&#20013;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;GPT-4&#21644;GPT-3.5-turbo&#65292;&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;GPT-4&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#26356;&#20154;&#31867;&#21270;&#30340;&#21453;&#24212;&#65292;&#20294;&#20854;&#22312;&#34394;&#24352;&#22768;&#21183;&#21644;&#39044;&#27979;&#23545;&#25163;&#34892;&#21160;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#26377;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#27880;&#20837;&#26356;&#22810;&#21019;&#26032;&#24615;&#21644;&#25361;&#25112;&#24615;&#30340;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21151;&#25928;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;&#35282;&#33394;&#25198;&#28436;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;&#65292;&#20855;&#20307;&#20307;&#29616;&#22312;&#33879;&#21517;&#30340;&#27745;&#33104;&#24335;&#28216;&#25103;Spyfall&#20013;&#12290;&#36890;&#36807;&#21033;&#29992;GPT-4&#20808;&#36827;&#30340;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23637;&#31034;&#35813;&#27169;&#22411;&#22312;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#29702;&#35299;&#12289;&#20915;&#31574;&#21644;&#20114;&#21160;&#28508;&#21147;&#12290;&#23545;&#27604;GPT-4&#19982;&#20854;&#21069;&#20219;GPT-3.5-turbo&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-4&#22312;&#28216;&#25103;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#25552;&#20986;&#20102;&#30456;&#20851;&#38382;&#39064;&#24182;&#24418;&#25104;&#20154;&#31867;&#33324;&#30340;&#22238;&#31572;&#26041;&#38754;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#34394;&#24352;&#22768;&#21183;&#21644;&#39044;&#27979;&#23545;&#25163;&#34892;&#21160;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#23545;&#28216;&#25103;&#24320;&#21457;&#12289;&#36130;&#21153;&#38480;&#21046;&#21644;&#30740;&#31350;&#30340;&#38750;&#35328;&#35821;&#38480;&#21046;&#36827;&#34892;&#20102;&#21453;&#24605;&#21644;&#35752;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#30456;&#23545;&#26089;&#26399;&#27169;&#22411;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#26377;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#27880;&#20837;&#26356;&#22810;&#21019;&#26032;&#24615;&#21644;&#25361;&#25112;&#24615;&#30340;&#20803;&#32032;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#22312;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#21644;&#37319;&#35775;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#29992;&#25143;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#30693;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#21152;&#37325;&#20102;&#29992;&#25143;&#30340;&#26435;&#34913;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.11653</link><description>&lt;p&gt;
"&#20844;&#24179;&#28216;&#25103;"&#65292;&#36824;&#26159;&#21527;&#65311;&#30740;&#31350;&#29992;&#25143;&#22312;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#22914;&#20309;&#22788;&#29702;&#25259;&#38706;&#39118;&#38505;&#21644;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
"It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. (arXiv:2309.11653v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#22312;&#23454;&#38469;&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#21644;&#37319;&#35775;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#29992;&#25143;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#29992;&#25143;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#30693;&#23384;&#22312;&#38382;&#39064;&#65292;&#32780;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#21152;&#37325;&#20102;&#29992;&#25143;&#30340;&#26435;&#34913;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#12290;&#26500;&#24314;&#23562;&#37325;&#29992;&#25143;&#38544;&#31169;&#30340;&#36947;&#24503;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#26368;&#20851;&#27880;&#29992;&#25143;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#65292;&#26080;&#27861;&#25552;&#20379;&#29992;&#25143;&#30340;&#35266;&#28857;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23454;&#38469;&#30340;ChatGPT&#23545;&#35805;&#20013;&#30340;&#25935;&#24863;&#25259;&#38706;&#65292;&#24182;&#23545;19&#21517;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#29992;&#25143;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#37319;&#35775;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;LLM&#22411;&#23545;&#35805;&#22411;&#26234;&#33021;&#21161;&#25163;&#26102;&#65292;&#29992;&#25143;&#19981;&#26029;&#38754;&#20020;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20415;&#21033;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#38169;&#35823;&#30340;&#24515;&#26234;&#27169;&#24335;&#21644;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#40657;&#26263;&#27169;&#24335;&#38480;&#21046;&#20102;&#20182;&#20204;&#23545;&#38544;&#31169;&#39118;&#38505;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21270;&#30340;&#20114;&#21160;&#40723;&#21169;&#20102;&#26356;&#22810;&#25935;&#24863;&#30340;&#25259;&#38706;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#22312;&#26435;&#34913;&#20013;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#38469;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guideli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36712;&#36947;&#19978;&#30340;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#20027;&#35201;&#20256;&#24863;&#22120;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11648</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36947;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#20027;&#21152;&#27833;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Orbital AI-based Autonomous Refuelling Solution. (arXiv:2309.11648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36712;&#36947;&#19978;&#30340;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#20027;&#35201;&#20256;&#24863;&#22120;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23567;&#22411;&#23610;&#23544;&#21644;&#20302;&#25104;&#26412;&#30340;&#21151;&#29575;&#12289;&#36136;&#37327;&#21644;&#20307;&#31215;&#65292;&#25668;&#20687;&#22836;&#27491;&#36805;&#36895;&#25104;&#20026;&#22826;&#31354;&#20132;&#20250;&#30340;&#36873;&#25321;&#26426;&#36733;&#20256;&#24863;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#25509;&#26041;&#38754;&#65292;&#23427;&#20204;&#36890;&#24120;&#36215;&#21040;&#27425;&#35201;&#20316;&#29992;&#65292;&#32780;&#20027;&#35201;&#24037;&#20316;&#30001;&#28608;&#20809;&#38647;&#36798;&#31561;&#20027;&#21160;&#20256;&#24863;&#22120;&#23436;&#25104;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23548;&#33322;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20351;&#33337;&#36733;&#21487;&#35265;&#27874;&#38271;&#25668;&#20687;&#22836;&#20316;&#20026;&#23545;&#25509;&#21644;&#36712;&#36947;&#26381;&#21153;&#30340;&#20027;&#35201;&#20256;&#24863;&#22120;&#25104;&#29087;&#36215;&#26469;&#65292;&#20943;&#23569;&#23545;&#28608;&#20809;&#38647;&#36798;&#30340;&#20381;&#36182;&#24182;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20351;&#24471;&#30456;&#23545;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#25193;&#23637;&#21040;&#22810;&#31181;&#24773;&#20917;&#65292;&#20363;&#22914;&#30446;&#26631;&#25110;&#29031;&#26126;&#26465;&#20214;&#65292;&#22312;&#20256;&#32479;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#24773;&#20917;&#37117;&#24517;&#39035;&#36827;&#34892;&#20010;&#26696;&#22788;&#29702;&#12290;&#22312;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#23545;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#39592;&#24178;&#26550;&#26500;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#21644;Residual Pixel Attention&#23618;&#65292;&#21033;&#29992;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11641</link><description>&lt;p&gt;
Attentive VQ-VAE&#65306;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attentive VQ-VAE. (arXiv:2309.11641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#21644;Residual Pixel Attention&#23618;&#65292;&#21033;&#29992;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#65288;AREN&#65289;&#21644;Residual Pixel Attention&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#23454;&#29992;&#30340;&#21442;&#25968;&#27700;&#24179;&#30340;&#21516;&#26102;&#25913;&#36827;VQ-VAE&#30340;&#24615;&#33021;&#12290;AREN&#32534;&#30721;&#22120;&#34987;&#35774;&#35745;&#25104;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#22810;&#20010;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#26550;&#26500;&#22797;&#26434;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#23558;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;AREN&#32534;&#30721;&#22120;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#23618;&#37319;&#29992;&#26368;&#23567;&#21442;&#25968;&#26041;&#27861;&#65292;&#30830;&#20445;&#21482;&#26377;&#22312;&#20854;&#20182;&#20687;&#32032;&#30340;&#30456;&#20851;&#20449;&#24687;&#21487;&#29992;&#26102;&#25165;&#20462;&#25913;&#28508;&#22312;&#21521;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in dat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#24037;&#20154;&#30340;&#26045;&#24037;&#25216;&#33021;&#36716;&#31227;&#21040;&#36741;&#21161;&#26426;&#22120;&#20154;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27785;&#28024;&#24335;&#34394;&#25311;&#28436;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#26045;&#24037;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11619</link><description>&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#24037;&#20154;&#21040;&#36741;&#21161;&#26426;&#22120;&#20154;&#30340;&#26045;&#24037;&#25216;&#33021;&#30340;&#21487;&#25193;&#23637;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots. (arXiv:2309.11619v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#24037;&#20154;&#30340;&#26045;&#24037;&#25216;&#33021;&#36716;&#31227;&#21040;&#36741;&#21161;&#26426;&#22120;&#20154;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27785;&#28024;&#24335;&#34394;&#25311;&#28436;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#26045;&#24037;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#21644;&#20307;&#21147;&#35201;&#27714;&#39640;&#30340;&#26045;&#24037;&#20219;&#21153;&#20998;&#37197;&#32473;&#26426;&#22120;&#20154;&#21487;&#20197;&#20943;&#36731;&#20154;&#31867;&#24037;&#20154;&#30340;&#32844;&#19994;&#20260;&#23475;&#12290;&#23558;&#24037;&#20154;&#30340;&#24039;&#22937;&#21644;&#36866;&#24212;&#24615;&#30340;&#25163;&#24037;&#33402;&#26045;&#24037;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#23545;&#20110;&#25104;&#21151;&#22996;&#25176;&#26045;&#24037;&#20219;&#21153;&#21644;&#23454;&#29616;&#39640;&#36136;&#37327;&#26426;&#22120;&#20154;&#26045;&#24037;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#23450;&#20041;&#30340;&#36816;&#21160;&#35268;&#21010;&#33050;&#26412;&#24448;&#24448;&#20250;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#26045;&#24037;&#29616;&#22330;&#29615;&#22659;&#20013;&#29983;&#25104;&#20725;&#30828;&#19988;&#26131;&#30896;&#25758;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20581;&#22766;&#21644;&#28789;&#27963;&#30340;&#25216;&#33021;&#36716;&#31227;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;IL&#31639;&#27861;&#20381;&#36182;&#20110;&#20154;&#31867;&#24037;&#20154;&#21453;&#22797;&#23637;&#31034;&#23436;&#25972;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#24314;&#31569;&#24037;&#20316;&#26469;&#35828;&#21487;&#33021;&#26159;&#36866;&#24471;&#20854;&#21453;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27785;&#28024;&#24335;&#30340;&#22522;&#20110;&#20113;&#26426;&#22120;&#20154;&#30340;&#34394;&#25311;&#28436;&#31034;&#26694;&#26550;&#65292;&#20855;&#22791;&#20004;&#20010;&#20027;&#35201;&#30446;&#30340;&#12290;&#31532;&#19968;&#65292;&#23427;&#25968;&#23383;&#21270;&#20102;&#28436;&#31034;&#36807;&#31243;&#65292;&#28040;&#38500;&#20102;&#26102;&#38388;&#21644;&#22320;&#28857;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26102;&#38388;&#34920;&#20197;&#25552;&#39640;&#24037;&#20154;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assigning repetitive and physically-demanding construction tasks to robots can alleviate human workers's exposure to occupational injuries. Transferring necessary dexterous and adaptive artisanal construction craft skills from workers to robots is crucial for the successful delegation of construction tasks and achieving high-quality robot-constructed work. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, Imitation Learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers to repeatedly demonstrate task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, this paper proposes an immersive, cloud robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#23545;&#25163;&#21183;&#36827;&#34892;&#35782;&#21035;&#65292;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#22312;HG14&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21457;&#29616;VGGNet&#21644;MobileNet&#27169;&#22411;&#30340;&#29256;&#26412;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.11610</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning. (arXiv:2309.11610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;&#38598;&#25104;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#23545;&#25163;&#21183;&#36827;&#34892;&#35782;&#21035;&#65292;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#22312;HG14&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21457;&#29616;VGGNet&#21644;MobileNet&#27169;&#22411;&#30340;&#29256;&#26412;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114; (HCI) &#26159;&#22810;&#24180;&#26469;&#30340;&#30740;&#31350;&#20027;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#36890;&#36807;&#21508;&#31181;&#25216;&#26415;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;&#24341;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;&#20854;&#22312;HCI&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#25506;&#32034;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#32467;&#26500;&#20174;&#22270;&#20687;&#20013;&#35782;&#21035;&#25163;&#21183;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;&#39640;&#24615;&#33021;&#28145;&#24230;&#32467;&#26500;&#22312;HG14&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;14&#20010;&#19981;&#21516;&#30340;&#25163;&#21183;&#31867;&#21035;&#12290;&#22312;22&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#65292;VGGNet&#21644;MobileNet&#27169;&#22411;&#30340;&#29256;&#26412;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VGG16&#21644;VGG19&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;94.64%&#21644;94.36%&#65292;&#32780;MobileNet&#21644;MobileNetV2&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;96.79%&#21644;94.43%&#12290;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Computer Interaction (HCI) has been the subject of research for many years, and recent studies have focused on improving its performance through various techniques. In the past decade, deep learning studies have shown high performance in various research areas, leading researchers to explore their application to HCI. Convolutional neural networks can be used to recognize hand gestures from images using deep architectures. In this study, we evaluated pre-trained high-performance deep architectures on the HG14 dataset, which consists of 14 different hand gesture classes. Among 22 different models, versions of the VGGNet and MobileNet models attained the highest accuracy rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of 94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand gesture recognition on the dataset using an ensemble learning technique, which combined 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;"&#25968;&#25454;&#38598;&#24037;&#21378;"&#30340;&#26041;&#27861;&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#19982;&#20803;&#25968;&#25454;&#20998;&#24320;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#22242;&#38431;&#21644;&#20010;&#20154;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#35268;&#27169;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.11608</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#24037;&#21378;&#65306;&#19968;&#20010;&#29983;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#24037;&#20855;&#38142;
&lt;/p&gt;
&lt;p&gt;
Dataset Factory: A Toolchain For Generative Computer Vision Datasets. (arXiv:2309.11608v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11608
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;"&#25968;&#25454;&#38598;&#24037;&#21378;"&#30340;&#26041;&#27861;&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#19982;&#20803;&#25968;&#25454;&#20998;&#24320;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#22242;&#38431;&#21644;&#20010;&#20154;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#35268;&#27169;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20316;&#27969;&#31243;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20219;&#21153;&#65292;&#22914;&#36890;&#36807;&#27880;&#37322;&#23383;&#27573;&#12289;&#21521;&#37327;&#36317;&#31163;&#25110;&#33258;&#23450;&#20041;&#20998;&#31867;&#22120;&#20135;&#29983;&#30340;&#20998;&#25968;&#26469;&#36807;&#28388;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#23481;&#37327;&#36805;&#36895;&#25509;&#36817;PB&#32423;&#65292;&#20351;&#24471;&#25968;&#25454;&#25972;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#20934;&#22791;&#30340;&#36845;&#20195;&#29305;&#24615;&#38656;&#35201;&#23454;&#29616;&#24378;&#22823;&#30340;&#25968;&#25454;&#38598;&#20849;&#20139;&#21644;&#29256;&#26412;&#25511;&#21046;&#26426;&#21046;&#65292;&#36825;&#20004;&#32773;&#37117;&#38590;&#20197;&#33258;&#36866;&#24212;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;"&#25968;&#25454;&#38598;&#24037;&#21378;"&#30340;&#26041;&#27861;&#65292;&#23558;&#26679;&#26412;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#19982;&#20803;&#25968;&#25454;&#20998;&#24320;&#65292;&#24182;&#20026;&#26426;&#22120;&#23398;&#20064;&#22242;&#38431;&#21644;&#20010;&#20154;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#35268;&#27169;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI workflows heavily rely on data-centric tasks - such as filtering samples by annotation fields, vector distances, or scores produced by custom classifiers. At the same time, computer vision datasets are quickly approaching petabyte volumes, rendering data wrangling difficult. In addition, the iterative nature of data preparation necessitates robust dataset sharing and versioning mechanisms, both of which are hard to implement ad-hoc. To solve these challenges, we propose a "dataset factory" approach that separates the storage and processing of samples from metadata and enables data-centric operations at scale for machine learning teams and individual researchers.
&lt;/p&gt;</description></item><item><title>CATS&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;&#23427;&#37319;&#29992;K-&#21311;&#21517;&#25216;&#26415;&#20445;&#38556;&#20102;&#20998;&#24067;&#32423;&#38544;&#31169;&#65292;&#36890;&#36807;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#21644;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#36712;&#36857;&#25968;&#25454;&#30340;&#21512;&#25104;&#21644;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.11587</link><description>&lt;p&gt;
CATS: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26465;&#20214;&#23545;&#25239;&#36712;&#36857;&#21512;&#25104;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches. (arXiv:2309.11587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11587
&lt;/p&gt;
&lt;p&gt;
CATS&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;&#23427;&#37319;&#29992;K-&#21311;&#21517;&#25216;&#26415;&#20445;&#38556;&#20102;&#20998;&#24067;&#32423;&#38544;&#31169;&#65292;&#36890;&#36807;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#21644;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#36712;&#36857;&#25968;&#25454;&#30340;&#21512;&#25104;&#21644;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#22788;&#19981;&#22312;&#30340;&#23450;&#20301;&#24863;&#30693;&#35774;&#22791;&#21644;&#31227;&#21160;&#20114;&#32852;&#32593;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#29992;&#25143;&#37027;&#37324;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#20010;&#20307;&#32423;&#36712;&#36857;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#36712;&#36857;&#22823;&#25968;&#25454;&#20026;&#20154;&#31867;&#31227;&#21160;&#24615;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#20301;&#32622;&#38544;&#31169;&#30340;&#20844;&#20247;&#20851;&#20999;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#21517;&#20026; Conditional Adversarial Trajectory Synthesis (CATS)&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#36712;&#36857;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#21457;&#24067;&#12290;CATS &#23558; K-&#21311;&#21517;&#24212;&#29992;&#20110;&#20154;&#31867;&#31227;&#21160;&#24615;&#30340;&#26102;&#31354;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20998;&#24067;&#32423;&#38544;&#31169;&#20445;&#38556;&#12290;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36712;&#36857;&#20840;&#23616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#21450;&#30456;&#37051;&#36712;&#36857;&#28857;&#30340;&#24490;&#29615;&#20108;&#37096;&#22270;&#21305;&#37197;&#65292;CATS &#33021;&#22815;&#20174;&#26465;&#20214;&#37319;&#26679;&#20301;&#32622;&#20013;&#37325;&#26500;&#36712;&#36857;&#25299;&#25169;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#20307;&#36712;&#36857;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;Adversarial Nibbler&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28508;&#22312;&#23545;&#25239;&#36755;&#20837;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23545;&#25552;&#31034;&#21644;&#22270;&#20687;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.11575</link><description>&lt;p&gt;
&#20174;&#23433;&#20840;&#22522;&#20934;&#20013;&#25552;&#28860;&#23545;&#25239;&#24615;&#25552;&#31034;&#65306;&#23545;&#23545;&#25239;&#24615;Nibbler&#25361;&#25112;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge. (arXiv:2309.11575v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;Adversarial Nibbler&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#28508;&#22312;&#23545;&#25239;&#36755;&#20837;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#23545;&#25552;&#31034;&#21644;&#22270;&#20687;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#23545;&#40784;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#38543;&#26426;&#29228;&#21462;&#30340;&#25968;&#21313;&#20159;&#20010;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20063;&#20250;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;&#20316;&#20026;&#23545;Adversarial Nibbler&#25361;&#25112;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;&#30340;&#23433;&#20840;&#22522;&#20934;&#20013;&#25552;&#28860;&#20102;&#19968;&#32452;&#36229;&#36807;1000&#20010;&#28508;&#22312;&#30340;&#23545;&#25239;&#36755;&#20837;&#12290;&#25105;&#20204;&#23545;&#25910;&#38598;&#21040;&#30340;&#25552;&#31034;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#36755;&#20837;&#36807;&#28388;&#22120;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#24403;&#21069;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#24615;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
&lt;/p&gt;</description></item><item><title>BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.11568</link><description>&lt;p&gt;
BTLM-3B-8K: &#19968;&#20010;3B&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;7B&#21442;&#25968;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11568
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bittensor&#35821;&#35328;&#27169;&#22411;, &#21517;&#20026;"BTLM-3B-8K", &#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#25317;&#26377;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;. BTLM-3B-8K&#22312;SlimPajama&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#20026;627B&#20010;token&#65292;&#37319;&#29992;&#20102;2048&#21644;8192&#30340;&#28151;&#21512;&#19978;&#19979;&#25991;&#38271;&#24230;. BTLM-3B-8K&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#29616;&#26377;&#30340;30&#20159;&#21442;&#25968;&#27169;&#22411;&#25552;&#39640;&#20102;2-5.5% &#65292;&#29978;&#33267;&#19982;&#19968;&#20123;70&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#23218;&#32654;. &#21478;&#22806;&#65292;BTLM-3B-8K&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#22312;&#38271;&#24230;&#20026;8192&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;MPT-7B-8K&#21644;XGen-7B-8K. &#25105;&#20204;&#22312;&#28165;&#29702;&#21644;&#21435;&#37325;&#30340;SlimPajama&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#27169;&#22411;&#65292;&#23545;&#181;P&#36229;&#21442;&#25968;&#21644;&#35843;&#24230;&#36827;&#34892;&#20102;&#35843;&#20248;&#65292;&#20351;&#29992;&#20102;ALiBi&#20301;&#32622;&#23884;&#20837;&#21644;SwiGLU&#38750;&#32447;&#24615;. &#22312;Hugging Face&#19978;&#65292;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;70&#20159;&#21442;&#25968;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#26356;&#20542;&#21521;&#20110;&#36136;&#37327;&#22823;&#23567;&#27604;&#20026;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;. &#23558;70&#20159;&#21442;&#25968;&#27169;&#22411;&#21387;&#32553;&#20026;30&#20159;&#21442;&#25968;&#65292;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#37324;&#31243;&#30865;.
&lt;/p&gt;
&lt;p&gt;
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.  On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31070;&#32463;&#24418;&#24577;&#21957;&#35273;&#30005;&#36335;&#20013;&#21957;&#35273;&#35782;&#21035;&#21644;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20256;&#24863;&#22120;&#28418;&#31227;&#21644;&#38750;&#38543;&#26426;&#27979;&#37327;&#21327;&#35758;&#65292;&#38480;&#21046;&#20102;&#23545;&#27668;&#21619;&#35782;&#21035;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#19988;&#27169;&#22411;&#21463;&#38480;&#20110;&#37325;&#22797;&#21576;&#29616;&#30456;&#21516;&#27668;&#20307;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11555</link><description>&lt;p&gt;
&#19968;&#20010;&#31070;&#32463;&#24418;&#24577;&#21957;&#35273;&#30005;&#36335;&#20013;&#21957;&#35273;&#35782;&#21035;&#21644;&#27867;&#21270;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit. (arXiv:2309.11555v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#31070;&#32463;&#24418;&#24577;&#21957;&#35273;&#30005;&#36335;&#20013;&#21957;&#35273;&#35782;&#21035;&#21644;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20256;&#24863;&#22120;&#28418;&#31227;&#21644;&#38750;&#38543;&#26426;&#27979;&#37327;&#21327;&#35758;&#65292;&#38480;&#21046;&#20102;&#23545;&#27668;&#21619;&#35782;&#21035;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#19988;&#27169;&#22411;&#21463;&#38480;&#20110;&#37325;&#22797;&#21576;&#29616;&#30456;&#21516;&#27668;&#20307;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26159;&#30446;&#21069;&#23569;&#25968;&#20855;&#26377;&#28508;&#21147;&#26174;&#33879;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21151;&#32791;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;Imam&#65286;Cleland&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#24418;&#24577;&#26550;&#26500;&#19978;&#36816;&#34892;&#30340;&#21957;&#35273;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21463;&#21040;&#21754;&#20083;&#21160;&#29289;&#21957;&#35273;&#29699;&#20013;&#25551;&#36848;&#30340;&#30005;&#36335;&#30340;&#21551;&#21457;&#12290;&#20182;&#20204;&#36890;&#36807;&#19968;&#32452;&#27668;&#20307;&#20256;&#24863;&#22120;&#35760;&#24405;&#23545;&#27668;&#21619;&#30340;&#19981;&#21516;&#21576;&#29616;&#65292;&#24182;&#36890;&#36807;&#33033;&#20914;&#22122;&#22768;&#23545;&#20854;&#36827;&#34892;&#25200;&#21160;&#26469;&#35780;&#20272;&#31639;&#27861;&#22312;&#8220;&#24555;&#36895;&#22312;&#32447;&#23398;&#20064;&#21644;&#35782;&#21035;&#8221;&#27668;&#20307;&#27668;&#21619;&#21644;&#26080;&#27668;&#21619;&#27668;&#20307;&#65288;&#31616;&#31216;&#8220;&#27668;&#20307;&#8221;&#65289;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#37325;&#22797;&#20102;&#30740;&#31350;&#30340;&#37096;&#20998;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#24433;&#21709;&#19968;&#20123;&#32467;&#35770;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20256;&#24863;&#22120;&#28418;&#31227;&#21644;&#38750;&#38543;&#26426;&#27979;&#37327;&#21327;&#35758;&#65292;&#20351;&#20854;&#23545;&#27668;&#21619;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#30340;&#29992;&#22788;&#26377;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#37325;&#22797;&#21576;&#29616;&#30456;&#21516;&#27668;&#20307;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic computing is one of the few current approaches that have the potential to significantly reduce power consumption in Machine Learning and Artificial Intelligence. Imam &amp; Cleland presented an odour-learning algorithm that runs on a neuromorphic architecture and is inspired by circuits described in the mammalian olfactory bulb. They assess the algorithm's performance in "rapid online learning and identification" of gaseous odorants and odorless gases (short "gases") using a set of gas sensor recordings of different odour presentations and corrupting them by impulse noise. We replicated parts of the study and discovered limitations that affect some of the conclusions drawn. First, the dataset used suffers from sensor drift and a non-randomised measurement protocol, rendering it of limited use for odour identification benchmarks. Second, we found that the model is restricted in its ability to generalise over repeated presentations of the same gas. We demonstrate that the task t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11528</link><description>&lt;p&gt;
&#23398;&#20064;&#20851;&#31995;&#20043;&#38388;&#30340;&#23436;&#25972;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#20197;&#36827;&#34892;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction. (arXiv:2309.11528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#8212;&#8212;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#23454;&#20307;&#21487;&#33021;&#19981;&#21516;&#8212;&#8212;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#20197;&#23454;&#20307;&#26080;&#20851;&#30340;&#26041;&#24335;&#23436;&#25104;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#35768;&#22810;&#27969;&#34892;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#24314;&#27169;&#22270;&#32423;&#29305;&#24449;&#65292;&#32780;&#36793;&#32423;&#20132;&#20114;&#8212;&#8212;&#23588;&#20854;&#26159;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#8212;&#8212;&#21017;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35821;&#20041;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26159;&#36793;&#32423;&#21644;&#23454;&#20307;&#26080;&#20851;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#21363;TACO&#65292;&#26469;&#24314;&#27169;&#19982;&#20854;&#23376;&#22270;&#20869;&#30340;&#25299;&#25169;&#32467;&#26500;&#39640;&#24230;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns,
&lt;/p&gt;</description></item><item><title>TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11527</link><description>&lt;p&gt;
TrueLearn: &#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#20449;&#24687;&#25512;&#33616;&#30340;Python&#24211;&#65288;&#24102;&#26377;&#65288;&#38544;&#24335;&#65289;&#21453;&#39304;&#65289;
&lt;/p&gt;
&lt;p&gt;
TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback. (arXiv:2309.11527v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11527
&lt;/p&gt;
&lt;p&gt;
TrueLearn&#26159;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#26500;&#24314;&#20010;&#24615;&#21270;&#30340;&#20449;&#24687;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#65292;&#21487;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#20351;&#29992;&#12290;&#23427;&#37319;&#29992;&#20102;&#24320;&#25918;&#23398;&#20064;&#32773;&#30340;&#27010;&#24565;&#21644;&#20154;&#24615;&#21270;&#30340;&#29992;&#25143;&#34920;&#36798;&#26041;&#24335;&#65292;&#21516;&#26102;&#25903;&#25345;&#29992;&#25143;&#21487;&#35270;&#21270;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TrueLearn Python&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#25945;&#32946;&#65288;&#25110;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#20449;&#24687;&#65289;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#32452;&#27169;&#22411;&#26159;&#26681;&#25454;&#8220;&#24320;&#25918;&#23398;&#20064;&#32773;&#8221;&#30340;&#27010;&#24565;&#35774;&#35745;&#30340;&#65292;&#20351;&#29992;&#30452;&#35266;&#30340;&#29992;&#25143;&#34920;&#36798;&#12290;&#20026;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#35753;&#29992;&#25143;&#26377;&#25511;&#21046;&#24863;&#65292;TrueLearn&#24211;&#36824;&#21253;&#21547;&#19981;&#21516;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#21487;&#35270;&#21270;&#23398;&#20064;&#32773;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#23558;&#26469;&#29992;&#25143;&#19982;&#33258;&#24049;&#30340;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#19982;&#35813;&#24211;&#19968;&#36215;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20808;&#21069;&#20844;&#24320;&#21457;&#24067;&#30340;&#38544;&#24335;&#21453;&#39304;&#25945;&#32946;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#34913;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#32534;&#30721;&#31034;&#20363;&#20351;&#35813;&#24211;&#23545;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#20154;&#21592;&#21644;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#21644;&#23398;&#20064;&#20998;&#26512;&#20174;&#19994;&#32773;&#37117;&#38750;&#24120;&#26131;&#20110;&#20351;&#29992;&#12290;&#35813;&#24211;&#21644;&#24102;&#26377;&#31034;&#20363;&#30340;&#25903;&#25345;&#25991;&#26723;&#21487;&#22312;https&#65306;//&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https:/
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#22312;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.11510</link><description>&lt;p&gt;
&#20309;&#26102;&#31639;&#26159;&#22522;&#30784;&#27169;&#22411;&#65311;&#65288;arXiv:2309.11510v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
When is a Foundation Model a Foundation Model. (arXiv:2309.11510v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#29992;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#22312;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#20960;&#39033;&#30740;&#31350;&#25253;&#36947;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#21033;&#29992;&#26469;&#33258;Twitter&#21644;PubMed&#31561;&#22312;&#32447;&#25968;&#25454;&#28304;&#30340;&#22270;&#29255;&#36827;&#34892;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#29992;&#20110;&#22270;&#20687;-&#25991;&#26412;&#24314;&#27169;&#12290;&#22522;&#37329;&#20250;&#27169;&#22411;&#26159;&#22823;&#22411;&#12289;&#28145;&#24230;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#24322;&#24120;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#23398;&#20064;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#34920;&#31034;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#19982;&#36739;&#23567;&#30340;&#20256;&#32479;&#28145;&#24230;&#32593;&#32476;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies have reported on the fine-tuning of foundation models for image-text modeling in the field of medicine, utilizing images from online data sources such as Twitter and PubMed. Foundation models are large, deep artificial neural networks capable of learning the context of a specific domain through training on exceptionally extensive datasets. Through validation, we have observed that the representations generated by such models exhibit inferior performance in retrieval tasks within digital pathology when compared to those generated by significantly smaller, conventional deep networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11508</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30701;&#25991;&#26412;&#31572;&#26696;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#30340;&#35780;&#20998;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#12289;&#20027;&#35266;&#30340;&#12289;&#37325;&#22797;&#30340;&#19988;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#23383;&#21270;&#24102;&#26469;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#28044;&#20837;&#65292; greatly increased autograding textual responses&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#20915;&#31574;&#35282;&#33394;&#20132;&#32473;AI&#27169;&#22411;&#24341;&#36215;&#20102;&#20262;&#29702;&#32771;&#34385;&#65292;&#20027;&#35201;&#28304;&#20110;&#28508;&#22312;&#20559;&#35265;&#21644;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;LLMs&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#20854;&#35780;&#20998;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38024;&#23545;&#33258;&#21160;&#30701;&#25991;&#26412;&#31572;&#26696;&#35780;&#20998;&#65288;ASAG&#65289;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#19981;&#21516;&#35838;&#31243;&#30340;&#21508;&#31181;&#35821;&#35328;&#21644;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#34917;&#20805;&#30340;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading of exams is an important, labor intensive, subjective, repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide a valuable tool to provide a complementary perspective, their readiness
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.11506</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36890;&#24120;&#25317;&#26377;&#22823;&#37327;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#22823;&#22411;&#25968;&#25454;&#24211;&#25110;&#20225;&#19994;&#25968;&#25454;&#28246;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#20803;&#25968;&#25454;&#21644;&#20005;&#26684;&#30340;&#35775;&#38382;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#23545;&#25968;&#25454;&#20869;&#23481;&#30340;&#35775;&#38382;&#65292;&#24182;&#22240;&#27492;&#38480;&#21046;&#20102;&#32463;&#20856;&#30340;&#26816;&#32034;&#21644;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#21253;&#21547;&#25968;&#25454;&#26631;&#31614;&#21644;&#25551;&#36848;&#30340;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#21033;&#29992;&#21487;&#29992;&#25110;&#31574;&#21010;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35268;&#21017;&#25110;&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#21015;&#21517;&#21644;&#35789;&#27719;&#25551;&#36848;&#65288;&#25110;&#23427;&#20204;&#30340;&#21521;&#37327;&#23884;&#20837;&#65289;&#20043;&#38388;&#25214;&#21040;&#26368;&#21305;&#37197;&#30340;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#35768;&#22810;&#19994;&#21153;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprises often own large collections of structured data in the form of large databases or an enterprise data lake. Such data collections come with limited metadata and strict access policies that could limit access to the data contents and, therefore, limit the application of classic retrieval and analysis solutions. As a result, there is a need for solutions that can effectively utilize the available metadata. In this paper, we study the problem of matching table metadata to a business glossary containing data labels and descriptions. The resulting matching enables the use of an available or curated business glossary for retrieval and analysis without or before requesting access to the data contents. One solution to this problem is to use manually-defined rules or similarity measures on column names and glossary descriptions (or their vector embeddings) to find the closest match. However, such approaches need to be tuned through manual labeling and cannot handle many business gloss
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.11331</link><description>&lt;p&gt;
Gold-YOLO: &#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;YOLO&#31995;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#12289;&#22686;&#21152;&#25968;&#25454;&#21644;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#22522;&#32447;&#25552;&#21319;&#21040;&#20102;&#26356;&#39640;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#34429;&#28982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#36335;&#24452;&#32858;&#21512;&#32593;&#32476;&#65288;PANet&#65289;&#24050;&#32463;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23454;&#29616;&#12290;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#27169;&#22411;&#21517;&#20026;Gold-YOLO&#65292;&#25552;&#21319;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#33021;&#21147;&#65292;&#24182;&#22312;&#25152;&#26377;&#27169;&#22411;&#23610;&#24230;&#19978;&#23454;&#29616;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#24819;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;YOLO&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;YOLO&#31995;&#21015;&#27169;&#22411;&#21487;&#20197;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;Gold-YOLO-N&#22312;COCO val2017&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;39.9%&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11206</link><description>&lt;p&gt;
&#25552;&#21462;-&#25913;&#20889;-&#22238;&#31572;&#65306;&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#35760;&#24518;&#25152;&#26377;&#19990;&#30028;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20197;&#22686;&#24378;LLMs&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;KGQA&#20013;LLMs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#32570;&#20047;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#29702;&#34920;&#36848;KG&#30693;&#35782;&#65292;&#21363;&#24573;&#30053;&#20102;KG&#34920;&#31034;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;KG&#30693;&#35782;&#36716;&#21270;&#20026;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25991;&#26412;&#21270;&#38472;&#36848;&#65292;&#29992;&#20110;KGQA&#12290;&#22522;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;KGQA&#20219;&#21153;&#30340;&#22686;&#24378;&#22411;KG-to-Text LLMS&#26694;&#26550;&#12290;&#22312;&#20960;&#20010;KGQA&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;KG-to-Text&#22686;&#24378;LLMs&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperfor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#24402;&#22240;&#24615;&#33021;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#21644;&#35774;&#35745;&#32622;&#20449;&#24230;-based&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#25552;&#39640;&#24402;&#22240;&#20934;&#30830;&#24615;&#65292;&#24182;&#32531;&#35299;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.11132</link><description>&lt;p&gt;
&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#30340;&#23545;&#27604;&#20266;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Pseudo Learning for Open-World DeepFake Attribution. (arXiv:2309.11132v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11132
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#24320;&#25918;&#19990;&#30028;&#28145;&#24230;&#20266;&#36896;&#24402;&#22240;&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#24402;&#22240;&#24615;&#33021;&#30340;&#26032;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#21644;&#35774;&#35745;&#32622;&#20449;&#24230;-based&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#25552;&#39640;&#24402;&#22240;&#20934;&#30830;&#24615;&#65292;&#24182;&#32531;&#35299;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20026;&#20266;&#36896;&#38754;&#37096;&#36827;&#34892;&#24402;&#22240;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#22312;GAN&#29983;&#25104;&#30340;&#38754;&#37096;&#26041;&#38754;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#20294;&#19982;&#36523;&#20221;&#20132;&#25442;&#25110;&#34920;&#24773;&#36716;&#31227;&#30456;&#20851;&#30340;&#26356;&#20855;&#23041;&#32961;&#24615;&#30340;&#25915;&#20987;&#20173;&#28982;&#34987;&#24573;&#35270;&#12290;&#32780;&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#26410;&#26631;&#35760;&#38754;&#37096;&#20013;&#38544;&#34255;&#30340;&#20266;&#36896;&#30165;&#36857;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#25512;&#21160;&#30456;&#20851;&#30340;&#21069;&#27839;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Open-World DeepFake Attribution (OW-DFA)&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#23545;&#21508;&#31181;&#31867;&#22411;&#20266;&#36896;&#38754;&#37096;&#30340;&#24402;&#22240;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#20266;&#23398;&#20064;(Contrastive Pseudo Learning, CPL)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;OW-DFA&#20219;&#21153;&#65292;&#36890;&#36807;1)&#24341;&#20837;&#20840;&#23616;-&#23616;&#37096;&#25237;&#31080;&#27169;&#22359;&#26469;&#24341;&#23548;&#19981;&#21516;&#25805;&#32437;&#21306;&#22495;&#30340;&#20266;&#36896;&#38754;&#37096;&#29305;&#24449;&#23545;&#40784;&#65292;2)&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#36719;&#20266;&#26631;&#31614;&#31574;&#30053;&#26469;&#32531;&#35299;&#30001;&#30456;&#20284;&#36896;&#25104;&#30340;&#20266;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by simi
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.10532</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#30340;&#35748;&#30693;&#21551;&#21457;&#31070;&#32463;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Cognitively-Inspired Neural Architecture for Visual Abstract Reasoning Using Contrastive Perceptual and Conceptual Processing. (arXiv:2309.10532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10532
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#36861;&#27714;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#37319;&#29992;&#36845;&#20195;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#32593;&#32476;&#22312;RAVEN&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21551;&#21457;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;&#30001;&#20154;&#31867;&#25277;&#35937;&#25512;&#29702;&#36890;&#24120;&#23558;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20132;&#26367;&#36827;&#34892;&#20316;&#20026;&#28789;&#27963;&#12289;&#36845;&#20195;&#21644;&#21160;&#24577;&#35748;&#30693;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#35266;&#23519;&#25152;&#21551;&#21457;&#12290;&#21463;&#27492;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#35270;&#35273;&#25277;&#35937;&#25512;&#29702;&#24314;&#27169;&#20026;&#19968;&#31181;&#36845;&#20195;&#30340;&#12289;&#33258;&#23545;&#27604;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#36861;&#27714;&#35270;&#35273;&#21050;&#28608;&#30340;&#24863;&#30693;&#21644;&#27010;&#24565;&#22788;&#29702;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#27604;&#24863;&#30693;-&#27010;&#24565;&#32593;&#32476;&#65288;CPCNet&#65289;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#40486;&#25991;&#36827;&#38454;&#30697;&#38453;&#26234;&#21147;&#27979;&#35797;&#30340;&#30697;&#38453;&#25512;&#29702;&#38382;&#39064;&#26469;&#24037;&#20316;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;RAVEN&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CPCNet&#22312;&#20351;&#29992;&#26368;&#24369;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#25152;&#26377;&#24050;&#21457;&#34920;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#21407;&#22987;RAVEN&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#22823;&#37327;&#19988;&#20197;&#21069;&#27809;&#26377;&#34987;&#27880;&#24847;&#21040;&#30340;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new neural architecture for solving visual abstract reasoning tasks inspired by human cognition, specifically by observations that human abstract reasoning often interleaves perceptual and conceptual processing as part of a flexible, iterative, and dynamic cognitive process. Inspired by this principle, our architecture models visual abstract reasoning as an iterative, self-contrasting learning process that pursues consistency between perceptual and conceptual processing of visual stimuli. We explain how this new Contrastive Perceptual-Conceptual Network (CPCNet) works using matrix reasoning problems in the style of the well-known Raven's Progressive Matrices intelligence test. Experiments on the machine learning dataset RAVEN show that CPCNet achieves higher accuracy than all previously published models while also using the weakest inductive bias. We also point out a substantial and previously unremarked class imbalance in the original RAVEN dataset, and we propose a new
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09553</link><description>&lt;p&gt;
&#22240;&#26524;&#25925;&#20107;&#65306;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#23454;&#29616;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis. (arXiv:2309.09553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#25925;&#20107;&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25913;&#36827;&#35270;&#35273;&#25925;&#20107;&#21512;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#21382;&#21490;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#21270;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#36830;&#36143;&#35270;&#35273;&#25925;&#20107;&#30340;&#21512;&#25104;&#36827;&#23637;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#26631;&#39064;&#12289;&#21382;&#21490;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#30340;&#29305;&#24449;&#20316;&#20026;&#29983;&#25104;&#24403;&#21069;&#24103;&#30340;&#26465;&#20214;&#36827;&#34892;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#21382;&#21490;&#24103;&#21644;&#26631;&#39064;&#37117;&#35270;&#20026;&#21516;&#26679;&#30340;&#36129;&#29486;&#65292;&#24182;&#20197;&#30456;&#31561;&#30340;&#26435;&#37325;&#23558;&#23427;&#20204;&#36830;&#25509;&#36215;&#26469;&#65292;&#24573;&#35270;&#20102;&#24182;&#38750;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#37117;&#19982;&#29983;&#25104;&#24403;&#21069;&#24103;&#30456;&#20851;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#25925;&#20107;&#12290;&#35813;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#26631;&#39064;&#12289;&#24103;&#21644;&#24403;&#21069;&#26631;&#39064;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23616;&#37096;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#12290;&#36890;&#36807;&#26681;&#25454;&#36825;&#31181;&#20851;&#31995;&#20998;&#37197;&#26435;&#37325;&#65292;&#22240;&#26524;&#25925;&#20107;&#29983;&#25104;&#24403;&#21069;&#24103;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25925;&#20107;&#29983;&#25104;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;PororoSV&#21644;FlintstonesSV&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The excellent text-to-image synthesis capability of diffusion models has driven progress in synthesizing coherent visual stories. The current state-of-the-art method combines the features of historical captions, historical frames, and the current captions as conditions for generating the current frame. However, this method treats each historical frame and caption as the same contribution. It connects them in order with equal weights, ignoring that not all historical conditions are associated with the generation of the current frame. To address this issue, we propose Causal-Story. This model incorporates a local causal attention mechanism that considers the causal relationship between previous captions, frames, and current captions. By assigning weights based on this relationship, Causal-Story generates the current frame, thereby improving the global consistency of story generation. We evaluated our model on the PororoSV and FlintstonesSV datasets and obtained state-of-the-art FID score
&lt;/p&gt;</description></item><item><title>FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09517</link><description>&lt;p&gt;
FedGKD:&#22312;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37322;&#25918;&#21327;&#20316;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
FedGKD: Unleashing the Power of Collaboration in Federated Graph Neural Networks. (arXiv:2309.09517v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09517
&lt;/p&gt;
&lt;p&gt;
FedGKD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#24449;&#24182;&#24341;&#20837;&#24863;&#30693;&#20840;&#23616;&#21327;&#20316;&#32467;&#26500;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#30001;&#20110;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#22815;&#22312;&#25968;&#25454;&#38548;&#31163;&#22330;&#26223;&#19979;&#25191;&#34892;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#32852;&#37030;&#35757;&#32451;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;GNN&#31995;&#32479;&#20013;&#30340;&#22270;&#24322;&#26500;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32479;&#35745;&#37327;&#26469;&#34920;&#31034;&#23616;&#37096;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#32858;&#21512;&#26426;&#21046;&#23558;&#23427;&#20204;&#32852;&#31995;&#36215;&#26469;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#25928;&#29575;&#26377;&#38480;&#65306;&#20219;&#21153;&#30456;&#20851;&#24615;&#37327;&#21270;&#30340;&#36136;&#37327;&#20302;&#21644;&#21033;&#29992;&#21327;&#20316;&#32467;&#26500;&#30340;&#26080;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedGKD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;GNN&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#22270;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#25552;&#21462;&#26356;&#22909;&#22320;&#25551;&#36848;&#20219;&#21153;&#30456;&#20851;&#24615;&#30340;&#20219;&#21153;&#29305;&#24449;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#24863;&#30693;&#21040;&#20840;&#23616;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;FedGKD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated training of Graph Neural Networks (GNN) has become popular in recent years due to its ability to perform graph-related tasks under data isolation scenarios while preserving data privacy. However, graph heterogeneity issues in federated GNN systems continue to pose challenges. Existing frameworks address the problem by representing local tasks using different statistics and relating them through a simple aggregation mechanism. However, these approaches suffer from limited efficiency from two aspects: low quality of task-relatedness quantification and inefficacy of exploiting the collaboration structure. To address these issues, we propose FedGKD, a novel federated GNN framework that utilizes a novel client-side graph dataset distillation method to extract task features that better describe task-relatedness, and introduces a novel server-side aggregation mechanism that is aware of the global collaboration structure. We conduct extensive experiments on six real-world datasets of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08587</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Compositional Foundation Models for Hierarchical Planning. (arXiv:2309.08587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#30340;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#36890;&#36807;&#31526;&#21495;&#35745;&#21010;&#12289;&#35270;&#39057;&#25193;&#25955;&#21644;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26032;&#29615;&#22659;&#20013;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#38656;&#35201;&#36827;&#34892;&#36328;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#30340;&#23618;&#27425;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#23618;&#27425;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#20998;&#21035;&#23545;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#21160;&#20316;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20849;&#21516;&#35299;&#20915;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22312;&#29615;&#22659;&#20013;&#25166;&#26681;&#30340;&#31526;&#21495;&#35745;&#21010;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#29983;&#25104;&#30340;&#35270;&#39057;&#35745;&#21010;&#36890;&#36807;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#19982;&#35270;&#35273;-&#21160;&#20316;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#22312;&#27492;&#23618;&#27425;&#32467;&#26500;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#24378;&#21046;&#20445;&#25345;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustr
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09683</link><description>&lt;p&gt;
PubMed&#21450;&#20854;&#20182;&#65306;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
PubMed and Beyond: Recent Advances and Best Practices in Biomedical Literature Search. (arXiv:2307.09683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26816;&#32034;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20171;&#32461;&#20102;&#38024;&#23545;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#28385;&#36275;&#20854;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20854;&#20013;&#24456;&#22810;&#21482;&#33021;&#36890;&#36807;&#25991;&#29486;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#26816;&#32034;&#26159;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#22312;&#20808;&#21069;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23558;&#21151;&#33021;&#25193;&#23637;&#21040;&#20102;&#36229;&#36234;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25628;&#32034;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#23545;&#20020;&#24202;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#36824;&#27604;&#36739;&#38476;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#39640;&#25928;&#22320;&#28385;&#36275;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;PubMed&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#25913;&#36827;&#21644;&#20173;&#28982;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20116;&#31181;&#29305;&#23450;&#20449;&#24687;&#38656;&#27714;&#30340;&#25991;&#29486;&#26816;&#32034;&#24037;&#20855;&#65306;1.&#20026;&#24490;&#35777;&#21307;&#23398;&#23547;&#25214;&#39640;&#36136;&#37327;&#20020;&#24202;&#30740;&#31350;&#12290;2.&#20026;&#31934;&#20934;&#21307;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#26816;&#32034;&#22522;&#22240;&#30456;&#20851;&#20449;&#24687;&#12290;3.&#26681;&#25454;&#24847;&#20041;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, inc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31227;&#38500;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36741;&#21161;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21482;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.01701</link><description>&lt;p&gt;
&#21512;&#25104;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#31227;&#38500;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#36741;&#21161;&#25968;&#25454;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data. (arXiv:2307.01701v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31227;&#38500;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36741;&#21161;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21482;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#27491;&#22312;&#25104;&#20026;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20849;&#20139;&#20010;&#20307;&#32423;&#25968;&#25454;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#24433;&#23376;&#24314;&#27169;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38544;&#31169;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#30446;&#21069;&#20551;&#35774;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31867;&#20284;&#20998;&#24067;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#36825;&#24448;&#24448;&#26159;&#19968;&#20010;&#38750;&#24120;&#24378;&#30340;&#20551;&#35774;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#21457;&#29983;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#22914;&#20309;&#31227;&#38500;&#36825;&#20010;&#20551;&#35774;&#65292;&#20197;&#21450;&#22914;&#20309;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20173;&#28982;&#25104;&#21151;&#65292;&#28041;&#21450;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23457;&#35745;&#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#35775;&#38382;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#20551;&#35774;&#21487;&#20197;&#25918;&#26494;&#20197;&#36827;&#34892;&#23454;&#38469;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data is emerging as the most promising solution to share individual-level data while safeguarding privacy. Membership inference attacks (MIAs), based on shadow modeling, have become the standard to evaluate the privacy of synthetic data. These attacks, however, currently assume the attacker to have access to an auxiliary dataset sampled from a similar distribution as the training dataset. This often is a very strong assumption that would make an attack unlikely to happen in practice. We here show how this assumption can be removed and how MIAs can be performed using only the synthetic data. More specifically, in three different attack scenarios using only synthetic data, our results demonstrate that MIAs are still successful, across two real-world datasets and two synthetic data generators. These results show how the strong hypothesis made when auditing synthetic data releases access to an auxiliary dataset - can be relaxed to perform an actual attack.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.16524</link><description>&lt;p&gt;
HNO&#65306;&#29992;&#20110;&#35299;&#20915;PDE&#30340;&#39715;&#29399;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HNO: Hyena Neural Operator for solving PDEs. (arXiv:2306.16524v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;&#39715;&#29399;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#23427;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;PDE&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27714;&#35299;PDE&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#31163;&#25955;&#21270;&#20197;&#35299;&#26512;&#24517;&#35201;&#30340;&#26102;&#31354;&#23610;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;PDE&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#12290;&#31070;&#32463;&#31639;&#23376;&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#33021;&#22815;&#22522;&#20110;&#25968;&#25454;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;&#39715;&#29399;&#65288;Hyena&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#31639;&#23376;&#37319;&#29992;&#30001;&#22810;&#23618;&#24863;&#30693;&#22120;&#21442;&#25968;&#21270;&#30340;&#38271;&#21367;&#31215;&#28388;&#27874;&#22120;&#12290;&#39715;&#29399;&#31639;&#23376;&#26159;&#19968;&#31181;&#20855;&#26377;&#27425;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#25805;&#20316;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#21442;&#25968;&#21270;&#20855;&#26377;&#20840;&#23616;&#24863;&#21463;&#37326;&#30340;&#38271;&#21367;&#31215;&#12290;&#36825;&#31181;&#26426;&#21046;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#65292;&#24182;&#33021;&#22815;&#20026;&#19981;&#21516;&#30340;PDE&#23454;&#20363;&#25552;&#20379;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#12290;&#20026;&#20102;&#34913;&#37327;&#21508;&#20010;&#23618;&#22312;&#35299;&#20915;PDE&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerically solving partial differential equations (PDEs) typically requires fine discretization to resolve necessary spatiotemporal scales, which can be computationally expensive. Recent advances in deep learning have provided a new approach to solving PDEs that involves the use of neural operators. Neural operators are neural network architectures that learn mappings between function spaces and have the capability to solve partial differential equations based on data. This study utilizes a novel neural operator called Hyena, which employs a long convolutional filter that is parameterized by a multilayer perceptron. The Hyena operator is an operation that enjoys sub-quadratic complexity and state space model to parameterize long convolution that enjoys global receptive field. This mechanism enhances the model's comprehension of the input's context and enables data-dependent weight for different PDE instances. To measure how effective the layers are in solving PDEs, we conduct experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#25235;&#21462;&#32467;&#26524;&#65292;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25235;&#21462;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36229;&#36807;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#20934;&#30830;&#30340;&#25235;&#21462;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#25235;&#21462;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14437</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25235;&#21462;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Self-supervised Contrastive Learning Method for Grasp Outcomes Prediction. (arXiv:2306.14437v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#25235;&#21462;&#32467;&#26524;&#65292;&#36890;&#36807;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25235;&#21462;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36229;&#36807;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#39046;&#22495;&#30340;&#28508;&#21147;&#21644;&#20934;&#30830;&#30340;&#25235;&#21462;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#25235;&#21462;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#39044;&#27979;&#25235;&#21462;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20221;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25235;&#21462;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;&#21160;&#24577;&#23383;&#20856;&#21644;&#21160;&#37327;&#26356;&#26032;&#25216;&#26415;&#30340;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#35302;&#35273;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#36798;&#21040;&#20102;81.83%&#30340;&#28385;&#24847;&#20934;&#30830;&#29575;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#20934;&#30830;&#30340;&#25235;&#21462;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#31283;&#23450;&#25235;&#21462;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the effectiveness of contrastive learning methods for predicting grasp outcomes in an unsupervised manner. By utilizing a publicly available dataset, we demonstrate that contrastive learning methods perform well on the task of grasp outcomes prediction. Specifically, the dynamic-dictionary-based method with the momentum updating technique achieves a satisfactory accuracy of 81.83% using data from one single tactile sensor, outperforming other unsupervised methods. Our results reveal the potential of contrastive learning methods for applications in the field of robot grasping and highlight the importance of accurate grasp prediction for achieving stable grasps.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#30340;&#26131;&#21463;&#25915;&#20987;&#35760;&#24405;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#35760;&#24405;&#19982;&#26368;&#36817;&#37051;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10308</link><description>&lt;p&gt;
Achilles' Heels: &#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#20013;&#26131;&#21463;&#25915;&#20987;&#30340;&#35760;&#24405;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Achilles' Heels: Vulnerable Record Identification in Synthetic Data Publishing. (arXiv:2306.10308v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#30340;&#26131;&#21463;&#25915;&#20987;&#35760;&#24405;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#35760;&#24405;&#19982;&#26368;&#36817;&#37051;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20849;&#20139;&#20010;&#20154;&#32423;&#21035;&#25968;&#25454;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#24433;&#23376;&#24314;&#27169;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38544;&#31169;&#39118;&#38505;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#34429;&#28982;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#23427;&#20204;&#38656;&#35201;&#21019;&#24314;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#27169;&#22411;&#26469;&#35780;&#20272;&#21333;&#20010;&#35760;&#24405;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#36890;&#36807;&#22312;&#36873;&#23450;&#30340;&#23569;&#25968;&#35760;&#24405;&#19978;&#36816;&#34892;MIAs&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#36825;&#37324;&#25552;&#20986;&#20102;&#25105;&#20204;&#35748;&#20026;&#26159;&#30446;&#21069;&#20026;&#27490;&#31532;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#26131;&#21463;&#25915;&#20987;&#35760;&#24405;&#35782;&#21035;&#25216;&#26415;&#65292;&#21033;&#29992;&#35760;&#24405;&#19982;&#26368;&#36817;&#37051;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#22120;&#20043;&#38388;&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;&#29305;&#23450;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;MIA&#36873;&#25321;&#21644;&#29305;&#23450;&#21442;&#25968;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data is seen as the most promising solution to share individual-level data while preserving privacy. Shadow modeling-based Membership Inference Attacks (MIAs) have become the standard approach to evaluate the privacy risk of synthetic data. While very effective, they require a large number of datasets to be created and models trained to evaluate the risk posed by a single record. The privacy risk of a dataset is thus currently evaluated by running MIAs on a handful of records selected using ad-hoc methods. We here propose what is, to the best of our knowledge, the first principled vulnerable record identification technique for synthetic data publishing, leveraging the distance to a record's closest neighbors. We show our method to strongly outperform previous ad-hoc methods across datasets and generators. We also show evidence of our method to be robust to the choice of MIA and to specific choice of parameters. Finally, we show it to accurately identify vulnerable records whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16755</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26174;&#33879;&#30340;&#36127;&#38754;&#22768;&#26126;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#26174;&#33879;&#65288;&#26377;&#36259;&#30340;&#65289;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;; &#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#20013;&#28044;&#29616;&#20986;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#28857;&#21644;k&#27425;&#26080;&#32422;&#26463;&#25506;&#38024;&#26469;&#25506;&#27979;LLMs&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21542;&#23450;&#29983;&#25104;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27169;&#24335;&#30340;&#25991;&#26412;&#25552;&#21462;&#21644;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#20197;&#21450;&#20247;&#21253;&#37329;&#26631;&#35821;&#21477;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20027;&#39064;&#29983;&#25104;&#21015;&#34920;&#30340;&#27491;&#30830;&#24615;&#21644;&#26174;&#30528;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25351;&#23548;&#30340;&#25506;&#38024;&#30830;&#23454;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#36127;&#38754;&#38472;&#36848;&#30340;&#36136;&#37327;&#65292;&#19982;&#26080;&#25351;&#23548;&#30340;&#21464;&#20307;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#25552;&#31034;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#36127;&#38754;&#20107;&#23454;&#30340;&#27010;&#24565;&#65292;&#24120;&#24120;&#29983;&#25104;&#35768;&#22810;&#21547;&#31946;&#19981;&#28165;&#30340;&#38472;&#36848;&#65292;&#25110;&#32773;&#24102;&#26377;&#36127;&#38754;&#20851;&#38190;&#35789;&#20294;&#20855;&#26377;&#31215;&#26497;&#24847;&#20041;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the ability of large language models (LLMs) to generate salient (interesting) negative statements about real-world entities; an emerging research topic of the last few years. We probe the LLMs using zero- and k-shot unconstrained probes, and compare with traditional methods for negation generation, i.e., pattern-based textual extractions and knowledge-graph-based inferences, as well as crowdsourced gold statements. We measure the correctness and salience of the generated lists about subjects from different domains. Our evaluation shows that guided probes do in fact improve the quality of generated negatives, compared to the zero-shot variant. Nevertheless, using both prompts, LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13706</link><description>&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#30340;&#20256;&#36755;&#35843;&#24230;&#65306;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Transmission Scheduling: a Monotonicity-driven Deep Reinforcement Learning Approach. (arXiv:2305.13706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13706
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35843;&#24615;&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;6G&#26102;&#20195;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#38382;&#39064;&#12290;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;6G&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#65292;&#38656;&#35201;&#35821;&#20041;&#20256;&#36755;&#26469;&#36830;&#25509;&#20998;&#24067;&#24335;&#35774;&#22791;&#65292;&#20197;&#20445;&#35777;&#24212;&#29992;&#23618;&#24615;&#33021;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#20013;&#20110;&#36890;&#20449;&#23618;&#24615;&#33021;&#12290;&#35821;&#20041;&#22312;&#36825;&#37324;&#26159;&#20449;&#24687;&#20256;&#36755;&#26377;&#29992;&#24615;&#30340;&#34913;&#37327;&#12290;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35821;&#20041;&#24863;&#30693;&#20256;&#36755;&#35843;&#24230;&#24120;&#24120;&#28041;&#21450;&#24222;&#22823;&#30340;&#20915;&#31574;&#31354;&#38388;&#65292;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#26368;&#20248;&#35821;&#20041;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#28982;&#21518;&#26681;&#25454;&#29702;&#35770;&#25351;&#23548;&#21407;&#21017;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#20934;&#31639;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
For cyber-physical systems in the 6G era, semantic communications connecting distributed devices for dynamic control and remote state estimation are required to guarantee application-level performance, not merely focus on communication-centric performance. Semantics here is a measure of the usefulness of information transmissions. Semantic-aware transmission scheduling of a large system often involves a large decision-making space, and the optimal policy cannot be obtained by existing algorithms effectively. In this paper, we first investigate the fundamental properties of the optimal semantic-aware scheduling policy and then develop advanced deep reinforcement learning (DRL) algorithms by leveraging the theoretical guidelines. Our numerical results show that the proposed algorithms can substantially reduce training time and enhance training performance compared to benchmark algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#30340;&#21516;&#24577;&#21464;&#25442;&#12290;&#36890;&#36807;&#23450;&#20041;&#21516;&#24577;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#30340;&#21512;&#24182;&#12289;&#20559;&#24207;&#20851;&#31995;&#21644;&#38381;&#21253;&#36827;&#34892;&#20102;&#36716;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.13135</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#24577;&#21464;&#25442;&#36716;&#21270;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Transforming Geospatial Ontologies by Homomorphisms. (arXiv:2305.13135v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#30340;&#21516;&#24577;&#21464;&#25442;&#12290;&#36890;&#36807;&#23450;&#20041;&#21516;&#24577;&#26144;&#23556;&#65292;&#25105;&#20204;&#23558;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#30340;&#21512;&#24182;&#12289;&#20559;&#24207;&#20851;&#31995;&#21644;&#38381;&#21253;&#36827;&#34892;&#20102;&#36716;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#65292;&#23558;&#19968;&#32452;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#21644;&#19968;&#32452;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#25805;&#20316;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#36827;&#34892;&#30740;&#31350;&#65292;&#32780;&#19981;&#38656;&#35201;&#20851;&#27880;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#21644;&#25805;&#20316;&#30340;&#20869;&#37096;&#32454;&#33410;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#20043;&#38388;&#30340;&#21516;&#24577;&#26144;&#23556;&#65292;&#21363;&#20445;&#25345;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#25805;&#20316;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#23545;&#26412;&#20307;&#38598;&#21512;&#36827;&#34892;&#32858;&#31867;&#65292;&#21363;&#23558;&#38598;&#21512;&#21010;&#20998;&#20026;&#31561;&#20215;&#31867;&#25110;&#24418;&#25104;&#21830;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;&#23884;&#20837;&#25805;&#20316;&#23558;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#31995;&#32479;&#30340;&#21516;&#24577;&#26144;&#23556;&#20998;&#35299;&#20026;&#21830;&#38598;&#21644;&#23884;&#20837;&#26144;&#23556;&#30340;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#22320;&#29702;&#31354;&#38388;&#26412;&#20307;&#21512;&#24182;&#31995;&#32479;&#12289;&#31995;&#32479;&#20013;&#30340;&#33258;&#28982;&#20559;&#24207;&#20851;&#31995;&#20197;&#21450;&#26412;&#20307;&#21512;&#24182;&#38381;&#21253;&#36827;&#34892;&#20102;&#36716;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the geospatial ontologies that we are interested in together as a geospatial ontology system, consisting of a set of the geospatial ontologies and a set of geospatial ontology operations, without any internal details of the geospatial ontologies and their operations being needed, algebraically. A homomorphism between two geospatial ontology systems is a function between two sets of geospatial ontologies in the systems, which preserves the geospatial ontology operations. We view clustering a set of the ontologies as partitioning the set or defining an equivalence relation on the set or forming a quotient set of the set or obtaining the surjective image of the set. Each geospatial ontology system homomorphism can be factored as a surjective clustering to a quotient space, followed by an embedding. Geospatial ontology merging systems, natural partial orders on the systems, and geospatial ontology merging closures in the systems are then transformed under geospatial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AWFSD&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07712</link><description>&lt;p&gt;
AWFSD:&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#22270;&#20687;&#20808;&#39564;&#30340;&#21152;&#36895;Wirtinger&#27969;&#29992;&#20110;&#27850;&#26494;&#39640;&#26031;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AWFSD: Accelerated Wirtinger Flow with Score-based Diffusion Image Prior for Poisson-Gaussian Holographic Phase Retrieval. (arXiv:2305.07712v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AWFSD&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#24674;&#22797;&#26159;&#35768;&#22810;&#30456;&#24178;&#25104;&#20687;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20809;&#23398;&#25104;&#20687;&#31995;&#32479;&#24341;&#36215;&#30340;&#27979;&#37327;&#25968;&#25454;&#34987;&#28151;&#21512;&#30340;&#27850;&#26494;&#21644;&#39640;&#26031;&#22122;&#22768;&#65288;PG&#22122;&#22768;&#65289;&#25152;&#30772;&#22351;&#30340;&#20840;&#24687;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#36895;Wirtinger&#27969;&#30340;&#26032;&#31639;&#27861;&#65292;&#20351;&#29992;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#20808;&#39564;&#65288;AWFSD&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#21253;&#21547;&#25968;&#25454;&#36866;&#37197;&#39033;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;PG&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#21450;&#20854;&#30456;&#24212;&#30340;Lipschitz&#24120;&#25968;&#65292;&#20174;&#32780;&#20445;&#35777;&#23454;&#38469;&#27979;&#37327;&#30340;&#26356;&#20934;&#30830;&#30340;&#25968;&#25454;&#19968;&#33268;&#24615;&#39033;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35780;&#20998;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#33719;&#65288;&#26799;&#24230;&#65289;&#22270;&#20687;&#20808;&#39564;&#20998;&#24067;&#65292;&#23558;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#25105;&#20204;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#24341;&#20837;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#20808;&#39564;&#19982;&#22270;&#20687;&#30456;&#20301;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#38190;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase retrieval (PR) is an essential problem in a number of coherent imaging systems. This work aims at resolving the holographic phase retrieval problem in real world scenarios where the measurements are corrupted by a mixture of Poisson and Gaussian (PG) noise that stems from optical imaging systems. To solve this problem, we develop a novel algorithm based on Accelerated Wirtinger Flow that uses Score-based Diffusion models as the generative prior (AWFSD). In particular, we frame the PR problem as an optimization task that involves both a data fidelity term and a regularization term. We derive the gradient of the PG log-likelihood function along with its corresponding Lipschitz constant, ensuring a more accurate data consistency term for practical measurements. We introduce a generative prior as part of our regularization approach by using a score-based diffusion model to capture (the gradient of) the image prior distribution. We provide theoretical analysis that establishes a criti
&lt;/p&gt;</description></item><item><title>CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12654</link><description>&lt;p&gt;
CoDi: &#28151;&#21512;&#31867;&#22411;&#34920;&#26684;&#29983;&#25104;&#30340;&#20849;&#21516;&#28436;&#21270;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12654
&lt;/p&gt;
&lt;p&gt;
CoDi &#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#24182;&#30456;&#20114;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32465;&#23450;&#65292;&#23637;&#29616;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#27880;&#24847;&#21147;&#34987;&#25918;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#23558;&#32508;&#21512;&#34920;&#26684;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#23581;&#35797;&#24050;&#32463;&#21521;&#21508;&#31181;&#22330;&#26223;&#25193;&#23637;&#12290;&#30001;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#34920;&#26684;&#25968;&#25454;&#32508;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#21464;&#24471;&#22797;&#26434;&#32780;&#30495;&#23454;&#12290;&#20294;&#26159;&#65292;&#24314;&#27169;&#34920;&#26684;&#25968;&#25454;&#30340;&#31163;&#25955;&#21464;&#37327;&#65288;&#21015;&#65289;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20004;&#20010;&#23545;&#27604;&#25193;&#25955;&#27169;&#22411;&#21333;&#29420;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21464;&#37327;&#65288;&#20294;&#30456;&#20114;&#26465;&#20214;&#21270;&#65289;&#12290;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#24444;&#27492;&#35835;&#21462;&#26465;&#20214;&#22312;&#35757;&#32451;&#20013;&#20849;&#21516;&#28436;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#32465;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36127;&#37319;&#26679;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;11&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;8&#20010;&#22522;&#20934;&#26041;&#27861;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861; CoDi &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11530</link><description>&lt;p&gt;
&#36890;&#36807;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Ensuring Trustworthy Medical Artificial Intelligencethrough Ethical and Philosophical Principles. (arXiv:2304.11530v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#21644;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20013;&#24102;&#26469;&#20102;&#26356;&#22810;&#25361;&#25112;&#65292;&#24517;&#39035;&#35299;&#20915;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#31561;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#21150;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#21307;&#30103;&#25252;&#29702;&#26041;&#38754;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#39640;&#21307;&#30103;&#19987;&#23478;&#21644;&#24739;&#32773;&#30340;&#20307;&#39564;&#26469;&#24443;&#24213;&#25913;&#21464;&#20247;&#22810;&#21307;&#30103;&#25252;&#29702;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#22914;&#26524;&#33021;&#22815;&#34920;&#29616;&#20986;&#33394;&#29978;&#33267;&#19982;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#30456;&#24403;&#65292;&#23601;&#21487;&#20197;&#20135;&#29983;&#24040;&#22823;&#30340;&#25928;&#30410;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#20013;&#22269;&#23478;&#21487;&#20197;&#25552;&#20379;&#20808;&#36827;&#30340;&#21307;&#30103;&#25252;&#29702;&#26381;&#21153;&#65292;&#24182;&#35299;&#20915;&#32570;&#20047;&#19987;&#19994;&#21307;&#30103;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#36164;&#28304;&#21644;&#25972;&#20307;&#27835;&#30103;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#25581;&#31034;&#22823;&#37327;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#21307;&#23398;&#25552;&#20379;&#26032;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#20063;&#24102;&#26469;&#20102;&#20960;&#20010;&#20262;&#29702;&#21644;&#21746;&#23398;&#19978;&#30340;&#38382;&#39064;&#65292;&#22914;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#12289;&#33258;&#20027;&#26435;&#12289;&#36131;&#20219;&#21644;&#38382;&#36131;&#21046;&#65292;&#36825;&#20123;&#38382;&#39064;&#24517;&#39035;&#22312;&#23558;&#36825;&#20123;&#24037;&#20855;&#25972;&#21512;&#21040;&#20020;&#24202;&#29615;&#22659;&#20043;&#21069;&#24471;&#21040;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#20197;&#21450;&#32771;&#34385;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#20197;&#30830;&#20445;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#21307;&#30103;&#25252;&#29702;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#20851;&#30340;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#20559;&#35265;&#12289;&#36879;&#26126;&#24230;&#30340;&#38656;&#35201;&#12289;&#33258;&#20027;&#20915;&#31574;&#30340;&#38382;&#39064;&#20197;&#21450;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#22312;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#20445;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#30340;&#26694;&#26550;&#20197;&#21450;&#25351;&#23548;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#32771;&#34385;&#20262;&#29702;&#21407;&#21017;&#30340;&#25351;&#21335;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#23454;&#26045;&#20262;&#29702;&#21644;&#21746;&#23398;&#21407;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#20445;&#24320;&#21457;&#20986;&#31526;&#21512;&#35786;&#25152;&#35774;&#32622;&#30340;&#21463;&#20449;&#20219;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) methods have great potential to revolutionize numerous medical care by enhancing the experience of medical experts and patients. AI based computer-assisted diagnosis tools can have a tremendous benefit if they can outperform or perform similarly to the level of a clinical expert. As a result, advanced healthcare services can be affordable in developing nations, and the problem of a lack of expert medical practitioners can be addressed. AI based tools can save time, resources, and overall cost for patient treatment. Furthermore, in contrast to humans, AI can uncover complex relations in the data from a large set of inputs and even lead to new evidence-based knowledge in medicine. However, integrating AI in healthcare raises several ethical and philosophical concerns, such as bias, transparency, autonomy, responsibility and accountability, which must be addressed before integrating such tools into clinical settings. In this article, we emphasize recent advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13773</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65306;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26356;&#26377;&#25928;&#22320;&#35843;&#24230;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#12290;&#22312;&#31163;&#32447;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#65288;ONTS&#65289;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#36712;&#36947;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26368;&#20339;&#23433;&#25490;&#65292;&#21516;&#26102;&#32771;&#34385;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#20248;&#20808;&#32423;&#65292;&#26368;&#23567;&#21644;&#26368;&#22823;&#28608;&#27963;&#20107;&#20214;&#65292;&#25191;&#34892;&#26102;&#38388;&#26694;&#26550;&#65292;&#21608;&#26399;&#21644;&#25191;&#34892;&#31383;&#21475;&#65292;&#20197;&#21450;&#21355;&#26143;&#30005;&#21147;&#36164;&#28304;&#21644;&#33021;&#37327;&#25910;&#38598;&#21644;&#31649;&#29702;&#30340;&#22797;&#26434;&#24615;&#30340;&#32422;&#26463;&#12290;ONTS&#38382;&#39064;&#24050;&#32463;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#31934;&#30830;&#26041;&#27861;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#35843;&#24230;&#38382;&#39064;&#21644;&#35774;&#26045;&#25918;&#32622;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ONTS&#38382;&#39064;&#30340;MILP&#23454;&#20363;&#23436;&#20840;&#34920;&#31034;&#25104;&#20108;&#20998;&#22270;&#32593;&#32476;&#32467;&#26500;&#26469;&#24212;&#29992;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-Music&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#26032;&#22320;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26465;&#20214;&#22240;&#32032;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#38899;&#20048;&#27874;&#24418;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26500;&#24314;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04456</link><description>&lt;p&gt;
ERNIE-Music: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-Music&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#26032;&#22320;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26465;&#20214;&#22240;&#32032;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#38899;&#20048;&#27874;&#24418;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26500;&#24314;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#20102;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#26080;&#38480;&#21046;&#30340;&#25991;&#26412;&#25552;&#31034;&#30452;&#25509;&#21512;&#25104;&#38899;&#20048;&#27874;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#36129;&#29486;&#65292;&#21363;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26377;&#26465;&#20214;&#30340;&#22240;&#32032;&#65292;&#20197;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#20869;&#30340;&#27874;&#24418;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26469;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#20219;&#21153;&#24471;&#21040;&#20102;&#24369;&#30417;&#30563;&#25216;&#26415;&#30340;&#24110;&#21161;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#38899;&#20048;&#26631;&#31614;&#21644;&#26080;&#32422;&#26463;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the burgeoning interest in diffusion models has led to significant advances in image and speech generation. Nevertheless, the direct synthesis of music waveforms from unrestricted textual prompts remains a relatively underexplored domain. In response to this lacuna, this paper introduces a pioneering contribution in the form of a text-to-waveform music generation model, underpinned by the utilization of diffusion models. Our methodology hinges on the innovative incorporation of free-form textual prompts as conditional factors to guide the waveform generation process within the diffusion model framework. Addressing the challenge of limited text-music parallel data, we undertake the creation of a dataset by harnessing web resources, a task facilitated by weak supervision techniques. Furthermore, a rigorous empirical inquiry is undertaken to contrast the efficacy of two distinct prompt formats for text conditioning, namely, music tags and unconstrained textual description
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.03044</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;Transformers&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#23548;&#31070;&#32463;&#26550;&#26500;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#20063;&#20986;&#29616;&#20102;&#31867;&#20284;&#30340;&#20351;&#29992;Transformers&#30340;&#28526;&#27969;&#65292;&#20294;&#38754;&#20020;&#30528;RL&#30340;&#29305;&#27530;&#35774;&#35745;&#36873;&#25321;&#21644;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;Transformers&#22312;RL&#20013;&#30340;&#21457;&#23637;&#23578;&#26410;&#34987;&#20805;&#20998;&#25581;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22312;RL&#20013;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#21644;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#35752;&#35770;&#20102;&#27599;&#20010;&#23376;&#39046;&#22495;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#21327;&#21516;&#39134;&#34892;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#30524;&#29699;&#36861;&#36394;&#21644;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#39134;&#34892;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24182;&#34892;&#33258;&#20027;&#12290;&#26681;&#25454;&#27880;&#24847;&#21147;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#31995;&#32479;&#21487;&#20197;&#35753;&#39134;&#34892;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#26469;&#36827;&#34892;&#25511;&#21046;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2212.11084</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#21327;&#21516;&#39134;&#34892;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Cooperative Flight Control Using Visual-Attention. (arXiv:2212.11084v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#21327;&#21516;&#39134;&#34892;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#30524;&#29699;&#36861;&#36394;&#21644;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#39134;&#34892;&#21592;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#24182;&#34892;&#33258;&#20027;&#12290;&#26681;&#25454;&#27880;&#24847;&#21147;&#29305;&#24449;&#30340;&#24046;&#24322;&#65292;&#31995;&#32479;&#21487;&#20197;&#35753;&#39134;&#34892;&#21592;&#25110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#26469;&#36827;&#34892;&#25511;&#21046;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39134;&#34892;&#25511;&#21046;&#20013;&#65292;&#20154;&#31867;&#39134;&#34892;&#21592;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21327;&#21516;&#21512;&#20316;&#23454;&#29616;&#20102;&#24182;&#34892;&#33258;&#20027;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#20013;&#23432;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#30524;&#29699;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#39134;&#34892;&#21592;&#19982;&#24182;&#34892;&#31471;&#21040;&#31471;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#31354;&#20013;&#23432;&#25252;&#31995;&#32479;&#23558;&#22240;&#26524;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19982;&#21327;&#20316;&#23618;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24863;&#30693;&#20854;&#27880;&#24847;&#21147;&#29305;&#24449;&#24046;&#24322;&#23454;&#29616;&#20102;&#39134;&#34892;&#21592;&#19982;&#25511;&#21046;&#31995;&#32479;&#20043;&#38388;&#30340;&#24182;&#34892;&#33258;&#20027;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#29305;&#24449;&#36890;&#36807;VisualBackProp&#31639;&#27861;&#35745;&#31639;&#24471;&#21040;&#65292;&#32780;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#29305;&#24449;&#21017;&#36890;&#36807;&#39134;&#34892;&#21592;&#30340;&#30524;&#29699;&#36861;&#36394;&#25110;&#27169;&#20223;&#20154;&#31867;&#39134;&#34892;&#21592;&#35757;&#32451;&#24471;&#21040;&#30340;&#32593;&#32476;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#33719;&#21462;&#12290;&#24403;&#39134;&#34892;&#21592;&#21644;&#31354;&#20013;&#23432;&#25252;&#31995;&#32479;&#30340;&#27880;&#24847;&#21147;&#29305;&#24449;&#19968;&#33268;&#26102;&#65292;&#39134;&#34892;&#21592;&#36827;&#34892;&#25511;&#21046;&#20915;&#31574;&#65307;&#21542;&#21017;&#65292;&#31354;&#20013;&#23432;&#25252;&#31995;&#32479;&#36827;&#34892;&#24178;&#39044;&#24182;&#25509;&#31649;&#39134;&#34892;&#22120;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cooperation of a human pilot with an autonomous agent during flight control realizes parallel autonomy. We propose an air-guardian system that facilitates cooperation between a pilot with eye tracking and a parallel end-to-end neural control system. Our vision-based air-guardian system combines a causal continuous-depth neural network model with a cooperation layer to enable parallel autonomy between a pilot and a control system based on perceived differences in their attention profiles. The attention profiles for neural networks are obtained by computing the networks' saliency maps (feature importance) through the VisualBackProp algorithm, while the attention profiles for humans are either obtained by eye tracking of human pilots or saliency maps of networks trained to imitate human pilots. When the attention profile of the pilot and guardian agents align, the pilot makes control decisions. Otherwise, the air-guardian makes interventions and takes over the control of the aircraft.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.05355</link><description>&lt;p&gt;
&#20998;&#31867;&#25351;&#26631;&#30340;&#20998;&#26512;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24120;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#34913;&#37327;&#30828;&#20915;&#31574;&#36136;&#37327;&#30340;&#26631;&#20934;&#21644;&#24179;&#34913;&#20934;&#30830;&#29575;&#12289;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#12289;F-beta&#20998;&#25968;&#21644;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#31561;&#25351;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#21644;&#20854;&#20182;&#25351;&#26631;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26399;&#26395;&#25104;&#26412;&#65288;EC&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#27599;&#20010;&#32479;&#35745;&#23398;&#20064;&#35838;&#31243;&#20013;&#37117;&#20171;&#32461;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#37117;&#26159;EC&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;EC&#19982;F&#20998;&#25968;&#21644;MCC&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;EC&#25351;&#26631;&#20248;&#20110;&#20256;&#32479;&#25351;&#26631;&#65292;&#22240;&#20854;&#26356;&#20855;&#26377;&#20248;&#38597;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#19988;&#22522;&#20110;&#32479;&#35745;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#26412;&#25991;&#20013;&#20171;&#32461;&#30340;&#25351;&#26631;&#22343;&#29992;&#20110;&#24230;&#37327;&#30828;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#20998;&#31867;&#31995;&#32479;&#36755;&#20986;&#36830;&#32493;&#24471;&#20998;&#65292;&#32780;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#23454;&#36341;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#36825;&#20123;&#36830;&#32493;&#24471;&#20998;&#20013;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#37325;&#26032;&#21019;&#36896;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#24182;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#30740;&#31350;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#20154;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#30340;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#26234;&#33021;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2205.04952</link><description>&lt;p&gt;
&#20998;&#26512;&#29615;&#22659;&#21644;&#31038;&#20132;&#35821;&#22659;&#65292;&#20351;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#36866;&#24212;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts. (arXiv:2205.04952v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#65292;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;&#36890;&#36807;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#37325;&#26032;&#21019;&#36896;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#24182;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#30740;&#31350;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#20154;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#30340;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#26234;&#33021;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#27491;&#24335;&#12289;&#23433;&#38745;&#12289;&#40657;&#26263;&#30340;&#29615;&#22659;&#25110;&#26126;&#20142;&#12289;&#28909;&#38393;&#12289;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#24212;&#35813;&#22914;&#20309;&#35828;&#35805;&#65311;&#36890;&#36807;&#35774;&#35745;&#26426;&#22120;&#20154;&#20197;&#26356;&#31038;&#20132;&#21644;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#24335;&#35828;&#35805;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#20154;&#20204;&#23545;&#36825;&#20123;&#20195;&#29702;&#20154;&#30340;&#24863;&#30693;&#24847;&#35782;&#21644;&#26234;&#33021;&#31243;&#24230;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#22120;&#20154;&#35821;&#38899;&#39118;&#26684;&#20197;&#36798;&#21040;&#31038;&#20132;&#36866;&#24212;&#24615;&#21644;&#29615;&#22659;&#24863;&#30693;&#30340;&#36807;&#31243;&#21644;&#32467;&#26524;&#12290;&#30001;&#20110;&#37326;&#22806;&#35821;&#38899;&#33719;&#21462;&#30340;&#22256;&#38590;&#65292;&#29702;&#35299;&#20154;&#31867;&#22312;&#19981;&#21516;&#22768;&#23398;&#29615;&#22659;&#20013;&#22914;&#20309;&#35843;&#25972;&#33258;&#24049;&#30340;&#22768;&#38899;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;(a) &#22312;&#34394;&#25311;&#30340;Zoom&#29615;&#22659;&#20013;&#25910;&#38598;&#21644;&#39564;&#35777;&#35821;&#38899;&#25968;&#25454;&#20132;&#20114;&#65292;(b) &#25506;&#32034;&#21644;&#32858;&#31867;&#20154;&#31867;&#30340;&#35821;&#38899;&#35805;&#35821;&#20197;&#35782;&#21035;&#20027;&#35201;&#30340;&#35821;&#38899;&#39118;&#26684;&#65292;&#20197;&#21450;(c) &#20351;&#29992;&#25237;&#24433;&#12289;&#28783;&#20809;&#21644;&#22768;&#38899;&#22312;&#20877;&#29616;&#30340;&#29615;&#22659;&#20013;&#27979;&#35797;&#26426;&#22120;&#20154;&#30340;&#35821;&#38899;&#39118;&#26684;&#12290;&#25105;&#20204;&#20197;&#39184;&#39278;&#26381;&#21153;&#22330;&#26223;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;Pepper&#26426;&#22120;&#20154;&#30340;&#19981;&#21516;&#39118;&#26684;&#30340;&#35821;&#38899;&#30340;&#32467;&#26524;&#65292;&#26397;&#30528;&#22312;&#29305;&#23450;&#35821;&#22659;&#19979;&#35828;&#35805;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
How should a robot speak in a formal, quiet and dark, or a bright, lively and noisy environment? By designing robots to speak in a more social and ambient-appropriate manner we can improve perceived awareness and intelligence for these agents. We describe a process and results toward selecting robot voice styles for perceived social appropriateness and ambiance awareness. Understanding how humans adapt their voices in different acoustic settings can be challenging due to difficulties in voice capture in the wild. Our approach includes 3 steps: (a) Collecting and validating voice data interactions in virtual Zoom ambiances, (b) Exploration and clustering human vocal utterances to identify primary voice styles, and (c) Testing robot voice styles in recreated ambiances using projections, lighting and sound. We focus on food service scenarios as a proof-of-concept setting. We provide results using the Pepper robot's voice with different styles, towards robots that speak in a contextually a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#21644;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.02998</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#20256;&#25773;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Propagation for Graph Neural Networks. (arXiv:2205.02998v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#21644;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65292;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#22266;&#23450;&#30340;&#22270;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20449;&#24687;&#31232;&#32570;&#12289;&#22122;&#22768;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#22270;&#25299;&#25169;&#12289;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21021;&#22987;&#36755;&#20837;&#22270;&#21487;&#33021;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#20010;&#24615;&#21270;PageRank&#20256;&#25773;&#30697;&#38453;&#20197;&#21450;&#19979;&#28216;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#26368;&#20248;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#20302;&#31209;&#36924;&#36817;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#20943;&#23569;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#22522;&#20934;&#26041;&#27861;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#21151;&#25928;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved tremendous success in a variety of real-world applications by relying on the fixed graph data as input. However, the initial input graph might not be optimal in terms of specific downstream tasks, because of information scarcity, noise, adversarial attacks, or discrepancies between the distribution in graph topology, features, and groundtruth labels. In this paper, we propose a bi-level optimization approach for learning the optimal graph structure via directly learning the Personalized PageRank propagation matrix as well as the downstream semi-supervised node classification simultaneously. We also explore a low-rank approximation model for further reducing the time complexity. Empirical evaluations show the superior efficacy and robustness of the proposed model over all baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2204.05232</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#26159;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#20197;&#30830;&#23450;&#65306;a)&#27491;&#22312;&#23457;&#26597;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b)&#23646;&#20110;&#21738;&#20010;&#39640;&#32423;&#26041;&#38754;&#65292;c)&#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;ABSA&#30340;&#20247;&#22810;&#20294;&#20998;&#25955;&#30340;&#35821;&#26009;&#24211;&#20351;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#24555;&#36895;&#30830;&#23450;&#26368;&#36866;&#21512;&#29305;&#23450;ABSA&#23376;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;ABSA&#21644;&#20854;&#23376;&#20219;&#21153;&#30340;&#20027;&#35201;&#35821;&#26009;&#24211;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#35821;&#26009;&#24211;&#26102;&#24212;&#32771;&#34385;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25910;&#38598;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#20026;&#26410;&#26469;&#35821;&#26009;&#24211;&#21019;&#24314;&#25552;&#20986;&#24314;&#35758;&#12290;&#26412;&#35843;&#26597;&#23457;&#26680;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;25&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;45&#20010;&#33521;&#35821;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#35265;-&#20107;&#20214;&#22522;&#20934;&#65288;VisEvent&#65289;&#65292;&#36890;&#36807;&#21487;&#35265;&#25668;&#20687;&#26426;&#21644;&#20107;&#20214;&#25668;&#20687;&#26426;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#29289;&#20307;&#36319;&#36394;&#12290;&#26681;&#25454;VisEvent&#65292;&#25105;&#20204;&#23558;&#20107;&#20214;&#27969;&#36716;&#21270;&#20026;&#20107;&#20214;&#22270;&#20687;&#65292;&#24182;&#26500;&#24314;&#20102;30&#22810;&#20010;...</title><link>http://arxiv.org/abs/2108.05015</link><description>&lt;p&gt;
VisEvent&#65306;&#36890;&#36807;&#24103;&#21644;&#20107;&#20214;&#27969;&#30340;&#21327;&#20316;&#23454;&#29616;&#21487;&#38752;&#30340;&#29289;&#20307;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#35265;-&#20107;&#20214;&#22522;&#20934;&#65288;VisEvent&#65289;&#65292;&#36890;&#36807;&#21487;&#35265;&#25668;&#20687;&#26426;&#21644;&#20107;&#20214;&#25668;&#20687;&#26426;&#30340;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#29289;&#20307;&#36319;&#36394;&#12290;&#26681;&#25454;VisEvent&#65292;&#25105;&#20204;&#23558;&#20107;&#20214;&#27969;&#36716;&#21270;&#20026;&#20107;&#20214;&#22270;&#20687;&#65292;&#24182;&#26500;&#24314;&#20102;30&#22810;&#20010;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#35760;&#24405;&#36880;&#24103;&#24378;&#24230;&#22270;&#20687;&#30340;&#21487;&#35265;&#25668;&#20687;&#26426;&#65292;&#29983;&#29289;&#21551;&#21457;&#24335;&#20107;&#20214;&#25668;&#20687;&#26426;&#20135;&#29983;&#19968;&#31995;&#21015;&#24322;&#27493;&#21644;&#31232;&#30095;&#20107;&#20214;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#35265;&#25668;&#20687;&#26426;&#21487;&#20197;&#26356;&#22909;&#22320;&#24863;&#30693;&#32441;&#29702;&#32454;&#33410;&#21644;&#24930;&#21160;&#20316;&#65292;&#32780;&#20107;&#20214;&#25668;&#20687;&#26426;&#21487;&#20197;&#25670;&#33073;&#36816;&#21160;&#27169;&#31946;&#65292;&#24182;&#20855;&#26377;&#26356;&#22823;&#30340;&#21160;&#24577;&#33539;&#22260;&#65292;&#20351;&#20854;&#22312;&#24555;&#36895;&#36816;&#21160;&#21644;&#20302;&#29031;&#26126;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#36825;&#20004;&#31181;&#20256;&#24863;&#22120;&#21487;&#20197;&#30456;&#20114;&#21512;&#20316;&#65292;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#29289;&#20307;&#36319;&#36394;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21487;&#35265;-&#20107;&#20214;&#22522;&#20934;&#65288;&#31216;&#20026;VisEvent&#65289;&#65292;&#22240;&#20026;&#27492;&#20219;&#21153;&#32570;&#20047;&#19968;&#20010;&#30495;&#23454;&#19988;&#20855;&#26377;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;820&#20010;&#35270;&#39057;&#23545;&#65292;&#28085;&#30422;&#20102;&#20302;&#29031;&#26126;&#12289;&#39640;&#36895;&#21644;&#32972;&#26223;&#26434;&#20081;&#22330;&#26223;&#65292;&#24182;&#20998;&#20026;&#35757;&#32451;&#23376;&#38598;&#21644;&#27979;&#35797;&#23376;&#38598;&#65292;&#20998;&#21035;&#21253;&#21547;500&#20010;&#21644;320&#20010;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 b
&lt;/p&gt;</description></item></channel></rss>