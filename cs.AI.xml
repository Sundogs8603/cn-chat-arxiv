<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#24182;&#25903;&#25345;&#20854;&#20256;&#24863;&#22120;&#36816;&#21160;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23398;&#29983;&#19982;&#25945;&#24072;&#20043;&#38388;&#30340;&#20248;&#21270;&#24046;&#36317;&#23613;&#37327;&#23567;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#23398;&#29983;&#39550;&#39542;&#34892;&#20026;&#30340;&#23398;&#20064;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.10014</link><description>&lt;p&gt;
&#25351;&#23548;&#21487;&#25945;&#23548;&#23398;&#29983;&#65306;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#20256;&#24863;&#22120;&#36816;&#21160;&#20195;&#29702;&#39550;&#39542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Coaching a Teachable Student. (arXiv:2306.10014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#24182;&#25903;&#25345;&#20854;&#20256;&#24863;&#22120;&#36816;&#21160;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23398;&#29983;&#19982;&#25945;&#24072;&#20043;&#38388;&#30340;&#20248;&#21270;&#24046;&#36317;&#23613;&#37327;&#23567;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#23398;&#29983;&#39550;&#39542;&#34892;&#20026;&#30340;&#23398;&#20064;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#25945;&#23548;&#20256;&#24863;&#22120;&#36816;&#21160;&#20195;&#29702;&#22312;&#29305;&#26435;&#25945;&#24072;&#20195;&#29702;&#30340;&#30417;&#30563;&#19979;&#39550;&#39542;&#12290;&#30446;&#21069;&#38024;&#23545;&#20256;&#24863;&#22120;&#36816;&#21160;&#20195;&#29702;&#30340;&#33976;&#39311;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#23398;&#29983;&#30340;&#39550;&#39542;&#34892;&#20026;&#23398;&#20064;&#19981;&#20339;&#65292;&#25105;&#20204;&#29468;&#27979;&#36825;&#26159;&#30001;&#20110;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#36755;&#20837;&#12289;&#24314;&#27169;&#33021;&#21147;&#21644;&#20248;&#21270;&#36807;&#31243;&#26041;&#38754;&#22266;&#26377;&#30340;&#24046;&#24322;&#25152;&#33268;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33976;&#39311;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#24182;&#32553;&#23567;&#20256;&#24863;&#22120;&#36816;&#21160;&#20195;&#29702;&#19982;&#20854;&#29305;&#26435;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#35774;&#35745;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#23398;&#29983;&#21487;&#20197;&#23398;&#20064;&#23558;&#20854;&#36755;&#20837;&#29305;&#24449;&#19982;&#25945;&#24072;&#30340;&#29305;&#26435;&#40479;&#30640;&#31354;&#38388;&#23545;&#40784;&#12290;&#28982;&#21518;&#23398;&#29983;&#21487;&#20197;&#20174;&#25945;&#24072;&#23545;&#20869;&#37096;&#34920;&#31034;&#23398;&#20064;&#30340;&#30452;&#25509;&#30417;&#30563;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#25903;&#25345;&#22256;&#38590;&#30340;&#20256;&#24863;&#22120;&#36816;&#21160;&#23398;&#20064;&#20219;&#21153;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#23398;&#29983;&#33410;&#22863;&#30340;&#36741;&#21161;&#25351;&#23548;&#26426;&#21046;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26080;&#30417;&#30563;&#36741;&#21161;&#23398;&#20064;&#26426;&#21046;&#21644;&#19968;&#31181;&#23398;&#29983;&#36873;&#25321;&#30417;&#30563;&#30340;&#33258;&#36866;&#24212;&#33976;&#39311;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel knowledge distillation framework for effectively teaching a sensorimotor student agent to drive from the supervision of a privileged teacher agent. Current distillation for sensorimotor agents methods tend to result in suboptimal learned driving behavior by the student, which we hypothesize is due to inherent differences between the input, modeling capacity, and optimization processes of the two agents. We develop a novel distillation scheme that can address these limitations and close the gap between the sensorimotor agent and its privileged teacher. Our key insight is to design a student which learns to align their input features with the teacher's privileged Bird's Eye View (BEV) space. The student then can benefit from direct supervision by the teacher over the internal representation learning. To scaffold the difficult sensorimotor learning task, the student model is optimized via a student-paced coaching mechanism with various auxiliary supervision. We further 
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10001</link><description>&lt;p&gt;
&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65306;&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Group Orthogonalization Regularization For Vision Models Adaptation and Robustness. (arXiv:2306.10001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31216;&#20026;&#32452;&#27491;&#20132;&#27491;&#21017;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#33258;&#36866;&#24212;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#28145;&#65292;&#21442;&#25968;&#20869;&#37096;&#30340;&#20887;&#20313;&#24230;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#36825;&#31181;&#29616;&#35937;&#23548;&#33268;&#20102;&#35768;&#22810;&#35797;&#22270;&#20943;&#23569;&#21367;&#31215;&#28388;&#27874;&#22120;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#40723;&#21169;&#21516;&#19968;&#23618;&#20869;&#28388;&#27874;&#22120;&#32452;&#20043;&#38388;&#30340;&#27491;&#20132;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#36866;&#24212;&#26041;&#27861;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViTs&#65289;&#26102;&#65292;&#36825;&#31181;&#27491;&#21017;&#21270;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#24403;&#22312;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#21046;&#36827;&#34892;&#32452;&#27491;&#20132;&#24615;&#26102;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/YoavKurtz/GOR&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks become deeper, the redundancy within their parameters increases. This phenomenon has led to several methods that attempt to reduce the correlation between convolutional filters. We propose a computationally efficient regularization technique that encourages orthonormality between groups of filters within the same layer. Our experiments show that when incorporated into recent adaptation methods for diffusion models and vision transformers (ViTs), this regularization improves performance on downstream tasks. We further show improved robustness when group orthogonality is enforced during adversarial training. Our code is available at https://github.com/YoavKurtz/GOR.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(PbRL)&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#25511;&#21046;&#31574;&#30053;&#65292;&#26082;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#21448;&#33021;&#22815;&#20844;&#24179;&#22320;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(FPbRL)&#26041;&#27861;&#12290;FPbRL&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#26032;&#30340;&#31119;&#21033;&#20559;&#22909;&#32780;&#19981;&#26159;PbRL&#20013;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#20559;&#22909;&#26469;&#23398;&#20064;&#19982;&#22810;&#30446;&#26631;&#20851;&#32852;&#30340;&#21521;&#37327;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;FPbRL&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;IEEE Data Port &#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#38382;&#39064;&#65292;&#25552;&#39640;&#29983;&#21629;&#20960;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09989</link><description>&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#30340;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ensemble Framework for Cardiovascular Disease Prediction. (arXiv:2306.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;IEEE Data Port &#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#38382;&#39064;&#65292;&#25552;&#39640;&#29983;&#21629;&#20960;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30149;&#26159;&#20840;&#29699;&#38750;&#20256;&#26579;&#24615;&#21644;&#32516;&#40664;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#24515;&#33039;&#30142;&#30149;&#25110;&#24515;&#34880;&#31649;&#30142;&#30149;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#20896;&#29366;&#21160;&#33033;&#24515;&#33039;&#30149;&#12289;&#24515;&#21147;&#34928;&#31469;&#12289;&#20808;&#22825;&#24615;&#24515;&#33039;&#30149;&#21644;&#24515;&#32908;&#30149;&#12290;&#26089;&#26399;&#20934;&#30830;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#23545;&#20110;&#36991;&#20813;&#36827;&#19968;&#27493;&#25439;&#20260;&#24182;&#25405;&#25937;&#24739;&#32773;&#30340;&#29983;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#21464;&#25104;&#21361;&#26426;&#20043;&#21069;&#39044;&#27979;&#20854;&#21457;&#29983;&#30340;&#31995;&#32479;&#12290;&#26426;&#22120;&#23398;&#20064;&#24341;&#36215;&#20102;&#21307;&#23398;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#24515;&#33039;&#30142;&#30149;&#39044;&#27979;&#65292;&#30740;&#31350;&#20154;&#21592;&#23454;&#26045;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26041;&#27861;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;IEEE Data Port&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;&#21487;&#29992;&#20110;&#24515;&#34880;&#31649;&#30142;&#30149;&#20010;&#20307;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#30001;&#21256;&#29273;&#21033;&#65292;&#20811;&#37324;&#22827;&#20848;&#65292;&#38271;&#28393;VA&#65292;&#29790;&#22763;&#21644;Statlog&#25968;&#25454;&#38598;&#32452;&#25104;&#65292;&#20855;&#26377;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#26368;&#22823;&#24515;&#29575;
&lt;/p&gt;
&lt;p&gt;
Heart disease is the major cause of non-communicable and silent death worldwide. Heart diseases or cardiovascular diseases are classified into four types: coronary heart disease, heart failure, congenital heart disease, and cardiomyopathy. It is vital to diagnose heart disease early and accurately in order to avoid further injury and save patients' lives. As a result, we need a system that can predict cardiovascular disease before it becomes a critical situation. Machine learning has piqued the interest of researchers in the field of medical sciences. For heart disease prediction, researchers implement a variety of machine learning methods and approaches. In this work, to the best of our knowledge, we have used the dataset from IEEE Data Port which is one of the online available largest datasets for cardiovascular diseases individuals. The dataset isa combination of Hungarian, Cleveland, Long Beach VA, Switzerland &amp; Statlog datasets with important features such as Maximum Heart Rate Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09980</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21019;&#24314;&#22810;&#32423;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating Multi-Level Skill Hierarchies in Reinforcement Learning. (arXiv:2306.09980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#25216;&#33021;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20160;&#20040;&#26679;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#23545;&#20110;&#33258;&#20027;&#20195;&#29702;&#20154;&#26159;&#26377;&#29992;&#30340;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#22270;&#24418;&#32467;&#26500;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20998;&#23618;&#22270;&#21010;&#20998;&#26469;&#25581;&#31034;&#22270;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#23618;&#27425;&#19978;&#65292;&#25216;&#33021;&#23558;&#20195;&#29702;&#20154;&#31227;&#21160;&#21040;&#29366;&#24577;&#31354;&#38388;&#20013;&#20114;&#30456;&#36830;&#25509;&#32039;&#23494;&#20294;&#30456;&#20114;&#36830;&#25509;&#36739;&#24369;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#33021;&#23618;&#27425;&#32467;&#26500;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a useful skill hierarchy for an autonomous agent? We propose an answer based on the graphical structure of an agent's interaction with its environment. Our approach uses hierarchical graph partitioning to expose the structure of the graph at varying timescales, producing a skill hierarchy with multiple levels of abstraction. At each level of the hierarchy, skills move the agent between regions of the state space that are well connected within themselves but weakly connected to each other. We illustrate the utility of the proposed skill hierarchy in a wide variety of domains in the context of reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#35821;&#38899;&#34920;&#24449;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#21463;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23884;&#20837;&#19982;&#35828;&#35805;&#20154;&#39564;&#35777;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;MOS&#65292;&#34920;&#26126;Whisper&#27169;&#22411;&#26159;&#26368;&#20026;&#21512;&#36866;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.09979</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#38899;&#34920;&#24449;&#23545;MOS&#39044;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Speech Representations for MOS prediction. (arXiv:2306.09979v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#35821;&#38899;&#34920;&#24449;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#21463;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23884;&#20837;&#19982;&#35828;&#35805;&#20154;&#39564;&#35777;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;MOS&#65292;&#34920;&#26126;Whisper&#27169;&#22411;&#26159;&#26368;&#20026;&#21512;&#36866;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#29992;&#20110;&#39044;&#27979;&#35821;&#38899;&#36136;&#37327;&#30340;&#29305;&#24449;&#25552;&#21462;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#32467;&#26500;&#26469;&#27604;&#36739;&#21463;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23884;&#20837;&#19982;&#35828;&#35805;&#20154;&#39564;&#35777;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;&#24230;&#37327;MOS&#12290;&#25105;&#20204;&#22312;VCC2018&#25968;&#25454;&#38598;&#21644;&#20026;&#36825;&#39033;&#24037;&#20316;&#21019;&#24314;&#30340;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;BRSpeechMOS&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;Whisper&#27169;&#22411;&#22312;&#25152;&#26377;&#22330;&#26223;&#19979;&#37117;&#26159;&#21512;&#36866;&#30340;&#65306;&#20351;&#29992;VCC2018&#21644;BRSpeechMOS&#25968;&#25454;&#38598;&#12290;&#22312;&#20351;&#29992;BRSpeechMOS&#30340;&#21463;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;Whisper-Small&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;0.6980&#65292;&#35828;&#35805;&#20154;&#39564;&#35777;&#27169;&#22411;SpeakerNet&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#20026;0.6963&#12290;&#22312;&#20351;&#29992;VCC2018&#26102;&#65292;&#26368;&#20339;&#30340;&#21463;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;Whisper-Large&#33719;&#24471;&#20102;0.7274&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#32780;&#26368;&#22909;&#30340;&#27169;&#22411;&#35828;&#35805;&#20154;&#39564;&#35777;&#27169;&#22411;TitaNet&#33719;&#24471;&#20102;0.6933&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate feature extraction models for predicting speech quality. We also propose a model architecture to compare embeddings of supervised learning and self-supervised learning models with embeddings of speaker verification models to predict the metric MOS. Our experiments were performed on the VCC2018 dataset and a Brazilian-Portuguese dataset called BRSpeechMOS, which was created for this work. The results show that the Whisper model is appropriate in all scenarios: with both the VCC2018 and BRSpeech- MOS datasets. Among the supervised and self-supervised learning models using BRSpeechMOS, Whisper-Small achieved the best linear correlation of 0.6980, and the speaker verification model, SpeakerNet, had linear correlation of 0.6963. Using VCC2018, the best supervised and self-supervised learning model, Whisper-Large, achieved linear correlation of 0.7274, and the best model speaker verification, TitaNet, achieved a linear correlation of 0.6933. Although the results of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.09970</link><description>&lt;p&gt;
HePCo&#65306;&#29992;&#20110;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#24322;&#26500;&#25552;&#31034;&#21512;&#24182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#19968;&#32452;&#23458;&#25143;&#31471;&#36890;&#20449;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#12290;&#30001;&#20110;&#26469;&#33258;&#36830;&#32493;&#21644;&#32852;&#37030;&#23398;&#20064;&#35282;&#24230;&#30340;&#25361;&#25112;&#65292;&#27492;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21463;&#21040;&#20102;&#21152;&#21095;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36951;&#24536;&#21644;&#24322;&#26500;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#24182;&#20445;&#25345;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#38750;&#32447;&#24615;&#20999;&#25442;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21306;&#20998;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#36807;&#20272;&#35745;&#26410;&#30693;&#21160;&#24577;&#21644;&#25512;&#26029;&#26410;&#30693;&#35268;&#33539;&#65292;&#20197;&#21450;&#20248;&#21270;&#31639;&#27861;&#26469;&#20998;&#26512;&#23398;&#20064;/&#25512;&#26029;&#27169;&#22411;-&#20219;&#21153;&#23545;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#32553;&#23567;&#25512;&#26029;&#35268;&#33539;&#22823;&#23567;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#21306;&#20998;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09966</link><description>&lt;p&gt;
&#21033;&#29992;&#26102;&#38388;&#36923;&#36753;&#25512;&#29702;&#30340;&#38750;&#32447;&#24615;&#20999;&#25442;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Model Discrimination of Switched Nonlinear Systems with Temporal Logic Inference. (arXiv:2306.09966v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#38750;&#32447;&#24615;&#20999;&#25442;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21306;&#20998;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#36807;&#20272;&#35745;&#26410;&#30693;&#21160;&#24577;&#21644;&#25512;&#26029;&#26410;&#30693;&#35268;&#33539;&#65292;&#20197;&#21450;&#20248;&#21270;&#31639;&#27861;&#26469;&#20998;&#26512;&#23398;&#20064;/&#25512;&#26029;&#27169;&#22411;-&#20219;&#21153;&#23545;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24182;&#25552;&#20986;&#32553;&#23567;&#25512;&#26029;&#35268;&#33539;&#22823;&#23567;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#21306;&#20998;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#38024;&#23545;&#26410;&#30693;&#38750;&#32447;&#24615;&#20999;&#25442;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#21306;&#20998;&#38382;&#39064;&#65292;&#20854;&#20013;&#36825;&#20123;&#20999;&#25442;&#31995;&#32479;&#30340;&#27169;&#24335;&#24207;&#21015;&#21463;&#20854;&#20219;&#21153;&#65292;&#21363;&#26410;&#30693;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#35201;&#27714;&#25152;&#25511;&#21046;&#65292;&#21482;&#26377;&#26410;&#30693;&#30340;&#21160;&#24577;&#21644;&#20219;&#21153;&#30340;&#37319;&#26679;&#25968;&#25454;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#36807;&#20272;&#35745;&#26410;&#30693;&#21160;&#24577;&#21644;&#25512;&#26029;&#26410;&#30693;&#35268;&#33539;&#65292;&#20174;&#32780;&#20445;&#35777;&#26410;&#30693;&#21160;&#24577;&#30340;&#38598;&#21512;&#25104;&#21592;&#27169;&#22411;&#21644;LTL&#20844;&#24335;&#22343;&#21253;&#25324;&#30495;&#23454;&#27169;&#22411;&#21644;&#35268;&#33539;/&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#26469;&#20998;&#26512;&#19968;&#32452;&#23398;&#20064;/&#25512;&#26029;&#30340;&#27169;&#22411;-&#20219;&#21153;&#23545;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#20197;&#21450;&#29992;&#20110;&#25490;&#38500;&#22312;&#36816;&#34892;&#26102;&#19982;&#26032;&#35266;&#27979;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;-&#20219;&#21153;&#23545;&#30340;&#27169;&#22411;&#21306;&#20998;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32553;&#23567;&#25512;&#26029;&#35268;&#33539;&#22823;&#23567;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#21306;&#20998;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of data-driven model discrimination for unknown switched systems with unknown linear temporal logic (LTL) specifications, representing tasks, that govern their mode sequences, where only sampled data of the unknown dynamics and tasks are available. To tackle this problem, we propose data-driven methods to over-approximate the unknown dynamics and to infer the unknown specifications such that both set-membership models of the unknown dynamics and LTL formulas are guaranteed to include the ground truth model and specification/task. Moreover, we present an optimization-based algorithm for analyzing the distinguishability of a set of learned/inferred model-task pairs as well as a model discrimination algorithm for ruling out model-task pairs from this set that are inconsistent with new observations at run time. Further, we present an approach for reducing the size of inferred specifications to increase the computational efficiency of the model discriminatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36742;&#20986;&#29616;&#20449;&#24687;&#29983;&#25104;&#28909;&#21147;&#22270;&#30340;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#26041;&#27861;&#65292;&#24182;&#22312;PKLot&#21644;CNRPark-EXT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09940</link><description>&lt;p&gt;
&#22522;&#20110;&#36710;&#36742;&#20986;&#29616;&#30340;&#20572;&#36710;&#20301;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vehicle Occurrence-based Parking Space Detection. (arXiv:2306.09940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36710;&#36742;&#20986;&#29616;&#20449;&#24687;&#29983;&#25104;&#28909;&#21147;&#22270;&#30340;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#26041;&#27861;&#65292;&#24182;&#22312;PKLot&#21644;CNRPark-EXT&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20572;&#36710;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20256;&#24863;&#22120;&#12289;&#30456;&#26426;&#21644;&#25968;&#25454;&#20998;&#26512;&#25552;&#39640;&#20572;&#36710;&#25928;&#29575;&#12289;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#36817;&#24180;&#26469;&#24191;&#27867;&#29992;&#20110;&#20572;&#36710;&#22330;&#31649;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#20294;&#22823;&#22810;&#25968;&#20316;&#21697;&#20551;&#23450;&#20572;&#36710;&#20301;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#24433;&#21709;&#37096;&#32626;&#30340;&#25104;&#26412;&#21644;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#20572;&#36710;&#20301;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25509;&#25910;&#19968;&#20010;&#20572;&#36710;&#22330;&#22270;&#20687;&#24207;&#21015;&#65292;&#24182;&#36820;&#22238;&#19968;&#20010;&#26631;&#35782;&#26816;&#27979;&#21040;&#30340;&#20572;&#36710;&#20301;&#30340;&#22352;&#26631;&#21015;&#34920;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#23454;&#20363;&#20998;&#21106;&#26469;&#35782;&#21035;&#27773;&#36710;&#65292;&#24182;&#20351;&#29992;&#36710;&#36742;&#20986;&#29616;&#26469;&#29983;&#25104;&#20572;&#36710;&#20301;&#30340;&#28909;&#21147;&#22270;&#12290;&#26469;&#33258;PKLot&#21644;CNRPark-EXT&#20572;&#36710;&#22330;&#25968;&#25454;&#38598;&#30340;12&#20010;&#19981;&#21516;&#23376;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#39640;95.60&#65285;&#30340;AP25&#20998;&#25968;&#21644;&#26368;&#39640;79.90&#65285;&#30340;AP50&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart-parking solutions use sensors, cameras, and data analysis to improve parking efficiency and reduce traffic congestion. Computer vision-based methods have been used extensively in recent years to tackle the problem of parking lot management, but most of the works assume that the parking spots are manually labeled, impacting the cost and feasibility of deployment. To fill this gap, this work presents an automatic parking space detection method, which receives a sequence of images of a parking lot and returns a list of coordinates identifying the detected parking spaces. The proposed method employs instance segmentation to identify cars and, using vehicle occurrence, generate a heat map of parking spaces. The results using twelve different subsets from the PKLot and CNRPark-EXT parking lot datasets show that the method achieved an AP25 score up to 95.60\% and AP50 score up to 79.90\%.
&lt;/p&gt;</description></item><item><title>LLMs&#26377;&#28508;&#21147;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#23545;&#31185;&#23398;&#20570;&#20986;&#21464;&#38761;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#26469;&#35299;&#20915;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.09928</link><description>&lt;p&gt;
&#26159;&#21451;&#36824;&#26159;&#25932;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31185;&#23398;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friend or Foe? Exploring the Implications of Large Language Models on the Science System. (arXiv:2306.09928v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09928
&lt;/p&gt;
&lt;p&gt;
LLMs&#26377;&#28508;&#21147;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#23545;&#31185;&#23398;&#20570;&#20986;&#21464;&#38761;&#65292;&#20294;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#26469;&#35299;&#20915;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI&#24320;&#21457;&#30340;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23427;&#23545;&#31185;&#23398;&#21644;&#39640;&#31561;&#25945;&#32946;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25945;&#32946;&#30340;&#24433;&#21709;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#20294;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22522;&#20110;LLMs&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#31185;&#23398;&#21644;&#31185;&#23398;&#23454;&#36341;&#30340;&#24433;&#21709;&#30340;&#23454;&#35777;&#30740;&#31350;&#26377;&#38480;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;Delphi&#30740;&#31350;&#65292;&#28041;&#21450;72&#20301;&#19987;&#38376;&#20174;&#20107;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19987;&#23478;&#12290;&#35813;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;LLMs&#30340;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#31185;&#23398;&#31995;&#32479;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#32771;&#34385;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#26377;&#25928;&#20351;&#29992;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;LLMs&#22312;&#31185;&#23398;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#25919;&#12289;&#21019;&#36896;&#24615;&#21644;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#19982;&#20559;&#35265;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#36136;&#37327;&#20445;&#35777;&#26377;&#20851;&#30340;&#39118;&#38505;&#38656;&#35201;&#36890;&#36807;&#31215;&#26497;&#30340;&#30417;&#31649;&#21644;&#31185;&#23398;&#25945;&#32946;&#21152;&#20197;&#35299;&#20915;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26377;&#20851;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#21644;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24433;&#21709;&#30340;&#30693;&#24773;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 experts specialising in research and AI. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and he
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.09922</link><description>&lt;p&gt;
&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#19982;&#34394;&#25311;&#26426;&#22120;&#20154;&#36807;&#21435;&#21160;&#20316;&#30456;&#20851;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#25191;&#34892;&#38271;&#24207;&#21015;&#30340;&#21160;&#20316;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#36731;&#26494;&#12289;&#21487;&#38752;&#22320;&#20102;&#35299;&#23427;&#20204;&#25152;&#20570;&#30340;&#20107;&#24773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#36807;&#21435;&#21160;&#20316;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#19968;&#20010;&#26680;&#24515;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#19968;&#31995;&#32479;&#34987;&#35757;&#32451;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#34394;&#25311;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#24103;&#21644;&#38382;&#39064;&#25552;&#31034;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#20026;&#20102;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#20851;&#20110;&#23545;&#35937;&#12289;&#21160;&#20316;&#21644;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#26399;&#38388;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#39034;&#24207;&#30340;&#33521;&#25991;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#23558;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#38382;&#39064;&#20351;&#24471;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#30340;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#33021;&#22815;&#25552;&#39640;&#21160;&#20316;&#24635;&#32467;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#20013;&#26631;&#31614;&#25805;&#20316;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20215;&#20307;&#31995;&#30340;&#35266;&#28857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09918</link><description>&lt;p&gt;
&#29702;&#24615;&#19968;&#26080;&#25152;&#24863;&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#37325;&#26032;&#25805;&#20316;&#20013;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference. (arXiv:2306.09918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#20013;&#26631;&#31614;&#25805;&#20316;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20215;&#20307;&#31995;&#30340;&#35266;&#28857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#19968;&#30452;&#26159;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#30707;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;NLI&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#19977;&#20998;&#31867;&#27169;&#24335;&#22312;&#35780;&#20272;&#27169;&#22411;&#25429;&#25417;&#20154;&#31867;&#25512;&#29702;&#30340;&#24494;&#22937;&#20043;&#22788;&#26041;&#38754;&#23384;&#22312;&#24050;&#30693;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;NLI&#25968;&#25454;&#38598;&#20013;&#20013;&#31435;&#26631;&#31614;&#30340;&#25805;&#20316;&#21270;&#25928;&#24230;&#24456;&#20302;&#65292;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#36890;&#24120;&#24573;&#30053;&#33267;&#23569;&#19968;&#20010;&#37325;&#35201;&#30340;&#20013;&#31435;&#24847;&#20041;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#32570;&#28857;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23548;&#33268;&#27880;&#37322;&#25968;&#25454;&#38598;&#23454;&#38469;&#19978;&#38477;&#20302;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#36817;&#30340;&#19968;&#20010;NLI&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#25805;&#20316;&#21270;&#30340;&#27880;&#37322;&#32773;&#30740;&#31350;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#24076;&#26395;&#24341;&#21457;NLP&#31038;&#21306;&#23545;&#20110;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35752;&#35770;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Inference (NLI) has been a cornerstone task in evaluating language models' inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models' ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.09884</link><description>&lt;p&gt;
Jumanji: JAX&#20013;&#19968;&#22871;&#22810;&#26679;&#21270;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX. (arXiv:2306.09884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09884
&lt;/p&gt;
&lt;p&gt;
Jumanji&#26159;JAX&#20013;&#19968;&#22871;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#24555;&#36895;&#12289;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22312;&#25512;&#21160;AI&#31639;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#38656;&#35201;&#27169;&#25311;&#29615;&#22659;&#20855;&#22791;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27169;&#22359;&#21270;&#29305;&#28857;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jumanji&#65292;&#36825;&#26159;&#19968;&#22871;&#35774;&#35745;&#29992;&#20110;&#24555;&#36895;&#12289;&#28789;&#27963;&#21644;&#21487;&#25193;&#23637;&#30340;&#19981;&#21516;RL&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;Jumanji&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#29615;&#22659;&#65292;&#19987;&#27880;&#20110;&#24037;&#19994;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#20197;&#21450;&#25361;&#25112;&#24615;&#30340;&#19968;&#33324;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;JAX&#21644;GPU&#12289;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#29575;&#65292;Jumanji&#33021;&#22815;&#36805;&#36895;&#36845;&#20195;&#30740;&#31350;&#24605;&#36335;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#26368;&#32456;&#36171;&#33021;&#26356;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#22871;&#20214;&#19981;&#21516;&#65292;Jumanji&#20855;&#26377;&#39640;&#24230;&#21487;&#23450;&#21046;&#24615;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#35843;&#25972;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#38382;&#39064;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source reinforcement learning (RL) environments have played a crucial role in driving progress in the development of AI algorithms. In modern RL research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present Jumanji, a suite of diverse RL environments specifically designed to be fast, flexible, and scalable. Jumanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of JAX and hardware accelerators like GPUs and TPUs, Jumanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing RL environment suites, Jumanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Further
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#20559;&#24046;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#23545;&#20110;&#20844;&#24179;&#24615;&#30340;&#20381;&#36182;&#24615;&#21487;&#33021;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.09835</link><description>&lt;p&gt;
&#23384;&#22312;&#20559;&#24046;&#26102;&#22522;&#20110;&#22810;&#20010;&#25490;&#21517;&#30340;&#23376;&#38598;&#36873;&#25321;&#65306;&#22810;&#36194;&#23478;&#25237;&#31080;&#35780;&#20998;&#20989;&#25968;&#30340;&#20844;&#24179;&#38480;&#21046;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subset Selection Based On Multiple Rankings in the Presence of Bias: Effectiveness of Fairness Constraints for Multiwinner Voting Score Functions. (arXiv:2306.09835v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#20559;&#24046;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#23545;&#20110;&#20844;&#24179;&#24615;&#30340;&#20381;&#36182;&#24615;&#21487;&#33021;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#22810;&#20010;&#39033;&#30446;&#30340;&#25490;&#21517;&#65292;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#39640;&#8220;&#36136;&#37327;&#8221;&#30340;&#23376;&#38598;&#12290;&#26469;&#33258;&#22810;&#36194;&#23478;&#25237;&#31080;&#25991;&#29486;&#30340;&#35780;&#20998;&#20989;&#25968;&#24050;&#29992;&#20110;&#23558;&#25490;&#21517;&#32858;&#21512;&#20026;&#23376;&#38598;&#30340;&#36136;&#37327;&#24471;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27492;&#35774;&#32622;&#19979;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#24403;&#25490;&#21517;&#21487;&#33021;&#21253;&#21547;&#23545;&#19968;&#32452;&#39033;&#30446;&#30340;&#31995;&#32479;&#24615;&#25110;&#26080;&#24847;&#35782;&#30340;&#20559;&#35265;&#26102;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20860;&#39038;&#20844;&#24179;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#39640;&#36873;&#25321;&#36136;&#37327;&#12290;&#23545;&#20110;&#36755;&#20837;&#25490;&#21517;&#21644;&#20559;&#35265;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#25105;&#20204;&#34920;&#26126;&#35201;&#27714;&#25152;&#36873;&#30340;&#23376;&#38598;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#36873;&#25321;&#22312;&#27809;&#26377;&#20559;&#35265;&#30340;&#25490;&#21517;&#20013;&#30340;&#36136;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#20110;&#20844;&#24179;&#24615;&#38480;&#21046;&#35201;&#26377;&#25928;&#65292;&#19981;&#21516;&#30340;&#22810;&#36194;&#23478;&#35780;&#20998;&#20989;&#25968;&#21487;&#33021;&#38656;&#35201;&#26497;&#20854;&#19981;&#21516;&#25968;&#37327;&#30340;&#25490;&#21517;&#65306;&#23545;&#20110;&#19968;&#20123;&#20989;&#25968;&#65292;&#20844;&#24179;&#24615;&#38480;&#21046;&#38656;&#35201;&#25351;&#25968;&#32423;&#25968;&#37327;&#30340;&#25490;&#21517;&#25165;&#33021;&#24674;&#22797;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#20989;&#25968;&#65292;&#36825;&#31181;&#20381;&#36182;&#24615;&#20165;&#20026;&#22810;&#39033;&#24335;&#12290;&#36825;&#20010;&#32467;&#26524;&#20381;&#36182;&#20110;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of subset selection where one is given multiple rankings of items and the goal is to select the highest ``quality'' subset. Score functions from the multiwinner voting literature have been used to aggregate rankings into quality scores for subsets. We study this setting of subset selection problems when, in addition, rankings may contain systemic or unconscious biases toward a group of items. For a general model of input rankings and biases, we show that requiring the selected subset to satisfy group fairness constraints can improve the quality of the selection with respect to unbiased rankings. Importantly, we show that for fairness constraints to be effective, different multiwinner score functions may require a drastically different number of rankings: While for some functions, fairness constraints need an exponential number of rankings to recover a close-to-optimal solution, for others, this dependency is only polynomial. This result relies on a novel notion 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PK-iL&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20026;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.09824</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#31243;&#30693;&#35782;&#27880;&#20837;&#30340;&#21307;&#29983;&#21451;&#22909;&#22411;&#35299;&#37322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Process Knowledge-infused Learning for Clinician-friendly Explanations. (arXiv:2306.09824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PK-iL&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20026;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#12290;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#24086;&#23376;&#21644;&#20132;&#27969;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26816;&#27979;&#34920;&#26126;&#25233;&#37057;&#30151;&#12289;&#28966;&#34385;&#30151;&#25110;&#33258;&#26432;&#20542;&#21521;&#31561;&#24515;&#29702;&#20581;&#24247;&#24773;&#20917;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#36890;&#36807;&#20851;&#38190;&#35789;&#12289;&#35821;&#35328;&#26631;&#35760;&#21644;&#24773;&#24863;&#26469;&#27934;&#23519;&#20010;&#20307;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#12289;&#24178;&#39044;&#21644;&#25903;&#25345;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25913;&#21892;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#23545;&#24515;&#29702;&#20581;&#24247;&#36827;&#34892;&#35780;&#20272;&#26377;&#20004;&#20010;&#23616;&#38480;&#24615;&#65306;(1)&#23427;&#20204;&#19981;&#20250;&#23558;&#24086;&#23376;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#35786;&#26029;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;(2)&#20351;&#29992;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#30340;&#27010;&#24565;&#65288;&#21363;&#21307;&#29983;&#21451;&#22909;&#22411;&#35299;&#37322;&#65289;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#27969;&#31243;&#30693;&#35782;&#27880;&#20837;&#30340;&#23398;&#20064;&#65288;PK-iL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have the potential to assess mental health using social media data. By analyzing online posts and conversations, these models can detect patterns indicating mental health conditions like depression, anxiety, or suicidal thoughts. They examine keywords, language markers, and sentiment to gain insights into an individual's mental well-being. This information is crucial for early detection, intervention, and support, improving mental health care and prevention strategies. However, using language models for mental health assessments from social media has two limitations: (1) They do not compare posts against clinicians' diagnostic processes, and (2) It's challenging to explain language model outputs using concepts that the clinician can understand, i.e., clinician-friendly explanations. In this study, we introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm that layers clinical process knowledge structures on language model outputs, enabling clinicia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ORIBA&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#23450;&#21046;&#30340;AI&#20195;&#29702;&#65292;&#33021;&#22815;&#19982;&#25554;&#30011;&#23478;&#30340;&#21407;&#21019;&#35282;&#33394;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#21152;&#24378;&#21019;&#24847;&#39046;&#22495;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2306.09776</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;ORIBA&#23558;&#33402;&#26415;&#23478;&#30340;&#21407;&#21019;&#35282;&#33394;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#28608;&#21457;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Inspire creativity with ORIBA: Transform Artists' Original Characters into Chatbots through Large Language Model. (arXiv:2306.09776v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ORIBA&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21487;&#23450;&#21046;&#30340;AI&#20195;&#29702;&#65292;&#33021;&#22815;&#19982;&#25554;&#30011;&#23478;&#30340;&#21407;&#21019;&#35282;&#33394;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#21152;&#24378;&#21019;&#24847;&#39046;&#22495;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#25554;&#22270;&#33402;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20132;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25554;&#30011;&#23478;&#22914;&#20309;&#19982;&#20195;&#34920;&#20854;&#21407;&#21019;&#35282;&#33394;&#65288;OC&#65289;&#30340;AI&#20195;&#29702;&#36827;&#34892;&#20114;&#21160;&#12290;&#25105;&#20204;&#25512;&#20986;&#20102;&#8220;ORIBA&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20801;&#35768;&#25554;&#30011;&#23478;&#19982;&#20854;OC&#36827;&#34892;&#23545;&#35805;&#12290;&#27492;&#26041;&#27861;&#19981;&#20165;&#20351;&#33402;&#26415;&#23478;&#33021;&#22815;&#25910;&#21040;OC&#30340;&#22238;&#24212;&#65292;&#36824;&#33021;&#35266;&#23519;&#20854;&#20869;&#24515;&#29420;&#30333;&#21644;&#34892;&#20026;&#12290;&#23613;&#31649;&#23384;&#22312;&#30528;&#33402;&#26415;&#23478;&#21644;AI&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#21019;&#26032;&#21512;&#20316;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#21019;&#24847;&#39046;&#22495;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#20854;&#28508;&#22312;&#24212;&#29992;&#21487;&#20197;&#25193;&#23637;&#21040;&#20132;&#20114;&#24335;&#21465;&#20107;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research delves into the intersection of illustration art and artificial intelligence (AI), focusing on how illustrators engage with AI agents that embody their original characters (OCs). We introduce 'ORIBA', a customizable AI chatbot that enables illustrators to converse with their OCs. This approach allows artists to not only receive responses from their OCs but also to observe their inner monologues and behavior. Despite the existing tension between artists and AI, our study explores innovative collaboration methods that are inspiring to illustrators. By examining the impact of AI on the creative process and the boundaries of authorship, we aim to enhance human-AI interactions in creative fields, with potential applications extending beyond illustration to interactive storytelling and more.
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.09752</link><description>&lt;p&gt;
&#31036;&#35980;&#21051;&#26495;&#21360;&#35937;&#21644;&#25915;&#20987;&#21521;&#37327;&#65306;&#26085;&#38889;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09752
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36319;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#20351;&#29992;&#65292;&#24615;&#21035;&#20559;&#35265;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#38750;&#33521;&#35821;&#20559;&#35265;&#30740;&#31350;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#22312;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#24615;&#21035;&#20559;&#35265;&#21644;&#31036;&#35980;&#27700;&#24179;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#22797;&#21046;&#36825;&#20123;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#26495;&#20998;&#26512;&#30007;&#24615;&#21644;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#30340;&#30456;&#23545;&#39044;&#27979;&#27010;&#29575;&#65292;&#24182;&#21457;&#29616;&#38750;&#27491;&#24335;&#31036;&#35980;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#65292;&#32780;&#31895;&#40065;&#21644;&#27491;&#24335;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#30007;&#24615;&#35821;&#27861;&#24615;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#25915;&#20987;&#21521;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25216;&#24039;&#22238;&#36991;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques ab
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.09746</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Temporal Difference Learning with Experience Replay. (arXiv:2306.09746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#39033;&#30340;&#20998;&#35299;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22238;&#25918;&#32531;&#20914;&#21306;&#21644;&#23567;&#25209;&#37327;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#31639;&#27861;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#65292;&#21253;&#25324;&#22343;&#26041;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#38480;&#12290;&#22312;&#32463;&#39564;&#26041;&#38754;&#65292;&#32463;&#39564;&#22238;&#25918;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#25928;&#24212;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39532;&#23572;&#31185;&#22827;&#22122;&#22768;&#39033;&#30340;&#31616;&#21333;&#20998;&#35299;&#65292;&#24182;&#20026;&#20855;&#26377;&#32463;&#39564;&#22238;&#25918;&#30340;TD&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39532;&#23572;&#31185;&#22827;&#35266;&#27979;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#22343;&#36845;&#20195;&#21644;&#26368;&#32456;&#36845;&#20195;&#24773;&#20917;&#19979;&#65292;&#24120;&#25968;&#27493;&#38271;&#24341;&#36215;&#30340;&#35823;&#24046;&#26415;&#35821;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#22823;&#23567;&#21644;&#20174;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#25277;&#26679;&#30340;&#23567;&#25209;&#37327;&#26469;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.09719</link><description>&lt;p&gt;
&#25512;&#21160; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649; ChatGPT &#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#65292;&#20854;&#34920;&#29616;&#20173;&#36828;&#20302;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#34920;&#29616;&#27424;&#20339;&#30340;&#21407;&#22240;&#20027;&#35201;&#26377;&#65306;&#65288;1&#65289;&#25552;&#31034;&#31526;&#20013;&#30340;&#20196;&#29260;&#38480;&#21046;&#19981;&#20801;&#35768;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;ChatGPT &#29983;&#25104;&#24615;&#36136;&#19982; NLP &#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65307;&#65288;3&#65289;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#12289;&#36807;&#24230;&#20851;&#27880;&#29305;&#23450;&#20851;&#38190;&#35789;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#25512;&#21160; ChatGPT &#22312; NLP &#20219;&#21153;&#19978;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#31181;&#36755;&#20837;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#65307;&#65288;2&#65289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#65307;&#65288;3&#65289;&#23558;&#20219;&#21153;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#65307;&#65288;4&#65289;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#25105;&#27880;&#24847;&#21147;&#28151;&#21512;&#31574;&#30053;&#30340;&#25239;&#22122;&#22768;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;DNN&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09718</link><description>&lt;p&gt;
&#33258;&#25105;&#27880;&#24847;&#21147;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#23481;&#24525;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Label-noise-tolerant medical image classification via self-attention and self-supervised learning. (arXiv:2306.09718v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#33258;&#25105;&#27880;&#24847;&#21147;&#28151;&#21512;&#31574;&#30053;&#30340;&#25239;&#22122;&#22768;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;DNN&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#23601;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#20934;&#30830;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#22122;&#22768;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#21307;&#23398;&#22270;&#20687;&#27880;&#37322;&#36807;&#31243;&#20013;&#34987;&#24341;&#20837;&#65292;&#22240;&#20026;&#26631;&#27880;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#27880;&#37322;&#20154;&#21592;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#21516;&#26102;&#65292;DNN&#23481;&#26131;&#36807;&#25311;&#21512;&#22122;&#22768;&#26631;&#31614;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#32452;&#20869;&#27880;&#24847;&#21147;&#28151;&#21512;&#31574;&#30053;&#32435;&#20837;&#22522;&#30784;&#30417;&#30563;&#23398;&#20064;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;&#26377;&#21161;&#20110;&#22686;&#24378;DNN&#30340;&#35270;&#35273;&#34920;&#31034;&#33021;&#21147;&#12290;&#32452;&#20869;&#27880;&#24847;&#21147;&#28151;&#21512;&#27169;&#22359;&#26500;&#24314;&#32452;&#24182;&#20026;&#32452;&#20869;&#26679;&#26412;&#20998;&#37197;&#33258;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#38543;&#21518;&#25972;&#21512;&#21152;&#26435;&#29305;&#24449;&#20197;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;DNN&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been widely applied in medical image classification and achieve remarkable classification performance. These achievements heavily depend on large-scale accurately annotated training data. However, label noise is inevitably introduced in the medical image annotation, as the labeling process heavily relies on the expertise and experience of annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the performance of models. Therefore, in this work, we innovatively devise noise-robust training approach to mitigate the adverse effects of noisy labels in medical image classification. Specifically, we incorporate contrastive learning and intra-group attention mixup strategies into the vanilla supervised learning. The contrastive learning for feature extractor helps to enhance visual representation of DNNs. The intra-group attention mixup module constructs groups and assigns self-attention weights for group-wise samples, and subsequently inte
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.09712</link><description>&lt;p&gt;
&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19982;&#29615;&#22659;&#20132;&#20114;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#24335;&#65306;&#22312;&#32447;&#21644;&#31163;&#32447;&#12290;&#22312;&#32447;&#26041;&#27861;&#25506;&#32034;&#29615;&#22659;&#25152;&#38656;&#26102;&#38388;&#36739;&#38271;&#65292;&#32780;&#31163;&#32447;&#26041;&#27861;&#36890;&#36807;&#29306;&#29298;&#25506;&#32034;&#33021;&#21147;&#26377;&#25928;&#22320;&#33719;&#24471;&#22870;&#21169;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#31163;&#32447;RL&#65292;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#24179;&#28369;&#22320;&#20174;&#31163;&#32447;&#36716;&#25442;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#24179;&#34913;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#24182;&#20026;&#27604;&#36739;&#19981;&#21516;RL&#35774;&#32622;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#22522;&#20110;&#21322;&#31163;&#32447;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21322;&#31163;&#32447;&#26041;&#27861;&#25928;&#29575;&#39640;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29305;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.09668</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Classification using One-versus-One Deep Learning Strategy with Joint Probability Estimates. (arXiv:2306.09668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#30340;&#19968;&#23545;&#19968;&#28145;&#24230;&#23398;&#20064;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#29305;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
One-versus-One&#65288;OvO&#65289;&#31574;&#30053;&#26159;&#19968;&#31181;&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#23427;&#20391;&#37325;&#20110;&#35757;&#32451;&#27599;&#19968;&#23545;&#31867;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;OvO&#22810;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#32852;&#21512;&#27010;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26657;&#20934;&#20108;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#27010;&#29575;&#30340;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#23545;&#20027;&#20307;&#30340;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The One-versus-One (OvO) strategy is an approach of multi-classification models which focuses on training binary classifiers between each pair of classes. While the OvO strategy takes advantage of balanced training data, the classification accuracy is usually hindered by the voting mechanism to combine all binary classifiers. In this paper, a novel OvO multi-classification model incorporating a joint probability measure is proposed under the deep learning framework. In the proposed model, a two-stage algorithm is developed to estimate the class probability from the pairwise binary classifiers. Given the binary classifiers, the pairwise probability estimate is calibrated by a distance measure on the separating feature hyperplane. From that, the class probability of the subject is estimated by solving a joint probability-based distance minimization problem. Numerical experiments in different applications show that the proposed model achieves generally higher classification accuracy than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09662</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20063;&#32463;&#24120;&#26159;&#27425;&#20248;&#30340;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MOMA-DDPG&#65289;&#65292;&#20351;&#29992;&#34928;&#20943;&#26435;&#37325;&#26469;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#30340;&#22810;&#20010;&#22870;&#21169;&#39033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#19968;&#20010;&#20122;&#27954;&#22269;&#23478;&#30340;&#20132;&#36890;&#25668;&#20687;&#22836;&#25910;&#38598;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#23616;&#26234;&#33021;&#20307;&#65292;&#20294;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20998;&#25955;&#30340;&#65292;&#22240;&#20026;&#36825;&#20010;&#26234;&#33021;&#20307;&#22312;&#25512;&#29702;&#38454;&#27573;&#19981;&#20877;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;MOMA-DDPG&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26368;&#23567;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISCUIT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.09643</link><description>&lt;p&gt;
BISCUIT: &#20174;&#20108;&#36827;&#21046;&#20132;&#20114;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
BISCUIT: Causal Representation Learning from Binary Interactions. (arXiv:2306.09643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISCUIT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#21644;&#23454;&#20307;AI&#31561;&#24212;&#29992;&#20013;&#65292;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#22240;&#26524;&#21464;&#37327;&#20197;&#21450;&#22914;&#20309;&#23545;&#23427;&#20204;&#36827;&#34892;&#24178;&#39044;&#20855;&#26377;&#26680;&#24515;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;&#35774;&#32622;&#20013;&#65292;&#20363;&#22914;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411;&#20013;&#20173;&#28982;&#21487;&#20197;&#30830;&#23450;&#22240;&#26524;&#21464;&#37327;&#65292;&#22914;&#26524;&#26234;&#33021;&#20307;&#19982;&#22240;&#26524;&#21464;&#37327;&#30340;&#20132;&#20114;&#21487;&#20197;&#29992;&#26410;&#30693;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#19968;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#25552;&#20986;BISCUIT&#65292;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22240;&#26524;&#21464;&#37327;&#21450;&#20854;&#23545;&#24212;&#20108;&#36827;&#21046;&#20132;&#20114;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;&#22312;&#19977;&#20010;&#26426;&#22120;&#20154;&#21551;&#21457;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;BISCUIT&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#22240;&#26524;&#21464;&#37327;&#65292;&#29978;&#33267;&#21487;&#20197;&#25193;&#23637;&#21040;&#22797;&#26434;&#30340;&#12289;&#36924;&#30495;&#30340;&#23454;&#20307;AI&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the causal variables of an environment and how to intervene on them is of core value in applications such as robotics and embodied AI. While an agent can commonly interact with the environment and may implicitly perturb the behavior of some of these causal variables, often the targets it affects remain unknown. In this paper, we show that causal variables can still be identified for many common setups, e.g., additive Gaussian noise models, if the agent's interactions with a causal variable can be described by an unknown binary variable. This happens when each causal variable has two different mechanisms, e.g., an observational and an interventional one. Using this identifiability result, we propose BISCUIT, a method for simultaneously learning causal variables and their corresponding binary interaction variables. On three robotic-inspired datasets, BISCUIT accurately identifies causal variables and can even be scaled to complex, realistic environments for embodied AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39575;&#65288;Clickbait&#65289;&#20250;&#36890;&#36807;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#29978;&#33267;&#24341;&#20154;&#20837;&#32988;&#30340;&#26631;&#39064;&#26469;&#35825;&#23548;&#29992;&#25143;&#36827;&#34892;&#28857;&#20987;&#65292;&#20960;&#20046;&#28183;&#36879;&#21040;&#25152;&#26377;&#22312;&#32447;&#20869;&#23481;&#21457;&#24067;&#32773;&#65292;&#22914;&#26032;&#38395;&#38376;&#25143;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;NLP&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;LLM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#31995;&#32479;&#36824;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLM&#22312;&#22810;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21644;&#24494;&#35843;PLM&#26041;&#27861;&#30456;&#27604;&#65292;LLM&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20154;&#31867;&#30452;&#35273;&#19981;&#21516;&#65292;&#23454;&#39564;&#34920;&#26126;LLM&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09595</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22270;&#27169;&#22411;&#20808;&#39564;&#19979;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Cooperative Learning with Graphical Model Priors. (arXiv:2306.09595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#35774;&#22791;&#20043;&#38388;&#36890;&#36807;&#21327;&#20316;&#23436;&#25104;&#20998;&#25955;&#20219;&#21153;&#12290;&#36890;&#36807;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#30340;&#21327;&#20316;&#22270;&#65292;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20998;&#25955;&#35774;&#22791;&#19978;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20010;&#24615;&#21270;&#24314;&#27169;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#23616;&#37096;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#32467;&#26500;&#21270;&#21327;&#20316;&#23398;&#20064;&#65288;SCooL&#65289;&#8221;&#65292;&#20854;&#20013;&#19968;&#20010;&#36328;&#35774;&#22791;&#30340;&#21327;&#20316;&#22270;&#30001;&#22270;&#27169;&#22411;&#20808;&#39564;&#29983;&#25104;&#65292;&#20197;&#33258;&#21160;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#12290;&#36890;&#36807;&#36873;&#25321;&#26045;&#21152;&#19981;&#21516;&#32467;&#26500;&#30340;&#22270;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#25512;&#23548;&#20986;&#19968;&#31867;&#20016;&#23500;&#30340;&#29616;&#26377;&#21644;&#26032;&#22411;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#31181; SCooL &#30340;&#31034;&#20363;&#65292;&#22312;&#20854;&#20013;&#20197; Dirac &#20998;&#24067;&#12289;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#21644;&#27880;&#24847;&#21147;&#20316;&#20026;&#29983;&#25104;&#21327;&#20316;&#22270;&#30340;&#20808;&#39564;&#12290;&#36825;&#20123; EM &#31867;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26356;&#26032;&#21327;&#20316;&#22270;&#21644;&#21327;&#21516;&#26412;&#22320;&#27169;&#22411;&#23398;&#20064;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#65292;&#20165;&#36890;&#36807;&#30417;&#35270;&#27169;&#22411;&#26356;&#26032;&#26469;&#20248;&#21270;&#21327;&#20316;&#22270;&#65292;&#20174;&#32780;&#21487;&#20197;&#33258;&#21160;&#25429;&#25417;&#35774;&#22791;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to train personalized models for different tasks on decentralized devices with limited local data. We propose "Structured Cooperative Learning (SCooL)", in which a cooperation graph across devices is generated by a graphical model prior to automatically coordinate mutual learning between devices. By choosing graphical models enforcing different structures, we can derive a rich class of existing and novel decentralized learning algorithms via variational inference. In particular, we show three instantiations of SCooL that adopt Dirac distribution, stochastic block model (SBM), and attention as the prior generating cooperation graphs. These EM-type algorithms alternate between updating the cooperation graph and cooperative learning of local models. They can automatically capture the cross-task correlations among devices by only monitoring their model updating in order to optimize the cooperation graph. We evaluate SCooL and compare it with existing decentralized learning met
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CMLM-CSE&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#36229;&#36234;SimCSE&#12290;</title><link>http://arxiv.org/abs/2306.09594</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;MLM&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;CMLM-CSE
&lt;/p&gt;
&lt;p&gt;
CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CMLM-CSE&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#36229;&#36234;SimCSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27604;&#36739;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;&#30452;&#25509;&#20351;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#21477;&#23376;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#21477;&#23376;&#20027;&#20307;&#65292;&#32780;&#24573;&#30053;&#20102;&#21477;&#23376;&#20013;&#19968;&#20123;&#35789;&#23545;&#21477;&#23376;&#35821;&#20041;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CMLM-CSE&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#22686;&#21152;&#19968;&#20010;&#38468;&#21152;&#30340;&#32593;&#32476;&#26469;&#38598;&#25104;&#21477;&#23376;&#23884;&#20837;&#20197;&#25191;&#34892;MLM&#20219;&#21153;&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#24403;&#20351;&#29992;Bertbase&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#27604;SimCSE&#39640;0.55&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#20351;&#29992;Robertabase&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24179;&#22343;&#36229;&#36807;SimCSE 0.3&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional comparative learning sentence embedding directly uses the encoder to extract sentence features, and then passes in the comparative loss function for learning. However, this method pays too much attention to the sentence body and ignores the influence of some words in the sentence on the sentence semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive learning framework based on conditional MLM. On the basis of traditional contrastive learning, an additional auxiliary network is added to integrate sentence embedding to perform MLM tasks, forcing sentence embedding to learn more masked word information. Finally, when Bertbase was used as the pretraining language model, we exceeded SimCSE by 0.55 percentage points on average in textual similarity tasks, and when Robertabase was used as the pretraining language model, we exceeded SimCSE by 0.3 percentage points on average in textual similarity tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09586</link><description>&lt;p&gt;
&#19968;&#20010;&#32622;&#20449;&#38598;&#30340;&#25968;&#37327;&#26159;&#21542;&#26159;&#19968;&#31181;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#22909;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the Volume of a Credal Set a Good Measure for Epistemic Uncertainty?. (arXiv:2306.09586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#32622;&#20449;&#38598;&#32780;&#38750;&#21333;&#19968;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#24182;&#21457;&#29616;&#65292;&#22312;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#20449;&#20219;&#38598;&#30340;&#20307;&#31215;&#26159;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#34913;&#37327;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#21017;&#27809;&#26377;&#36825;&#31181;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#21644;&#37327;&#21270;&#22312;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#20316;&#20026;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20449;&#20219;&#38598;&#65288;&#19968;&#32452;&#27010;&#29575;&#20998;&#24067;&#30340;&#20984;&#38598;&#65289;&#12290;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#20316;&#20026;$d$&#32500;&#22810;&#38754;&#20307;&#24847;&#21619;&#30528;&#23545;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#20960;&#20309;&#30452;&#35273;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20108;&#20803;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20449;&#20219;&#38598;&#30340;&#20960;&#20309;&#34920;&#31034;&#30340;&#20307;&#31215;&#26159;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#19968;&#31181;&#26377;&#24847;&#20041;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#22312;&#22810;&#31867;&#20998;&#31867;&#26102;&#21017;&#19981;&#37027;&#20040;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25351;&#23450;&#21644;&#20351;&#29992;&#27491;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#20197;&#21450;&#24847;&#35782;&#21040;&#21487;&#33021;&#30340;&#39118;&#38505;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adequate uncertainty representation and quantification have become imperative in various scientific disciplines, especially in machine learning and artificial intelligence. As an alternative to representing uncertainty via one single probability measure, we consider credal sets (convex sets of probability measures). The geometric representation of credal sets as $d$-dimensional polytopes implies a geometric intuition about (epistemic) uncertainty. In this paper, we show that the volume of the geometric representation of a credal set is a meaningful measure of epistemic uncertainty in the case of binary classification, but less so for multi-class classification. Our theoretical findings highlight the crucial role of specifying and employing uncertainty measures in machine learning in an appropriate way, and for being aware of possible pitfalls.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26085;&#35821;&#36825;&#31181;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#26368;&#20339;&#30340;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.09572</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#22312;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#20197;&#26085;&#35821;&#20026;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26085;&#35821;&#36825;&#31181;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#26368;&#20339;&#30340;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#20197;&#26085;&#35821;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36825;&#31181;&#35821;&#35328;&#30340;&#20998;&#35789;&#22120;&#36890;&#24120;&#30001;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#21644;&#23376;&#35789;&#20998;&#35789;&#22120;&#32452;&#25104;&#65292;&#38656;&#35201;&#25105;&#20204;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#32508;&#21512;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#36825;&#31181;&#20840;&#38754;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22823;&#37327;&#30340;&#20998;&#35789;&#22120;&#38598;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;PLM&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#27979;&#37327;&#20102;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#26368;&#20339;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#32780;&#19981;&#26159;WordPiece&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;CAE&#25968;&#25454;&#36716;&#20026;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#27169;&#25311;&#25968;&#25454;&#24182;&#31361;&#20986;&#26410;&#25506;&#32034;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#23545;&#19981;&#21516;&#35774;&#35745;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#20851;&#27880;&#20102;&#36710;&#36742;&#30896;&#25758;&#23433;&#20840;&#24615;&#20998;&#26512;&#20013;&#30340;&#36127;&#33655;&#36335;&#24452;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09538</link><description>&lt;p&gt;
&#29992;&#20110;&#21327;&#21161;&#36710;&#36742;&#30896;&#25758;&#27169;&#25311;&#25968;&#25454;&#20998;&#26512;&#30340;&#22270;&#24418;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Graph Extraction for Assisting Crash Simulation Data Analysis. (arXiv:2306.09538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;CAE&#25968;&#25454;&#36716;&#20026;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#27169;&#25311;&#25968;&#25454;&#24182;&#31361;&#20986;&#26410;&#25506;&#32034;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#23545;&#19981;&#21516;&#35774;&#35745;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#20851;&#27880;&#20102;&#36710;&#36742;&#30896;&#25758;&#23433;&#20840;&#24615;&#20998;&#26512;&#20013;&#30340;&#36127;&#33655;&#36335;&#24452;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35745;&#31639;&#26426;&#36741;&#21161;&#24037;&#31243;&#65288;CAE&#65289;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#36716;&#25442;&#25104;&#22270;&#24418;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#22270;&#24418;&#21270;&#21576;&#29616;CAE&#25968;&#25454;&#21487;&#20197;&#20248;&#21270;&#35774;&#35745;&#25351;&#21335;&#12289;&#25903;&#25345;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#27169;&#25311;&#25968;&#25454;&#26469;&#31361;&#20986;&#26410;&#25506;&#32034;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#24182;&#23545;&#19981;&#21516;&#35774;&#35745;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36710;&#36742;&#35774;&#35745;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#23376;&#23398;&#31185;&#8212;&#8212;&#30896;&#25758;&#23433;&#20840;&#24615;&#20998;&#26512;&#20013;&#30340;&#36127;&#33655;&#36335;&#24452;&#12290;&#36127;&#33655;&#36335;&#24452;&#26159;&#21560;&#25910;&#30896;&#25758;&#33021;&#37327;&#30340;&#22823;&#37096;&#20998;&#38646;&#37096;&#20214;&#30340;&#39034;&#24207;&#12290;&#20026;&#20102;&#26816;&#27979;&#36127;&#33655;&#36335;&#24452;&#65292;&#25105;&#20204;&#20174;CAE&#25968;&#25454;&#29983;&#25104;&#20102;&#19968;&#20010;&#26377;&#21521;&#24102;&#26435;&#22270;&#12290;&#33410;&#28857;&#20195;&#34920;&#36710;&#36742;&#38646;&#37096;&#20214;&#65292;&#36793;&#20195;&#34920;&#38646;&#37096;&#20214;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#25277;&#35937;&#12290;&#36793;&#30340;&#26041;&#21521;&#36981;&#24490;&#30896;&#25758;&#30340;&#26102;&#38388;&#39034;&#24207;&#65292;&#20854;&#20013;&#36793;&#30340;&#26435;&#37325;&#21453;&#26144;&#20102;&#33021;&#37327;&#21560;&#25910;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#22270;&#24418;&#25552;&#21462;&#26041;&#27861;&#21644;&#19968;&#31181;&#36827;&#19968;&#27493;&#26356;&#26032;&#27599;&#20010;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#24207;&#21015;&#25968;&#25454;&#26469;&#25552;&#39640;CAE&#25968;&#25454;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we establish a method for abstracting information from Computer Aided Engineering (CAE) into graphs. Such graph representations of CAE data can improve design guidelines and support recommendation systems by enabling the comparison of simulations, highlighting unexplored experimental designs, and correlating different designs. We focus on the load-path in crashworthiness analysis, a complex sub-discipline in vehicle design. The load-path is the sequence of parts that absorb most of the energy caused by the impact. To detect the load-path, we generate a directed weighted graph from the CAE data. The vertices represent the vehicle's parts, and the edges are an abstraction of the connectivity of the parts. The edge direction follows the temporal occurrence of the collision, where the edge weights reflect aspects of the energy absorption. We introduce and assess three methods for graph extraction and an additional method for further updating each graph with the sequences of a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.09525</link><description>&lt;p&gt;
&#21033;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#35299;&#37322;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#27861;&#24459;&#24320;&#25918;&#24615;&#26415;&#35821;&#30340;&#21547;&#20041;&#26159;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20808;&#21069;&#27861;&#38498;&#26696;&#20363;&#20013;&#35813;&#26415;&#35821;&#30340;&#24212;&#29992;&#26159;&#35299;&#37322;&#20854;&#21547;&#20041;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;GPT-4&#34987;&#30452;&#25509;&#35201;&#27714;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#30340;&#22522;&#20934;&#35774;&#32622;&#30340;&#24615;&#33021;&#19982;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#34987;&#29992;&#26469;&#20026;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#65292;&#21363;&#26469;&#33258;&#26696;&#20363;&#27861;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#24212;&#29992;GPT-4&#20135;&#29983;&#30340;&#35299;&#37322;&#22312;&#34920;&#38754;&#19978;&#20284;&#20046;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#37322;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#24182;&#20284;&#20046;&#28040;&#38500;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21457;&#26126;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09520</link><description>&lt;p&gt;
&#38024;&#23545;&#28508;&#22312;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#32467;&#26524;&#30340;&#26356;&#32039;&#23494;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30830;&#20999;&#20010;&#20307;&#27835;&#30103;&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;&#24456;&#23569;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25913;&#36827;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#32467;&#26524;&#21306;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#26377;&#26102;&#20250;&#32473;&#20986;&#26080;&#20449;&#24687;&#37327;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;Caus-Modens&#65292;&#29992;&#20110;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#32479;&#35745;&#21644;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;Caus-Modens&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20998;&#31163;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#21306;&#38388;&#22823;&#23567;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#35206;&#30422;&#29575;&#12290;&#26368;&#21518;&#19968;&#20010;&#22522;&#20934;&#26159;&#20351;&#29992;&#26410;&#30693;&#20294;&#21487;&#25506;&#26126;&#30340;&#22522;&#30784;&#20107;&#23454;&#24320;&#23637;&#35266;&#23519;&#23454;&#39564;&#30340;GPT-4&#30340;&#26032;&#22411;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.09509</link><description>&lt;p&gt;
Granger&#22240;&#26524;&#30340;&#20998;&#23618;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Granger-Causal Hierarchical Skill Discovery. (arXiv:2306.09509v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#36973;&#21463;&#20302;&#26679;&#26412;&#25928;&#29575;&#21644;&#26377;&#38480;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#20132;&#20114;&#26816;&#27979;&#22120;&#26469;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#12290;&#21463;Granger&#22240;&#26524;&#24615;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#25429;&#25417;&#21040;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23398;&#20064;&#26377;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;HIntS - &#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#20854;&#20182;RL&#21644;HRL&#26041;&#27861;&#37117;&#34920;&#29616;&#19981;&#20339;&#12290;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#19981;&#20165;&#23637;&#31034;&#20102;&#20351;&#29992;Breakout&#30340;&#21464;&#20307;&#30340;&#36716;&#31227;&#65292;&#32780;&#19988;&#19982;&#21487;&#27604;&#36739;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30456;&#27604;&#65292;&#36824;&#34920;&#29616;&#20986;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;HIntS&#19968;&#36215;&#35777;&#26126;&#20102;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown promising results learning policies for complex tasks, but can often suffer from low sample efficiency and limited transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm, which uses learned interaction detectors to discover and train a hierarchy of skills that manipulate factors in factored environments. Inspired by Granger causality, these unsupervised detectors capture key events between factors to sample efficiently learn useful skills and transfer those skills to other related tasks -- tasks where many reinforcement learning techniques struggle. We evaluate HIntS on a robotic pushing task with obstacles -- a challenging domain where other RL and HRL methods fall short. The learned skills not only demonstrate transfer using variants of Breakout, a common RL benchmark, but also show 2-3x improvement in both sample efficiency and final performance compared to comparable RL baselines. Together, HIntS demonstrates a proof of co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2306.09491</link><description>&lt;p&gt;
&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#26816;&#27979;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Feature Selection and Construction Method for Detection of Wind Turbine Generator Heating Faults. (arXiv:2306.09491v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09491
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#39044;&#22788;&#29702;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26377;&#25928;&#35774;&#35745;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#29305;&#24449;&#26500;&#36896;&#21644;&#36873;&#21462;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#39118;&#21147;&#28065;&#36718;&#26426;&#21457;&#30005;&#26426;&#21152;&#28909;&#25925;&#38556;&#30340;&#29305;&#24449;&#36873;&#21462;&#21644;&#26500;&#36896;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#30417;&#25511;&#25511;&#21046;&#21644;&#25968;&#25454;&#37319;&#38598;&#65288;SCADA&#65289;&#31995;&#32479;&#25910;&#38598;&#25968;&#25454;&#65292;&#24314;&#31435;&#21253;&#21547;&#39118;&#21147;&#29305;&#24449;&#12289;&#25805;&#20316;&#25968;&#25454;&#12289;&#28201;&#24230;&#27979;&#37327;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#21407;&#22987;&#29305;&#24449;&#65292;&#24182;&#22312;&#29305;&#24449;&#26500;&#36896;&#27493;&#39588;&#20013;&#21019;&#24314;&#26032;&#29305;&#24449;&#20197;&#33719;&#24471;&#26356;&#26377;&#21147;&#30340;&#25925;&#38556;&#25351;&#31034;&#20449;&#24687;&#12290;&#26500;&#36896;&#26032;&#29305;&#24449;&#21518;&#65292;&#37319;&#29992;&#28151;&#21512;&#29305;&#24449;&#36873;&#21462;&#25216;&#26415;&#22312;&#25972;&#20010;&#29305;&#24449;&#38598;&#20013;&#25214;&#20986;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#31934;&#24230;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preprocessing of information is an essential step for the effective design of machine learning applications. Feature construction and selection are powerful techniques used for this aim. In this paper, a feature selection and construction approach is presented for the detection of wind turbine generator heating faults. Data were collected from Supervisory Control and Data Acquisition (SCADA) system of a wind turbine. The original features directly collected from the data collection system consist of wind characteristics, operational data, temperature measurements and status information. In addition to these original features, new features were created in the feature construction step to obtain information that can be more powerful indications of the faults. After the construction of new features, a hybrid feature selection technique was implemented to find out the most relevant features in the overall set to increase the classification accuracy and decrease the computational burden. Fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35270;&#39057;&#22797;&#21046;&#26816;&#27979;&#21644;&#23450;&#20301;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#27861;&#24182;&#20998;&#26512;&#20102;&#25361;&#25112;&#32467;&#26524;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09489</link><description>&lt;p&gt;
2023&#24180;&#35270;&#39057;&#30456;&#20284;&#24230;&#25968;&#25454;&#38598;&#21450;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The 2023 Video Similarity Dataset and Challenge. (arXiv:2306.09489v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35270;&#39057;&#22797;&#21046;&#26816;&#27979;&#21644;&#23450;&#20301;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26041;&#27861;&#24182;&#20998;&#26512;&#20102;&#25361;&#25112;&#32467;&#26524;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35270;&#39057;&#22797;&#21046;&#26816;&#27979;&#21644;&#23450;&#20301;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#30830;&#23450;&#19968;&#20010;&#26597;&#35810;&#35270;&#39057;&#26159;&#21542;&#19982;&#19968;&#20010;&#21442;&#32771;&#35270;&#39057;&#20849;&#20139;&#20869;&#23481;&#65288;&#8220;&#26816;&#27979;&#8221;&#65289;&#65292;&#24182;&#22312;&#27599;&#20010;&#35270;&#39057;&#20013;&#23450;&#20301;&#20849;&#20139;&#30340;&#20869;&#23481;&#65288;&#8220;&#23450;&#20301;&#8221;&#65289;&#12290;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#27169;&#25311;&#20102;&#19968;&#20010;&#29616;&#23454;&#20013;&#30340;&#22823;&#28023;&#25438;&#38024;&#30340;&#24773;&#22659;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26597;&#35810;&#21644;&#21442;&#32771;&#35270;&#39057;&#37117;&#26159;&#21253;&#21547;&#27809;&#26377;&#22797;&#21046;&#20869;&#23481;&#30340;&#8220;&#24178;&#25200;&#39033;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21453;&#26144;&#26816;&#27979;&#21644;&#23450;&#20301;&#20934;&#30830;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#30456;&#20851;&#30340;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#30456;&#24212;&#30340;&#36712;&#36947;&#65292;&#27599;&#20010;&#36712;&#36947;&#37117;&#26377;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#32447;&#30340;&#23454;&#29616;&#20195;&#30721;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25361;&#25112;&#20013;&#25490;&#21517;&#21069;&#20960;&#21517;&#30340;&#25552;&#20132;&#32467;&#26524;&#21644;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#12289;&#22522;&#32447;&#26041;&#27861;&#21644;&#35780;&#20272;&#20195;&#30721;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#65292;&#23558;&#36827;&#34892;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a dataset, benchmark, and challenge for the problem of video copy detection and localization. The problem comprises two distinct but related tasks: determining whether a query video shares content with a reference video ("detection"), and additionally temporally localizing the shared content within each video ("localization"). The benchmark is designed to evaluate methods on these two tasks, and simulates a realistic needle-in-haystack setting, where the majority of both query and reference videos are "distractors" containing no copied content. We propose a metric that reflects both detection and localization accuracy. The associated challenge consists of two corresponding tracks, each with restrictions that reflect real-world settings. We provide implementation code for evaluation and baselines. We also analyze the results and methods of the top submissions to the challenge. The dataset, baseline methods and evaluation code is publicly available and will be discus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#20837;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#26469;&#26377;&#25928;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#29289;&#20307;&#65292;&#36824;&#21253;&#25324;&#25277;&#35937;&#27010;&#24565;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09482</link><description>&lt;p&gt;
&#26377;&#25928;&#23398;&#20064;&#26032;&#35270;&#35273;&#27010;&#24565;&#30340;&#26679;&#26412;&#33410;&#32422;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Learning of Novel Visual Concepts. (arXiv:2306.09482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21152;&#20837;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#26469;&#26377;&#25928;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#29289;&#20307;&#65292;&#36824;&#21253;&#25324;&#25277;&#35937;&#27010;&#24565;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35270;&#35273;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23569;&#25968;&#25552;&#20379;&#26377;&#38480;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#35782;&#21035;&#26032;&#29289;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20808;&#36827;&#35782;&#21035;&#27169;&#22411;&#20013;&#21152;&#20837;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#23569;&#26679;&#26412;&#20998;&#31867;&#12290;&#35813;&#31070;&#32463;&#31526;&#21495;&#23398;&#26550;&#26500;&#21450;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#30001;&#23569;&#37327;&#31034;&#20363;&#25552;&#21462;&#30340;&#38468;&#21152;&#20851;&#31995;&#21152;&#20837;&#21040;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#30456;&#20114;&#36830;&#25509;&#23454;&#20307;&#30340;&#23384;&#22312;&#65292;&#25552;&#39640;&#20102;&#20854;&#35782;&#21035;&#26032;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#29289;&#20307;&#65292;&#36824;&#21253;&#25324;&#25277;&#35937;&#27010;&#24565;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the advances made in visual object recognition, state-of-the-art deep learning models struggle to effectively recognize novel objects in a few-shot setting where only a limited number of examples are provided. Unlike humans who excel at such tasks, these models often fail to leverage known relationships between entities in order to draw conclusions about such objects. In this work, we show that incorporating a symbolic knowledge graph into a state-of-the-art recognition model enables a new approach for effective few-shot classification. In our proposed neuro-symbolic architecture and training methodology, the knowledge graph is augmented with additional relationships extracted from a small set of examples, improving its ability to recognize novel objects by considering the presence of interconnected entities. Unlike existing few-shot classifiers, we show that this enables our model to incorporate not only objects but also abstract concepts and affordances. The existence of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.09479</link><description>&lt;p&gt;
&#36870;&#21521;&#32553;&#25918;&#65306;&#21464;&#24471;&#26356;&#22823;&#24182;&#19981;&#24847;&#21619;&#30528;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#25439;&#22833;&#27604;&#20363;&#26377;&#21487;&#39044;&#27979;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;LMs&#20063;&#21487;&#33021;&#26174;&#31034;&#36870;&#21521;&#32553;&#25918;&#65292;&#21363;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20219;&#21153;&#24615;&#33021;&#36234;&#26469;&#36234;&#24046;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#30340;&#32570;&#38519;&#25152;&#33268;&#12290;&#26412;&#25991;&#36890;&#36807;&#20844;&#24320;&#27604;&#36187;&#65292;Inverse Scaling Prize&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#21450;&#20854;&#20182;&#23454;&#20363;&#65292;&#25105;&#20204;&#35748;&#20026;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#65288;i&#65289;&#20542;&#21521;&#20110;&#37325;&#22797;&#35760;&#24518;&#30340;&#24207;&#21015;&#32780;&#38750;&#36319;&#38543;&#19978;&#19979;&#25991;&#25351;&#31034;&#65292;&#65288;ii&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#19981;&#33391;&#27169;&#24335;&#65292;&#65288;iii&#65289;&#20219;&#21153;&#20013;&#26377;&#19968;&#20010;&#26131;&#20110;&#24178;&#25200;LMs&#30340;&#20219;&#21153;&#65292;&#23558;&#20854;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#36739;&#38590;&#30340;&#20219;&#21153;&#65292;&#65288;iv&#65289;&#20219;&#21153;&#30340;&#27491;&#30830;&#31034;&#33539;&#35823;&#23548;LMs&#12290;&#20316;&#32773;&#36824;&#20844;&#24067;&#20102;&#27604;&#36187;&#30340;&#33719;&#32988;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#32467;&#21512;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26412;&#22320;&#25209;&#35780;&#32773;&#30340;&#26041;&#27861;&#65292;&#21152;&#36895;&#22312;&#26410;&#30693;&#22330;&#26223;&#19982;&#38556;&#30861;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.09470</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#25209;&#35780;&#32773;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#25913;&#21892;&#36335;&#24452;&#35268;&#21010;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Path Planning Performance through Multimodal Generative Models with Local Critics. (arXiv:2306.09470v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#32467;&#21512;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26412;&#22320;&#25209;&#35780;&#32773;&#30340;&#26041;&#27861;&#65292;&#21152;&#36895;&#22312;&#26410;&#30693;&#22330;&#26223;&#19982;&#38556;&#30861;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25104;&#21151;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#26410;&#30693;&#22330;&#26223;&#19982;&#38556;&#30861;&#20013;&#21033;&#29992;&#24102;&#26377;&#28176;&#21464;&#24809;&#32602;&#65288;GP&#65289;&#30340;Wasserstein&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65288;WGANs&#65289;&#26469;&#36817;&#20284;&#33258;&#30001;&#26465;&#20214;&#37197;&#32622;&#31354;&#38388;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WGAN-GP&#19982;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;WGAN-GP&#35757;&#32451;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#33021;&#23545;&#20110;&#20174;&#22270;&#20687;&#21040;&#37197;&#32622;&#31354;&#38388;&#30340;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;KL&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#38543;&#26426;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#37197;&#32622;&#31354;&#38388;&#31616;&#21270;&#20026;&#19968;&#32452;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#20960;&#20010;&#26412;&#22320;&#27169;&#22411;&#12290;&#36825;&#20351;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#23398;&#20064;&#27169;&#22411;&#65292;&#36824;&#21487;&#20197;&#21152;&#36895;&#20854;&#25910;&#25947;&#12290;&#25105;&#20204;&#20351;&#29992;&#27969;&#24418;&#30340;&#21516;&#35843;&#31209;&#21644;&#20960;&#20309;&#24471;&#20998;&#35780;&#20272;&#37325;&#26500;&#30340;&#37197;&#32622;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#24230;&#37327;&#26412;&#22320;&#25209;&#35780;&#32773;&#22312;&#20026;&#22810;&#27169;&#24577;Wasserstein&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;with Gradient Penalty&#25552;&#20379;&#21453;&#39304;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36335;&#24452;&#35268;&#21010;&#36895;&#24230;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel method for accelerating path planning tasks in unknown scenes with obstacles by utilizing Wasserstein Generative Adversarial Networks (WGANs) with Gradient Penalty (GP) to approximate the distribution of the free conditioned configuration space. Our proposed approach involves conditioning the WGAN-GP with a Variational Auto-Encoder in a continuous latent space to handle multimodal datasets. However, training a Variational Auto-Encoder with WGAN-GP can be challenging for image-to-configuration-space problems, as the Kullback-Leibler loss function often converges to a random distribution. To overcome this issue, we simplify the configuration space as a set of Gaussian distributions and divide the dataset into several local models. This enables us to not only learn the model but also speed up its convergence. We evaluate the reconstructed configuration space using the homology rank of manifolds for datasets with the geometry score. Furthermore, we propose a nov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09468</link><description>&lt;p&gt;
FFB:&#38754;&#21521;&#22788;&#29702;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods. (arXiv:2306.09468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#26694;&#26550;&#65288;FFB&#65289;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20844;&#24179;&#20844;&#27491;&#22522;&#20934;&#65288;FFB&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22788;&#29702;&#20013;&#32452;&#20844;&#24179;&#26041;&#27861;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30340;&#20844;&#24179;&#24615;&#23545;&#20110;&#31526;&#21512;&#36947;&#24503;&#21644;&#27861;&#24459;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#35774;&#32622;&#30340;&#19981;&#19968;&#33268;&#65292;&#32570;&#20047;&#26131;&#20110;&#35775;&#38382;&#30340;&#31639;&#27861;&#23454;&#29616;&#20197;&#21450;&#24403;&#21069;&#20844;&#24179;&#24230;&#37327;&#24037;&#20855;&#30340;&#26377;&#38480;&#21487;&#25193;&#23637;&#24615;&#65292;&#23384;&#22312;&#27604;&#36739;&#21644;&#24320;&#21457;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#12289;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22788;&#29702;&#20013;&#30340;&#32452;&#20844;&#24179;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#30830;&#20445;&#19981;&#21516;&#27665;&#26063;/&#31181;&#26063;&#32676;&#20307;&#20844;&#24179;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#20197;&#19979;&#20851;&#38190;&#36129;&#29486;&#65306;&#25552;&#20379;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#26497;&#31616;&#21644;&#38754;&#21521;&#30740;&#31350;&#30340;&#24320;&#28304;&#20195;&#30721;&#65307;&#24314;&#31435;&#32479;&#19968;&#30340;&#20844;&#24179;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#27969;&#27700;&#32447;&#65307;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174; $\mathbf{45,079}$ &#20010;&#23454;&#39564;&#20013;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Fair Fairness Benchmark (\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is critical for ethical and legal compliance. However, there exist challenges in comparing and developing of fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source, standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\mathbf{45,079}$ experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Kriging&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Kriging&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09463</link><description>&lt;p&gt;
Kriging&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Kriging Convolutional Networks. (arXiv:2306.09463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09463
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Kriging&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;Kriging&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#24182;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25554;&#20540;&#26159;&#19968;&#31867;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#24050;&#30693;&#20540;&#30340;&#20301;&#32622;&#29992;&#20110;&#20272;&#35745;&#20854;&#20182;&#20301;&#32622;&#30340;&#20540;&#65292;&#30528;&#37325;&#20110;&#21033;&#29992;&#31354;&#38388;&#23616;&#37096;&#24615;&#21644;&#36235;&#21183;&#12290;&#20256;&#32479;&#30340;Kriging&#26041;&#27861;&#20855;&#26377;&#24378;&#28872;&#30340;&#39640;&#26031;&#20551;&#35774;&#65292;&#22240;&#27492;&#24120;&#24120;&#26080;&#27861;&#25429;&#25417;&#25968;&#25454;&#20869;&#37096;&#30340;&#22797;&#26434;&#24615;&#12290;&#21463;&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Kriging&#21367;&#31215;&#32593;&#32476;&#65288;KCN&#65289;&#65292;&#23427;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;Kriging&#30340;&#20248;&#28857;&#30456;&#32467;&#21512;&#12290;&#19982;&#26631;&#20934;GCN&#30456;&#27604;&#65292;KCN&#22312;&#20135;&#29983;&#39044;&#27979;&#26102;&#30452;&#25509;&#21033;&#29992;&#30456;&#37051;&#35266;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;KCN&#36824;&#23558;Kriging&#26041;&#27861;&#20316;&#20026;&#29305;&#23450;&#37197;&#32622;&#21253;&#21547;&#22312;&#20869;&#12290;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27880;&#24847;&#21147;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#20960;&#20010;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20248;&#20110;GCN&#21644;Kriging&#12290;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;KCN&#22312;GitHub&#23384;&#20648;&#24211;&#19978;&#20844;&#24320;&#65306;https://github.com/tufts-ml/kcn-torch&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial interpolation is a class of estimation problems where locations with known values are used to estimate values at other locations, with an emphasis on harnessing spatial locality and trends. Traditional Kriging methods have strong Gaussian assumptions, and as a result, often fail to capture complexities within the data. Inspired by the recent progress of graph neural networks, we introduce Kriging Convolutional Networks (KCN), a method of combining the advantages of Graph Convolutional Networks (GCN) and Kriging. Compared to standard GCNs, KCNs make direct use of neighboring observations when generating predictions. KCNs also contain the Kriging method as a specific configuration. We further improve the model's performance by adding attention. Empirically, we show that this model outperforms GCNs and Kriging in several applications. The implementation of KCN using PyTorch is publicized at the GitHub repository: https://github.com/tufts-ml/kcn-torch.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#26550;&#26500;&#21644;&#19982;&#20854;&#30456;&#20851;&#30340;&#34917;&#20805;&#26694;&#26550;&#12290;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#24615;&#12289;&#21709;&#24212;&#26102;&#38388;&#12289;&#36816;&#21160;&#26197;&#36710;&#21644;&#20248;&#21270;&#25216;&#26415;&#31561;&#26041;&#38754;&#30340;&#25216;&#26415;&#32454;&#33410;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09462</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#39542;&#33298;&#36866;&#20248;&#21270;&#65306;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques. (arXiv:2306.09462v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#20154;&#31867;&#33298;&#36866;&#24230;&#30340;&#33258;&#21160;&#39550;&#39542;&#26550;&#26500;&#21644;&#19982;&#20854;&#30456;&#20851;&#30340;&#34917;&#20805;&#26694;&#26550;&#12290;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#24615;&#12289;&#21709;&#24212;&#26102;&#38388;&#12289;&#36816;&#21160;&#26197;&#36710;&#21644;&#20248;&#21270;&#25216;&#26415;&#31561;&#26041;&#38754;&#30340;&#25216;&#26415;&#32454;&#33410;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20154;&#31867;&#33298;&#36866;&#24615;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#21160;&#39550;&#39542;&#30340;&#26550;&#26500;&#21644;&#30456;&#20851;&#34917;&#20805;&#26694;&#26550;&#12290;&#20171;&#32461;&#20102;&#34913;&#37327;&#33258;&#21160;&#39550;&#39542;&#29992;&#25143;&#33298;&#36866;&#24615;&#21644;&#24515;&#29702;&#20998;&#26512;&#30340;&#25216;&#26415;&#20803;&#32032;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19982;&#33258;&#21160;&#39550;&#39542;&#32467;&#26500;&#21644;&#21453;&#24212;&#26102;&#38388;&#30456;&#20851;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#33258;&#21160;&#39550;&#39542;&#33298;&#36866;&#31995;&#32479;&#12289;AV&#39550;&#39542;&#21592;&#21709;&#24212;&#26102;&#38388;&#12289;AV&#33298;&#36866;&#27700;&#24179;&#12289;&#36816;&#21160;&#26197;&#36710;&#20197;&#21450;&#30456;&#20851;&#20248;&#21270;&#25216;&#26415;&#30340;&#25216;&#26415;&#32454;&#33410;&#12290;&#20256;&#24863;&#22120;&#30340;&#21151;&#33021;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20256;&#24863;&#22120;&#20027;&#35201;&#24863;&#30693;&#36710;&#36742;&#21608;&#22260;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;&#8220;&#22825;&#27668;&#8221;&#65292;&#22240;&#27492;&#22312;&#19981;&#21516;&#30340;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#20108;&#25163;&#20256;&#24863;&#22120;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;&#33258;&#21160;&#39550;&#39542;&#30340;&#33298;&#36866;&#24615;&#21644;&#23433;&#20840;&#24615;&#20063;&#26159;&#24433;&#21709;&#33258;&#20027;&#21457;&#23637;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article outlines the architecture of autonomous driving and related complementary frameworks from the perspective of human comfort. The technical elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis are listed here. At the same time, this article introduces the technology related to the structure of automatic driving and the reaction time of automatic driving. We also discuss the technical details related to the automatic driving comfort system, the response time of the AV driver, the comfort level of the AV, motion sickness, and related optimization technologies. The function of the sensor is affected by various factors. Since the sensor of automatic driving mainly senses the environment around a vehicle, including "the weather" which introduces the challenges and limitations of second-hand sensors in autonomous vehicles under different weather conditions. The comfort and safety of autonomous driving are also factors that affect the development of autono
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#32467;&#21512; NIDS &#21644; HIDS&#65292;&#21033;&#29992;&#29305;&#24449;&#23637;&#24179;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#21327;&#20316;&#20998;&#31867;&#36827;&#34892;&#21512;&#20316;&#20998;&#31867;&#20197;&#25552;&#39640;&#20837;&#20405;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09451</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#23637;&#24179;&#21644;&#20004;&#38454;&#27573;&#21327;&#20316;&#20998;&#31867;&#22120;&#30340;&#20027;&#26426;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Host-Based Network Intrusion Detection via Feature Flattening and Two-stage Collaborative Classifier. (arXiv:2306.09451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09451
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#32467;&#21512; NIDS &#21644; HIDS&#65292;&#21033;&#29992;&#29305;&#24449;&#23637;&#24179;&#25216;&#26415;&#21644;&#20004;&#38454;&#27573;&#21327;&#20316;&#20998;&#31867;&#36827;&#34892;&#21512;&#20316;&#20998;&#31867;&#20197;&#25552;&#39640;&#20837;&#20405;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#36890;&#36807;&#30417;&#25511;&#30495;&#23454;&#32593;&#32476;&#27969;&#37327;&#24182;&#20998;&#26512;&#21487;&#30097;&#27963;&#21160;&#26469;&#36827;&#34892;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20294;&#26159;&#65292;NIDS &#22312;&#26816;&#27979;&#29305;&#23450;&#31867;&#22411;&#30340;&#25915;&#20987;&#65288;&#22914;&#39640;&#32423;&#25345;&#32493;&#24615;&#23041;&#32961;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21152;&#23494;&#27969;&#37327;&#25110;&#32570;&#20047;&#26435;&#38480;&#65292;NIDS &#21463;&#21046;&#20110;&#35266;&#23519;&#23436;&#25972;&#30340;&#27969;&#37327;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#23558; NIDS &#21644;&#22522;&#20110;&#20027;&#26426;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;HIDS&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20837;&#20405;&#26816;&#27979;&#24615;&#33021;&#12290;&#23558;&#29305;&#24449;&#23637;&#24179;&#25216;&#26415;&#24212;&#29992;&#20110;&#23558;&#20108;&#32500;&#22522;&#20110;&#20027;&#26426;&#30340;&#29305;&#24449;&#23637;&#24179;&#20026;&#19968;&#32500;&#21521;&#37327;&#65292;&#21487;&#30452;&#25509;&#20379;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#29992;&#12290;&#24212;&#29992;&#20004;&#38454;&#27573;&#21327;&#20316;&#20998;&#31867;&#22120;&#36827;&#34892;&#21512;&#20316;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#20837;&#20405;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Intrusion Detection Systems (NIDS) have been extensively investigated by monitoring real network traffic and analyzing suspicious activities. However, there are limitations in detecting specific types of attacks with NIDS, such as Advanced Persistent Threats (APT). Additionally, NIDS is restricted in observing complete traffic information due to encrypted traffic or a lack of authority. To address these limitations, a Host-based Intrusion Detection system (HIDS) evaluates resources in the host, including logs, files, and folders, to identify APT attacks that routinely inject malicious files into victimized nodes. In this study, a hybrid network intrusion detection system that combines NIDS and HIDS is proposed to improve intrusion detection performance. The feature flattening technique is applied to flatten two-dimensional host-based features into one-dimensional vectors, which can be directly used by traditional Machine Learning (ML) models. A two-stage collaborative classifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#39033;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#25928;&#29992;&#29702;&#35770;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#31574;&#30053;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#26368;&#20248;&#25928;&#29992;&#21644;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.09445</link><description>&lt;p&gt;
&#20102;&#35299;&#25928;&#29992;&#29702;&#35770;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey. (arXiv:2306.09445v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#39033;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#25928;&#29992;&#29702;&#35770;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#31574;&#30053;&#26469;&#23454;&#29616;&#31995;&#32479;&#30340;&#26368;&#20248;&#25928;&#29992;&#21644;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#32463;&#27982;&#23398;&#12289;&#21338;&#24328;&#35770;&#21644;&#36816;&#31609;&#23398;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#27010;&#24565;&#65292;&#25928;&#29992;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#34987;&#29992;&#26469;&#35780;&#20272;&#20010;&#20307;&#38656;&#27714;&#12289;&#20559;&#22909;&#21644;&#21033;&#30410;&#27700;&#24179;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;/&#26426;&#22120;&#20154;&#31995;&#32479;&#65288;MAS/MRS&#65289;&#30340;&#20915;&#31574;&#21644;&#23398;&#20064;&#20013;&#65292;&#21512;&#36866;&#30340;&#25928;&#29992;&#27169;&#22411;&#21487;&#20197;&#25351;&#23548;&#26234;&#33021;&#20307;&#36873;&#25321;&#21512;&#29702;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#20854;&#24403;&#21069;&#38656;&#27714;&#24182;&#23398;&#20250;&#21512;&#20316;&#21644;&#32452;&#32455;&#20854;&#34892;&#20026;&#65292;&#20248;&#21270;&#31995;&#32479;&#30340;&#25928;&#29992;&#65292;&#24314;&#31435;&#31283;&#23450;&#21487;&#38752;&#30340;&#20851;&#31995;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#32676;&#20307;&#25104;&#21592;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#31038;&#20250;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#30340;&#22797;&#26434;&#12289;&#22823;&#35268;&#27169;&#21644;&#38271;&#26399;&#30340;&#34892;&#20026;&#24456;&#22823;&#31243;&#24230;&#19978;&#30001;&#24213;&#23618;&#20851;&#31995;&#30340;&#22522;&#26412;&#29305;&#24615;&#20915;&#23450;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#23545;&#26426;&#21046;&#30340;&#29702;&#35770;&#26041;&#38754;&#21644;&#24212;&#29992;&#39046;&#22495;&#30340;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#25928;&#29992;&#20026;&#23548;&#21521;&#30340;&#38656;&#27714;&#33539;&#24335;&#65292;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09433</link><description>&lt;p&gt;
&#23454;&#29992;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09433
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#36807;&#31243;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#22270;&#20197;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#20294;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#25968;&#25454;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#24182;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;FedC2SL&#26159;&#19968;&#31181;&#32852;&#37030;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#35813;&#26816;&#39564;&#22312;&#19981;&#25910;&#38598;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#26597;&#20004;&#20010;&#21464;&#37327;&#22312;&#19968;&#32452;&#26465;&#20214;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;FedC2SL&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#26356;&#24369;&#21644;&#26356;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#24182;&#26356;&#24378;&#22320;&#25269;&#24481;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#24322;&#12290;FedPC&#21644;FedFCI&#26159;FedC2SL&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#22240;&#26524;&#20805;&#20998;&#24615;&#21644;&#22240;&#26524;&#19981;&#20805;&#20998;&#24615;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
&lt;/p&gt;</description></item><item><title>Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09417</link><description>&lt;p&gt;
Diff-TTSG: &#21435;&#22122;&#27010;&#29575;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09417
&lt;/p&gt;
&lt;p&gt;
Diff-TTSG&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#26368;&#26032;&#25216;&#26415;&#30340;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26391;&#35835;&#35821;&#38899;&#21512;&#25104;&#23454;&#29616;&#39640;&#33258;&#28982;&#24230;&#35780;&#20998;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#21512;&#25104;&#33258;&#28982;&#35328;&#35821;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#38754;&#23545;&#38754;&#30340;&#33258;&#21457;&#23545;&#35805;&#26082;&#26377;&#21475;&#22836;&#30340;&#65292;&#20063;&#26377;&#38750;&#35821;&#35328;&#30340;&#65288;&#20363;&#22914;&#65292;&#20849;&#21516;&#35328;&#35821;&#25163;&#21183;&#65289;&#12290;&#26368;&#36817;&#25165;&#24320;&#22987;&#30740;&#31350;&#32852;&#21512;&#21512;&#25104;&#36825;&#20004;&#31181;&#27169;&#24577;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#31995;&#32479;&#20013;&#30340;&#22909;&#22788;&#12290;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#20351;&#29992;&#38750;&#27010;&#29575;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#35762;&#35805;&#21644;&#36816;&#21160;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#30340;&#20266;&#24433;&#21644;&#27425;&#20248;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026; Diff-TTSG&#65292;&#20849;&#21516;&#23398;&#20064;&#21512;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#32452;&#23567;&#24515;&#30340;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#20027;&#35266;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#31995;&#32479;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#21512;&#25104;&#30340;&#26679;&#20363;&#32780;&#35328;&#65292;Diff-TTSG&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#20135;&#29983;&#26356;&#36924;&#30495;&#21644;&#22810;&#26679;&#21270;&#30340;&#38598;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With read-aloud speech synthesis achieving high naturalness scores, there is a growing research interest in synthesising spontaneous speech. However, human spontaneous face-to-face conversation has both spoken and non-verbal aspects (here, co-speech gestures). Only recently has research begun to explore the benefits of jointly synthesising these two modalities in a single system. The previous state of the art used non-probabilistic methods, which fail to capture the variability of human speech and motion, and risk producing oversmoothing artefacts and sub-optimal synthesis quality. We present the first diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to synthesise speech and gestures together. Our method can be trained on small datasets from scratch. Furthermore, we describe a set of careful uni- and multi-modal subjective tests for evaluating integrated speech and gesture synthesis systems, and use them to validate our proposed approach. For synthesised examp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.09390</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#33258;&#26432;&#39118;&#38505;&#35780;&#20272;&#65306;&#27169;&#22411;&#24615;&#33021;&#12289;&#28508;&#21147;&#21644;&#38480;&#21046;&#30340;&#37327;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#37327;&#21270;&#35780;&#20272;&#20132;&#20114;&#24335; ChatGPT &#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#36827;&#34892;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#39532;&#37324;&#20848;&#22823;&#23398; Reddit &#33258;&#26432;&#20542;&#21521;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23454;&#39564;&#26469;&#23545; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#25216;&#26415;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#22522;&#20110; transformer &#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#28201;&#24230;&#21442;&#25968;&#23545; ChatGPT &#21709;&#24212;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#22522;&#20110; ChatGPT &#19981;&#30830;&#23450;&#24615;&#29575;&#30340;&#26368;&#20339;&#28201;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#38416;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972; ChatGPT &#30340;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#20854;&#22312;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#20013;&#24110;&#21161;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;PINN&#30340;&#31934;&#24230;&#21644;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; ST-PINN &#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#30340;&#29289;&#29702;&#30693;&#35782;&#24182;&#21463;&#30410;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09389</link><description>&lt;p&gt;
ST-PINN: &#19968;&#31181;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ST-PINN: A Self-Training Physics-Informed Neural Network for Partial Differential Equations. (arXiv:2306.09389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09389
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#29616;&#26377;PINN&#30340;&#31934;&#24230;&#21644;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; ST-PINN &#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#30340;&#29289;&#29702;&#30693;&#35782;&#24182;&#21463;&#30410;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#29289;&#29702;&#21644;&#24037;&#31243;&#35745;&#31639;&#26680;&#24515;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#26080;&#32593;&#26684;&#26041;&#27861;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#24555;&#36895;&#27714;&#35299;PDE&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;ST-PINN&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;PINN&#30340;&#20302;&#31934;&#24230;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-PINN&#22312;&#35757;&#32451;&#26399;&#38388;&#24341;&#20837;&#20102;&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#33258;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#23558;&#25511;&#21046;&#26041;&#31243;&#20316;&#20026;&#20266;&#26631;&#35760;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#20174;&#26679;&#26412;&#28857;&#20013;&#36873;&#25321;&#26368;&#39640;&#32622;&#20449;&#24230;&#30340;&#31034;&#20363;&#26469;&#38468;&#21152;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#26159;&#39318;&#20808;&#23558;&#33258;&#25105;&#35757;&#32451;&#26426;&#21046;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#23398;&#20064;&#30340;&#20154;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#22330;&#26223;&#20013;&#23545;&#20116;&#20010;PDE&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#26356;&#22810;&#29289;&#29702;&#20449;&#24687;&#24182;&#21463;&#30410;&#20110;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are an essential computational kernel in physics and engineering. With the advance of deep learning, physics-informed neural networks (PINNs), as a mesh-free method, have shown great potential for fast PDE solving in various applications. To address the issue of low accuracy and convergence problems of existing PINNs, we propose a self-training physics-informed neural network, ST-PINN. Specifically, ST-PINN introduces a pseudo label based self-learning algorithm during training. It employs governing equation as the pseudo-labeled evaluation index and selects the highest confidence examples from the sample points to attach the pseudo labels. To our best knowledge, we are the first to incorporate a self-training mechanism into physics-informed learning. We conduct experiments on five PDE problems in different fields and scenarios. The results demonstrate that the proposed method allows the network to learn more physical information and benefit conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#65292;AHSTN&#22312;&#33410;&#28857;&#32423;&#21035;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#22359;&#65292;&#26469;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#26469;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09386</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hierarchical SpatioTemporal Network for Traffic Forecasting. (arXiv:2306.09386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#26102;&#31354;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#65292;AHSTN&#22312;&#33410;&#28857;&#32423;&#21035;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#22359;&#65292;&#26469;&#33258;&#36866;&#24212;&#22320;&#22788;&#29702;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#26469;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31995;&#32479;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#35299;&#20915;&#22478;&#24066;&#20132;&#36890;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#30740;&#31350;&#30528;&#37325;&#20110;&#24314;&#27169;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388; - &#26102;&#38388;&#21160;&#24577;&#65292;&#20854;&#20013;&#65292;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26159;&#21033;&#29992;&#36335;&#32593;&#26684;&#20013;&#23884;&#20837;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;GCN&#30340;&#26041;&#27861;&#20165;&#22312;&#33410;&#28857;&#32423;&#21035;&#65288;&#20363;&#22914;&#36947;&#36335;&#21644;&#20132;&#21449;&#21475;&#65289;&#19978;&#36816;&#34892;&#65292;&#32780;&#24573;&#30053;&#20102;&#25972;&#20010;&#22478;&#24066;&#30340;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#12290;&#35832;&#22914;&#20132;&#21449;&#21475;&#21644;&#36947;&#36335;&#27573;&#20043;&#31867;&#30340;&#33410;&#28857;&#21487;&#20197;&#24418;&#25104;&#31751;&#65288;&#20363;&#22914;&#21306;&#22495;&#65289;&#65292;&#36825;&#20123;&#33410;&#28857;&#22312;&#26356;&#39640;&#30340;&#23618;&#27425;&#19978;&#36824;&#21487;&#20197;&#30456;&#20114;&#20316;&#29992;&#24182;&#20849;&#20139;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#23618;&#31354;&#38388;&#26102;&#38388;&#32593;&#32476;&#65288;AHSTN&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#31354;&#38388;&#23618;&#27425;&#32467;&#26500;&#21644;&#24314;&#27169;&#22810;&#23610;&#24230;&#31354;&#38388;&#30456;&#20851;&#24615;&#20419;&#36827;&#20132;&#36890;&#39044;&#27979;&#12290;&#38500;&#20102;&#33410;&#28857;&#32423;&#21035;&#30340;&#26102;&#31354;&#22359;&#65292;AHSTN&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#26102;&#31354;&#22359;&#65292;&#20998;&#21035;&#25429;&#25417;&#26412;&#22320;&#12289;&#21306;&#22495;&#21644;&#20840;&#29699;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#19981;&#21516;&#23618;&#27425;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#23610;&#24230;&#30340;&#20449;&#24687;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;AHSTN&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting is vital to intelligent transportation systems, which are widely adopted to solve urban traffic issues. Existing traffic forecasting studies focus on modeling spatial-temporal dynamics in traffic data, among which the graph convolution network (GCN) is at the center for exploiting the spatial dependency embedded in the road network graphs. However, these GCN-based methods operate intrinsically on the node level (e.g., road and intersection) only whereas overlooking the spatial hierarchy of the whole city. Nodes such as intersections and road segments can form clusters (e.g., regions), which could also have interactions with each other and share similarities at a higher level. In this work, we propose an Adaptive Hierarchical SpatioTemporal Network (AHSTN) to promote traffic forecasting by exploiting the spatial hierarchy and modeling multi-scale spatial correlations. Apart from the node-level spatiotemporal blocks, AHSTN introduces the adaptive spatiotempor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#31934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#31561;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2306.09385</link><description>&lt;p&gt;
&#24212;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21387;&#21147;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Employing Multimodal Machine Learning for Stress Detection. (arXiv:2306.09385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#34701;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#31934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#31561;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#26816;&#27979;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#26102;&#20195;&#65292;&#20154;&#31867;&#30340;&#29983;&#27963;&#26041;&#24335;&#36234;&#26469;&#36234;&#27880;&#37325;&#30693;&#35782;&#65292;&#23548;&#33268;&#20037;&#22352;&#30340;&#24037;&#20316;&#26041;&#24335;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;&#36825;&#23548;&#33268;&#35768;&#22810;&#20581;&#24247;&#21644;&#24515;&#29702;&#38556;&#30861;&#12290;&#24515;&#29702;&#20581;&#24247;&#26159;&#24403;&#20170;&#19990;&#30028;&#19978;&#26368;&#34987;&#24573;&#35270;&#20294;&#20063;&#26159;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#12290;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#20250;&#30452;&#25509;&#25110;&#38388;&#25509;&#22320;&#24433;&#21709;&#21040;&#20154;&#20307;&#20854;&#20182;&#37096;&#20998;&#65292;&#24182;&#22952;&#30861;&#20010;&#20154;&#30340;&#26085;&#24120;&#27963;&#21160;&#21644;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21387;&#21147;&#24182;&#25214;&#20986;&#23548;&#33268;&#20005;&#37325;&#24515;&#29702;&#30142;&#30149;&#30340;&#21387;&#21147;&#36235;&#21183;&#23545;&#20110;&#20010;&#20154;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#28041;&#21450;&#22810;&#20010;&#22240;&#32032;&#12290;&#36890;&#36807;&#34701;&#21512;&#30001;&#34892;&#20026;&#27169;&#24335;&#20135;&#29983;&#30340;&#22810;&#20010;&#27169;&#24577;&#65288;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65289;&#21487;&#20197;&#20934;&#30830;&#22320;&#23454;&#29616;&#36825;&#31181;&#35782;&#21035;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30830;&#23450;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#27492;&#30446;&#30340;&#25552;&#20986;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;AI&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#38754;&#37096;&#34920;&#24773;&#12289;&#35821;&#38899;&#27169;&#24335;&#12289;&#29983;&#29702;&#20449;&#21495;&#21644;&#33258;&#25253;&#25968;&#25454;&#65292;&#20934;&#30830;&#30417;&#27979;&#20154;&#30340;&#24037;&#20316;&#34892;&#20026;&#21644;&#21387;&#21147;&#27700;&#24179;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(Convolutional Neural Networks, CNNs)&#12289;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(Time-delay Neural Networks, TDNNs)&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;(Multi-Layer Perceptron, MLP)&#31561;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#20998;&#31867;&#39640;&#21387;&#21147;&#21644;&#20302;&#21387;&#21147;&#29366;&#24577;&#12290;&#22312;&#20844;&#20849;&#21387;&#21147;&#25968;&#25454;&#38598;&#21644;&#20174;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#37027;&#37324;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20934;&#30830;&#26816;&#27979;&#21387;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current age, human lifestyle has become more knowledge oriented leading to generation of sedentary employment. This has given rise to a number of health and mental disorders. Mental wellness is one of the most neglected but crucial aspects of today's world. Mental health issues can, both directly and indirectly, affect other sections of human physiology and impede an individual's day-to-day activities and performance. However, identifying the stress and finding the stress trend for an individual leading to serious mental ailments is challenging and involves multiple factors. Such identification can be achieved accurately by fusing these multiple modalities (due to various factors) arising from behavioral patterns. Certain techniques are identified in the literature for this purpose; however, very few machine learning-based methods are proposed for such multimodal fusion tasks. In this work, a multimodal AI-based framework is proposed to monitor a person's working behavior and st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26377;&#25928;&#35757;&#32451;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#36229;&#21442;&#25968;&#20540;&#38656;&#35201;&#22312;&#24615;&#33021;&#24230;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.09384</link><description>&lt;p&gt;
MobileASR: &#19968;&#31181;&#38754;&#21521;&#31227;&#21160;&#30005;&#35805;&#30340;&#36164;&#28304;&#24863;&#30693;&#26412;&#22320;&#20010;&#24615;&#21270;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MobileASR: A resource-aware on-device personalisation framework for automatic speech recognition in mobile phones. (arXiv:2306.09384v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#26377;&#25928;&#35757;&#32451;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#36229;&#21442;&#25968;&#20540;&#38656;&#35201;&#22312;&#24615;&#33021;&#24230;&#37327;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#20351;&#29992;&#25143;&#25968;&#25454;&#21644;&#27169;&#22411;&#22312;&#26412;&#22320;&#23384;&#20648;&#21644;&#20351;&#29992;&#65292;&#20174;&#32780;&#24320;&#21457;&#29992;&#25143;&#35821;&#38899;&#20010;&#24615;&#21270;&#30340;ASR&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#24863;&#30693;&#30340;&#23376;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#31227;&#21160;&#35774;&#22791;&#30340;RAM&#21644;&#30005;&#27744;&#23481;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#29992;&#36164;&#28304;&#19982;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31361;&#20986;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;&#23376;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#31227;&#21160;&#35774;&#22791;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#30005;&#27744;&#38480;&#21046;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#26377;&#25928;&#30340;&#35757;&#32451;&#24182;&#30456;&#24212;&#22320;&#20572;&#27490;&#35813;&#36807;&#31243;&#12290;&#20026;&#20102;&#27169;&#25311;&#30495;&#23454;&#29992;&#25143;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#21508;&#31181;&#21475;&#38899;&#30340;&#21457;&#35328;&#32773;&#12290;&#28982;&#21518;&#65292;&#22312;&#21508;&#20010;&#21697;&#29260;&#30340;&#21508;&#31181;&#31227;&#21160;&#35774;&#22791;&#19978;&#27979;&#35797;&#25972;&#20010;&#26412;&#22320;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#35843;&#27169;&#22411;&#21644;&#36873;&#25321;&#27491;&#30830;&#30340;&#36229;&#21442;&#25968;&#20540;&#26159;&#24615;&#33021;&#24230;&#37327;&#26368;&#20302;&#30340;&#21487;&#36798;&#21040;&#24615;&#21644;&#26412;&#22320;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a comprehensive methodology for developing user-voice personalised ASR models by effectively training models on mobile phones, allowing user data and models to be stored and used locally. To achieve this, we propose a resource-aware sub-model based training approach that considers the RAM, and battery capabilities of mobile phones. We also investigate the relationship between available resources and training time, highlighting the effectiveness of using sub-models in such scenarios. By taking into account the evaluation metric and battery constraints of the mobile phones, we are able to perform efficient training and halt the process accordingly. To simulate real users, we use speakers with various accents. The entire on-device training and evaluation framework was then tested on various mobile phones across brands. We show that fine-tuning the models and selecting the right hyperparameter values is a trade-off between the lowest achievable performance metric, on-device tra
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#26799;&#24230;&#33539;&#22260;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;Transformer&#20013;&#21442;&#25968;&#20849;&#20139;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#24615;&#26159;&#20854;&#20013;&#20027;&#35201;&#21407;&#22240;&#65292;&#27169;&#22411;&#22797;&#26434;&#24230;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09380</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#20013;&#30340;&#21442;&#25968;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Understanding Parameter Sharing in Transformers. (arXiv:2306.09380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#26799;&#24230;&#33539;&#22260;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;Transformer&#20013;&#21442;&#25968;&#20849;&#20139;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#24615;&#26159;&#20854;&#20013;&#20027;&#35201;&#21407;&#22240;&#65292;&#27169;&#22411;&#22797;&#26434;&#24230;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;Transformer&#19978;&#30340;&#20808;&#21069;&#24037;&#20316;&#38598;&#20013;&#22312;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#20849;&#20139;&#21442;&#25968;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#26469;&#25552;&#39640;&#26377;&#38480;&#21442;&#25968;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20026;&#20160;&#20040;&#27492;&#26041;&#27861;&#26377;&#25928;&#12290;&#39318;&#20808;&#65292;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#20250;&#20351;&#27169;&#22411;&#26356;&#21152;&#22797;&#26434;&#65292;&#25105;&#20204;&#20551;&#35774;&#21407;&#22240;&#19982;&#27169;&#22411;&#22797;&#26434;&#24230;&#65288;&#25351;FLOPs&#65289;&#26377;&#20851;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#27599;&#20010;&#20849;&#20139;&#21442;&#25968;&#22312;&#21521;&#21069;&#20256;&#25773;&#20013;&#20250;&#21442;&#19982;&#32593;&#32476;&#35745;&#31639;&#22810;&#27425;&#65292;&#20854;&#23545;&#24212;&#30340;&#26799;&#24230;&#20540;&#19982;&#21407;&#27169;&#22411;&#30340;&#26799;&#24230;&#20540;&#33539;&#22260;&#19981;&#21516;&#65292;&#36825;&#23558;&#24433;&#21709;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#35757;&#32451;&#25910;&#25947;&#24615;&#20063;&#26159;&#20854;&#20013;&#20043;&#19968;&#30340;&#21407;&#22240;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26174;&#31034;&#27492;&#26041;&#27861;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#24402;&#22240;&#20110;&#22686;&#21152;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;&#36825;&#21551;&#21457;&#20102;&#25105;&#20204;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter sharing has proven to be a parameter-efficient approach. Previous work on Transformers has focused on sharing parameters in different layers, which can improve the performance of models with limited parameters by increasing model depth. In this paper, we study why this approach works from two perspectives. First, increasing model depth makes the model more complex, and we hypothesize that the reason is related to model complexity (referring to FLOPs). Secondly, since each shared parameter will participate in the network computation several times in forward propagation, its corresponding gradient will have a different range of values from the original model, which will affect the model convergence. Based on this, we hypothesize that training convergence may also be one of the reasons. Through further analysis, we show that the success of this approach can be largely attributed to better convergence, with only a small part due to the increased model complexity. Inspired by this
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09376</link><description>&lt;p&gt;
&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27169;&#22359;&#21270;&#65306;&#19968;&#31181;&#26032;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;(MwT)&#65292;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#27169;&#22411;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#65292;&#36827;&#32780;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#65292;&#33021;&#22815;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#24050;&#25104;&#20026;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#20013;&#36234;&#26469;&#36234;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;DNN&#27169;&#22411;&#36890;&#24120;&#22312;&#26102;&#38388;&#21644;&#25104;&#26412;&#26041;&#38754;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#24320;&#22987;&#20851;&#27880;&#37325;&#29992;&#29616;&#26377;&#30340;DNN&#27169;&#22411;-&#20511;&#37492;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#20195;&#30721;&#37325;&#29992;&#24605;&#24819;&#12290;&#20294;&#26159;&#65292;&#37325;&#29992;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#20250;&#36896;&#25104;&#39069;&#22806;&#30340;&#24320;&#38144;&#25110;&#20174;&#19981;&#38656;&#35201;&#30340;&#21151;&#33021;&#20013;&#32487;&#25215;&#24369;&#28857;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20998;&#35299;&#25104;&#27169;&#22359;&#65292;&#21363;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#65292;&#24182;&#23454;&#29616;&#27169;&#22359;&#30340;&#37325;&#29992;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24182;&#19981;&#26159;&#20026;&#20102;&#27169;&#22359;&#21270;&#32780;&#26500;&#24314;&#30340;&#65292;&#25152;&#20197;&#35757;&#32451;&#21518;&#30340;&#27169;&#22359;&#21270;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#24320;&#38144;&#21644;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#27169;&#22359;&#21270;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21363;&#22312;&#35757;&#32451;&#26102;&#27169;&#22359;&#21270;&#65288;MwT&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#20855;&#26377;&#32467;&#26500;&#19978;&#30340;&#27169;&#22359;&#21270;&#33021;&#21147;&#65292;&#36825;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22359;&#20869;&#30340;&#20869;&#32858;&#24615;&#21644;&#27169;&#22359;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#30495;&#27491;&#30340;&#27169;&#22359;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#21518;&#27169;&#22359;&#21270;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#39062;&#30340;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#36827;&#34892;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#20998;&#25968;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09374</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#24211;&#20462;&#22797;&#21040;&#25968;&#25454;&#24211;&#22240;&#26524;&#21450;&#20854;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Database Repairs to Causality in Databases and Beyond. (arXiv:2306.09374v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26032;&#39062;&#30340;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#36827;&#34892;&#25968;&#25454;&#24211;&#26597;&#35810;&#31572;&#26696;&#20998;&#25968;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20123;&#26368;&#36817;&#29992;&#20110;&#25968;&#25454;&#24211;&#20013;&#26597;&#35810;&#31572;&#26696;&#20998;&#25968;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#37325;&#28857;&#20171;&#32461;&#20102;&#20316;&#32773;&#21450;&#20854;&#21512;&#20316;&#32773;&#30340;&#24037;&#20316;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#36827;&#34892;&#20998;&#25968;&#35268;&#33539;&#21644;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#23637;&#31034;&#20102;&#20960;&#20010;&#31034;&#20363;&#20197;&#35828;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe some recent approaches to score-based explanations for query answers in databases. The focus is on work done by the author and collaborators. Special emphasis is placed on the use of counterfactual reasoning for score specification and computation. Several examples that illustrate the flexibility of these methods are shown.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09373</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#19988;&#30456;&#20114;&#31454;&#20105;&#30340;&#30456;&#20851;&#24615;&#65292;&#21333;&#32431;&#22320;&#35757;&#32451;&#25152;&#26377;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#23398;&#20064;&#65292;&#21363;&#19968;&#20123;&#20219;&#21153;&#34987;&#24456;&#22909;&#22320;&#23398;&#20064;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#34987;&#24573;&#35270;&#12290;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;MTO&#65289;&#26088;&#22312;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20219;&#21153;&#25439;&#22833;&#35268;&#27169;&#25110;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;MTL&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#26356;&#26032;&#20849;&#20139;&#21442;&#25968;&#26102;&#65292;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65288;&#21363;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20540;&#38500;&#20197;&#20854;&#21407;&#22987;&#26799;&#24230;&#33539;&#25968;&#30340;&#20540;&#65289;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;EMTL&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;MTL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#28155;&#21152;&#20102;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#20351;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26356;&#20855;&#21487;&#27604;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
&lt;/p&gt;</description></item><item><title>Warpformer&#26159;&#19968;&#31181;&#33021;&#22815;&#23436;&#25972;&#32771;&#34385;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09368</link><description>&lt;p&gt;
Warpformer: &#19968;&#31181;&#29992;&#20110;&#19981;&#35268;&#21017;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#22810;&#23610;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Warpformer: A Multi-scale Modeling Approach for Irregular Clinical Time Series. (arXiv:2306.09368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09368
&lt;/p&gt;
&lt;p&gt;
Warpformer&#26159;&#19968;&#31181;&#33021;&#22815;&#23436;&#25972;&#32771;&#34385;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#24456;&#24120;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#21576;&#29616;&#20986;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#21644;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#12290;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#25351;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#36890;&#24120;&#22312;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#35760;&#24405;&#65292;&#32780;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#21017;&#25351;&#19981;&#21516;&#24207;&#21015;&#20043;&#38388;&#30340;&#37319;&#26679;&#29575;&#26174;&#33879;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#24207;&#21015;&#20869;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#32780;&#24573;&#30053;&#20102;&#24207;&#21015;&#38388;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;Warpformer&#65292;&#23427;&#20805;&#20998;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled multivariate time series are ubiquitous in various fields, particularly in healthcare, and exhibit two key characteristics: intra-series irregularity and inter-series discrepancy. Intra-series irregularity refers to the fact that time-series signals are often recorded at irregular intervals, while inter-series discrepancy refers to the significant variability in sampling rates among diverse series. However, recent advances in irregular time series have primarily focused on addressing intra-series irregularity, overlooking the issue of inter-series discrepancy. To bridge this gap, we present Warpformer, a novel approach that fully considers these two characteristics. In a nutshell, Warpformer has several crucial designs, including a specific input representation that explicitly characterizes both intra-series irregularity and inter-series discrepancy, a warping module that adaptively unifies irregular time series in a given scale, and a customized attention module fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#32467;&#21512;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;</title><link>http://arxiv.org/abs/2306.09365</link><description>&lt;p&gt;
&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#24863;&#24212;&#30005;&#21160;&#26426;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fault Detection in Induction Motors using Functional Dimensionality Reduction Methods. (arXiv:2306.09365v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#32467;&#21512;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26059;&#36716;&#30005;&#26426;&#19978;&#23454;&#26045;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#31574;&#30053;&#23545;&#20110;&#29616;&#20195;&#24037;&#19994;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20256;&#32479;&#30340;&#30005;&#26426;&#30005;&#27969;&#29305;&#24449;&#20998;&#26512;&#31574;&#30053;&#19982;&#20989;&#25968;&#38477;&#32500;&#26041;&#27861;&#65288;&#21363;&#20989;&#25968;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#20989;&#25968;&#25193;&#25955;&#26144;&#23556;&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#26816;&#27979;&#21644;&#20998;&#31867;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#30340;&#25925;&#38556;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#33719;&#24471;&#20102;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#65292;&#19981;&#20165;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#24863;&#24212;&#30005;&#21160;&#26426;&#20013;&#25925;&#38556;&#30340;&#23384;&#22312;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#20998;&#26512;&#35782;&#21035;&#26356;&#22810;&#31867;&#22411;&#30340;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The implementation of strategies for fault detection and diagnosis on rotating electrical machines is crucial for the reliability and safety of modern industrial systems. The contribution of this work is a methodology that combines conventional strategy of Motor Current Signature Analysis with functional dimensionality reduction methods, namely Functional Principal Components Analysis and Functional Diffusion Maps, for detecting and classifying fault conditions in induction motors. The results obtained from the proposed scheme are very encouraging, revealing a potential use in the future not only for real-time detection of the presence of a fault in an induction motor, but also in the identification of a greater number of types of faults present through an offline analysis.
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09363</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#38754;&#21521;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Data Augmentation for Feature Distribution Skewed Federated Learning. (arXiv:2306.09363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#26500;&#24615;&#65288;&#21363;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65289;&#65292;&#23427;&#30340;&#24615;&#33021;&#24517;&#28982;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;FL&#22330;&#26223;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#30001;&#26412;&#22320;&#25968;&#25454;&#38598;&#20043;&#38388;&#28508;&#22312;&#20998;&#24067;&#19981;&#21516;&#23548;&#33268;&#30340;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) facilitates collaborative learning among multiple clients in a distributed manner, while ensuring privacy protection. However, its performance is inevitably degraded as suffering data heterogeneity, i.e., non-IID data. In this paper, we focus on the feature distribution skewed FL scenario, which is widespread in real-world applications. The main challenge lies in the feature shift caused by the different underlying distributions of local datasets. While the previous attempts achieved progress, few studies pay attention to the data itself, the root of this issue. Therefore, the primary goal of this paper is to develop a general data augmentation technique at the input level, to mitigate the feature shift. To achieve this goal, we propose FedRDN, a simple yet remarkably effective data augmentation method for feature distribution skewed FL, which randomly injects the statistics of the dataset from the entire federation into the client's data. By this, our method ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#23433;&#20840;&#31574;&#30053;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#36136;&#37327;RL&#31639;&#27861;&#23454;&#29616;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#12290;&#35813;&#22871;&#20214;&#21487;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.09303</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Datasets and Benchmarks for Offline Safe Reinforcement Learning. (arXiv:2306.09303v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#23433;&#20840;&#31574;&#30053;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#36136;&#37327;RL&#31639;&#27861;&#23454;&#29616;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#12290;&#35813;&#22871;&#20214;&#21487;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25361;&#25112;&#30340;&#32508;&#21512;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#37096;&#32626;&#38454;&#27573;&#20013;&#30340;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#22871;&#20214;&#21253;&#21547;&#19977;&#20010;&#32452;&#20214;&#65306;1&#65289;&#19987;&#23478;&#21046;&#20316;&#30340;&#23433;&#20840;&#31574;&#30053;&#65292;2&#65289;D4RL&#26679;&#24335;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#29615;&#22659;&#21253;&#35013;&#22120;&#65292;&#20197;&#21450;3&#65289;&#39640;&#36136;&#37327;&#30340;&#31163;&#32447;&#23433;&#20840;RL&#22522;&#20934;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#26465;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#30001;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#25903;&#25345;&#65292;&#21487;&#20197;&#20419;&#36827;&#22312;38&#20010;&#21463;&#27426;&#36814;&#30340;&#23433;&#20840;RL&#20219;&#21153;&#20013;&#29983;&#25104;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#20174;&#26426;&#22120;&#20154;&#25511;&#21046;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#21518;&#22788;&#29702;&#36807;&#28388;&#22120;&#65292;&#33021;&#22815;&#20462;&#25913;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27169;&#25311;&#21508;&#31181;&#25968;&#25454;&#25910;&#38598;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#31163;&#32447;&#23433;&#20840;RL&#31639;&#27861;&#30340;&#31934;&#32654;&#19988;&#21487;&#25193;&#23637;&#30340;&#23454;&#29616;&#65292;&#20197;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290; &#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive benchmarking suite tailored to offline safe reinforcement learning (RL) challenges, aiming to foster progress in the development and evaluation of safe learning algorithms in both the training and deployment phases. Our benchmark suite contains three packages: 1) expertly crafted safe policies, 2) D4RL-styled datasets along with environment wrappers, and 3) high-quality offline safe RL baseline implementations. We feature a methodical data collection pipeline powered by advanced safe RL algorithms, which facilitates the generation of diverse datasets across 38 popular safe RL tasks, from robot control to autonomous driving. We further introduce an array of data post-processing filters, capable of modifying each dataset's diversity, thereby simulating various data collection conditions. Additionally, we provide elegant and extensible implementations of prevalent offline safe RL algorithms to accelerate research in this area. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09147</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Learning of Multivariate Time Series with Temporal Irregularity. (arXiv:2306.09147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#30340;&#22810;&#20803;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#34920;&#29616;&#20986;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#26102;&#38388;&#38388;&#38548;&#21644;&#32452;&#20214;&#38169;&#20301;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#22343;&#21248;&#30340;&#38388;&#36317;&#21644;&#24322;&#27493;&#26159;&#25968;&#25454;&#20869;&#29983;&#29305;&#24449;&#32780;&#19981;&#26159;&#19981;&#36275;&#35266;&#23519;&#30340;&#32467;&#26524;&#65292;&#21017;&#36825;&#20123;&#19981;&#35268;&#21017;&#24615;&#30340;&#20449;&#24687;&#20869;&#23481;&#22312;&#34920;&#24449;&#22810;&#20803;&#20381;&#36182;&#32467;&#26500;&#26102;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#35201;&#20040;&#26131;&#21463;&#21040;&#25554;&#34917;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#20551;&#35774;&#24378;&#21152;&#20110;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#32452;&#20214;&#21551;&#29992;&#21807;&#19968;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#20415;&#21040;&#36798;&#26102;&#38388;&#21487;&#20197;&#25351;&#23548;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#21738;&#20010;&#38544;&#34255;&#29366;&#24577;&#24212;&#35813;&#26356;&#26032;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#35266;&#27979;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#20102;&#36328;&#32452;&#20214;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#21644;&#26465;&#20214;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden state
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08388</link><description>&lt;p&gt;
Skill-Critic: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#25216;&#33021;&#30340;&#31579;&#36873;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08388
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23558;&#19968;&#20010;&#31574;&#30053;&#20998;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#21152;&#24555;&#38271;&#26399;&#20915;&#31574;&#30340;&#36895;&#24230;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#65292;&#25216;&#33021;&#21363;&#21407;&#22987;&#21160;&#20316;&#30340;&#24207;&#21015;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25216;&#33021;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#31574;&#30053;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#65292;&#20294;&#30001;&#20110;&#28436;&#31034;&#35206;&#30422;&#33539;&#22260;&#20302;&#25110;&#20998;&#24067;&#36716;&#31227;&#65292;&#25152;&#24471;&#21040;&#30340;&#20302;&#23618;&#31574;&#30053;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-tuning&#20302;&#23618;&#31574;&#30053;&#19982;&#39640;&#23618;&#25216;&#33021;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;Skill-Critic&#31639;&#27861;&#20248;&#21270;&#20102;&#20302;&#23618;&#21644;&#39640;&#23618;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#21021;&#22987;&#21270;&#21644;&#35268;&#33539;&#21270;&#65292;&#20197;&#24341;&#23548;&#32852;&#21512;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Gran Turismo Sport&#20013;&#26032;&#30340;&#31232;&#30095;&#22870;&#21169;&#33258;&#20027;&#36187;&#36710;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Skill-Critic&#30340;&#20302;&#23618;&#31574;&#30053;Fine-tuning&#21644;&#28436;&#31034;&#24341;&#23548;&#31574;&#30053;&#21021;&#22987;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.03414</link><description>&lt;p&gt;
DreamSparse: &#21033;&#29992; 2D &#25193;&#25955;&#27169;&#22411;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#21512;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views. (arXiv:2306.03414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#37327;&#35270;&#35282;&#20013;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#25110;&#22312;&#27492;&#31867;&#23569;&#35270;&#35282;&#35774;&#32622;&#20013;&#38656;&#35201;&#36880;&#20010;&#23545;&#35937;&#20248;&#21270;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#20449;&#24687;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#26469;&#21512;&#25104;&#26032;&#39062;&#30340;&#35270;&#35282;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;2D &#25193;&#25955;&#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#65292;&#23548;&#33268;&#22270;&#20687;&#21512;&#25104;&#22833;&#30495;&#65292;&#24433;&#21709;&#20102;&#22270;&#20687;&#30340;&#35782;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DreamSparse&#65292;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#20960;&#20309;&#21644;&#35782;&#21035;&#32852;&#21512;&#19968;&#33268;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamSparse &#21253;&#25324;&#19968;&#20010;&#20960;&#20309;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#35270;&#35282;&#33719;&#21462; 3D &#29305;&#24449;&#20316;&#20026; 3D &#20808;&#39564;&#65292;&#38543;&#21518;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#23558;&#36825;&#20123; 3D &#29305;&#24449;&#22270;&#36716;&#25442;&#20026;&#29983;&#25104;&#36807;&#31243;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#28982;&#21518;&#29992;&#20110;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#25351;&#23548;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DreamSparse &#22312;&#23569;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29289;&#20307;&#20960;&#20309;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.14065</link><description>&lt;p&gt;
&#19981;&#35201;&#35757;&#32451;&#23427;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS-GNN&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32487;&#25215;&#20102;&#20256;&#32479;NAS&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20248;&#21270;&#38590;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;NAS&#26041;&#27861;&#24573;&#35270;&#20102;GNN&#30340;&#29420;&#29305;&#24615;&#65292;&#21363;GNN&#20855;&#26377;&#26080;&#38656;&#35757;&#32451;&#23601;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#28857;&#12290;&#37319;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#30446;&#26631;&#23547;&#25214;&#26368;&#20248;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#24182;&#24471;&#20986;&#19968;&#31181;&#26032;&#30340;NAS-GNN&#26041;&#27861;&#65292;&#21363;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;NAC&#22312;GNN&#19978;&#23454;&#29616;&#20102;&#26080;&#26356;&#26032;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#39640;&#25928;&#35745;&#31639;&#12290;&#22312;&#22810;&#20010;GNN&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07142</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#20173;&#28982;&#26159;&#22810;&#35828;&#35805;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#38899;&#20998;&#31163;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#19968;&#20123;&#27169;&#22411;&#38656;&#35201;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#36739;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32553;&#30701;&#35757;&#32451;&#31034;&#20363;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24212;&#29992;&#36825;&#20123;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#65288;TSL&#65289;&#38480;&#21046;&#23545;&#20004;&#20010;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65288;SepFormer&#65292;&#19968;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21644;Conv-TasNet&#65292;&#19968;&#20010;&#21367;&#31215;&#27169;&#22411;&#65289;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;WJS0-2Mix&#65292;WHAMR&#21644;Libri2Mix&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#21450;&#20854;&#23545;&#35757;&#32451;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20998;&#24067;&#65292;&#24212;&#29992;&#29305;&#23450;&#30340;TSL&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23545;&#27874;&#24418;&#36215;&#22987;&#32034;&#24341;&#36827;&#34892;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#26356;&#22810;&#29420;&#29305;&#30340;&#31034;&#20363;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18101</link><description>&lt;p&gt;
&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields. (arXiv:2303.18101v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#39046;&#22495;&#30340;&#24863;&#30693;&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#37117;&#21463;&#38480;&#65292;&#36825;&#24433;&#21709;&#20102;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#35757;&#32451;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#38024;&#23545;&#20892;&#19994;&#39046;&#22495;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65288;INoD&#65289;&#65292;&#21033;&#29992;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;INoD&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#32534;&#30721;&#20013;&#20132;&#38169;&#29305;&#24449;&#22270;&#65292;&#24182;&#39044;&#27979;&#20135;&#29983;&#30340;&#29305;&#24449;&#22270;&#30340;&#25968;&#25454;&#38598;&#38582;&#23646;&#20851;&#31995;&#20316;&#20026;&#39044;&#25991;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#30340;&#26126;&#30830;&#34920;&#31034;&#65292;&#21516;&#26102;&#19982;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20284;&#29305;&#24449;&#19968;&#36215;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception datasets for agriculture are limited both in quantity and diversity which hinders effective training of supervised learning approaches. Self-supervised learning techniques alleviate this problem, however, existing methods are not optimized for dense prediction tasks in agriculture domains which results in degraded performance. In this work, we address this limitation with our proposed Injected Noise Discriminator (INoD) which exploits principles of feature replacement and dataset discrimination for self-supervised representation learning. INoD interleaves feature maps from two disjoint datasets during their convolutional encoding and predicts the dataset affiliation of the resultant feature map as a pretext task. Our approach enables the network to learn unequivocal representations of objects seen in one dataset while observing them in conjunction with similar features from the disjoint dataset. This allows the network to reason about higher-level semantics of the entailed o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.18005</link><description>&lt;p&gt;
&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Ovarian Cancer Histopathology: A Systematic Review. (arXiv:2303.18005v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18005
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;36&#31687;&#25991;&#31456;&#30340;&#32508;&#36848;&#65292;&#35813;&#30740;&#31350;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#21463;&#21040;&#23567;&#26679;&#26412;&#37327;&#65292;&#28508;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;-&#29305;&#24449;&#21270;&#21644;&#35780;&#20272;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#21033;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#25968;&#25454;&#36827;&#34892;&#21365;&#24034;&#30284;&#35786;&#26029;&#25110;&#39044;&#21518;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;&#26041;&#27861;-&#22312;2022&#24180;1&#26376;12&#26085;&#20043;&#21069;&#65292;&#23545;5&#20010;&#26469;&#28304;&#36827;&#34892;&#25628;&#32034;&#12290;&#21253;&#25324;&#26631;&#20934;&#35201;&#27714;&#30740;&#31350;&#35780;&#20272;AI&#22312;&#21365;&#24034;&#30284;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#19978;&#65292;&#23545;&#21365;&#24034;&#30284;&#65292;&#21253;&#25324;&#36755;&#21365;&#31649;&#21365;&#24034;&#21644;&#33145;&#33180;&#32959;&#30244;&#30340;&#35786;&#26029;&#25110;&#39044;&#21518;&#25512;&#26029;&#12290;&#25490;&#38500;&#35780;&#35770;&#21644;&#38750;&#33521;&#35821;&#25991;&#31456;&#12290;&#23545;&#27599;&#20010;&#21253;&#21547;&#30340;&#27169;&#22411;&#20351;&#29992;PROBAST&#35780;&#20272;&#20559;&#20506;&#39118;&#38505;&#12290;&#32467;&#26524;-&#20849;&#21457;&#29616;1434&#31687;&#30740;&#31350;&#25991;&#31456;&#65292;&#20854;&#20013;36&#31687;&#31526;&#21512;&#32435;&#20837;&#26631;&#20934;&#12290;&#36825;&#20123;&#30740;&#31350;&#25253;&#21578;&#20102;62&#20010;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;35&#20010;&#20998;&#31867;&#22120;&#65292;14&#20010;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#65292;7&#20010;&#20998;&#21106;&#27169;&#22411;&#21644;6&#20010;&#22238;&#24402;&#27169;&#22411;&#12290;&#20351;&#29992;1-1375&#24352;&#20174;1-664&#20010;&#21365;&#24034;&#30284;&#24739;&#32773;&#20013;&#24471;&#21040;&#30340;&#24187;&#28783;&#29255;&#24320;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#39044;&#27979;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24635;&#20307;&#29983;&#23384;&#65288;9/62&#65289;&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#65288;7/62&#65289;&#21644;&#28107;&#24052;&#32467;&#29366;&#24577;&#65288;6/62&#65289;&#12290;&#32467;&#35770;-&#22522;&#20110;&#21487;&#29992;&#30340;&#25991;&#29486;&#65292;AI&#27169;&#22411;&#22312;&#21365;&#24034;&#30284;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#21463;&#21040;&#26679;&#26412;&#37327;&#23567;&#12289;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#32570;&#20047;&#22806;&#37096;&#39564;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose - To characterise and assess the quality of published research evaluating artificial intelligence (AI) methods for ovarian cancer diagnosis or prognosis using histopathology data. Methods - A search of 5 sources was conducted up to 01/12/2022. The inclusion criteria required that research evaluated AI on histopathology images for diagnostic or prognostic inferences in ovarian cancer, including tubo-ovarian and peritoneal tumours. Reviews and non-English language articles were excluded. The risk of bias was assessed for every included model using PROBAST. Results - A total of 1434 research articles were identified, of which 36 were eligible for inclusion. These studies reported 62 models of interest, including 35 classifiers, 14 survival prediction models, 7 segmentation models, and 6 regression models. Models were developed using 1-1375 slides from 1-664 ovarian cancer patients. A wide array of outcomes were predicted, including overall survival (9/62), histological subtypes (7
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#26426;&#22120;&#20154;&#21253;&#35013;&#20013;&#30340;&#20256;&#36865;&#24102;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#20313;&#19979;&#25511;&#21046;&#31995;&#32479;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2303.14693</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#26426;&#22120;&#20154;&#21253;&#35013;
&lt;/p&gt;
&lt;p&gt;
Robotic Packaging Optimization with Reinforcement Learning. (arXiv:2303.14693v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#26426;&#22120;&#20154;&#21253;&#35013;&#20013;&#30340;&#20256;&#36865;&#24102;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#20313;&#19979;&#25511;&#21046;&#31995;&#32479;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26497;&#22823;&#22320;&#25552;&#21319;&#29983;&#20135;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#26234;&#33021;&#21046;&#36896;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#33258;&#21160;&#21270;&#20108;&#27425;&#39135;&#21697;&#21253;&#35013;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#39135;&#21697;&#20135;&#21697;&#20174;&#20256;&#36865;&#24102;&#36716;&#31227;&#21040;&#23481;&#22120;&#20013;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#19981;&#21516;&#30340;&#20135;&#21697;&#20379;&#24212;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#20135;&#29575;&#30340;&#24613;&#21095;&#19979;&#38477;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#24448;&#24448;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#20250;&#23548;&#33268;&#34892;&#19994;&#35201;&#27714;&#30340;&#36829;&#35268;&#12290;&#24378;&#21270;&#23398;&#20064;&#21017;&#36890;&#36807;&#23398;&#20064;&#21709;&#24212;&#21644;&#39044;&#27979;&#31574;&#30053;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25511;&#21046;&#26041;&#26696;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20256;&#36865;&#24102;&#36895;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#23545;&#20313;&#19979;&#25511;&#21046;&#31995;&#32479;&#30340;&#24178;&#25200;&#12290;&#32463;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#35813;&#26694;&#26550;&#30340;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent manufacturing is becoming increasingly important due to the growing demand for maximizing productivity and flexibility while minimizing waste and lead times. This work investigates automated secondary robotic food packaging solutions that transfer food products from the conveyor belt into containers. A major problem in these solutions is varying product supply which can cause drastic productivity drops. Conventional rule-based approaches, used to address this issue, are often inadequate, leading to violation of the industry's requirements. Reinforcement learning, on the other hand, has the potential of solving this problem by learning responsive and predictive policy, based on experience. However, it is challenging to utilize it in highly complex control schemes. In this paper, we propose a reinforcement learning framework, designed to optimize the conveyor belt speed while minimizing interference with the rest of the control system. When tested on real-world data, the fram
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#36825;&#31181;&#35299;&#37322;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#19979;&#30340;&#34892;&#20026;&#12290;&#20027;&#20307;&#21487;&#20197;&#22312;&#25509;&#25910;&#30340;&#35299;&#37322;&#20013;&#37096;&#20998;&#22320;&#23653;&#34892;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21644;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#65292;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.11111</link><description>&lt;p&gt;
&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#30410;&#22788;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Iterative Partial Fulfillment of Counterfactual Explanations: Benefits and Risks. (arXiv:2303.11111v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11111
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#37096;&#20998;&#28385;&#36275;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#36825;&#31181;&#35299;&#37322;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#19979;&#30340;&#34892;&#20026;&#12290;&#20027;&#20307;&#21487;&#20197;&#22312;&#25509;&#25910;&#30340;&#35299;&#37322;&#20013;&#37096;&#20998;&#22320;&#23653;&#34892;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21644;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#65292;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#65288;CF&#65289;&#35299;&#37322;&#65292;&#20063;&#31216;&#20026;&#23545;&#27604;&#35299;&#37322;&#21644;&#31639;&#27861;&#22238;&#24402;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#38024;&#23545;&#19968;&#20010;&#21463;&#21040;&#36127;&#38754;&#39044;&#27979;&#30340;&#20027;&#20307;&#65288;&#20363;&#22914;&#65292;&#25298;&#32477;&#25151;&#36151;&#30003;&#35831;&#65289;&#65292;CF&#35299;&#37322;&#26159;&#30456;&#20284;&#30340;&#24773;&#20917;&#65292;&#20294;&#39044;&#27979;&#32467;&#26524;&#20026;&#27491;&#38754;&#65292;&#36825;&#21578;&#30693;&#20027;&#20307;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#21508;&#31181;&#23646;&#24615;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20363;&#22914;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23646;&#24615;&#65306;&#22312;&#36845;&#20195;&#37096;&#20998;&#23653;&#34892;&#65288;IPF&#65289;&#19979;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#25509;&#25910;&#21040;CF&#35299;&#37322;&#21518;&#65292;&#20027;&#20307;&#21487;&#33021;&#21482;&#33021;&#37096;&#20998;&#22320;&#23653;&#34892;&#23427;&#65292;&#28982;&#21518;&#35831;&#27714;&#19968;&#20010;&#26032;&#30340;&#39044;&#27979;&#21450;&#26032;&#30340;&#35299;&#37322;&#65292;&#37325;&#22797;&#27492;&#36807;&#31243;&#30452;&#21040;&#39044;&#27979;&#20026;&#27491;&#38754;&#12290;&#36825;&#31181;&#37096;&#20998;&#23653;&#34892;&#21487;&#33021;&#26159;&#30001;&#20110;&#20027;&#20307;&#30340;&#33021;&#21147;&#26377;&#38480;&#65288;&#20363;&#22914;&#65292;&#27492;&#26102;&#21482;&#33021;&#25903;&#20184;&#22235;&#24352;&#20449;&#29992;&#21345;&#24080;&#25143;&#20013;&#30340;&#20004;&#24352;&#65289;&#25110;&#35797;&#22270;&#20882;&#38505;&#65288;&#20363;&#22914;&#65292;&#25276;&#27880;800&#32654;&#20803;&#30340;&#26376;&#34218;&#22686;&#38271;&#36275;&#22815;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual (CF) explanations, also known as contrastive explanations and algorithmic recourses, are popular for explaining machine learning models in high-stakes domains. For a subject that receives a negative model prediction (e.g., mortgage application denial), the CF explanations are similar instances but with positive predictions, which informs the subject of ways to improve. While their various properties have been studied, such as validity and stability, we contribute a novel one: their behaviors under iterative partial fulfillment (IPF). Specifically, upon receiving a CF explanation, the subject may only partially fulfill it before requesting a new prediction with a new explanation, and repeat until the prediction is positive. Such partial fulfillment could be due to the subject's limited capability (e.g., can only pay down two out of four credit card accounts at this moment) or an attempt to take the chance (e.g., betting that a monthly salary increase of $800 is enough eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.02393</link><description>&lt;p&gt;
Seq-HyGAN: &#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Seq-HyGAN: Sequence Classification via Hypergraph Attention Network. (arXiv:2303.02393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;Seq-HyGAN&#65292;&#36890;&#36807;&#21019;&#24314;&#36229;&#22270;&#21644;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#20998;&#31867;&#22312;&#19981;&#21516;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#22522;&#22240;&#32452;&#20998;&#31867;&#21644;&#22312;&#21830;&#19994;&#39046;&#22495;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#25968;&#25454;&#20013;&#32570;&#20047;&#26174;&#24335;&#30340;&#29305;&#24449;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20165;&#38480;&#20110;&#25429;&#33719;&#30456;&#37051;&#32467;&#26500;&#36830;&#25509;&#24182;&#24573;&#30053;&#24207;&#21015;&#20043;&#38388;&#30340;&#20840;&#23616;&#12289;&#39640;&#38454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Seq-HyGAN&#12290;&#20026;&#20102;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22797;&#26434;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20854;&#20013;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#36229;&#36793;&#65292;&#20174;&#24207;&#21015;&#20013;&#25552;&#21462;&#30340;&#23376;&#24207;&#21015;&#34987;&#25551;&#32472;&#20026;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#21452;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sequence classification has a wide range of real-world applications in different domains, such as genome classification in health and anomaly detection in business. However, the lack of explicit features in sequence data makes it difficult for machine learning models. While Neural Network (NN) models address this with learning features automatically, they are limited to capturing adjacent structural connections and ignore global, higher-order information between the sequences. To address these challenges in the sequence classification problems, we propose a novel Hypergraph Attention Network model, namely Seq-HyGAN. To capture the complex structural similarity between sequence data, we first create a hypergraph where the sequences are depicted as hyperedges and subsequences extracted from sequences are depicted as nodes. Additionally, we introduce an attention-based Hypergraph Neural Network model that utilizes a two-level attention mechanism. This model generates a sequence representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24110;&#21161;&#31616;&#21270;DNS&#23457;&#26597;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;</title><link>http://arxiv.org/abs/2302.02031</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#35268;&#21017;&#22411;&#22495;&#21517;&#31995;&#32479;DNS&#23457;&#26597;&#26816;&#27979;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning. (arXiv:2302.02031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24110;&#21161;&#31616;&#21270;DNS&#23457;&#26597;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#26816;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#23457;&#26597;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#22823;&#37327;&#30417;&#27979;&#21644;&#26333;&#20809;&#30340;&#27979;&#37327;&#24179;&#21488;&#30340;&#21457;&#23637;&#12290;&#22495;&#21517;&#31995;&#32479;&#65288;DNS&#65289;&#30340;&#23457;&#26597;&#26159;&#19981;&#21516;&#22269;&#23478;&#20351;&#29992;&#30340;&#20851;&#38190;&#26426;&#21046;&#65292;&#30446;&#21069;&#36890;&#36807;&#23545;&#29305;&#23450;&#30446;&#30340;&#22320;&#30340;DNS&#26597;&#35810;&#21644;&#21709;&#24212;&#65288;&#25506;&#38024;&#65289;&#26679;&#26412;&#24212;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#26082;&#19982;&#24179;&#21488;&#29305;&#23450;&#30456;&#20851;&#65292;&#20063;&#21457;&#29616;&#24403;&#23457;&#26597;&#32773;&#25913;&#21464;&#20854;&#38459;&#27490;&#34892;&#20026;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#38656;&#35201;&#26356;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#23457;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#22914;&#20309;&#65288;1&#65289;&#24110;&#21161;&#31616;&#21270;&#26816;&#27979;&#36807;&#31243;&#65292;&#65288;2&#65289;&#25552;&#39640;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#26597;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#65288;3&#65289;&#21457;&#29616;&#29616;&#26377;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38169;&#36807;&#30340;&#26032;&#23457;&#26597;&#23454;&#20363;&#21644;&#38459;&#27490;&#26631;&#24535;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#19987;&#23478;&#27966;&#29983;&#26631;&#31614;&#35757;&#32451;&#30340;&#30417;&#30563;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26816;&#27979;&#23457;&#26597;&#21644;&#24322;&#24120;&#30340;&#24050;&#30693;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship.  In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the potential of using large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the det
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAP&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#26469;&#25913;&#21892;&#34892;&#36710;&#35270;&#39057;&#20013;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#12290;&#36825;&#20010;&#26041;&#27861;&#32771;&#34385;&#20102;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#21644;&#29615;&#22659;&#21464;&#21270;&#31561;&#25361;&#25112;&#65292;&#24076;&#26395;&#33021;&#20026;&#26410;&#26469;&#30340;&#23433;&#20840;&#39550;&#39542;&#31995;&#32479;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2212.09381</link><description>&lt;p&gt;
&#34892;&#36710;&#22330;&#26223;&#20013;&#30340;&#35748;&#30693;&#20107;&#25925;&#39044;&#27979;:&#19968;&#31181;&#22810;&#27169;&#24577;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cognitive Accident Prediction in Driving Scenes: A Multimodality Benchmark. (arXiv:2212.09381v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAP&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#26469;&#25913;&#21892;&#34892;&#36710;&#35270;&#39057;&#20013;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#12290;&#36825;&#20010;&#26041;&#27861;&#32771;&#34385;&#20102;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#21644;&#29615;&#22659;&#21464;&#21270;&#31561;&#25361;&#25112;&#65292;&#24076;&#26395;&#33021;&#20026;&#26410;&#26469;&#30340;&#23433;&#20840;&#39550;&#39542;&#31995;&#32479;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#36710;&#35270;&#39057;&#30340;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#26088;&#22312;&#25552;&#26089;&#21457;&#29616;&#20107;&#25925;&#30340;&#21457;&#29983;&#65292;&#25903;&#25345;&#23433;&#20840;&#39550;&#39542;&#31995;&#32479;&#30340;&#20915;&#31574;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20851;&#27880;&#29289;&#20307;&#32423;&#19978;&#19979;&#25991;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#22826;&#36866;&#21512;&#20869;&#22312;&#30340;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#20005;&#37325;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20107;&#25925;&#39044;&#27979;&#65288;CAP&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#21033;&#29992;&#20102;&#20154;&#31867;&#21551;&#21457;&#30340;&#23545;&#35270;&#35273;&#35266;&#23519;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#19978;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#35748;&#30693;&#26469;&#20419;&#36827;&#27169;&#22411;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25991;&#26412;&#25551;&#36848;&#20026;&#20132;&#36890;&#22330;&#26223;&#30340;&#20027;&#35201;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#23494;&#38598;&#30340;&#35821;&#20041;&#25551;&#36848;&#25351;&#23548;&#65292;&#32780;&#39550;&#39542;&#21592;&#30340;&#27880;&#24847;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#29301;&#24341;&#21147;&#65292;&#20351;&#20854;&#19987;&#27880;&#20110;&#19982;&#23433;&#20840;&#39550;&#39542;&#23494;&#20999;&#30456;&#20851;&#30340;&#20851;&#38190;&#21306;&#22495;&#12290;CAP&#30001;&#27880;&#37325;&#25991;&#26412;&#21040;&#35270;&#35273;&#36716;&#31227;&#34701;&#21512;&#27169;&#22359;&#12289;&#27880;&#37325;&#22330;&#26223;&#19978;&#19979;&#25991;&#36716;&#31227;&#27169;&#22359;&#21644;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#20107;&#25925;&#39044;&#27979;&#32593;&#32476;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic accident prediction in driving videos aims to provide an early warning of the accident occurrence, and supports the decision making of safe driving systems. Previous works usually concentrate on the spatial-temporal correlation of object-level context, while they do not fit the inherent long-tailed data distribution well and are vulnerable to severe environmental change. In this work, we propose a Cognitive Accident Prediction (CAP) method that explicitly leverages human-inspired cognition of text description on the visual observation and the driver attention to facilitate model training. In particular, the text description provides a dense semantic description guidance for the primary context of the traffic scene, while the driver attention provides a traction to focus on the critical region closely correlating with safe driving. CAP is formulated by an attentive text-to-vision shift fusion module, an attentive scene context transfer module, and the driver attention guided acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#25512;&#29702;&#33258;&#36523;&#30693;&#35782;&#12289;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#20132;&#27969;&#12289;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#23558;&#20854;&#19982;IFOL&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#30456;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.07935</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#30340;&#24378;&#20154;&#24037;&#26234;&#33021;&#33258;&#25105;&#30830;&#20999;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Strong-AI Autoepistemic Robots Build on Intensional First Order Logic. (arXiv:2212.07935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#25512;&#29702;&#33258;&#36523;&#30693;&#35782;&#12289;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#20132;&#27969;&#12289;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#23558;&#20854;&#19982;IFOL&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#35797;&#22270;&#20197;&#20114;&#34917;&#30340;&#26041;&#24335;&#38598;&#25104;&#31070;&#32463;&#21644;&#31526;&#21495;&#26550;&#26500;&#65292;&#20197;&#24212;&#23545;&#27599;&#31181;&#26550;&#26500;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#25903;&#25345;&#20855;&#26377;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#35748;&#30693;&#24314;&#27169;&#33021;&#21147;&#30340;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;(IFOL)&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#21442;&#29031;&#21644;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#25512;&#29702;&#20854;&#33258;&#36523;&#30693;&#35782;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#26426;&#22120;&#20154;&#20351;&#29992;&#20854;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#26469;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#24182;&#23558;&#36825;&#31181;&#32463;&#39564;&#19982;IFOL&#30340;PRP&#65288;&#23646;&#24615;/&#20851;&#31995;/&#21629;&#39064;&#65289;&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#65288;&#29305;&#23450;/&#20010;&#20307;&#21644;&#26222;&#36941;&#27010;&#24565;&#65289;&#30340;&#25366;&#25496;&#65288;&#24863;&#35273;&#65289;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#32771;&#34385;&#26426;&#22120;&#20154;&#30340;&#22235;&#32423;&#30693;&#35782;&#32467;&#26500;&#65306;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#30340;&#35821;&#27861;&#23618;&#38754;&#65288;&#24847;&#22823;&#21033;&#35821;&#12289;&#27861;&#35821;&#31561;&#65289;&#12289;&#20004;&#20010;&#36890;&#29992;&#35821;&#35328;&#23618;&#38754;&#65306;&#20854;&#35821;&#20041;&#20851;&#31995;&#21450;&#23427;&#30340;&#20869;&#28085;&#36923;&#36753;&#32467;&#26500;&#21644;&#20854;&#23427;&#35821;&#35328;&#27010;&#24565;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust strong AI capable of reasoning, learning, and cognitive modeling. In this paper we consider the  intensional First Order Logic (IFOL) as a symbolic architecture of modern robots, able to use natural languages to communicate with humans and to reason about their own knowledge with self-reference and abstraction language property.  We intend to obtain the grounding of robot's language by experience of how it uses its neuronal architectures and hence by associating this experience with the mining (sense) of non-defined language concepts (particulars/individuals and universals) in PRP (Properties/Relations/Propositions) theory of IFOL.  We consider the robot's four-levels knowledge structure: The syntax level of particular natural language (Italian, French, etc..), two universal language levels: its sem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.00979</link><description>&lt;p&gt;
PASTA&#65306;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#29992;&#20110; Syn-to-Real &#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27604;&#20363;&#24133;&#24230;&#35889;&#35757;&#32451;&#22686;&#24378;&#30340;&#26041;&#27861; PASTA&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010; Syn-to-Real &#20219;&#21153;&#19978;&#22343;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25552;&#20379;&#24265;&#20215;&#19988;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#35780;&#20272;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#34920;&#29616;&#26174;&#33879;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Proportional Amplitude Spectrum Training Augmentation (PASTA)&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#25552;&#39640;&#21512;&#25104;&#21040;&#30495;&#23454;&#65288;Syn-to-Real&#65289;&#27867;&#21270;&#24615;&#33021;&#12290; PASTA &#22312; Fourier &#39046;&#22495;&#20013;&#25200;&#21160;&#21512;&#25104;&#22270;&#20687;&#30340;&#24133;&#24230;&#35889;&#20197;&#29983;&#25104;&#22686;&#24378;&#35270;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992; PASTA&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#25200;&#21160;&#31574;&#30053;&#65292;&#20854;&#20013;&#39640;&#39057;&#20998;&#37327;&#30456;&#23545;&#20110;&#20302;&#39057;&#20998;&#37327;&#26356;&#23481;&#26131;&#21463;&#21040;&#25200;&#21160;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65288;GTAV-to-Real&#65289;&#65292;&#30446;&#26631;&#26816;&#27979;&#65288;Sim10K-to-Real&#65289;&#21644;&#23545;&#35937;&#35782;&#21035;&#65288;VisDA-C Syn-to-Real&#65289;&#20219;&#21153;&#65292;&#22312;&#24635;&#20849;5&#20010; Syn-to-Real &#36716;&#31227;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616; PASTA &#30340;&#24615;&#33021;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#30340;&#27867;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#20855;&#26377;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
&lt;/p&gt;</description></item><item><title>Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.00296</link><description>&lt;p&gt;
&#36890;&#36807; Lov\'asz Local Lemma &#36827;&#34892;&#37319;&#26679;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Combinatorial Structures via Markov Random Fields with Sampling through Lov\'asz Local Lemma. (arXiv:2212.00296v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00296
&lt;/p&gt;
&lt;p&gt;
Nelson&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;Lov\'asz Local Lemma&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#30001;&#20110;&#23398;&#20064;&#30446;&#26631;&#21463;&#21040;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#21046;&#32422;&#65292;&#20854;&#26799;&#24230;&#20272;&#35745;&#38750;&#24120;&#22797;&#26434;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Lov\'asz Local Lemma &#30340;&#31070;&#32463;&#32593;&#32476;&#65288;Nelson&#65289;&#65292;&#23427;&#33021;&#22815;&#20174;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#27169;&#22411;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#28385;&#36275;&#32452;&#21512;&#32422;&#26463;&#26465;&#20214;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models for learning combinatorial structures have transformative impacts in many applications. However, existing approaches fail to offer efficient and accurate learning results. Because of the highly intractable nature of the gradient estimation of the learning objective subject to combinatorial constraints. Existing gradient estimation methods would easily run into exponential time/memory space, or incur huge estimation errors due to improper approximation. We develop NEural Lovasz Sampler (Nelson), a neural network based on Lov\'asz Local Lemma (LLL). We show it guarantees to generate samples satisfying combinatorial constraints from the distribution of the constrained Markov Random Fields model (MRF) under certain conditions. We further present a fully differentiable contrastive-divergence-based learning framework on constrained MRF (Nelson-CD). Meanwhile, Nelson-CD being fully differentiable allows us to take advantage of the parallel computing power of GPUs, resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;</title><link>http://arxiv.org/abs/2211.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Explainable Action Advising for Multi-Agent Reinforcement Learning. (arXiv:2211.07882v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07882
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#35299;&#37322;&#34892;&#20026;&#24314;&#35758;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#29983;&#21487;&#20197;&#29702;&#35299;&#25152;&#23398;&#30340;&#20869;&#23481;&#24182;&#36827;&#34892;&#25512;&#29702;&#20174;&#32780;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#24314;&#35758;&#26159;&#19968;&#31181;&#22522;&#20110;&#24072;&#29983;&#33539;&#24335;&#30340;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;&#36716;&#31227;&#25216;&#26415;&#12290;&#19987;&#23478;&#32769;&#24072;&#22312;&#35757;&#32451;&#26399;&#38388;&#25552;&#20379;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#23398;&#29983;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#31574;&#30053;&#34920;&#29616;&#12290;&#36825;&#31181;&#24314;&#35758;&#36890;&#24120;&#20197;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20351;&#24471;&#23398;&#29983;&#38590;&#20197;&#25512;&#29702;&#21644;&#24212;&#29992;&#20110;&#26032;&#39062;&#29366;&#24577;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#24314;&#35758;&#65292;&#20854;&#20013;&#32769;&#24072;&#25552;&#20379;&#34892;&#20026;&#24314;&#35758;&#21644;&#30456;&#20851;&#30340;&#35299;&#37322;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36873;&#21462;&#35813;&#34892;&#20026;.&#36825;&#20801;&#35768;&#23398;&#29983;&#33258;&#25105;&#21453;&#24605;&#25152;&#23398;&#30340;&#20869;&#23481;&#65292;&#23454;&#29616;&#24314;&#35758;&#30340;&#27867;&#21270;&#65292;&#24182;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#30340;&#25552;&#39640;&#8212;&#8212;&#21363;&#20351;&#22312;&#32769;&#24072;&#19981;&#29702;&#24819;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#31574;&#30053;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student's sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05247</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Debiased Subnetworks with Contrastive Weight Pruning. (arXiv:2210.05247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23384;&#22312;&#24378;&#20551;&#30456;&#20851;&#30340;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#21098;&#26525;&#26435;&#37325;&#35757;&#32451;&#23454;&#29616;&#21435;&#20559;&#32622;&#23376;&#32593;&#32476;&#30340;&#31639;&#27861; DCWP&#65292;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#20559;&#32622;&#24615;&#65292;&#23548;&#33268;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#32479;&#35745;&#35777;&#25454;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#22312;&#20559;&#32622;&#32593;&#32476;&#20013;&#25552;&#21462;&#26368;&#20248;&#26080;&#20559;&#21151;&#33021;&#23376;&#32593;&#32476;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#25506;&#32034;&#20855;&#26377;&#24378;&#20551;&#30456;&#20851;&#24615;&#30340;&#26080;&#20559;&#23376;&#32593;&#32476;&#23384;&#22312;&#38480;&#21046;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#23545;&#32467;&#26500;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#30340;&#65288;&#20266;&#65289;&#26080;&#20559;&#26679;&#26412;&#21644;&#36873;&#25321;&#24615;&#20559;&#24046;&#20914;&#31361;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#21435;&#20559;&#32622;&#23545;&#27604;&#21098;&#26525;&#65288;DCWP&#65289;&#31639;&#27861;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102; DCWP &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are often biased to spuriously correlated features that provide misleading statistical evidence that does not generalize. This raises an interesting question: ``Does an optimal unbiased functional subnetwork exist in a severely biased network? If so, how to extract such subnetwork?" While empirical evidence has been accumulated about the existence of such unbiased subnetworks, these observations are mainly based on the guidance of ground-truth unbiased samples. Thus, it is unexplored how to discover the optimal subnetworks with biased training datasets in practice. To address this, here we first present our theoretical insight that alerts potential limitations of existing algorithms in exploring unbiased subnetworks in the presence of strong spurious correlations. We then further elucidate the importance of bias-conflicting samples on structure learning. Motivated by these observations, we propose a Debiased Contrastive Weight Pruning (DCWP) algorithm, which probes unbi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#21487;&#33021;&#23548;&#33268;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01874</link><description>&lt;p&gt;
&#26368;&#20339;&#20915;&#31574;&#24182;&#19981;&#26159;&#26368;&#20339;&#24314;&#35758;&#65306;&#21046;&#23450;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
The Best Decisions Are Not the Best Advice: Making Adherence-Aware Recommendations. (arXiv:2209.01874v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01874
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26469;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#21487;&#33021;&#23548;&#33268;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39640;&#39118;&#38505;&#20915;&#31574;&#36981;&#24490;&#19987;&#23478;&#21442;&#19982;&#30340;&#32467;&#26500;&#65292;&#21363;&#20154;&#31867;&#25805;&#20316;&#21592;&#20174;&#31639;&#27861;&#20013;&#25509;&#25910;&#24314;&#35758;&#20294;&#26368;&#32456;&#26159;&#20915;&#31574;&#21046;&#23450;&#32773;&#12290;&#22240;&#27492;&#65292;&#31639;&#27861;&#30340;&#24314;&#35758;&#21487;&#33021;&#19982;&#23454;&#36341;&#20013;&#23454;&#38469;&#25191;&#34892;&#30340;&#20915;&#31574;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#24314;&#35758;&#26159;&#36890;&#36807;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#33719;&#24471;&#30340;&#65292;&#35813;&#38382;&#39064;&#20551;&#23450;&#24314;&#35758;&#23558;&#23436;&#32654;&#25191;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26381;&#20174;&#24230;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#25429;&#33719;&#25512;&#33616;&#31574;&#30053;&#21644;&#23454;&#38469;&#25191;&#34892;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#20998;&#26512;&#37096;&#20998;&#26381;&#20174;&#24230;&#23545;&#26368;&#20339;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24573;&#30053;&#37096;&#20998;&#26381;&#20174;&#24230;&#29616;&#35937;&#65292;&#19982;&#30446;&#21069;&#22823;&#22810;&#25968;&#25512;&#33616;&#24341;&#25806;&#25152;&#20570;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#19982;&#24403;&#21069;&#20154;&#31867;&#22522;&#32447;&#24615;&#33021;&#21644;&#25512;&#33616;&#31639;&#27861;&#39044;&#26399;&#24615;&#33021;&#30456;&#27604;&#65292;&#20986;&#29616;&#20219;&#24847;&#20005;&#37325;&#30340;&#24615;&#33021;&#24694;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#26469;&#20998;&#26512;&#32467;&#26500;&#21644;&#35745;&#31639;&#26368;&#20339;&#25512;&#33616;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many high-stake decisions follow an expert-in-loop structure in that a human operator receives recommendations from an algorithm but is the ultimate decision maker. Hence, the algorithm's recommendation may differ from the actual decision implemented in practice. However, most algorithmic recommendations are obtained by solving an optimization problem that assumes recommendations will be perfectly implemented. We propose an adherence-aware optimization framework to capture the dichotomy between the recommended and the implemented policy and analyze the impact of partial adherence on the optimal recommendation. We show that overlooking the partial adherence phenomenon, as is currently being done by most recommendation engines, can lead to arbitrarily severe performance deterioration, compared with both the current human baseline performance and what is expected by the recommendation algorithm. Our framework also provides useful tools to analyze the structure and to compute optimal recom
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.04039</link><description>&lt;p&gt;
&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#36523;&#20221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04039
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#38656;&#35201;&#20154;&#31867;&#20132;&#20114;&#25110;&#21028;&#26029;&#30340;&#30740;&#31350;&#38382;&#39064;&#26041;&#38754;&#65292;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#30001;&#20110;&#25191;&#34892;&#30340;&#20219;&#21153;&#22810;&#26679;&#21270;&#21644;&#25968;&#25454;&#29992;&#36884;&#30340;&#22810;&#26679;&#24615;&#65292;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#23558;&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#24037;&#20154;(&#32780;&#38750;&#20154;&#31867;&#21463;&#35797;&#32773;)&#12290;&#36825;&#20123;&#22256;&#38590;&#21152;&#21095;&#20102;&#25919;&#31574;&#30340;&#20914;&#31361;&#65292;&#19968;&#20123;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#23558;&#25152;&#26377;ML&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#20154;&#31867;&#21463;&#35797;&#32773;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#23427;&#20204;&#24456;&#23569;&#26500;&#25104;&#20154;&#31867;&#21463;&#35797;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21253;&#25324;&#20247;&#21253;&#24037;&#20316;&#30340;&#40092;&#26377;ML&#35770;&#25991;&#25552;&#21040;IRB&#30340;&#30417;&#30563;&#65292;&#24341;&#21457;&#20102;&#36829;&#21453;&#36947;&#24503;&#21644;&#27861;&#35268;&#35201;&#27714;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#20247;&#21253;&#30740;&#31350;&#30340;&#36866;&#24403;&#21010;&#23450;&#65292;&#24182;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26292;&#38706;&#20986;&#30340;&#29420;&#29305;&#30740;&#31350;&#30417;&#30563;&#25361;&#25112;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#32654;&#22269;&#20844;&#20849;&#35268;&#21017;&#19979;&#65292;&#36825;&#20123;&#21028;&#26029;&#21462;&#20915;&#20110;&#20851;&#20110;&#38382;&#39064;&#30340;&#30830;&#23450;&#65292;&#28041;&#21450;&#35841;(&#25110;&#20160;&#20040;)&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#21452;&#23556;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#25214;&#21040;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35782;&#21035;&#32431;&#26420;-&#21452;&#23556;&#30340;&#35770;&#35777;&#26694;&#26550;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#20837;&#24230;&#26377;&#38480;&#30340;&#26694;&#26550;&#20013;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#31616;&#21270;&#30340;&#33258;&#21355;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#20043;&#38388;&#23384;&#22312;&#21452;&#23556;&#12290;</title><link>http://arxiv.org/abs/2202.05506</link><description>&lt;p&gt;
&#35770;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#65306;&#19982;&#32431;&#26420;&#38598;&#21512;&#30340;&#21452;&#23556;
&lt;/p&gt;
&lt;p&gt;
On the preferred extensions of argumentation frameworks: bijections with naive sets. (arXiv:2202.05506v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#21452;&#23556;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#25214;&#21040;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#35782;&#21035;&#32431;&#26420;-&#21452;&#23556;&#30340;&#35770;&#35777;&#26694;&#26550;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#20837;&#24230;&#26377;&#38480;&#30340;&#26694;&#26550;&#20013;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#31616;&#21270;&#30340;&#33258;&#21355;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#20043;&#38388;&#23384;&#22312;&#21452;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#21452;&#23556;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#26159;&#32431;&#26420;-&#21452;&#23556;&#30340;&#24773;&#20917;&#65306;&#20854;&#32431;&#26420;&#38598;&#21512;&#21644;&#20248;&#20808;&#25512;&#29702;&#30456;&#31561;&#12290;&#35782;&#21035;&#32431;&#26420;-&#21452;&#23556;&#30340;&#35770;&#35777;&#26694;&#26550;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20837;&#24230;&#26377;&#38480;&#30340;&#26694;&#26550;&#20013;&#36825;&#26159;&#21487;&#34892;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#19982;&#19968;&#20010;&#19982;&#20854;&#30456;&#21516;&#30340;&#35770;&#28857;&#38598;&#19978;&#30340;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#26159;&#21487;&#25509;&#21463;&#23553;&#38381;&#30340;&#65288;&#20004;&#20010;&#21487;&#25509;&#21463;&#38598;&#21512;&#30340;&#20132;&#26159;&#21487;&#25509;&#21463;&#30340;&#65289;&#30340;&#21452;&#23556;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37492;&#21035;&#21487;&#25509;&#21463;&#23553;&#38381;&#30340;&#35770;&#35777;&#26694;&#26550;&#26159;coNP&#23436;&#22791;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21487;&#31616;&#21270;&#30340;&#33258;&#21355;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#36825;&#20123;&#38598;&#21512;&#19981;&#26159;&#20854;&#20182;&#38598;&#21512;&#30340;&#24182;&#38598;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#19968;&#20010;&#35770;&#35777;&#26694;&#26550;&#30340;&#20248;&#20808;&#25512;&#29702;&#19982;&#21478;&#19968;&#20010;&#26694;&#26550;&#30340;&#32431;&#26420;&#38598;&#21512;&#20043;&#38388;&#23384;&#22312;&#21452;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with the problem of finding the preferred extensions of an argumentation framework by means of a bijection with the naive sets of another framework. First, we consider the case where an argumentation framework is naive-bijective: its naive sets and preferred extensions are equal. Recognizing naive-bijective argumentation frameworks is hard, but we show that it is tractable for frameworks with bounded in-degree. Next, we give a bijection between the preferred extensions of an argumentation framework being admissible-closed (the intersection of two admissible sets is admissible) and the naive sets of another framework on the same set of arguments. On the other hand, we prove that identifying admissible-closed argumentation frameworks is coNP-complete. At last, we introduce the notion of irreducible self-defending sets as those that are not the union of others. It turns out there exists a bijection between the preferred extensions of an argumentation framework and the nai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27010;&#29575;&#29702;&#35770;(GPT)&#30340;&#32852;&#24819;&#35760;&#24518;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#26356;&#22909;&#22320;&#22788;&#29702;&#20855;&#26377;&#25351;&#25968;&#32423;&#20248;&#21183;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2201.12305</link><description>&lt;p&gt;
&#21518;&#37327;&#23376;&#32852;&#24819;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
A Post-Quantum Associative Memory. (arXiv:2201.12305v2 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#27010;&#29575;&#29702;&#35770;(GPT)&#30340;&#32852;&#24819;&#35760;&#24518;&#30340;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;GPT&#21487;&#20197;&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#26356;&#22909;&#22320;&#22788;&#29702;&#20855;&#26377;&#25351;&#25968;&#32423;&#20248;&#21183;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#24819;&#35760;&#24518;&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#37096;&#20998;&#20449;&#24687;&#26816;&#32034;&#23436;&#25972;&#20449;&#24687;&#30340;&#35774;&#22791;&#12290;&#22312;&#24191;&#20041;&#27010;&#29575;&#35770;&#65288;GPT&#65289;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#24819;&#35760;&#24518;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#25152;&#21463;&#21040;&#30340;&#26497;&#38480;&#38480;&#21046;&#12290;&#22312;GPT&#30340;&#35268;&#23450;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#20197;&#23481;&#32435;$2^m$&#20010;&#29366;&#24577;&#19988;&#20854;&#20013;&#30340;&#20219;&#24847;$N$&#20010;&#29366;&#24577;&#37117;&#26159;&#23436;&#20840;&#21487;&#21306;&#20998;&#30340;&#26368;&#23567;GPT&#32500;&#24230;$d(N,m)$&#12290;&#36890;&#36807;&#24341;&#29992;Danzer&#21644;Gr&#252;nbaum&#30340;&#32769;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;$m$&#20026;2&#26102;$d(2,m)=m+1$&#65292;&#32780;&#22312;&#38656;&#35201;GPT&#20998;&#21035;&#20026;&#32463;&#20856;&#25110;&#37327;&#23376;&#29702;&#35770;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#32500;&#24230;&#20998;&#21035;&#20026;$O(2^m)$&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#20363;&#23376;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;GPT&#27604;&#32463;&#20856;&#21644;&#37327;&#23376;&#29702;&#35770;&#37117;&#26377;&#25351;&#25968;&#32423;&#30340;&#20248;&#21183;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22266;&#23450;$N$&#21644;&#28176;&#36817;&#22823;&#30340;$m$&#30340;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;$d(N,m) \leq m^{1+o_N(1)}$&#65288;&#24403;$m\to\infty$&#26102;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Associative memories are devices storing information that can be fully retrieved given partial disclosure of it. We examine a toy model of associative memory and the ultimate limitations it is subjected to within the framework of general probabilistic theories (GPTs), which represent the most general class of physical theories satisfying some basic operational axioms. We ask ourselves how large the dimension of a GPT should be so that it can accommodate $2^m$ states with the property that any $N$ of them are perfectly distinguishable. Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and Gr\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the GPT is required to be either classical or quantum. This yields an example of a task where GPTs outperform both classical and quantum theory exponentially. More generally, we resolve the case of fixed $N$ and asymptotically large $m$, proving that $d(N,m) \leq m^{1+o_N(1)}$ (as $m\to\infty$) for every 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#25351;&#21335;&#65292;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08265</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge-driven Active Learning. (arXiv:2110.08265v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#25351;&#21335;&#65292;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#30417;&#30563;&#25968;&#25454;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26088;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#22823;&#22810;&#25968;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#36873;&#25321;&#65292;&#36890;&#24120;&#20165;&#38480;&#20110;&#20301;&#20110;&#20915;&#31574;&#36793;&#30028;&#38468;&#36817;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#22522;&#20110;&#20869;&#23481;&#23545;&#25152;&#36873;&#26679;&#26412;&#30340;&#29702;&#35299;&#24182;&#19981;&#30452;&#35266;&#65292;&#36825;&#36827;&#19968;&#27493;&#23548;&#33268;&#38750;&#19987;&#19994;&#20154;&#22763;&#23558;&#28145;&#24230;&#23398;&#20064;&#35270;&#20026;&#40657;&#30418;&#23376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#32771;&#34385;&#36890;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20351;&#38750;&#19987;&#19994;&#29992;&#25143;&#33021;&#22815;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;(KAL)&#26694;&#26550;&#20013;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#32422;&#26463;&#65292;&#24182;&#26816;&#26597;&#20854;&#36829;&#21453;&#20316;&#20026;&#26679;&#26412;&#36873;&#25321;&#30340;&#33258;&#28982;&#25351;&#21335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#25968;&#25454;&#21644;&#36755;&#20986;&#31867;&#21035;&#20043;&#38388;&#30340;&#31616;&#21333;&#20851;&#31995;&#20063;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of Deep Learning (DL) models is still precluded in those contexts where the amount of supervised data is limited. To answer this issue, active learning strategies aim at minimizing the amount of labelled data required to train a DL model. Most active strategies are based on uncertain sample selection, and even often restricted to samples lying close to the decision boundary. These techniques are theoretically sound, but an understanding of the selected samples based on their content is not straightforward, further driving non-experts to consider DL as a black-box. For the first time, here we propose to take into consideration common domain-knowledge and enable non-expert users to train a model with fewer samples. In our Knowledge-driven Active Learning (KAL) framework, rule-based knowledge is converted into logic constraints and their violation is checked as a natural guide for sample selection. We show that even simple relationships among data and output classes offer a
&lt;/p&gt;</description></item></channel></rss>