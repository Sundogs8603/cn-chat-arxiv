<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2402.13249</link><description>&lt;p&gt;
TofuEval&#65306;&#35780;&#20272;LLM&#22312;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13249
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;TofuEval&#65292;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#24182;&#34920;&#26126;&#24403;LLMs&#20805;&#24403;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#20854;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25991;&#26723;&#26032;&#38395;&#25688;&#35201;&#22312;&#24544;&#23454;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#36827;&#27493;&#65292;&#36825;&#24471;&#30410;&#20110;&#23545;&#20107;&#23454;&#19968;&#33268;&#24615;&#25110;&#24187;&#35273;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#36827;&#23637;&#26159;&#21542;&#33021;&#24310;&#20280;&#21040;&#20854;&#20182;&#25991;&#26412;&#25688;&#35201;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20027;&#39064;&#23545;&#35805;&#25688;&#35201;&#35780;&#20272;&#22522;&#20934;&#65292;&#30001;&#19981;&#21516;&#35268;&#27169;&#30340;LLMs&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20123;&#25688;&#35201;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#20108;&#20803;&#21477;&#32423;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#21450;&#23545;&#20107;&#23454;&#19981;&#19968;&#33268;&#21477;&#23376;&#30340;&#35814;&#32454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;LLMs&#22312;&#23545;&#35805;&#39046;&#22495;&#23384;&#22312;&#22823;&#37327;&#20107;&#23454;&#38169;&#35823;&#30340;&#24187;&#35273;&#65292;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;LLMs&#65288;&#21253;&#25324;GPT-4&#65289;&#20805;&#24403;&#20108;&#20803;&#20107;&#23454;&#35780;&#20272;&#22120;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#21487;&#20197;&#34987;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#19987;&#38376;&#20107;&#23454;&#35780;&#20272;&#24230;&#37327;&#25152;&#36229;&#36234;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24187;&#35273;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13241</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Federated Causal Discovery from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#25968;&#25454;&#65292;&#36825;&#19982;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#25968;&#25454;&#30340;&#20998;&#25955;&#24615;&#36136;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#25512;&#21160;&#20102;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#65288;FCD&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FCD&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#20854;&#23545;&#21487;&#35782;&#21035;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#25110; homogeneous&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#26032;&#22411;FCD&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19982;&#23458;&#25143;&#31471;&#32034;&#24341;&#23545;&#24212;&#30340;&#26367;&#20195;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#39592;&#26550;&#21457;&#29616;&#30340;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;FCIT&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#30340;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#65288;FICP&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>NeRF&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27010;&#24565;&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#34920;&#31034;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#24471;&#21040;&#39640;&#32500;MR&#22270;&#20687;&#65292;&#24182;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.13226</link><description>&lt;p&gt;
NeRF&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NeRF Solves Undersampled MRI Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13226
&lt;/p&gt;
&lt;p&gt;
NeRF&#25216;&#26415;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#27010;&#24565;&#35299;&#20915;&#20102;MRI&#37325;&#24314;&#20013;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;&#65292;&#36890;&#36807;&#31070;&#32463;&#34920;&#31034;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#24471;&#21040;&#39640;&#32500;MR&#22270;&#20687;&#65292;&#24182;&#30740;&#31350;&#20102;&#26377;&#25928;&#30340;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#27010;&#24565;&#30340;&#26032;&#22411;&#27424;&#37319;&#26679;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25216;&#26415;&#12290;&#36890;&#36807;&#24452;&#21521;&#27424;&#37319;&#26679;&#65292;&#30456;&#24212;&#30340;&#25104;&#20687;&#38382;&#39064;&#21487;&#20197;&#20174;&#31232;&#30095;&#35270;&#22270;&#28210;&#26579;&#25968;&#25454;&#20013;&#37325;&#22609;&#20026;&#22270;&#20687;&#24314;&#27169;&#20219;&#21153;&#65307;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#20013;&#33719;&#24471;&#39640;&#32500;MR&#22270;&#20687;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#29992;&#20110;&#20174;&#31354;&#38388;&#22352;&#26631;&#36755;&#20986;&#22270;&#20687;&#24378;&#24230;&#65292;&#35813;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#32473;&#23450;&#27979;&#37327;&#25968;&#25454;&#21644;&#26399;&#26395;&#22270;&#20687;&#20043;&#38388;&#30340;MR&#29289;&#29702;&#39537;&#21160;&#28210;&#26579;&#20851;&#31995;&#12290;&#30740;&#31350;&#20102;&#29992;&#20110;&#39640;&#36136;&#37327;&#31070;&#32463;&#34920;&#31034;&#30340;&#26377;&#25928;&#27424;&#37319;&#26679;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;(i) &#23398;&#20064;&#23436;&#20840;&#22522;&#20110;&#21333;&#20010;&#27424;&#37319;&#26679;&#30340;$k$-space&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#19968;&#22534;&#27979;&#37327;&#25968;&#25454;&#21644;&#30446;&#26631;&#22270;&#20687;&#38598;&#12290;&#23427;&#21487;&#33021;&#29992;&#20110;&#35786;&#26029;&#24615;MR&#25104;&#20687;&#65292;&#20363;&#22914;&#32974;&#20799;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13226v1 Announce Type: cross  Abstract: This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF). With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation. A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image. Effective undersampling strategies for high-quality neural representation are investigated. The proposed method serves two benefits: (i) The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets. It can be used potentially for diagnostic MR imaging, such as fetal
&lt;/p&gt;</description></item><item><title>AgentMD&#26159;&#19968;&#31181;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#31579;&#36873;&#21644;&#24212;&#29992;&#21253;&#21547;2,164&#20010;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;RiskCalcs&#38598;&#21512;&#65292;&#20026;&#20811;&#26381;&#20020;&#24202;&#24037;&#20855;&#26131;&#29992;&#24615;&#25361;&#25112;&#21644;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.13225</link><description>&lt;p&gt;
AgentMD&#65306;&#20026;&#22823;&#35268;&#27169;&#20020;&#24202;&#24037;&#20855;&#23398;&#20064;&#36171;&#33021;&#35821;&#35328;&#20195;&#29702;&#20197;&#36827;&#34892;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13225
&lt;/p&gt;
&lt;p&gt;
AgentMD&#26159;&#19968;&#31181;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#33258;&#21160;&#31579;&#36873;&#21644;&#24212;&#29992;&#21253;&#21547;2,164&#20010;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;RiskCalcs&#38598;&#21512;&#65292;&#20026;&#20811;&#26381;&#20020;&#24202;&#24037;&#20855;&#26131;&#29992;&#24615;&#25361;&#25112;&#21644;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35745;&#31639;&#22120;&#36890;&#36807;&#20026;&#35832;&#22914;&#39044;&#21518;&#31561;&#21508;&#31181;&#30446;&#30340;&#25552;&#20379;&#20934;&#30830;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#39044;&#27979;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#21033;&#29992;&#24120;&#24120;&#21463;&#21040;&#26131;&#29992;&#24615;&#25361;&#25112;&#12289;&#20449;&#24687;&#20256;&#25773;&#19981;&#30021;&#21644;&#21151;&#33021;&#21463;&#38480;&#30340;&#38459;&#30861;&#12290;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24191;&#27867;&#30340;&#20020;&#24202;&#35745;&#31639;&#22120;&#38598;&#21512;&#30456;&#32467;&#21512;&#65292;&#20026;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#24182;&#25552;&#39640;&#24037;&#20316;&#27969;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#25163;&#21160;&#31579;&#36873;&#36807;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentMD&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#21508;&#31181;&#20020;&#24202;&#32972;&#26223;&#19979;&#31574;&#21010;&#21644;&#24212;&#29992;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;&#26032;&#22411;&#35821;&#35328;&#20195;&#29702;&#12290;AgentMD&#21033;&#29992;&#24050;&#21457;&#34920;&#30340;&#25991;&#29486;&#65292;&#33258;&#21160;&#31579;&#36873;&#20102;&#19968;&#32452;&#21253;&#21547;2,164&#20010;&#22810;&#26679;&#21270;&#20020;&#24202;&#35745;&#31639;&#22120;&#30340;&#38598;&#21512;&#65292;&#20855;&#26377;&#21487;&#25191;&#34892;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#25991;&#26723;&#65292;&#32479;&#31216;&#20026;RiskCalcs&#12290;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;RiskCalcs&#24037;&#20855;&#36798;&#21040;&#20102;&#19968;&#23450;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13225v1 Announce Type: cross  Abstract: Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#25511;&#21046;&#23460;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#21644;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#29366;&#24577;&#37327;&#36523;&#23450;&#21046;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.13219</link><description>&lt;p&gt;
&#20998;&#26512;&#25805;&#20316;&#21592;&#29366;&#24577;&#21644;AI&#22686;&#24378;&#20915;&#31574;&#25903;&#25345;&#23545;&#25511;&#21046;&#23460;&#30340;&#24433;&#21709;&#65306;&#19968;&#31181;&#20154;&#22312;&#22238;&#36335;&#20013;&#30340;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#24178;&#39044;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26088;&#22312;&#25913;&#21892;&#25511;&#21046;&#23460;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#21644;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#20026;&#20854;&#25552;&#20379;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#29366;&#24577;&#37327;&#36523;&#23450;&#21046;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#24037;&#19994;&#21644;&#21270;&#23398;&#36807;&#31243;&#25511;&#21046;&#23460;&#20013;&#65292;&#26377;&#25928;&#30340;&#20915;&#31574;&#23545;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#38598;&#25104;&#21040;&#25913;&#36827;&#30340;&#20154;&#26426;&#30028;&#38754;&#20013;&#30340;AI&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#24433;&#21709;&#21644;&#24212;&#29992;&#65292;&#20351;&#29992;&#21160;&#24577;&#24433;&#21709;&#22270;&#12289;&#38544;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#22686;&#24378;&#30340;&#25903;&#25345;&#31995;&#32479;&#26088;&#22312;&#20943;&#23569;&#25805;&#20316;&#21592;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;&#25552;&#39640;&#24773;&#22659;&#24847;&#35782;&#65292;&#24182;&#26681;&#25454;&#31995;&#32479;&#21644;&#20154;&#31867;&#34920;&#29616;&#30340;&#24403;&#21069;&#29366;&#24577;&#20026;&#25805;&#20316;&#21592;&#25552;&#20379;&#19981;&#21516;&#30340;&#24178;&#39044;&#31574;&#30053;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#22312;&#20449;&#24687;&#36807;&#36733;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#26377;&#29992;&#65292;&#24403;&#35768;&#22810;&#35686;&#25253;&#21644;&#36755;&#20837;&#21516;&#26102;&#21576;&#29616;&#22312;&#21516;&#19968;&#20010;&#26102;&#38388;&#31383;&#21475;&#20869;&#65292;&#25110;&#32773;&#22312;&#22521;&#35757;&#26399;&#38388;&#23545;&#21021;&#32423;&#25805;&#20316;&#21592;&#26469;&#35828;&#23588;&#20854;&#26377;&#29992;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20132;&#21449;&#25968;&#25454;&#20998;&#26512;&#65292;&#28041;&#21450;47&#21517;&#21442;&#19982;&#32773;&#21644;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#65292;&#22914;&#26234;&#33021;&#25163;&#34920;&#25351;&#26631;&#12289;&#30524;&#21160;&#25968;&#25454;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13219v1 Announce Type: new  Abstract: In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and effi- ciency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influ- ence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye- tracking data,
&lt;/p&gt;</description></item><item><title>VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13217</link><description>&lt;p&gt;
VideoPrism: &#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#22522;&#30784;&#35270;&#35273;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
VideoPrism: A Foundational Visual Encoder for Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13217
&lt;/p&gt;
&lt;p&gt;
VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VideoPrism&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#21333;&#20010;&#20923;&#32467;&#27169;&#22411;&#22788;&#29702;&#22810;&#26679;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;3600&#19975;&#39640;&#36136;&#37327;&#35270;&#39057;&#26631;&#39064;&#23545;&#21644;58.2&#20159;&#20010;&#24102;&#26377;&#22024;&#26434;&#24179;&#34892;&#25991;&#26412;&#65288;&#22914;ASR&#36716;&#24405;&#65289;&#30340;&#35270;&#39057;&#21098;&#36753;&#30340;&#24322;&#26500;&#35821;&#26009;&#24211;&#19978;&#23545;VideoPrism&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#19968;&#20010;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#25913;&#36827;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#65292;&#20351;VideoPrism&#33021;&#22815;&#20027;&#35201;&#19987;&#27880;&#20110;&#35270;&#39057;&#27169;&#24577;&#21516;&#26102;&#21033;&#29992;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#23453;&#36149;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#32452;&#19978;&#36827;&#34892;&#20102;&#23545;VideoPrism&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#20174;&#32593;&#32476;&#35270;&#39057;&#38382;&#31572;&#21040;&#31185;&#23398;CV&#65292; &#22312;33&#20010;&#35270;&#39057;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13217v1 Announce Type: cross  Abstract: We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>ConfHyena&#26159;&#19968;&#31181;&#22522;&#20110;Hyena&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#35821;&#38899;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#26174;&#33879;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.13208</link><description>&lt;p&gt;
&#20986;&#20110;&#24615;&#33021;&#32771;&#34385;&#65292;Hyena&#22914;&#20309;&#22788;&#29702;&#20154;&#31867;&#35821;&#38899;&#65311;&#20351;&#29992;ConfHyena&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13208
&lt;/p&gt;
&lt;p&gt;
ConfHyena&#26159;&#19968;&#31181;&#22522;&#20110;Hyena&#30340;&#25913;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#35821;&#38899;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#26174;&#33879;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;&#29616;&#20195;&#31070;&#32463;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#20294;&#30001;&#20110;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#36807;&#21435;&#20960;&#24180;&#30340;&#30740;&#31350;&#24037;&#20316;&#20391;&#37325;&#20110;&#23547;&#25214;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;Hyena&#65288;Poli&#31561;&#65292;2023&#24180;&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#27425;&#32447;&#24615;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22522;&#20110;&#36825;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConfHyena&#65292;&#36825;&#26159;&#19968;&#20010;Conformer&#65292;&#20854;&#32534;&#30721;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#34987;Hyena&#30340;&#19968;&#31181;&#21464;&#20307;&#21462;&#20195;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#38899;&#65292;&#20854;&#20013;&#38271;&#36755;&#20837;&#24207;&#21015;&#23548;&#33268;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;&#33521;&#35821;&#65289;&#21644;&#32763;&#35793;&#23454;&#39564;&#65288;&#20174;&#33521;&#35821;&#32763;&#35793;&#25104;8&#31181;&#30446;&#26631;&#35821;&#35328;&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26368;&#22909;&#30340;ConfHyena&#27169;&#22411;&#23558;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#20102;27%&#65292;&#32780;&#21697;&#36136;&#25439;&#22833;&#20165;&#20026;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13208v1 Announce Type: cross  Abstract: The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%
&lt;/p&gt;</description></item><item><title>&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#24182;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21387;&#32553;&#20197;&#36866;&#24212;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.13201</link><description>&lt;p&gt;
&#20351;&#29992;&#20915;&#31574;&#21464;&#25442;&#22120;&#36827;&#34892;&#22235;&#36275;&#21160;&#20316;&#30340;&#23567;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13201
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#24182;&#20351;&#29992;&#22870;&#21169;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21387;&#32553;&#20197;&#36866;&#24212;&#36164;&#28304;&#21463;&#38480;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13201v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21463;&#36164;&#28304;&#38480;&#21046;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#29305;&#21035;&#36866;&#29992;&#20110;&#38656;&#35201;&#20302;&#25104;&#26412;&#30828;&#20214;&#26367;&#20195;&#26041;&#26696;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23384;&#22312;&#22833;&#21435;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;&#65292;&#27604;&#22914;&#22312;&#25628;&#25937;&#24212;&#29992;&#20013;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#35774;&#22791;&#65292;&#27604;&#22914;&#22312;&#32676;&#20307;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#22312;&#20110;&#25214;&#21040;&#26426;&#21046;&#65292;&#20351;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36866;&#24212;&#36825;&#20123;&#36229;&#20302;&#25104;&#26412;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#20302;&#35745;&#31639;&#33021;&#21147;&#21644;&#36739;&#23567;&#20869;&#23384;&#23481;&#37327;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#27169;&#20223;&#23398;&#20064;&#37096;&#32626;&#21040;&#21463;&#36164;&#28304;&#38480;&#21046;&#30340;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#26469;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#38468;&#21152;&#20102;&#33258;&#23450;&#20041;&#22870;&#21169;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#35757;&#32451;&#20915;&#31574;&#21464;&#25442;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#20214;&#20248;&#21270;&#26041;&#26696;&#65292;&#21253;&#25324;&#37327;&#21270;&#21644;&#20462;&#21098;&#65292;&#26469;&#21387;&#32553;&#29983;&#25104;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13201v1 Announce Type: cross  Abstract: Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in si
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13178</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Retrieval-Augmented Generation for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13178
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#39318;&#20010;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;MedRAG&#24037;&#20855;&#21253;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#24191;&#27867;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#24187;&#35273;&#21644;&#36807;&#26102;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;RAG&#31995;&#32479;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#28789;&#27963;&#30340;&#32452;&#20214;&#65292;&#24182;&#19988;&#32570;&#20047;&#20851;&#20110;&#21508;&#31181;&#21307;&#23398;&#30446;&#30340;&#30340;&#26368;&#20339;RAG&#35774;&#32622;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#35780;&#20272;(MIRAGE)&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#21019;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;7,663&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;MIRAGE&#65292;&#25105;&#20204;&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;MedRAG&#24037;&#20855;&#21253;&#65292;&#22312;41&#31181;&#19981;&#21516;&#35821;&#26009;&#24211;&#12289;&#26816;&#32034;&#22120;&#21644;&#39592;&#24178;LLMs&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#36229;&#36807;1.8&#19975;&#20159;&#30340;&#25552;&#31034;&#26631;&#35760;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;MedRAG&#25552;&#39640;&#20102;&#20845;&#31181;&#19981;&#21516;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs 
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.13145</link><description>&lt;p&gt;
CMDAG: &#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#20013;&#25991;&#38544;&#21947;&#25968;&#25454;&#38598;&#20316;&#20026;&#8220;CoT&#8221;&#26469;&#25552;&#21319;&#38544;&#21947;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#25991;&#23398;&#20013;&#26174;&#33879;&#30340;&#20462;&#36766;&#25163;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#28155;&#20102;&#33394;&#24425;&#12289;&#24418;&#35937;&#21644;&#24378;&#35843;&#65292;&#20197;&#22686;&#24378;&#26377;&#25928;&#20132;&#27969;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;28K&#21477;&#26469;&#33258;&#21508;&#31181;&#20013;&#25991;&#25991;&#23398;&#26469;&#28304;&#65288;&#22914;&#35799;&#27468;&#12289;&#25955;&#25991;&#12289;&#27468;&#35789;&#31561;&#65289;&#12290;&#20026;&#30830;&#20445;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#28085;&#30422;&#20102;&#38544;&#21947;&#26631;&#27880;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#35782;&#21035;&#23545;&#35937;&#12289;&#36733;&#20307;&#21644;&#22522;&#30784;&#65292;&#20197;&#22788;&#29702;&#27604;&#21947;&#12289;&#25311;&#20154;&#12289;&#24182;&#21015;&#21644;&#22840;&#24352;&#31561;&#22797;&#26434;&#24615;&#12290;&#25171;&#30772;&#20256;&#32479;&#65292;&#25105;&#20204;&#30340;&#38544;&#21947;&#29983;&#25104;&#26041;&#27861;&#24378;&#35843;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;&#36890;&#36807;&#23558;&#8220;&#22522;&#30784;&#8221;&#20316;&#20026;&#8220;CoT&#8221;&#65288;&#24605;&#32500;&#38142;&#65289;&#36755;&#20837;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13145v1 Announce Type: cross  Abstract: Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that re
&lt;/p&gt;</description></item><item><title>VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13126</link><description>&lt;p&gt;
VGMShield&#65306;&#32531;&#35299;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;
&lt;/p&gt;
&lt;p&gt;
VGMShield: Mitigating Misuse of Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13126
&lt;/p&gt;
&lt;p&gt;
VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#21033;&#29992;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#31526;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#25216;&#26415;&#34987;&#29992;&#20110;&#21019;&#20316;&#21644;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VGMShield&#65306;&#19968;&#22871;&#21253;&#21547;&#19977;&#39033;&#30452;&#25509;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#29992;&#20110;&#38450;&#33539;&#34394;&#20551;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#8220;&#34394;&#20551;&#35270;&#39057;&#26816;&#27979;&#8221;&#24320;&#22987;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#30340;&#35270;&#39057;&#20013;&#26159;&#21542;&#23384;&#22312;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#21306;&#20998;&#23427;&#20204;&#19982;&#30495;&#23454;&#35270;&#39057;&#30340;&#19981;&#21516;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#8220;&#28335;&#28304;&#8221;&#38382;&#39064;&#65292;&#21363;&#23558;&#19968;&#27573;&#34394;&#20551;&#35270;&#39057;&#36861;&#28335;&#22238;&#29983;&#25104;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20851;&#27880;&#8220;&#26102;&#31354;&#21160;&#24577;&#8221;&#30340;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
&lt;/p&gt;</description></item><item><title>TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;</title><link>https://arxiv.org/abs/2402.13125</link><description>&lt;p&gt;
TreeEval&#65306;&#36890;&#36807;&#26641;&#35268;&#21010;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13125
&lt;/p&gt;
&lt;p&gt;
TreeEval&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#26641;&#35268;&#21010;&#31574;&#30053;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24314;&#31435;&#20102;&#35768;&#22810;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#35745;&#31639;&#25972;&#20307;&#24471;&#20998;&#25110;&#20351;&#29992;&#21478;&#19968;&#20010;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#22522;&#20934;&#30340;&#20844;&#24320;&#35775;&#38382;&#21644;&#35780;&#20272;&#36807;&#31243;&#30340;&#19981;&#28789;&#27963;&#32780;&#36973;&#21463;&#25968;&#25454;&#27844;&#28431;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TreeEval&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#22522;&#20934;&#35780;&#20272;&#26041;&#27861;&#65292;&#35753;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;LLM&#20027;&#25345;&#19968;&#20010;&#19981;&#21487;&#37325;&#29616;&#30340;&#35780;&#20272;&#20250;&#35805;&#65292;&#20174;&#26681;&#26412;&#19978;&#36991;&#20813;&#20102;&#25968;&#25454;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;LLM&#20805;&#24403;&#19968;&#20010;&#32771;&#23448;&#65292;&#25552;&#20986;&#19968;&#31995;&#21015;&#20851;&#20110;&#19968;&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#26641;&#35268;&#21010;&#31574;&#30053;&#65292;&#32771;&#34385;&#24403;&#21069;&#30340;&#35780;&#20272;&#29366;&#24577;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#38382;&#39064;&#30340;&#29983;&#25104;&#65292;&#30830;&#20445;&#35780;&#20272;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;6&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;7B&#12289;13B&#21644;33B&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#30456;&#20851;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13125v1 Announce Type: cross  Abstract: Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coef
&lt;/p&gt;</description></item><item><title>BuffGraph&#36890;&#36807;&#25554;&#20837;&#32531;&#20914;&#33410;&#28857;&#21040;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#65292;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.13114</link><description>&lt;p&gt;
BuffGraph: &#36890;&#36807;&#32531;&#20914;&#33410;&#28857;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13114
&lt;/p&gt;
&lt;p&gt;
BuffGraph&#36890;&#36807;&#25554;&#20837;&#32531;&#20914;&#33410;&#28857;&#21040;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#65292;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#21363;&#27425;&#35201;&#31867;&#21035;&#26126;&#26174;&#20195;&#34920;&#19981;&#36275;&#65292;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#29983;&#25104;&#26032;&#30340;&#23569;&#25968;&#33410;&#28857;&#65292;&#24182;&#36830;&#25509;&#26032;&#33410;&#28857;&#21040;&#21407;&#22987;&#22270;&#20013;&#65292;&#20351;&#24471;&#31867;&#21035;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#26410;&#35299;&#20915;&#20027;&#35201;&#31867;&#21035;&#36890;&#36807;&#21407;&#22987;&#22270;&#20013;&#30340;&#36793;&#21521;&#27425;&#35201;&#33410;&#28857;&#20256;&#25773;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#23545;&#20027;&#35201;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BuffGraph&#65292;&#23558;&#32531;&#20914;&#33410;&#28857;&#25554;&#20837;&#22270;&#20013;&#65292;&#35843;&#33410;&#20027;&#35201;&#31867;&#21035;&#30340;&#24433;&#21709;&#65292;&#20197;&#25913;&#21892;&#27425;&#35201;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;BuffGraph&#22312;&#33258;&#28982;&#35774;&#32622;&#21644;&#19981;&#24179;&#34913;&#35774;&#32622;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#20013;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://anonymous.4open.scien&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13114v1 Announce Type: cross  Abstract: Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.scien
&lt;/p&gt;</description></item><item><title>CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13109</link><description>&lt;p&gt;
CIF-Bench&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13109
&lt;/p&gt;
&lt;p&gt;
CIF-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#35328;&#19978;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26469;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22686;&#24378;&#20102;&#36890;&#36807;&#25351;&#20196;&#36981;&#24490;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#26410;&#35265;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22914;&#20013;&#25991;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#24120;&#24120;&#20250;&#20943;&#24369;&#65292;&#21463;&#21040;&#25968;&#25454;&#27844;&#28431;&#24341;&#36215;&#30340;&#20559;&#35265;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#20154;&#23545;&#23427;&#20204;&#30495;&#27491;&#30340;&#27867;&#21270;&#33021;&#21147;&#21040;&#26032;&#35821;&#35328;&#39046;&#22495;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#25991;&#25351;&#20196;&#36981;&#24490;&#22522;&#20934;&#65288;CIF-Bench&#65289;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#23545;&#20013;&#25991;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;CIF-Bench &#21253;&#21547;150&#20010;&#20219;&#21153;&#21644;15,000&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#30001;&#27597;&#35821;&#32773;&#24320;&#21457;&#65292;&#29992;&#20110;&#27979;&#35797;&#36328;&#36234;20&#20010;&#31867;&#21035;&#30340;&#22797;&#26434;&#25512;&#29702;&#21644;&#20013;&#22269;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#12290;&#20026;&#20102;&#20943;&#23569;&#35780;&#20272;&#20559;&#35265;&#65292;&#25105;&#20204;&#21482;&#20844;&#24320;&#20102;&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#31169;&#23494;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#20197;&#26368;&#23567;&#21270;&#24471;&#20998;&#26041;&#24046;&#65292;&#20849;&#35745;45,000&#20010;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13109v1 Announce Type: cross  Abstract: The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our eval
&lt;/p&gt;</description></item><item><title>ELAD&#25552;&#20986;&#20102;&#19968;&#31181;Explanation-Guided LLMs Active Distillation&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#21644;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.13098</link><description>&lt;p&gt;
ELAD: &#35299;&#37322;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20027;&#21160;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
ELAD: Explanation-Guided Large Language Models Active Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13098
&lt;/p&gt;
&lt;p&gt;
ELAD&#25552;&#20986;&#20102;&#19968;&#31181;Explanation-Guided LLMs Active Distillation&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#21644;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#21644;&#24212;&#29992;&#21463;&#21040;&#23427;&#20204;&#30340;&#20869;&#23384;&#25928;&#29575;&#12289;&#35745;&#31639;&#35201;&#27714;&#21644;&#39640;&#25104;&#26412;&#30340;API&#25512;&#26029;&#30340;&#38459;&#30861;&#12290;&#20256;&#32479;&#30340;&#33976;&#39311;&#26041;&#27861;&#24448;&#24448;&#26410;&#33021;&#30830;&#23450;&#30693;&#35782;&#26159;&#21542;&#24050;&#32463;&#34987;&#20805;&#20998;&#36716;&#31227;&#65292;&#21487;&#33021;&#23548;&#33268;&#39640;&#25104;&#26412;&#25110;&#19981;&#23436;&#25972;&#30340;&#33976;&#39311;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Explanation-Guided LLMs Active Distillation&#65288;ELAD&#65289;&#26694;&#26550;&#65292;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#20248;&#21270;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#25913;&#36827;&#26377;&#25928;&#30340;&#26679;&#26412;&#36873;&#25321;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35299;&#37322;&#27493;&#39588;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#25361;&#25112;&#20854;&#25512;&#29702;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;LLM-&#27880;&#37322;&#35299;&#37322;&#20462;&#35746;&#25216;&#26415;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#26816;&#27979;&#24182;&#32416;&#27491;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13098v1 Announce Type: cross  Abstract: The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the stu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.13093</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Event-level Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13093
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#36890;&#36807;&#30452;&#25509;&#32534;&#36753;&#26032;&#20107;&#20214;&#21040;LLMs&#20013;&#65292;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#36807;&#26102;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#20107;&#23454;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#32423;&#21035;&#19978;&#32534;&#36753;LLMs&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#30693;&#35782;&#26356;&#26032;&#26469;&#33258;&#26032;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#26356;&#25913;&#20107;&#23454;&#19977;&#20803;&#32452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#35774;&#32622;&#65306;&#20107;&#20214;&#32423;&#30693;&#35782;&#32534;&#36753;&#65292;&#30452;&#25509;&#23558;&#26032;&#20107;&#20214;&#32534;&#36753;&#21040;LLMs&#20013;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#23436;&#25972;&#24615;&#19978;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#19977;&#20803;&#32452;&#32423;&#21035;&#32534;&#36753;&#12290;(1)&#25928;&#29575;&#12290;&#21333;&#20010;&#20107;&#20214;&#32534;&#36753;&#20250;&#23548;&#33268;&#22810;&#20010;&#25512;&#26029;&#30693;&#35782;&#19977;&#20803;&#32452;&#30340;&#26356;&#26032;&#12290;(2)&#23436;&#25972;&#24615;&#12290;&#38500;&#20102;&#26356;&#26032;&#20107;&#23454;&#30693;&#35782;&#22806;&#65292;&#20107;&#20214;&#32423;&#21035;&#30340;&#32534;&#36753;&#36824;&#38656;&#35201;&#32771;&#34385;&#20107;&#20214;&#24433;&#21709;&#65292;&#26356;&#26032;LLMs&#20851;&#20110;&#26410;&#26469;&#36235;&#21183;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#32423;&#21035;&#32534;&#36753;&#22522;&#20934;ELKEN&#65292;&#21253;&#25324;1,515&#20010;&#20107;&#20214;&#32534;&#36753;&#65292;6,449&#20010;&#20851;&#20110;&#20107;&#23454;&#30693;&#35782;&#30340;&#38382;&#39064;&#21644;10,150&#20010;&#20851;&#20110;&#26410;&#26469;&#21457;&#23637;&#36235;&#21183;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13093v1 Announce Type: cross  Abstract: Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.13089</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;MoE&#35774;&#35745;&#36873;&#25321;&#30340;&#23454;&#35777;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards an empirical understanding of MoE design choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36335;&#30001;&#22120;&#30340;&#23398;&#20064;&#19982;&#21021;&#22987;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#27604;&#36739;&#12289;&#24207;&#21015;&#32423;&#36335;&#30001;&#19982;&#26631;&#35760;&#32423;&#36335;&#30001;&#22312;&#19987;&#23478;&#19987;&#19994;&#21270;&#26041;&#38754;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;Mixture of Experts&#65288;MoEs&#65289;&#20013;&#24120;&#35265;&#35774;&#35745;&#36873;&#25321;&#23545;&#39564;&#35777;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#22312;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#36335;&#30001;&#22120;&#21644;&#20923;&#32467;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#36335;&#30001;&#22120;&#20043;&#38388;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#23398;&#20064;&#36335;&#30001;&#21487;&#33021;&#24182;&#38750;&#24517;&#19981;&#21487;&#23569;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#24207;&#21015;&#32423;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#20027;&#39064;&#30340;&#24369;&#19987;&#23478;&#19987;&#19994;&#21270;&#65292;&#19982;&#26631;&#35760;&#32423;&#21035;&#36335;&#30001;&#35266;&#23519;&#21040;&#30340;&#35821;&#27861;&#19987;&#19994;&#21270;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13089v1 Announce Type: cross  Abstract: In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25552;&#39640;&#25968;&#25454;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20511;&#21161;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13077</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Neural Networks for Scientific Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25552;&#39640;&#25968;&#25454;&#24314;&#27169;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#20511;&#21161;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#23427;&#22312;&#26631;&#20934;&#26550;&#26500;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#21046;&#27169;&#22359;&#65292;&#26126;&#30830;&#22320;&#23398;&#20064;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#34920;&#31034;&#65292;&#25581;&#31034;&#25968;&#25454;&#30340;&#22522;&#26412;&#21160;&#24577;&#65292;&#24182;&#22686;&#24378;&#20102;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26494;&#24347;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65288;NeuRLP&#65289;&#65292;&#21463;&#19968;&#31181;&#23558;&#27714;&#35299;&#32447;&#24615;ODE&#36716;&#21270;&#20026;&#27714;&#35299;&#32447;&#24615;&#35268;&#21010;&#30340;&#25216;&#26415;&#21551;&#21457;&#12290;&#23427;&#19982;&#31070;&#32463;&#32593;&#32476;&#24456;&#22909;&#22320;&#38598;&#25104;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;ODE&#27714;&#35299;&#22120;&#30340;&#23616;&#38480;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;GPU&#24182;&#34892;&#22788;&#29702;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26426;&#21046;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20174;&#26041;&#31243;&#21457;&#29616;&#21040;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20998;&#26512;&#21644;&#35299;&#37322;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13077v1 Announce Type: cross  Abstract: This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scient
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#24341;&#20837;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13058</link><description>&lt;p&gt;
&#38543;&#26426;&#22270;&#38598;&#21644;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Random Graph Set and Evidence Pattern Reasoning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#24341;&#20837;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#29702;&#35770;&#22312;&#20915;&#31574;&#21644;&#25512;&#29702;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#36716;&#31227;&#20449;&#24565;&#27169;&#22411;&#65288;TBM&#65289;&#26159;&#24120;&#29992;&#30340;&#35777;&#25454;&#20915;&#31574;&#27169;&#22411;&#65292;&#20294;TBM&#26159;&#19968;&#20010;&#38750;&#20559;&#22909;&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#31526;&#21512;&#20915;&#31574;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#35777;&#25454;&#27169;&#24335;&#25512;&#29702;&#27169;&#22411;&#65288;EPRM&#65289;&#12290;&#36890;&#36807;&#23450;&#20041;&#27169;&#24335;&#36816;&#31639;&#31526;&#21644;&#20915;&#31574;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#35774;&#32622;&#30456;&#24212;&#30340;&#20559;&#22909;&#12290;&#38543;&#26426;&#25490;&#21015;&#38598;&#65288;RPS&#65289;&#20026;&#35777;&#25454;&#29702;&#35770;&#25193;&#23637;&#20102;&#39034;&#24207;&#20449;&#24687;&#12290;RPS&#24456;&#38590;&#21051;&#30011;&#26679;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22914;&#24490;&#29615;&#12289;&#24182;&#34892;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#22270;&#38598;&#65288;RGS&#65289;&#26469;&#27169;&#25311;&#22797;&#26434;&#20851;&#31995;&#24182;&#34920;&#31034;&#26356;&#22810;&#20107;&#20214;&#31867;&#22411;&#12290;&#20026;&#20102;&#35828;&#26126;RGS&#21644;EPRM&#30340;&#37325;&#35201;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#39033;&#39134;&#26426;&#36895;&#24230;&#25490;&#24207;&#23454;&#39564;&#65292;&#24182;&#27169;&#25311;&#20102;10,000&#20010;&#26696;&#20363;&#12290;EPRM&#30340;&#23454;&#29616;&#31216;&#20026;&#20914;&#31361;&#35299;&#20915;Dec
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13058v1 Announce Type: new  Abstract: Evidence theory is widely used in decision-making and reasoning systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Dec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13055</link><description>&lt;p&gt;
&#35782;&#21035;&#35821;&#20041;&#24863;&#24212;&#22836;&#20197;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Identifying Semantic Induction Heads to Understand In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#65292;&#25581;&#31034;&#20102;&#32467;&#21512;&#20102;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#20986;&#29616;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#25512;&#29702;&#36923;&#36753;&#30340;&#19981;&#36879;&#26126;&#24615;&#24341;&#21457;&#20102;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#65292;&#25105;&#20204;&#23545;&#27880;&#24847;&#21147;&#22836;&#30340;&#25805;&#20316;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#26088;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#26159;&#21542;&#32534;&#30721;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#23384;&#22312;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#65306;&#20174;&#21477;&#23376;&#20013;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#21644;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#27880;&#24847;&#21147;&#22836;&#34920;&#29616;&#20986;&#19968;&#31181;&#27169;&#24335;&#65292;&#21363;&#24403;&#20851;&#27880;&#22836;&#26631;&#35760;&#26102;&#65292;&#23427;&#20204;&#20250;&#22238;&#24518;&#36215;&#23614;&#26631;&#35760;&#65292;&#24182;&#22686;&#21152;&#36825;&#20123;&#23614;&#26631;&#35760;&#30340;&#36755;&#20986;&#36923;&#36753;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#35821;&#20041;&#24863;&#24212;&#22836;&#30340;&#21046;&#23450;&#19982;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#20986;&#29616;&#23384;&#22312;&#23494;&#20999;&#20851;&#32852;&#12290;&#35821;&#20041;&#27880;&#24847;&#21147;&#22836;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13055v1 Announce Type: cross  Abstract: Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;</title><link>https://arxiv.org/abs/2402.13040</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Guided Molecule Generation with Diffusion Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13040
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#65292;&#22312;&#29983;&#25104;&#26377;&#25928;&#20998;&#23376;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#20248;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#30340;&#20998;&#23376;&#29983;&#25104;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#29305;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#12290;&#26368;&#36817;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;SMILES&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Text-Guided Molecule Generation with Diffusion Language Model&#65288;TGM-DLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#33258;&#22238;&#24402;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;TGM-DLM&#38598;&#20307;&#21644;&#36845;&#20195;&#22320;&#26356;&#26032;SMILES&#23383;&#31526;&#20018;&#20013;&#30340;&#26631;&#35760;&#23884;&#20837;&#65292;&#20351;&#29992;&#20004;&#38454;&#27573;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#24341;&#23548;&#26469;&#20248;&#21270;&#23884;&#20837;&#65292;&#32780;&#31532;&#20108;&#38454;&#27573;&#32416;&#27491;&#26080;&#25928;&#30340;SMILES&#23383;&#31526;&#20018;&#20197;&#24418;&#25104;&#26377;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;TGM-DLM&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;MolT5-Base&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;TGM-DLM&#22312;&#29983;&#25104;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13040v1 Announce Type: cross  Abstract: Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating cohe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.13037</link><description>&lt;p&gt;
&#23545;&#40784;&#24744;&#30340;&#24847;&#22270;&#65306;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align Your Intents: Offline Imitation Learning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#23398;&#20064;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#21363;&#20351;&#32570;&#20047;&#26126;&#30830;&#30340;&#22870;&#21169;&#25110;&#21160;&#20316;&#26631;&#31614;&#65292;&#27169;&#20223;&#20195;&#29702;&#20063;&#21487;&#20197;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;AILOT&#65288;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23545;&#40784;&#27169;&#20223;&#23398;&#20064;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24847;&#22270;&#30340;&#29305;&#27530;&#29366;&#24577;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#20869;&#30340;&#20004;&#20004;&#31354;&#38388;&#36317;&#31163;&#12290;&#22312;&#32473;&#23450;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#21644;&#20195;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#23450;&#20041;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#31216;AILOT&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13035</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#26597;&#65306;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#26657;&#27491;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13035
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#21644;&#26500;&#24314;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#19981;&#26029;&#21162;&#21147;&#36890;&#36807;&#33258;&#25105;&#26657;&#27491;&#26469;&#23436;&#21892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#22806;&#37096;&#20934;&#30830;&#30693;&#35782;&#30340;&#33258;&#25105;&#26657;&#27491;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#20851;&#20110;&#33258;&#25105;&#26657;&#27491;&#30340;&#38480;&#21046;&#21644;&#26377;&#25928;&#24615;&#30340;&#30097;&#38382;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLM&#30340;&#33258;&#26816;&#21151;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#25105;&#26657;&#27491;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#38169;&#35823;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#65292;&#31216;&#20026;&#8220;Step CoT Check&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26816;&#26597;-&#26657;&#27491;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#23558;&#21407;&#22987;CoT&#25968;&#25454;&#21644;&#26816;&#26597;&#26657;&#27491;&#25968;&#25454;&#25972;&#21512;&#21518;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20854;&#33258;&#26816;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#24182;&#28040;&#38500;&#20102;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13035v1 Announce Type: cross  Abstract: Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#30340;&#20107;&#23454;&#26680;&#26597;&#65292;&#21033;&#29992;&#24322;&#36136;&#35777;&#25454;&#22270;&#21644;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.13028</link><description>&lt;p&gt;
&#24322;&#36136;&#22270;&#25512;&#29702;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#25991;&#26412;&#21644;&#34920;&#26684;&#20013;&#30340;&#20107;&#23454;&#26680;&#26597;&#65292;&#21033;&#29992;&#24322;&#36136;&#35777;&#25454;&#22270;&#21644;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#26088;&#22312;&#36890;&#36807;&#25512;&#29702;&#22810;&#20010;&#35777;&#25454;&#20197;&#39044;&#27979;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#21518;&#32773;&#65292;&#21363;&#25512;&#29702;&#20851;&#20110;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21644;&#32467;&#26500;&#21270;&#34920;&#26684;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24322;&#36136;&#22270;&#30340;&#35789;&#32423;&#27169;&#22411;HeterFC&#65292;&#29992;&#20110;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24322;&#36136;&#35777;&#25454;&#22270;&#65292;&#20197;&#21333;&#35789;&#20026;&#33410;&#28857;&#65292;&#24182;&#19988;&#35774;&#35745;&#20102;&#20195;&#34920;&#19981;&#21516;&#35777;&#25454;&#23646;&#24615;&#30340;&#36793;&#32536;&#65292;&#36890;&#36807;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#65292;&#20419;&#36827;&#22768;&#26126;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13028v1 Announce Type: cross  Abstract: Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claim
&lt;/p&gt;</description></item><item><title>CFEVER&#26159;&#19968;&#20010;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#26631;&#20934;&#21644;&#23545;&#24212;&#35777;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#24037;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.13025</link><description>&lt;p&gt;
CFEVER&#65306;&#19968;&#20010;&#29992;&#20110;&#27721;&#35821;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CFEVER: A Chinese Fact Extraction and VERification Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13025
&lt;/p&gt;
&lt;p&gt;
CFEVER&#26159;&#19968;&#20010;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#26631;&#20934;&#21644;&#23545;&#24212;&#35777;&#25454;&#65292;&#21487;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#24037;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CFEVER&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#32780;&#35774;&#35745;&#30340;&#27721;&#35821;&#25968;&#25454;&#38598;&#12290;CFEVER&#21253;&#25324;30,012&#20010;&#22522;&#20110;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#20869;&#23481;&#25163;&#21160;&#21019;&#24314;&#30340;&#22768;&#26126;&#12290;&#27599;&#20010;CFEVER&#20013;&#30340;&#22768;&#26126;&#37117;&#26631;&#35760;&#20026;&#8220;&#25903;&#25345;&#8221;&#12289;&#8220;&#21453;&#39539;&#8221;&#25110;&#8220;&#20449;&#24687;&#19981;&#36275;&#8221;&#65292;&#20197;&#25551;&#36848;&#20854;&#20107;&#23454;&#31243;&#24230;&#12290;&#31867;&#20284;&#20110;FEVER&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#21644;&#21453;&#39539;&#31867;&#21035;&#20013;&#30340;&#22768;&#26126;&#20063;&#26631;&#26377;&#23545;&#24212;&#30340;&#35777;&#25454;&#21477;&#65292;&#36825;&#20123;&#35777;&#25454;&#21477;&#21462;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340;&#21333;&#20010;&#25110;&#22810;&#20010;&#39029;&#38754;&#12290;&#25105;&#20204;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#22312;&#20116;&#36335;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;0.7934&#30340;Fleiss' kappa&#20540;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;FEVER&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20197;&#21450;&#23545;CFEVER&#30340;&#31616;&#21333;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26032;&#30340;&#33499;&#21051;&#20107;&#23454;&#25552;&#21462;&#21644;&#39564;&#35777;&#22522;&#20934;&#65292;&#21487;&#36827;&#19968;&#27493;&#29992;&#20110;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#20943;&#36731;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#30340;&#24037;&#20316;&#37327;&#12290;CFEVER&#21487;&#22312;&#32593;&#22336;&#22788;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13025v1 Announce Type: cross  Abstract: We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as "Supports", "Refutes", or "Not Enough Info" to depict its degree of factualness. Similar to the FEVER dataset, claims in the "Supports" and "Refutes" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at htt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#20004;&#31181;&#24120;&#35265;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#20855;&#26377;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#65292;&#22312;&#22810;&#23610;&#24230;&#26041;&#27861;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#32593;&#32476;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.13019</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#25552;&#21319;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Neural-based Classification with Logical Background Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#20004;&#31181;&#24120;&#35265;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#20855;&#26377;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#65292;&#22312;&#22810;&#23610;&#24230;&#26041;&#27861;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#32593;&#32476;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13019v1 &#20844;&#21578;&#31867;&#22411;:&#26032; &#25277;&#35937;:&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#26085;&#30410;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#33021;&#21147;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#28151;&#21512;&#21487;&#20197;&#37319;&#29992;&#22810;&#31181;&#24418;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#21629;&#39064;&#32972;&#26223;&#30693;&#35782;&#30340;&#30417;&#30563;&#24335;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35821;&#20041;&#35843;&#33410;&#30340;&#26032;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#32422;&#26463;&#31995;&#32479;&#65292;&#32780;&#19981;&#24433;&#21709;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#21478;&#22806;&#20004;&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#8212;&#8212;&#35821;&#20041;&#35843;&#33410;&#21644;&#35821;&#20041;&#27491;&#21017;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#20248;&#21183;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#35780;&#20272;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#30340;&#22909;&#22788;&#38543;&#32593;&#32476;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35780;&#20272;&#24182;&#27604;&#36739;&#36825;&#19977;&#31181;&#25216;&#26415;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#20041;&#35843;&#33410;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13019v1 Announce Type: new  Abstract: Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.12993</link><description>&lt;p&gt;
&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#25968;&#25454;&#25366;&#25496;&#30340;&#33258;&#20027;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Large Language Model Agent for Chemical Literature Data Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12993
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21512;&#25104;&#23545;&#20110;&#25512;&#21160;&#26448;&#26009;&#21512;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#30528;&#21253;&#25324;&#29615;&#22659;&#31185;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#21270;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#19978;&#21319;&#20351;&#24471;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21270;&#23398;&#25968;&#25454;&#65292;&#25361;&#25112;&#30740;&#31350;&#20154;&#21592;&#21435;&#35782;&#21035;&#27169;&#24335;&#24182;&#32454;&#21270;&#21512;&#25104;&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#26469;&#20248;&#21270;&#21512;&#25104;&#24182;&#25552;&#39640;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#25991;&#29486;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21270;&#23398;&#25991;&#29486;&#30340;&#32467;&#26500;&#19981;&#35268;&#25972;&#65292;&#20889;&#20316;&#39118;&#26684;&#22810;&#26679;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#24191;&#27867;&#30340;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24555;&#36895;&#29983;&#25104;&#21644;&#36845;&#20195;&#20248;&#21270;&#12290;&#23427;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#30465;&#20154;&#21147;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12993v1 Announce Type: cross  Abstract: Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's ef
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#19982;LLMs&#21327;&#21516;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.12984</link><description>&lt;p&gt;
GNN&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;LLMs&#30340;&#33391;&#22909;&#36866;&#37197;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GNN be Good Adapter for LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#19982;LLMs&#21327;&#21516;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;&#21644;&#38646;&#27425;&#23398;&#20064;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20026;&#35768;&#22810;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#39046;&#22495;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#24314;&#27169;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GraphAdapter&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;LLMs&#30340;&#39640;&#25928;&#36866;&#37197;&#22120;&#65292;&#20197;&#24212;&#23545;TAGs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12984v1 Announce Type: cross  Abstract: Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable param
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12976</link><description>&lt;p&gt;
&#31034;&#33539;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#22810;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32500;&#20998;&#26512;&#21457;&#29616;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20854;&#20013;&#37096;&#20998;&#27169;&#22411;&#23545;&#31034;&#33539;&#36136;&#37327;&#19981;&#25935;&#24863;&#65292;&#32780;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#21487;&#20197;&#28040;&#38500;&#23545;&#31034;&#33539;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#31034;&#33539;&#32463;&#24120;&#34987;&#29992;&#20316;&#25512;&#29702;&#31574;&#30053;&#65292;&#22312;&#27492;&#31574;&#30053;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#31034;&#33539;&#26469;&#35299;&#20915;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#19982;&#21333;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#22810;&#35821;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#25105;&#20204;&#32570;&#20047;&#23545;&#35813;&#29615;&#22659;&#20013;&#31034;&#33539;&#20316;&#29992;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22810;&#35821;&#22659;&#23398;&#20064;&#36827;&#34892;&#20102;&#22810;&#32500;&#20998;&#26512;&#65292;&#23454;&#39564;&#37319;&#29992;&#20102;&#26469;&#33258;&#19981;&#21516;&#27169;&#22411;&#23478;&#26063;&#30340;5&#20010;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#22312;&#20869;&#30340;9&#20010;&#25968;&#25454;&#38598;&#65292;&#35206;&#30422;&#20102;56&#31181;&#31867;&#22411;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31034;&#33539;&#30340;&#26377;&#25928;&#24615;&#22312;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Llama 2-Chat&#12289;GPT-3.5&#21644;GPT-4&#23545;&#31034;&#33539;&#36136;&#37327;&#30340;&#25935;&#24863;&#24230;&#36739;&#20302;&#12290;&#30456;&#21453;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#26495;&#24448;&#24448;&#20250;&#28040;&#38500;&#19968;&#20123;&#27169;&#22411;&#23545;&#31034;&#33539;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12976v1 Announce Type: cross  Abstract: In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some t
&lt;/p&gt;</description></item><item><title>Gl'orIA&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35774;&#35745;&#30340;&#24378;&#22823;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;CALAME-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.12969</link><description>&lt;p&gt;
Gl'orIA - &#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#29983;&#25104;&#24335;&#24320;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gl\'orIA - A Generative and Open Large Language Model for Portuguese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12969
&lt;/p&gt;
&lt;p&gt;
Gl'orIA&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35774;&#35745;&#30340;&#24378;&#22823;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#20026;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24341;&#20837;&#20102;CALAME-PT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#37117;&#26377;&#20016;&#23500;&#30340;LLM&#65292;&#20294;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#30340;&#36825;&#31181;&#27169;&#22411;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#35299;&#30721;&#22120;LLM&#8212;&#8212;Gl'orIA&#12290;&#20026;&#20102;&#23545;Gl'orIA&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#21508;&#20010;&#26469;&#28304;&#30340;350&#20159;&#20010;tokens&#30340;&#20840;&#38754;PT-PT&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#28982;&#21518;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CALAME-PT&#65288;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33889;&#33796;&#29273;&#35821;&#38646;&#26679;&#26412;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12969v1 Announce Type: cross  Abstract: Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.12954</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conditional Logical Message Passing Transformer for Complex Query Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#24046;&#24322;&#65292;&#33021;&#21160;&#24577;&#27979;&#37327;&#28040;&#24687;&#37325;&#35201;&#24615;&#24182;&#25429;&#25417;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;KGs&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#27169;&#22411;&#26469;&#36890;&#36807;&#25191;&#34892;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;CQA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#19981;&#33021;&#21516;&#26102;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#30340;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#12290;&#34429;&#28982;&#22312;&#19968;&#36339;&#21644;&#22810;&#36339;&#26597;&#35810;&#19978;&#37117;&#26377;&#25928;&#65292;&#20294;&#23427;&#24573;&#30053;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26356;&#26032;&#38454;&#27573;&#65292;&#35813;&#26426;&#21046;&#19981;&#33021;&#21160;&#24577;&#34913;&#37327;&#19981;&#21516;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#23427;&#33021;&#21542;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#21464;&#21387;&#22120;&#65288;CLMPT&#65289;&#65292;&#32771;&#34385;&#20102;&#26597;&#35810;&#22270;&#20013;&#24120;&#37327;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#19988;&#20855;&#26377;&#21160;&#24577;&#27979;&#37327;&#19981;&#21516;&#28040;&#24687;&#37325;&#35201;&#24615;&#20197;&#21450;&#25429;&#25417;&#19982;&#33410;&#28857;&#21644;&#25509;&#25910;&#28040;&#24687;&#30456;&#20851;&#30340;&#38544;&#24335;&#36923;&#36753;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12954v1 Announce Type: cross  Abstract: Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the c
&lt;/p&gt;</description></item><item><title>QuanTest&#26159;&#19968;&#20010;&#22522;&#20110;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#26088;&#22312;&#25581;&#31034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.12950</link><description>&lt;p&gt;
QuanTest&#65306;&#22522;&#20110;&#32416;&#32544;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12950
&lt;/p&gt;
&lt;p&gt;
QuanTest&#26159;&#19968;&#20010;&#22522;&#20110;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#26088;&#22312;&#25581;&#31034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Neural Network&#65288;QNN&#65289;&#23558;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21407;&#29702;&#19982;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20855;&#26377;&#37327;&#23376;&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290; &#26368;&#36817;&#21457;&#29616;&#65292;QNN&#31995;&#32479;&#26377;&#31867;&#20284;&#20110;&#32463;&#20856;DL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290; &#24613;&#38656;&#19968;&#31181;&#26041;&#27861;&#26469;&#27979;&#35797;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290; &#20294;&#26159;&#65292;QNN&#31995;&#32479;&#19982;&#20256;&#32479;&#37327;&#23376;&#36719;&#20214;&#21644;&#32463;&#20856;DL&#31995;&#32479;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#23545;QNN&#27979;&#35797;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290; &#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#20256;&#32479;&#37327;&#23376;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#30340;&#19981;&#36866;&#29992;&#24615;&#65292;&#37327;&#23376;&#27979;&#35797;&#26679;&#26412;&#29983;&#25104;&#23545;&#25200;&#21160;&#31639;&#23376;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#37327;&#23376;&#31070;&#32463;&#20803;&#20013;&#32570;&#20047;&#26377;&#25928;&#20449;&#24687;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;QuanTest&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;QNN&#31995;&#32479;&#20013;&#28508;&#22312;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#23376;&#32416;&#32544;&#20805;&#20998;&#24615;&#20934;&#21017;&#26469;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12950v1 Announce Type: cross  Abstract: Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#21644;&#38477;&#32500;&#25216;&#26415;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#21457;&#29616;&#24182;&#25913;&#36827;&#20854;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#27425;&#20248;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.12939</link><description>&lt;p&gt;
&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#22312;&#28508;&#31354;&#38388;&#20013;&#21457;&#29616;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12939
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36712;&#36857;&#32858;&#31867;&#21644;&#38477;&#32500;&#25216;&#26415;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#20197;&#21457;&#29616;&#24182;&#25913;&#36827;&#20854;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#27425;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#30340;&#34892;&#20026;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#31574;&#30053;&#22797;&#26434;&#24615;&#24448;&#24448;&#20351;&#20854;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;DRL&#31574;&#30053;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#20013;&#21033;&#29992;&#38477;&#32500;&#21644;&#36712;&#36857;&#32858;&#31867;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;Pairwise Controlled Manifold Approximation Projection&#65288;PaCMAP&#65289;&#36827;&#34892;&#38477;&#32500;&#21644;TRACLUS&#36827;&#34892;&#36712;&#36857;&#32858;&#31867;&#65292;&#20998;&#26512;&#20102;&#22312;Mountain Car&#25511;&#21046;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;DRL&#31574;&#30053;&#30340;&#28508;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35782;&#21035;&#22810;&#26679;&#30340;&#34892;&#20026;&#27169;&#24335;&#21644;&#31574;&#30053;&#30340;&#27425;&#20248;&#36873;&#25321;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#20197;&#22686;&#24378;&#31574;&#30053;&#22312;&#29366;&#24577;&#31354;&#38388;&#29305;&#23450;&#21306;&#22495;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12939v1 Announce Type: cross  Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12928</link><description>&lt;p&gt;
&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20998;&#25955;&#30340;&#30693;&#35782;&#65292;&#25991;&#29486;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#25152;&#30740;&#31350;&#20027;&#39064;&#30340;&#20840;&#38754;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#65288;PAMI&#65289;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36807;&#22810;&#30340;&#32508;&#36848;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#35770;&#32773;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#20851;&#27880;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#20840;&#38754;&#23457;&#35270;PAMI&#39046;&#22495;&#30340;&#32508;&#36848;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12928v1 Announce Type: cross  Abstract: By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#25216;&#26415;&#20248;&#21270;&#25968;&#25454;&#31649;&#36947;&#65292;&#25552;&#21319;&#25968;&#25454;&#27969;&#26234;&#33021;&#24615;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#26500;&#24314;&#39640;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#25968;&#25454;&#29615;&#22659;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12916</link><description>&lt;p&gt;
&#25968;&#25454;&#31649;&#36947;&#35757;&#32451;&#65306;&#23558; AutoML &#38598;&#25104;&#21040;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#27969;&#20013;
&lt;/p&gt;
&lt;p&gt;
Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#25216;&#26415;&#20248;&#21270;&#25968;&#25454;&#31649;&#36947;&#65292;&#25552;&#21319;&#25968;&#25454;&#27969;&#26234;&#33021;&#24615;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#32467;&#26524;&#65292;&#24182;&#25581;&#31034;&#20102;&#26500;&#24314;&#39640;&#25928;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#25968;&#25454;&#29615;&#22659;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#36947;&#22312;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#21644;&#24320;&#21457;&#25968;&#25454;&#20135;&#21697;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#12290;&#38543;&#30528;&#25968;&#25454;&#28304;&#26085;&#30410;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#21270;&#65292;&#20197;&#21450;&#25968;&#25454;&#37327;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#26500;&#24314;&#39640;&#25928;&#30340;&#25968;&#25454;&#31649;&#36947;&#23545;&#20110;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#38598;&#25104; AutoML &#21040;&#25968;&#25454;&#31649;&#36947;&#20013;&#65292;&#20248;&#21270;&#25968;&#25454;&#27969;&#21160;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#22914;&#20309;&#21033;&#29992; AutoML &#25216;&#26415;&#25552;&#21319;&#25968;&#25454;&#31649;&#36947;&#30340;&#26234;&#33021;&#21270;&#65292;&#20174;&#32780;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#27969;&#30340;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26500;&#24314;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#29615;&#22659;&#30340;&#39640;&#25928;&#25968;&#25454;&#31649;&#36947;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#36825;&#19981;&#20165;&#21152;&#24555;&#20102;&#24314;&#27169;&#36807;&#31243;&#65292;&#36824;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20102;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26356;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12916v1 Announce Type: cross  Abstract: Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more sig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.12907</link><description>&lt;p&gt;
AI&#23545;&#40784;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#65306;&#31435;&#22330;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#65292;&#23545;&#31038;&#20250;&#27835;&#29702;&#21644;&#23433;&#20840;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#35299;&#20915;AI&#23545;&#40784;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25216;&#26415;&#26041;&#38754;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;AI&#31995;&#32479;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#24615;&#36136;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24320;&#21457;&#21644;&#37096;&#32626;&#32972;&#26223;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#26032;&#38382;&#39064;&#65306;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#33021;&#21628;&#21505;&#26356;&#22810;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#24357;&#21512;&#25216;&#26415;&#21644;&#31038;&#20250;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23454;&#29616;IC&#30340;&#19977;&#20010;&#32463;&#20856;&#21338;&#24328;&#38382;&#39064;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#22865;&#32422;&#29702;&#35770;&#21644;&#36125;&#21494;&#26031;&#35828;&#26381;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12907v1 Announce Type: new  Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion,
&lt;/p&gt;</description></item><item><title>&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20856;&#22411;&#24320;&#21457;&#38454;&#27573;&#21253;&#25324;&#32467;&#26500;&#24320;&#21457;&#21644;&#21442;&#25968;&#21270;&#65292;&#32780;&#24320;&#21457;&#36807;&#31243;&#20013;&#25191;&#34892;&#21021;&#27493;&#30340;&#31895;&#30053;&#21442;&#25968;&#21270;&#23545;&#20110;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#36866;&#24615;&#21644;&#21518;&#32493;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.12887</link><description>&lt;p&gt;
&#24320;&#21457;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#23450;&#24615;&#21442;&#25968;&#21270;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
The practice of qualitative parameterisation in the development of Bayesian networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12887
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20856;&#22411;&#24320;&#21457;&#38454;&#27573;&#21253;&#25324;&#32467;&#26500;&#24320;&#21457;&#21644;&#21442;&#25968;&#21270;&#65292;&#32780;&#24320;&#21457;&#36807;&#31243;&#20013;&#25191;&#34892;&#21021;&#27493;&#30340;&#31895;&#30053;&#21442;&#25968;&#21270;&#23545;&#20110;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#36866;&#24615;&#21644;&#21518;&#32493;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;(BN)&#32467;&#26500;&#21270;&#24320;&#21457;&#30340;&#20856;&#22411;&#38454;&#27573;&#21253;&#25324;&#30446;&#30340;&#21644;&#33539;&#22260;&#30340;&#35268;&#23450;&#12289;&#32467;&#26500;&#24320;&#21457;&#12289;&#21442;&#25968;&#21270;&#21644;&#39564;&#35777;&#12290; &#32467;&#26500;&#24320;&#21457;&#36890;&#24120;&#38598;&#20013;&#22312;&#23450;&#24615;&#38382;&#39064;&#19978;&#65292;&#32780;&#21442;&#25968;&#21270;&#38598;&#20013;&#22312;&#23450;&#37327;&#38382;&#39064;&#19978;&#65292;&#28982;&#32780;&#22312;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#20250;&#20986;&#29616;&#23450;&#24615;&#21644;&#23450;&#37327;&#38382;&#39064;&#12290; &#22312;&#21021;&#27493;&#32467;&#26500;&#24320;&#21457;&#21518;&#36890;&#24120;&#20250;&#25191;&#34892;&#31895;&#30053;&#21442;&#25968;&#21270;&#30340;&#24120;&#35265;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#20165;&#25429;&#25417;&#21644;&#35828;&#26126;&#27169;&#22411;&#30340;&#39044;&#26399;&#23450;&#24615;&#34892;&#20026;&#12290; &#36825;&#26159;&#22312;&#26356;&#20005;&#26684;&#30340;&#21442;&#25968;&#21270;&#20043;&#21069;&#23436;&#25104;&#30340;&#65292;&#30830;&#20445;&#32467;&#26500;&#31526;&#21512;&#39044;&#26399;&#65292;&#21516;&#26102;&#25903;&#25345;&#21518;&#32493;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#12290; &#22312;&#25105;&#20204;&#30340;&#38598;&#20307;&#32463;&#39564;&#21644;&#19982;&#20854;&#20182;&#24314;&#27169;&#32773;&#30340;&#35752;&#35770;&#20013;&#65292;&#36825;&#19968;&#27493;&#39588;&#26159;&#24320;&#21457;&#36807;&#31243;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24456;&#23569;&#25552;&#21450;&#12290; &#30001;&#20110;&#23454;&#36341;&#38598;&#20013;&#22312;&#23450;&#24615;&#38382;&#39064;&#19978;&#65292;&#23613;&#31649;&#26159;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12887v1 Announce Type: new  Abstract: The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation. Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases. A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model. This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation. In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature. Since the practice focuses on qualitative issues, despite being quanti
&lt;/p&gt;</description></item><item><title>&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.12865</link><description>&lt;p&gt;
&#21453;&#21521;&#38236;&#22836;&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
Backward Lens: Projecting Language Model Gradients into the Vocabulary Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12865
&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#20013;&#65292;&#25366;&#25496;&#20449;&#24687;&#22312;LMs&#20869;&#37096;&#30340;&#27969;&#21160;&#26041;&#24335;&#65292;&#25506;&#32034;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#22914;&#20309;&#23398;&#20064;&#21644;&#35760;&#24518;&#20449;&#24687;&#26159;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#23558;&#20174;&#21069;&#21521;&#20256;&#25773;&#20013;&#33719;&#24471;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#25237;&#24433;&#21040;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#20013;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;LMs&#20869;&#37096;&#20449;&#24687;&#27969;&#21160;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;LMs&#30340;&#21518;&#21521;&#20256;&#25773;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#26799;&#24230;&#30697;&#38453;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#20854;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#25773;&#36755;&#20837;&#30340;&#20302;&#31209;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#26041;&#27861;&#23558;&#36825;&#20123;&#26799;&#24230;&#25237;&#24433;&#21040;&#35789;&#27719;&#39033;&#20013;&#65292;&#24182;&#25506;&#35752;&#26032;&#20449;&#24687;&#22914;&#20309;&#23384;&#20648;&#22312;LMs&#30340;&#31070;&#32463;&#20803;&#20013;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12865v1 Announce Type: cross  Abstract: Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12847</link><description>&lt;p&gt;
&#35843;&#25972;&#36807;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#22909;&#30340;&#30693;&#35782;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Language Models are Better Knowledge Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25345;&#32493;&#39044;&#35757;&#32451;&#25991;&#26723;&#20043;&#21069;&#26292;&#38706;LLM&#21040;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#20197;&#20415;&#20174;&#22797;&#26434;&#25991;&#26723;&#20013;&#32534;&#30721;&#30693;&#35782;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#30693;&#35782;&#35775;&#38382;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21161;&#25163;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#24517;&#39035;&#33021;&#22815;&#36890;&#36807;&#25345;&#32493;&#22312;&#26032;&#25968;&#25454;&#19978;&#35757;&#32451;&#26469;&#26356;&#26032;&#23427;&#20204;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#20256;&#32479;&#20570;&#27861;&#28041;&#21450;&#22312;&#26032;&#25991;&#26723;&#19978;&#25345;&#32493;&#39044;&#22521;&#35757;&#65292;&#28982;&#21518;&#26681;&#25454;&#38382;&#39064;-&#31572;&#26696;&#65288;QA&#65289;&#23545;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12847v1 Announce Type: cross  Abstract: In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ConVQG&#26041;&#27861;&#36890;&#36807;&#21452;&#37325;&#23545;&#27604;&#30446;&#26631;&#21306;&#20998;&#20351;&#29992;&#20004;&#31181;&#27169;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#21644;&#22522;&#20110;&#21333;&#19968;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#22312;&#29983;&#25104;&#19987;&#27880;&#38382;&#39064;&#30340;&#21516;&#26102;&#30830;&#20445;&#19982;&#22270;&#20687;&#20869;&#23481;&#30340;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;.</title><link>https://arxiv.org/abs/2402.12846</link><description>&lt;p&gt;
ConVQG: &#23545;&#27604;&#24335;&#24102;&#22810;&#27169;&#24577;&#24341;&#23548;&#30340;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ConVQG: Contrastive Visual Question Generation with Multimodal Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ConVQG&#26041;&#27861;&#36890;&#36807;&#21452;&#37325;&#23545;&#27604;&#30446;&#26631;&#21306;&#20998;&#20351;&#29992;&#20004;&#31181;&#27169;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#21644;&#22522;&#20110;&#21333;&#19968;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#22312;&#29983;&#25104;&#19987;&#27880;&#38382;&#39064;&#30340;&#21516;&#26102;&#30830;&#20445;&#19982;&#22270;&#20687;&#20869;&#23481;&#30340;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#25361;&#25112;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35810;&#38382;&#20851;&#20110;&#35270;&#35273;&#29615;&#22659;&#30340;&#38382;&#39064;&#26159;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#20016;&#23500;&#22810;&#38754;&#22330;&#26223;&#30340;&#20851;&#38190;&#26041;&#24335;&#65292;&#36825;&#25552;&#39640;&#20102;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#65288;VQG&#65289;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;&#38500;&#20102;&#22522;&#20110;&#22270;&#20687;&#65292;&#29616;&#26377;&#30340;VQG&#31995;&#32479;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#32422;&#26463;&#65292;&#22914;&#26399;&#26395;&#22238;&#31572;&#25110;&#30693;&#35782;&#19977;&#20803;&#32452;&#65292;&#26469;&#29983;&#25104;&#19987;&#27880;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25991;&#26412;&#32422;&#26463;&#29983;&#25104;&#19987;&#27880;&#38382;&#39064;&#30340;&#21516;&#26102;&#30830;&#20445;&#19982;&#22270;&#20687;&#20869;&#23481;&#30340;&#39640;&#24230;&#30456;&#20851;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;VQG&#31995;&#32479;&#32463;&#24120;&#20250;&#24573;&#35270;&#19968;&#20010;&#25110;&#20004;&#20010;&#25511;&#21046;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24335;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#65288;ConVQG&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#21452;&#37325;&#23545;&#27604;&#30446;&#26631;&#26469;&#21306;&#20998;&#20351;&#29992;&#20004;&#31181;&#27169;&#24577;&#29983;&#25104;&#30340;&#38382;&#39064;&#21644;&#22522;&#20110;&#21333;&#19968;&#27169;&#24335;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12846v1 Announce Type: cross  Abstract: Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on
&lt;/p&gt;</description></item><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#23545;&#40784;&#21040;&#30456;&#21516;&#30340;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#26377;&#21033;&#20110;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#27169;&#24577;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#24615;&#33021;&#24182;&#20419;&#36827;&#38271;&#26399;&#25112;&#30053;&#24605;&#32500;&#12290;</title><link>https://arxiv.org/abs/2402.12845</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12845
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#23545;&#40784;&#21040;&#30456;&#21516;&#30340;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#26377;&#21033;&#20110;&#25552;&#21319;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#27169;&#24577;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#24615;&#33021;&#24182;&#20419;&#36827;&#38271;&#26399;&#25112;&#30053;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#21040;&#30456;&#21516;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#29702;&#35299;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25361;&#25112;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#38598;&#25104;&#22810;&#27169;&#24577;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#20174;&#22270;&#20687;&#20013;&#24471;&#21040;&#30340;&#29366;&#24577;&#20449;&#24687;&#21644;&#20174;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#20102;&#38271;&#26399;&#25112;&#30053;&#24605;&#32500;&#12290;&#25105;&#20204;&#24378;&#35843;&#35821;&#35328;&#30340;&#24773;&#22659;&#29702;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22914;&#20309;&#20174;&#23558;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#19982;&#35821;&#35328;&#34920;&#31034;&#23545;&#40784;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Atari&#21644;OpenAI Gym&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#26174;&#31034;&#26126;&#26174;&#20248;&#20110;&#24403;&#21069;&#22522;&#20934;&#12290;&#36825;&#26377;&#21161;&#20110;&#25512;&#21160;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12845v1 Announce Type: new  Abstract: Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the offline reinforcement learning (RL) challenge. More concretely, we transform it into a supervised learning task by integrating multimodal and pre-trained language models. Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states' and actions' representation with languages' representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing offline RL performance and efficie
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.12843</link><description>&lt;p&gt;
&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;:&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#19981;&#23436;&#32654;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12843
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#38382;&#39064;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22826;&#38451;&#33021;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#30417;&#25511;&#21644;&#32500;&#25252;&#26041;&#27861;&#26469;&#30830;&#20445;&#22826;&#38451;&#33021;&#38754;&#26495;&#23433;&#35013;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#20013;&#20934;&#30830;&#20998;&#21106;&#22826;&#38451;&#33021;&#38754;&#26495;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#36816;&#34892;&#38382;&#39064;&#21644;&#35780;&#20272;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#38754;&#26495;&#20998;&#21106;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20197;&#21450;&#25163;&#21160;&#26631;&#27880;&#23545;&#30417;&#30563;&#23398;&#20064;&#30340;&#21171;&#21160;&#23494;&#38598;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#24182;&#24212;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;SSL&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#23545;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#20026;&#20581;&#22766;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#22826;&#38451;&#33021;&#38754;&#26495;&#20998;&#21106;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12843v1 Announce Type: cross  Abstract: The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12835</link><description>&lt;p&gt;
PANDA: &#29992;&#20110;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12835
&lt;/p&gt;
&lt;p&gt;
PANDA&#26159;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#36798;&#21040;&#29305;&#23450;&#39046;&#22495;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22686;&#24378;LLMs&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#30340;&#19968;&#31181;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#32791;&#36153;&#36164;&#28304;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#26080;&#27861;&#24212;&#29992;&#20110;&#23553;&#38381;&#28304;&#21830;&#19994;LLMs&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PANDA&#30340;&#20559;&#22909;&#36866;&#24212;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#27169;&#22411;&#21709;&#24212;&#20559;&#22909;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;LLMs&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PANDA&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;PANDA&#30340;LLM&#29978;&#33267;&#36229;&#36807;&#20102;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12835v1 Announce Type: cross  Abstract: While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.12810</link><description>&lt;p&gt;
PIP-Net&#65306;&#22478;&#24066;&#20013;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PIP-Net: Pedestrian Intention Prediction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12810
&lt;/p&gt;
&lt;p&gt;
PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#23545;&#34892;&#20154;&#24847;&#22270;&#30340;&#39044;&#27979;&#26159;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#19968;&#39033;&#30740;&#31350;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PIP-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;AVs&#22312;&#29616;&#23454;&#19990;&#30028;&#22478;&#24066;&#22330;&#26223;&#20013;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38024;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#23433;&#35013;&#21644;&#35774;&#32622;&#35774;&#35745;&#30340;PIP-Net&#21464;&#31181;&#12290;&#21033;&#29992;&#26469;&#33258;&#34892;&#39542;&#22330;&#26223;&#30340;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#20026;&#20102;&#22686;&#24378;&#36947;&#36335;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#21450;&#20854;&#19982;&#33258;&#36710;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#28145;&#24230;&#29305;&#24449;&#22270;&#65292;&#32467;&#21512;&#23616;&#37096;&#36816;&#21160;&#27969;&#29305;&#24449;&#65292;&#20026;&#22330;&#26223;&#21160;&#24577;&#25552;&#20379;&#20016;&#23500;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25668;&#20687;&#22836;&#30340;&#35270;&#37326;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22260;&#32469;&#33258;&#36710;&#30340;&#19977;&#20010;&#25668;&#20687;&#22836;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12810v1 Announce Type: cross  Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;3D RM&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.12794</link><description>&lt;p&gt;
&#21033;&#29992;&#21327;&#20316;&#22235;&#36275;&#26426;&#22120;&#20154;&#21644;&#26080;&#20154;&#26426;&#36827;&#34892;&#25991;&#21270;&#36951;&#20135;&#36951;&#22336;&#30340;&#33258;&#20027;&#29616;&#23454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12794
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#65292;&#23454;&#29616;&#20102;&#31995;&#32479;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;3D RM&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20808;&#36827;&#20256;&#24863;&#22120;&#30340;&#20351;&#29992;&#65292;&#22914;&#22320;&#38754;3D&#28608;&#20809;&#25195;&#25551;&#20202;&#12289;&#31227;&#21160;LiDAR&#21644;&#26080;&#20154;&#26426;&#25668;&#24433;&#27979;&#37327;&#65292;&#24050;&#32463;&#25104;&#20026;&#25991;&#21270;&#36951;&#20135;(CH)&#22823;&#22411;&#25991;&#29289;&#30340;3D&#29616;&#23454;&#24314;&#27169;&#21644;&#25968;&#23383;&#21270;&#30340;&#20027;&#35201;&#23454;&#36341;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20010;&#36807;&#31243;&#19982;&#35843;&#26597;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#23494;&#20999;&#30456;&#20851;&#65292;&#22788;&#29702;&#38024;&#23545;&#27599;&#20010;&#36951;&#22336;&#29305;&#23450;&#35201;&#27714;&#21644;&#32422;&#26463;&#30340;&#32791;&#26102;&#35268;&#21010;&#21644;&#25191;&#34892;3D&#26144;&#23556;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#24178;&#39044;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#37197;&#22791;&#36866;&#24403;&#20256;&#24863;&#22120;&#30340;&#33258;&#20027;&#20223;&#29983;&#22235;&#36275;&#26426;&#22120;&#20154;&#20195;&#29702;&#21644;&#26080;&#20154;&#26426;&#26469;&#23454;&#29616;&#25991;&#21270;&#36951;&#20135;(CH)&#25991;&#29289;&#30340;&#33258;&#20027;3D&#29616;&#23454;&#24314;&#27169;&#12290;&#36825;&#20123;&#33258;&#20027;&#26426;&#22120;&#20154;&#20195;&#29702;&#20197;&#31995;&#32479;&#21270;&#19988;&#21487;&#37325;&#22797;&#30340;&#26041;&#24335;&#36827;&#34892;3D RM&#36807;&#31243;&#12290;&#36825;&#20010;&#33258;&#21160;&#21270;&#36807;&#31243;&#30340;&#32467;&#26524;&#21487;&#33021;&#22312;&#25968;&#23383;&#23402;&#29983;&#24179;&#21488;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12794v1 Announce Type: cross  Abstract: Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platfo
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12789</link><description>&lt;p&gt;
&#26080;&#38656;&#20844;&#24179;&#35757;&#32451;&#30340;&#20844;&#24179;&#20998;&#31867;&#22120;&#65306;&#19968;&#31181;&#21463;&#24433;&#21709;&#25968;&#25454;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12789
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#23454;&#26045;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#25277;&#26679;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#26469;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#24212;&#35813;&#30830;&#20445;&#26469;&#33258;&#19981;&#21516;&#32676;&#20307;&#30340;&#20154;&#20204;&#21463;&#30410;&#65292;&#32780;&#32676;&#20307;&#20449;&#24687;&#24448;&#24448;&#26159;&#25935;&#24863;&#30340;&#65292;&#19981;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19968;&#20010;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#20294;&#25490;&#38500;&#25935;&#24863;&#23646;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20844;&#24179;&#20998;&#31867;&#22120;&#32780;&#19981;&#23454;&#29616;&#20844;&#24179;&#35757;&#32451;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#21487;&#33021;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#20855;&#26377;&#36866;&#24403;&#20998;&#24067;&#20559;&#31227;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#21487;&#20197;&#21516;&#26102;&#20943;&#23569;&#20844;&#24179;&#24046;&#36317;&#30340;&#19978;&#38480;&#21644;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#65292;&#34920;&#26126;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#21487;&#20197;&#21516;&#27493;&#25552;&#39640;&#65292;&#21482;&#38656;&#31616;&#21333;&#22320;&#36827;&#34892;&#20256;&#32479;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25277;&#26679;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#36880;&#27493;&#36716;&#31227;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#35775;&#38382;&#26032;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT-4&#21644;GLM-4&#65292;&#21457;&#29616;&#26368;&#31616;&#21333;&#30452;&#25509;&#30340;&#25552;&#31034;&#31574;&#30053;&#33021;&#22815;&#20135;&#29983;&#26368;&#20339;&#30340;&#20195;&#30721;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#19988;&#25351;&#20986;&#34429;&#28982;GPT-4&#30053;&#20248;&#20110;GLM-4&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#24046;&#24322;&#24494;&#20046;&#20854;&#24494;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#30456;&#23545;&#20256;&#32479;&#32534;&#30721;&#35268;&#33539;30&#21040;100&#20493;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#29575;&#22686;&#21152;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#36741;&#21161;&#32534;&#30721;&#23558;&#24341;&#21457;&#32534;&#31243;&#39046;&#22495;&#33539;&#24335;&#36716;&#21464;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12782</link><description>&lt;p&gt;
&#25512;&#36827;GenAI&#36741;&#21161;&#32534;&#31243;--GPT-4&#21644;GLM-4&#20043;&#38388;&#25552;&#31034;&#25928;&#29575;&#21644;&#20195;&#30721;&#36136;&#37327;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing GenAI Assisted Programming--A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;GPT-4&#21644;GLM-4&#65292;&#21457;&#29616;&#26368;&#31616;&#21333;&#30452;&#25509;&#30340;&#25552;&#31034;&#31574;&#30053;&#33021;&#22815;&#20135;&#29983;&#26368;&#20339;&#30340;&#20195;&#30721;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#19988;&#25351;&#20986;&#34429;&#28982;GPT-4&#30053;&#20248;&#20110;GLM-4&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#24046;&#24322;&#24494;&#20046;&#20854;&#24494;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#30456;&#23545;&#20256;&#32479;&#32534;&#30721;&#35268;&#33539;30&#21040;100&#20493;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#29575;&#22686;&#21152;&#65292;&#24182;&#24378;&#35843;&#20102;GenAI&#36741;&#21161;&#32534;&#30721;&#23558;&#24341;&#21457;&#32534;&#31243;&#39046;&#22495;&#33539;&#24335;&#36716;&#21464;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;GPT-4&#21644;GLM-4&#20043;&#38388;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25506;&#35752;&#21033;&#29992;GenAI&#20316;&#20026;&#32534;&#31243;&#24037;&#20855;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#22797;&#26434;&#24615;&#27700;&#24179;&#19978;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#31616;&#21333;&#30452;&#25509;&#30340;&#25552;&#31034;&#31574;&#30053;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#20195;&#30721;&#29983;&#25104;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#31867;&#20284;CoT&#30340;&#21021;&#27493;&#30830;&#35748;&#27493;&#39588;&#23558;&#36827;&#19968;&#27493;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20248;&#20110;GLM-4&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#26469;&#35828;&#24046;&#24322;&#24494;&#20046;&#20854;&#24494;&#12290;&#22312;&#25105;&#20204;&#31616;&#21270;&#30340;&#35780;&#20272;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30475;&#21040;&#20195;&#30721;&#29983;&#25104;&#25928;&#29575;&#30456;&#23545;&#20256;&#32479;&#32534;&#30721;&#35268;&#33539;&#26174;&#33879;&#22686;&#21152;&#20102;30&#21040;100&#20493;&#12290;&#25105;&#20204;&#30340;GenAI&#32534;&#30721;&#30740;&#35752;&#20250;&#31361;&#26174;&#20102;&#26412;&#30740;&#31350;&#24320;&#21457;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;GenAI&#36741;&#21161;&#32534;&#30721;&#23558;&#24341;&#21457;&#32534;&#31243;&#39046;&#22495;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#36825;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#25215;&#25285;&#26032;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12782v1 Announce Type: cross  Abstract: This study aims to explore the best practices for utilizing GenAI as a programming tool, through a comparative analysis between GPT-4 and GLM-4. By evaluating prompting strategies at different levels of complexity, we identify that simplest and straightforward prompting strategy yields best code generation results. Additionally, adding a CoT-like preliminary confirmation step would further increase the success rate. Our results reveal that while GPT-4 marginally outperforms GLM-4, the difference is minimal for average users. In our simplified evaluation model, we see a remarkable 30 to 100-fold increase in code generation efficiency over traditional coding norms. Our GenAI Coding Workshop highlights the effectiveness and accessibility of the prompting methodology developed in this study. We observe that GenAI-assisted coding would trigger a paradigm shift in programming landscape, which necessitates developers to take on new roles revo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#20248;&#36873;&#25552;&#31034;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12760</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20248;&#36873;&#25552;&#31034;&#30340;&#29992;&#25143;&#21451;&#22909;&#26694;&#26550;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12760
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#20248;&#36873;&#25552;&#31034;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#24778;&#20154;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#25351;&#23548;&#65292;&#20294;&#26159;&#23545;&#20110;&#26032;&#25163;&#29992;&#25143;&#26469;&#35828;&#65292;&#36890;&#36807;&#25163;&#21160;&#36755;&#20837;&#25552;&#31034;&#26469;&#23454;&#29616;&#26399;&#26395;&#32467;&#26524;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#22240;&#20026;&#26032;&#25163;&#29992;&#25143;&#36755;&#20837;&#30340;&#25552;&#31034;&#19982;&#27169;&#22411;&#20248;&#36873;&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#24357;&#21512;&#29992;&#25143;&#36755;&#20837;&#34892;&#20026;&#21644;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31895;&#32454;&#31890;&#24230;&#25552;&#31034;&#25968;&#25454;&#38598;(CFP)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#25143;&#21451;&#22909;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;(UF-FGTG)&#29992;&#20110;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#12290;&#23545;&#20110;CFP&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#31895;&#31961;&#21644;&#32454;&#31890;&#24230;&#25552;&#31034;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#23545;&#20110;UF-FGTG&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#29992;&#25143;&#36755;&#20837;&#30340;&#25552;&#31034;&#36716;&#25442;&#20026;&#27169;&#22411;&#20248;&#36873;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12760v1 Announce Type: cross  Abstract: Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred promp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12750</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Model Composition for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#31034;&#20986;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#26397;&#30528;&#21019;&#24314;&#33021;&#22815;&#29702;&#35299;&#21508;&#31181;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#21151;&#33021;MLLMs&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36825;&#23545;&#36164;&#28304;&#35201;&#27714;&#39640;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29616;&#26377;MLLMs&#30340;&#27169;&#22411;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#26032;&#27169;&#22411;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#23454;&#29616;NaiveMC&#36890;&#36807;&#37325;&#29992;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#21512;&#24182;LLM&#21442;&#25968;&#23637;&#31034;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAMC&#26469;&#35299;&#20915;&#22312;&#21512;&#24182;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCUB&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;MLLMs&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#19987;&#23478;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#24515;&#29702;&#36741;&#23548;&#24773;&#22659;&#20013;&#19982;&#20154;&#31867;&#22238;&#22797;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.12738</link><description>&lt;p&gt;
&#33021;&#21542;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24515;&#29702;&#21672;&#35810;&#65311;&#23545;GPT-4&#29983;&#25104;&#30340;&#23545;&#35805;&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12738
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19987;&#23478;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#24515;&#29702;&#36741;&#23548;&#24773;&#22659;&#20013;&#19982;&#20154;&#31867;&#22238;&#22797;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12738v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#23545;&#29616;&#20195;&#31038;&#20250;&#26500;&#25104;&#26085;&#30410;&#20005;&#23803;&#25361;&#25112;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#21033;&#29992;&#20449;&#24687;&#25216;&#26415;&#35299;&#20915;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#21253;&#25324;&#26088;&#22312;&#24320;&#21457;&#21672;&#35810;&#23545;&#35805;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#26377;&#24517;&#35201;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21672;&#35810;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#36827;&#34892;&#26356;&#22810;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#28041;&#21450;&#19987;&#23478;&#36741;&#23548;&#21592;&#30340;&#35282;&#33394;&#25198;&#28436;&#24773;&#26223;&#25910;&#38598;&#20102;&#36741;&#23548;&#23545;&#35805;&#25968;&#25454;&#65292;&#24182;&#38024;&#23545;&#36741;&#23548;&#21592;&#30340;&#24847;&#22270;&#23545;&#35805;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20026;&#20102;&#30830;&#23450;&#23545;&#35805;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#36741;&#23548;&#24773;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#31532;&#19977;&#26041;&#36741;&#23548;&#21592;&#22312;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#25968;&#25454;&#20013;&#23545;&#20154;&#31867;&#36741;&#23548;&#21592;&#21644;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#30456;&#21516;&#24773;&#22659;&#19979;&#30340;&#24688;&#24403;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-4&#29983;&#25104;&#30340;&#22238;&#22797;&#19982;&#20154;&#31867;&#30340;&#22238;&#22797;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12738v1 Announce Type: cross  Abstract: Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#31216;&#20026;&#26657;&#20934;&#20391;&#35843;&#33410;&#65292;&#23558;&#25104;&#21151;&#24212;&#29992;&#20110;&#36716;&#25442;&#22120;&#30340;&#25216;&#26415;&#19982;ResNet&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#24182;&#20445;&#25345;&#24179;&#28369;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.12736</link><description>&lt;p&gt;
CST: &#21442;&#25968;&#21644;&#20869;&#23384;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#30340;&#26657;&#20934;&#20391;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#31216;&#20026;&#26657;&#20934;&#20391;&#35843;&#33410;&#65292;&#23558;&#25104;&#21151;&#24212;&#29992;&#20110;&#36716;&#25442;&#22120;&#30340;&#25216;&#26415;&#19982;ResNet&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#24182;&#20445;&#25345;&#24179;&#28369;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#29289;&#20307;&#26816;&#27979;&#30340;&#26222;&#36941;&#39640;&#20934;&#30830;&#24615;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#34892;&#19994;&#20013;&#30446;&#21069;&#30340;&#20027;&#35201;&#28966;&#28857;&#22312;&#20110;&#26816;&#27979;&#29305;&#23450;&#31867;&#21035;&#30340;&#29289;&#20307;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#19968;&#20010;&#25110;&#22810;&#20010;&#29289;&#20307;&#26816;&#27979;&#32593;&#32476;&#38656;&#35201;&#19968;&#23450;&#37327;&#30340;GPU&#20869;&#23384;&#29992;&#20110;&#35757;&#32451;&#21644;&#23384;&#20648;&#23481;&#37327;&#29992;&#20110;&#25512;&#26029;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#21327;&#35843;&#22810;&#20010;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#24494;&#35843;&#31574;&#30053;&#65292;&#31216;&#20026;&#26657;&#20934;&#20391;&#35843;&#33410;&#65292;&#23427;&#25972;&#21512;&#20102;&#36866;&#37197;&#22120;&#35843;&#33410;&#21644;&#20391;&#35843;&#33410;&#30340;&#26041;&#38754;&#65292;&#20197;&#36866;&#24212;&#36716;&#25442;&#22120;&#20013;&#25104;&#21151;&#25216;&#26415;&#22312;ResNet&#19978;&#30340;&#20351;&#29992;&#12290;&#26657;&#20934;&#20391;&#35843;&#33410;&#26550;&#26500;&#34701;&#21512;&#20102;&#26368;&#22823;&#36807;&#28193;&#26657;&#20934;&#65292;&#21033;&#29992;&#23569;&#37327;&#39069;&#22806;&#21442;&#25968;&#22686;&#24378;&#32593;&#32476;&#24615;&#33021;&#21516;&#26102;&#20445;&#25345;&#24179;&#31283;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12736v1 Announce Type: cross  Abstract: Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;BMLP&#65292;&#36890;&#36807;&#34892;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#27169;&#22359;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#21644;&#36141;&#20080;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12733</link><description>&lt;p&gt;
BMLP&#65306;&#29992;&#20110;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#30340;&#34892;&#20026;&#24863;&#30693;MLP
&lt;/p&gt;
&lt;p&gt;
BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;BMLP&#65292;&#36890;&#36807;&#34892;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#27169;&#22359;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#21644;&#36141;&#20080;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#28857;&#20987;&#21644;&#36141;&#20080;&#12290;&#29616;&#26377;&#30740;&#31350;&#26041;&#27861;&#34920;&#26126;&#65292;&#36890;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#34892;&#20026;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#30340;&#24322;&#26500;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22810;&#34892;&#20026;&#26041;&#27861;&#22312;&#23398;&#20064;&#19981;&#21516;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#24322;&#26500;&#24207;&#21015;&#25512;&#33616;&#26041;&#27861;&#65292;&#21363;&#34892;&#20026;&#24863;&#30693;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;BMLP&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#65292;&#21363;&#24322;&#26500;&#20852;&#36259;&#24863;&#30693;&#65288;HIP&#65289;&#27169;&#22359;&#65292;&#36890;&#36807;&#34892;&#20026;&#31867;&#22411;&#21644;&#36716;&#25442;&#20851;&#31995;&#22312;&#22810;&#20010;&#31890;&#24230;&#19978;&#24314;&#27169;&#34892;&#20026;&#65292;&#24182;&#36141;&#20080;&#24847;&#22270;&#24863;&#30693;&#65288;PIP&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#36741;&#21161;&#34892;&#20026;&#30340;&#23376;&#24207;&#21015;&#20197;&#25429;&#25417;&#29992;&#25143;&#30340;&#36141;&#20080;&#24847;&#22270;&#12290;&#19982;&#20027;&#27969;&#24207;&#21015;&#27169;&#22411;&#30456;&#27604;&#65292;MLP&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12733v1 Announce Type: cross  Abstract: In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accu
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTNP&#30340;&#31070;&#32463;&#36807;&#31243;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36755;&#31574;&#30053;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12729</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21487;&#38752;&#30340;&#22810;&#23610;&#24230;&#31070;&#32463;&#36807;&#31243;&#23884;&#20837;&#30693;&#35782;&#29992;&#20110;&#26234;&#33021;&#25925;&#38556;&#26816;&#27979;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12729
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GTNP&#30340;&#31070;&#32463;&#36807;&#31243;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#20256;&#36755;&#31574;&#30053;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#20998;&#26512;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#26159;&#26234;&#33021;&#25925;&#38556;&#26816;&#27979;&#65288;IFD&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#35757;&#32451;&#38598;&#65288;&#28304;&#22495;&#65289;&#21644;&#27979;&#35797;&#38598;&#65288;&#30446;&#26631;&#22495;&#65289;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#23548;&#33268;&#26041;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GTNP&#65289;&#30340;&#26032;&#39062;&#30340;DTL&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#21644;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12729v1 Announce Type: cross  Abstract: Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP). Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12728</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#24577;&#24863;&#30693;&#38598;&#25104;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#29992;&#20110;&#38024;&#23545;KVQA&#65292;&#36890;&#36807;&#32454;&#33268;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#26469;&#22788;&#29702;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;KVQA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22914;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#20960;&#31181;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38544;&#21547;&#30693;&#35782;&#28304;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#29983;&#25104;&#24187;&#35273;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#30693;&#35782;&#26469;&#28304;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#19981;&#33021;&#36731;&#26131;&#23545;&#40784;&#20197;&#24212;&#23545;&#22797;&#26434;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KVQA&#30340;&#26032;&#39062;&#30340;&#20855;&#26377;&#27169;&#24577;&#24863;&#30693;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65288;MAIL&#65289;&#12290;&#23427;&#31934;&#24515;&#21033;&#29992;&#22810;&#27169;&#24577;&#30693;&#35782;&#36827;&#34892;&#22270;&#20687;&#29702;&#35299;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#30340;&#20004;&#38454;&#27573;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#23494;&#38598;&#22320;&#34701;&#20837;&#24102;&#26377;&#35814;&#32454;&#35270;&#35273;&#29305;&#24449;&#30340;&#22330;&#26223;&#22270;&#20013;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#36890;&#36807;&#23558;&#25552;&#21040;&#30340;&#23454;&#20307;&#19982;&#22806;&#37096;&#20107;&#23454;&#32852;&#31995;&#36215;&#26469;&#26500;&#24314;&#19968;&#20010;&#32806;&#21512;&#30340;&#27010;&#24565;&#22270;&#65307;&#65288;iii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20266;&#23402;&#29983;&#22270;&#20013;&#20171;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#65306;&#22312;&#21152;&#23494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#20551;&#35774;&#19979;&#8212;&#8212;&#21333;&#21521;&#20989;&#25968;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#23454;&#20363;&#65292;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27599;&#20010;&#31639;&#27861;&#37117;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21363;&#20351;&#26080;&#26465;&#20214;&#25277;&#26679;&#21487;&#20197;&#35777;&#26126;&#26159;&#24555;&#36895;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.12727</link><description>&lt;p&gt;
&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Sampling is Computationally Intractable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12727
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#39564;&#25277;&#26679;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#65306;&#22312;&#21152;&#23494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#20551;&#35774;&#19979;&#8212;&#8212;&#21333;&#21521;&#20989;&#25968;&#23384;&#22312;&#30340;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#23454;&#20363;&#65292;&#23545;&#20110;&#36825;&#20123;&#23454;&#20363;&#65292;&#27599;&#20010;&#31639;&#27861;&#37117;&#38656;&#35201;&#36229;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21363;&#20351;&#26080;&#26465;&#20214;&#25277;&#26679;&#21487;&#20197;&#35777;&#26126;&#26159;&#24555;&#36895;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#23398;&#20064;&#21644;&#20174;&#20998;&#24067;$p(x)$&#20013;&#25277;&#26679;&#30340;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#22312;&#21518;&#39564;&#25277;&#26679;&#20013;&#65292;&#20154;&#20204;&#36824;&#20250;&#32473;&#20986;&#19968;&#20010;&#27979;&#37327;&#27169;&#22411;$p(y \mid x)$&#21644;&#19968;&#20010;&#27979;&#37327;$y$&#65292;&#24076;&#26395;&#20174;$p(x \mid y)$&#20013;&#25277;&#26679;&#12290;&#21518;&#39564;&#25277;&#26679;&#23545;&#20110;&#35832;&#22914;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#21644;MRI&#37325;&#24314;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#27492;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#32473;&#20986;&#20102;&#21551;&#21457;&#24335;&#36817;&#20284;&#31639;&#27861;&#65307;&#20294;&#27809;&#26377;&#19968;&#20010;&#24050;&#30693;&#33021;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12727v1 Announce Type: cross  Abstract: Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \mid x)$ and a measurement $y$, and would like to sample from $p(x \mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   In this paper we show that posterior sampling is \emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \emph{every} algorithm takes superpolynomial time, even though \emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12721</link><description>&lt;p&gt;
PAC-FNO&#65306;&#24182;&#34892;&#32467;&#26500;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35782;&#21035;&#20302;&#36136;&#37327;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411; PAC-FNO&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#25805;&#20316;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#31867;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26159;&#22312;&#29305;&#23450;&#22270;&#20687;&#20998;&#36776;&#29575;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#37096;&#32626;&#23427;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25512;&#29702;&#20013;&#65292;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21644;/&#25110;&#21463;&#21040;&#33258;&#28982;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22825;&#27668;&#21464;&#21270;&#12289;&#22122;&#22768;&#31867;&#22411;&#21644;&#21387;&#32553;&#20266;&#24433;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#20026;&#19981;&#21516;&#20998;&#36776;&#29575;&#25110;&#36755;&#20837;&#21464;&#21270;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#19981;&#21487;&#25193;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21363;&#24182;&#34892;&#32467;&#26500;&#21644;&#20840;&#32452;&#20998;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;PAC-FNO&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;PAC-FNO&#22312;&#39057;&#22495;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21333;&#20010;&#27169;&#22411;&#20869;&#22788;&#29702;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#30340;&#20462;&#25913;&#35757;&#32451;PAC-FNO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12721v1 Announce Type: cross  Abstract: A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the orig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#30340;&#23481;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#37327;&#23450;&#20041;&#31867;&#20284;&#20110;&#20449;&#36947;&#23481;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#32039;&#23494;&#20272;&#35745;&#20854;&#19978;&#30028;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#26469;&#23433;&#20840;&#20256;&#36755;&#36229;&#20986;&#23481;&#37327;&#30340;&#36523;&#20221;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.12720</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#30340;&#20449;&#24687;&#23481;&#37327;&#65306;&#19978;&#30028;&#20272;&#35745;&#19982;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#30340;&#23481;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#37327;&#23450;&#20041;&#31867;&#20284;&#20110;&#20449;&#36947;&#23481;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#32039;&#23494;&#20272;&#35745;&#20854;&#19978;&#30028;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#26469;&#23433;&#20840;&#20256;&#36755;&#36229;&#20986;&#23481;&#37327;&#30340;&#36523;&#20221;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29256;&#26435;&#65292;&#25152;&#26377;&#32773;&#21487;&#20197;&#23558;&#20854;&#36523;&#20221;&#20449;&#24687;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#20316;&#20026;&#27700;&#21360;&#12290;&#27700;&#21360;&#30340;&#23481;&#37327;&#37327;&#21270;&#20102;&#21487;&#20197;&#20174;&#24102;&#26377;&#27700;&#21360;&#27169;&#22411;&#20013;&#39564;&#35777;&#30340;&#26368;&#22823;&#20449;&#24687;&#37327;&#12290;&#24403;&#21069;&#20851;&#20110;&#23481;&#37327;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#36890;&#21435;&#38500;&#25915;&#20987;&#19979;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#20934;&#30830;&#24615;&#65292;&#24182;&#26410;&#25429;&#25417;&#21040;&#40065;&#26834;&#24615;&#21644;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#30340;&#23481;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20449;&#36947;&#23481;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27700;&#21360;&#23481;&#37327;&#30340;&#26032;&#23450;&#20041;&#65292;&#20998;&#26512;&#20102;&#20854;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22312;&#23545;&#25239;&#24615;&#35206;&#20889;&#19979;&#32039;&#23494;&#20272;&#35745;&#20854;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#36718;&#25152;&#26377;&#26435;&#39564;&#35777;&#26469;&#23433;&#20840;&#20256;&#36755;&#36229;&#20986;&#23481;&#37327;&#30340;&#36523;&#20221;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12720v1 Announce Type: cross  Abstract: To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark. The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model. Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity. This paper studies the capacity of deep neural network watermarks from an information theoretical perspective. We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting. We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification. Our observations provide evi
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35843;&#25972;&#20197;&#36866;&#24212;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#39640;&#25928;&#21019;&#24314;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.12702</link><description>&lt;p&gt;
&#20174;&#20113;&#31471;&#21040;&#36793;&#32536;&#65306;&#37325;&#26032;&#24605;&#32771;&#20302;&#36164;&#28304;&#35774;&#35745;&#25361;&#25112;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12702
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#35843;&#25972;&#20197;&#36866;&#24212;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#39640;&#25928;&#21019;&#24314;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#25216;&#26415;&#30340;&#21508;&#20010;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#36164;&#28304;&#30340;&#27785;&#37325;&#38656;&#27714;&#65292;&#23427;&#36890;&#24120;&#22312;&#22823;&#22411;&#35745;&#31639;&#22522;&#30784;&#26550;&#26500;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#32463;&#24120;&#20316;&#20026;&#22522;&#20110;&#20113;&#31471;&#30340;&#26381;&#21153;&#25552;&#20379;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#36793;&#32536;&#36827;&#34892;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#28508;&#21147;&#12289;&#25361;&#25112;&#21644;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#65292;&#20854;&#20013;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#12289;&#33021;&#32791;&#65288;&#30005;&#27744;&#65289;&#21644;&#32593;&#32476;&#36830;&#25509;&#21487;&#33021;&#26377;&#38480;&#12290;&#35843;&#25972;&#29983;&#25104;AI&#20197;&#36866;&#24212;&#36825;&#26679;&#30340;&#35774;&#32622;&#28041;&#21450;&#20811;&#26381;&#37325;&#22823;&#38556;&#30861;&#65292;&#20027;&#35201;&#26159;&#22914;&#20309;&#31616;&#21270;&#22797;&#26434;&#27169;&#22411;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26377;&#25928;&#36816;&#34892;&#12290;&#36825;&#38656;&#35201;&#22312;&#27169;&#22411;&#21387;&#32553;&#12289;&#39640;&#25928;&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#21487;&#33021;&#29978;&#33267;&#21033;&#29992;&#36793;&#32536;&#35745;&#31639;&#26041;&#38754;&#25552;&#20986;&#21019;&#26032;&#24615;&#26041;&#27861;&#12290;&#30446;&#26631;&#26159;&#21033;&#29992;&#29983;&#25104;AI&#30340;&#21147;&#37327;&#20026;&#35774;&#35745;&#38382;&#39064;&#21019;&#36896;&#37327;&#36523;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12702v1 Announce Type: new  Abstract: Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design. However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service. In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited. Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments. This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing. The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such
&lt;/p&gt;</description></item><item><title>XRL-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29366;&#24577;&#35299;&#37322;&#26041;&#27861;&#22312;RL&#20013;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26694;&#26550;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12685</link><description>&lt;p&gt;
XRL-Bench&#65306;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12685
&lt;/p&gt;
&lt;p&gt;
XRL-Bench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#32479;&#19968;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29366;&#24577;&#35299;&#37322;&#26041;&#27861;&#22312;RL&#20013;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26694;&#26550;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#29702;&#24615;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#26412;&#25991;&#30528;&#25163;&#20110;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#65288;XRL&#65289;&#65292;&#36825;&#26159;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#25581;&#31034;RL&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#29366;&#24577;&#35299;&#37322;&#25216;&#26415;&#65292;&#36825;&#26159;XRL&#26041;&#27861;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#23376;&#38598;&#65292;&#22240;&#20026;&#23427;&#20204;&#25581;&#31034;&#20102;&#24433;&#21709;&#20195;&#29702;&#31243;&#24207;&#22312;&#20219;&#20309;&#32473;&#23450;&#26102;&#38388;&#37319;&#21462;&#34892;&#21160;&#30340;&#28508;&#22312;&#22240;&#32032;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#38459;&#30861;&#20102;&#23545;&#23427;&#20204;&#20934;&#30830;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;XRL-Bench&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#19987;&#20026;&#35780;&#20272;&#21644;&#27604;&#36739;XRL&#26041;&#27861;&#32780;&#35774;&#35745;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#26631;&#20934;RL&#29615;&#22659;&#12289;&#22522;&#20110;&#29366;&#24577;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#22120;&#21644;&#26631;&#20934;&#35780;&#20272;&#22120;&#12290;XRL-Bench&#25903;&#25345;&#36733;&#20837;&#27169;&#22411;&#21644;&#35780;&#20272;&#22120;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#36866;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#21508;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12685v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent's actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12659</link><description>&lt;p&gt;
FinBen&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#36130;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
The FinBen: An Holistic Financial Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;FinBen&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#33021;&#21147;&#30340;&#20840;&#38754;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#24443;&#24213;&#30340;&#35780;&#20272;&#21644;&#37329;&#34701;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FinBen&#65292;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#35780;&#20272;&#22522;&#20934;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24443;&#24213;&#35780;&#20272;LLMs&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;FinBen&#21253;&#25324;23&#31181;&#37329;&#34701;&#20219;&#21153;&#30340;35&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#26681;&#25454;&#21345;&#29305;&#23572;-&#38669;&#24681;-&#21345;&#32599;&#23572;&#29702;&#35770;&#30340;&#28789;&#24863;&#32452;&#32455;&#25104;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#35889;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24402;&#32435;&#25512;&#29702;&#12289;&#32852;&#24819;&#35760;&#24518;&#12289;&#25968;&#37327;&#25512;&#29702;&#12289;&#26230;&#20307;&#26234;&#21147;&#31561;&#26041;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;15&#20010;&#20195;&#34920;&#24615;LLMs&#65288;&#21253;&#25324;GPT-4&#12289;ChatGPT&#21644;&#26368;&#26032;&#30340;Gemini&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12659v1 Announce Type: cross  Abstract: LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations withi
&lt;/p&gt;</description></item><item><title>HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.12656</link><description>&lt;p&gt;
HyperMoE: &#36890;&#36807;&#19987;&#23478;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#23454;&#29616;&#26356;&#22909;&#30340;&#19987;&#23478;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12656
&lt;/p&gt;
&lt;p&gt;
HyperMoE&#36890;&#36807;Hypernetworks&#26694;&#26550;&#25972;&#21512;&#30693;&#35782;&#20256;&#36882;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#19987;&#23478;&#30693;&#35782;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;(MoE)&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36335;&#30001;&#21040;&#29305;&#23450;&#30340;&#19987;&#23478;&#23376;&#38598;&#36827;&#34892;&#22788;&#29702;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#19987;&#23478;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#38754;&#20020;&#25361;&#25112;&#65306;&#36890;&#36807;&#22686;&#21152;&#23545;&#19987;&#23478;&#30693;&#35782;&#30340;&#20351;&#29992;&#26469;&#22686;&#24378;&#24615;&#33021;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#19987;&#23478;&#36873;&#25321;&#36807;&#31243;&#20013;&#31232;&#30095;&#24230;&#20943;&#23569;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;Hypernetworks&#20043;&#19978;&#30340;&#26032;&#39062;MoE&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;MoE&#30340;&#35745;&#31639;&#36807;&#31243;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#27010;&#24565;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#22522;&#20110;&#26410;&#36873;&#25321;&#19987;&#23478;&#20449;&#24687;&#29983;&#25104;&#30340;&#29305;&#23450;&#27169;&#22359;&#20316;&#20026;&#34917;&#20805;&#20449;&#24687;&#65292;&#20801;&#35768;&#26410;&#34987;&#36873;&#20013;&#30340;&#19987;&#23478;&#30340;&#30693;&#35782;&#22312;&#20445;&#25345;&#36873;&#25321;&#31232;&#30095;&#24615;&#30340;&#21516;&#26102;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12656v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#39640;&#25928;&#29256;&#26412;&#30340;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21644;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12646</link><description>&lt;p&gt;
&#36890;&#36807;&#22352;&#26631;&#25628;&#32034;&#31639;&#27861;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Artificial Neural Networks by Coordinate Search Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#39640;&#25928;&#29256;&#26412;&#30340;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#20102;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21644;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#12290; &#23613;&#31649;&#26799;&#24230;&#19979;&#38477;&#31561;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290; &#20363;&#22914;&#65292;&#23427;&#20204;&#38656;&#35201;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#19988;&#19981;&#33021;&#22522;&#20110;&#22810;&#20010;&#29420;&#31435;&#30340;&#38750;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#65307;&#20363;&#22914;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#20351;&#29992;&#30340; F1 &#20998;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#65292;&#24403;&#37319;&#29992;&#26080;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#26102;&#12290; &#27492;&#22806;&#65292;&#20219;&#20309; DNN &#20013;&#30340;&#35757;&#32451;&#21487;&#33021;&#21482;&#38656;&#24456;&#23569;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#26799;&#24230;&#22352;&#26631;&#25628;&#32034;&#65288;CS&#65289;&#31639;&#27861;&#30340;&#39640;&#25928;&#29256;&#26412;&#65292;&#23427;&#26159;&#36890;&#29992;&#27169;&#24335;&#25628;&#32034;&#26041;&#27861;&#30340;&#19968;&#31181;&#23454;&#20363;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12646v1 Announce Type: cross  Abstract: Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#24402;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12627</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#25968;&#25454;&#21464;&#21270;&#26041;&#38754;&#30340;&#32508;&#21512;&#35780;&#35770;&#65306;&#36328;&#39046;&#22495;&#36879;&#35270;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12627
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#24402;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#21508;&#20010;&#23398;&#26415;&#39046;&#22495;&#21644;&#34892;&#19994;&#37117;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#21160;&#24577;&#25968;&#25454;&#23548;&#33268;&#20102;&#37096;&#32626;AI&#27169;&#22411;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#24847;&#22806;&#30340;&#25968;&#25454;&#21464;&#21270;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#30456;&#20851;&#30740;&#31350;&#39046;&#22495;&#65292;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#65292;&#26681;&#25454;&#25968;&#25454;&#21464;&#21270;&#30340;&#35774;&#23450;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#21644;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#38382;&#39064;&#65292;&#20294;&#22522;&#26412;&#23646;&#24615;&#20173;&#28982;&#30456;&#20284;&#65292;&#36825;&#20063;&#40723;&#21169;&#37319;&#29992;&#31867;&#20284;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#36716;&#31227;&#21644;&#27010;&#24565;&#28418;&#31227;&#37325;&#26032;&#32452;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#21464;&#21270;&#38382;&#39064;&#65292;&#24182;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#38382;&#39064;&#20998;&#31867;&#26041;&#26696;&#65292;&#20197;&#23558;&#36825;&#20004;&#20010;&#25216;&#26415;&#39046;&#22495;&#30340;&#20851;&#38190;&#24605;&#24819;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12627v1 Announce Type: cross  Abstract: Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#22330;&#26223;&#20013;&#21033;&#29992;&#39640;&#25928;&#30340;&#23618;&#32423;&#21442;&#25968;&#38548;&#31163;&#26041;&#24335;&#65292;&#21487;&#20197;&#24110;&#21161;&#32593;&#32476;&#20445;&#25345;&#26816;&#27979;&#22120;&#24615;&#33021;&#65292;&#24182;&#22312;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20869;&#20419;&#36827;&#22686;&#37327;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.12624</link><description>&lt;p&gt;
&#39640;&#25928;&#21442;&#25968;&#25366;&#25496;&#21644;&#20923;&#32467;&#29992;&#20110;&#36830;&#32493;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Parameter Mining and Freezing for Continual Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#22330;&#26223;&#20013;&#21033;&#29992;&#39640;&#25928;&#30340;&#23618;&#32423;&#21442;&#25968;&#38548;&#31163;&#26041;&#24335;&#65292;&#21487;&#20197;&#24110;&#21161;&#32593;&#32476;&#20445;&#25345;&#26816;&#27979;&#22120;&#24615;&#33021;&#65292;&#24182;&#22312;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20869;&#20419;&#36827;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#30446;&#26631;&#26816;&#27979;&#23545;&#20110;&#20351;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#19982;&#20154;&#31867;&#36827;&#34892;&#31215;&#26497;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#21442;&#25968;&#38548;&#31163;&#31574;&#30053;&#22312;&#25345;&#32493;&#23398;&#20064;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#24050;&#34987;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#22312;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#22330;&#26223;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#21463;&#20808;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25366;&#25496;&#21333;&#20010;&#31070;&#32463;&#20803;&#21709;&#24212;&#20197;&#21450;&#25972;&#21512;&#26368;&#36817;&#31070;&#32463;&#21098;&#26525;&#21457;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21738;&#20123;&#23618;&#23545;&#20110;&#32593;&#32476;&#26469;&#35828;&#26159;&#26368;&#37325;&#35201;&#30340;&#65292;&#20197;&#22312;&#39034;&#24207;&#26356;&#26032;&#20013;&#32500;&#25345;&#26816;&#27979;&#22120;&#24615;&#33021;&#12290;&#25152;&#21576;&#29616;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#22312;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20869;&#36827;&#34892;&#23618;&#32423;&#21442;&#25968;&#38548;&#31163;&#20197;&#20419;&#36827;&#22686;&#37327;&#23398;&#20064;&#30340;&#37325;&#22823;&#20248;&#21183;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#21644;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12624v1 Announce Type: cross  Abstract: Continual Object Detection is essential for enabling intelligent agents to interact proactively with humans in real-world settings. While parameter-isolation strategies have been extensively explored in the context of continual learning for classification, they have yet to be fully harnessed for incremental object detection scenarios. Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural pruning, we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates. The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within object detection models, offering promising avenues for future research and application in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12617</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#65306;&#25361;&#25112;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Generative AI Security: Challenges and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12617
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#34892;&#19994;&#30340;&#19981;&#26029;&#25193;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#21644;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
&lt;/p&gt;</description></item><item><title>&#24739;&#32773;&#20013;&#24515;&#30693;&#35782;&#22270;&#35889;&#65288;PCKGs&#65289;&#20195;&#34920;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#36716;&#21464;&#65292;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#20581;&#24247;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#25928;&#30340;&#24739;&#32773;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12608</link><description>&lt;p&gt;
&#24739;&#32773;&#20013;&#24515;&#30693;&#35782;&#22270;&#35889;&#65306;&#24403;&#21069;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#24212;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12608
&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#20013;&#24515;&#30693;&#35782;&#22270;&#35889;&#65288;PCKGs&#65289;&#20195;&#34920;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#36716;&#21464;&#65292;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#20581;&#24247;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#25928;&#30340;&#24739;&#32773;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#20013;&#24515;&#30693;&#35782;&#22270;&#35889;&#65288;PCKGs&#65289;&#20195;&#34920;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#36716;&#21464;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#20197;&#25972;&#20307;&#21644;&#22810;&#32500;&#26041;&#24335;&#26144;&#23556;&#24739;&#32773;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#24739;&#32773;&#25252;&#29702;&#12290;PCKGs&#25972;&#21512;&#21508;&#31181;&#31867;&#22411;&#30340;&#20581;&#24247;&#25968;&#25454;&#65292;&#20026;&#21307;&#25252;&#20154;&#21592;&#25552;&#20379;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#25928;&#30340;&#25252;&#29702;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#19982;PCKGs&#30456;&#20851;&#30340;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#25972;&#21512;&#19981;&#21516;&#21307;&#30103;&#25968;&#25454;&#21644;&#36890;&#36807;&#32479;&#19968;&#30340;&#20581;&#24247;&#35270;&#35282;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#35813;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;PCKG&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#26412;&#20307;&#35774;&#35745;&#12289;&#25968;&#25454;&#25972;&#21512;&#25216;&#26415;&#12289;&#30693;&#35782;&#25552;&#21462;&#21644;&#30693;&#35782;&#32467;&#26500;&#21270;&#34920;&#24449;&#12290;&#23427;&#24378;&#35843;&#20102;&#22312;&#26500;&#24314;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25512;&#29702;&#12289;&#35821;&#20041;&#25628;&#32034;&#21644;&#25512;&#29702;&#26426;&#21046;&#31561;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12608v1 Announce Type: new  Abstract: Patient-Centric Knowledge Graphs (PCKGs) represent an important shift in healthcare that focuses on individualized patient care by mapping the patient's health information in a holistic and multi-dimensional way. PCKGs integrate various types of health data to provide healthcare professionals with a comprehensive understanding of a patient's health, enabling more personalized and effective care. This literature review explores the methodologies, challenges, and opportunities associated with PCKGs, focusing on their role in integrating disparate healthcare data and enhancing patient care through a unified health perspective. In addition, this review also discusses the complexities of PCKG development, including ontology design, data integration techniques, knowledge extraction, and structured representation of knowledge. It highlights advanced techniques such as reasoning, semantic search, and inference mechanisms essential in constructin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;GgNet&#26550;&#26500;&#65292;&#29992;&#20110;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.12598</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#65306;&#26469;&#33258;&#31232;&#30095;&#21644;&#37096;&#20998;&#22810;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#34394;&#25311;&#20256;&#24863;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;GgNet&#26550;&#26500;&#65292;&#29992;&#20110;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20256;&#24863;&#25216;&#26415;&#20801;&#35768;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#20301;&#32622;&#30340;&#29289;&#29702;&#20256;&#24863;&#22120;&#30340;&#26102;&#31354;&#27979;&#37327;&#26469;&#25512;&#26029;&#26032;&#20301;&#32622;&#30340;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#25110;&#20854;&#20182;&#38480;&#21046;&#23548;&#33268;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#21464;&#24471;&#31232;&#30095;&#65292;&#26080;&#27861;&#21033;&#29992;&#29289;&#29702;&#25509;&#36817;&#24615;&#25903;&#25345;&#25554;&#20540;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#21464;&#37327;&#19982;&#19968;&#32452;&#30456;&#20851;&#21464;&#37327;&#65288;&#21327;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#36825;&#20123;&#21327;&#21464;&#37327;&#21487;&#20197;&#32463;&#24120;&#19982;&#24863;&#20852;&#36259;&#30340;&#27599;&#20010;&#20301;&#32622;&#30456;&#20851;&#32852;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#21327;&#21464;&#37327;&#25552;&#20379;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#38382;&#39064;&#22312;&#20110;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#20301;&#32622;&#30340;&#35266;&#27979;&#32467;&#26524;&#25512;&#26029;&#26410;&#35266;&#27979;&#20449;&#36947;&#30340;&#20540;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#21464;&#37327;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;GgNet&#30340;&#22270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26469;&#23454;&#29616;&#35813;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12598v1 Announce Type: cross  Abstract: Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on p
&lt;/p&gt;</description></item><item><title>FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12572</link><description>&lt;p&gt;
FairProof&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#23494;&#21644;&#21487;&#35748;&#35777;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairProof : Confidential and Certifiable Fairness for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12572
&lt;/p&gt;
&lt;p&gt;
FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#28982;&#32780;&#27861;&#24459;&#21644;&#38544;&#31169;&#38382;&#39064;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#20445;&#23494;&#12290;&#22240;&#27492;&#65292;&#28040;&#36153;&#32773;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#36234;&#26469;&#36234;&#19981;&#20449;&#20219;&#65292;&#28040;&#36153;&#32773;&#36890;&#24120;&#26159;&#27169;&#22411;&#39044;&#27979;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairProof - &#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65288;&#19968;&#31181;&#23494;&#30721;&#21407;&#35821;&#65289;&#26469;&#20844;&#24320;&#39564;&#35777;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#35813;&#31995;&#32479;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;Gnark&#20013;&#23454;&#29616;&#20102;FairProof&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#23454;&#38469;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12572v1 Announce Type: cross  Abstract: Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.12563</link><description>&lt;p&gt;
&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#23427;&#20204;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35797;&#22270;&#35299;&#20915;&#20851;&#20110;&#20854;&#21487;&#34892;&#24615;&#30340;&#25345;&#32493;&#20105;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28508;&#22312;&#22240;&#32032; - LLMs&#30340;&#8220;&#20449;&#24515;&#8221; - &#22312;&#33258;&#25105;&#26657;&#27491;&#36807;&#31243;&#20013;&#12290;&#24573;&#35270;&#36825;&#19968;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25209;&#35780;&#33258;&#24049;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#33258;&#26657;&#27491;&#25928;&#26524;&#30340;&#21487;&#38752;&#32467;&#35770;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23454;&#39564;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#29702;&#35299;&#20854;&#33258;&#36523;&#22238;&#24212;&#8220;&#20449;&#24515;&#8221;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#23548;LLMs&#35780;&#20272;&#20854;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#65292;&#20419;&#36827;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;IoE&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
&lt;/p&gt;</description></item><item><title>CausalGym&#20171;&#32461;&#20102;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;DAS&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#29992;&#23427;&#26469;&#30740;&#31350;&#20102;pythia-1b&#20013;&#30340;&#20004;&#20010;&#22256;&#38590;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.12560</link><description>&lt;p&gt;
CausalGym&#65306;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23545;&#22240;&#26524;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CausalGym: Benchmarking causal interpretability methods on linguistic tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12560
&lt;/p&gt;
&lt;p&gt;
CausalGym&#20171;&#32461;&#20102;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;DAS&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#29992;&#23427;&#26469;&#30740;&#31350;&#20102;pythia-1b&#20013;&#30340;&#20004;&#20010;&#22256;&#38590;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#32431;&#34892;&#20026;&#27979;&#37327;&#65288;&#20363;&#22914;&#65292;&#24778;&#22855;&#27604;&#36739;&#65289;&#12290;&#21516;&#26102;&#65292;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#24050;&#24320;&#22987;&#38416;&#26126;&#22609;&#36896;LM&#34892;&#20026;&#30340;&#25277;&#35937;&#22240;&#26524;&#26426;&#21046;&#12290;&#20026;&#20102;&#24110;&#21161;&#23558;&#36825;&#20123;&#30740;&#31350;&#39046;&#22495;&#26356;&#32039;&#23494;&#22320;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;CausalGym&#12290;&#25105;&#20204;&#25913;&#32534;&#24182;&#25193;&#23637;&#20102;SyntaxGym&#20219;&#21153;&#22871;&#20214;&#65292;&#20197;&#22522;&#20934;&#27979;&#35797;&#35299;&#37322;&#26041;&#27861;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35828;&#26126;CausalGym&#30340;&#29992;&#36884;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;pythia&#27169;&#22411;&#65288;&#20174;14M&#21040;6.9B&#65289;&#24182;&#35780;&#20272;&#20102;&#24191;&#27867;&#35299;&#37322;&#26041;&#27861;&#30340;&#22240;&#26524;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#32447;&#24615;&#25506;&#38024;&#21644;&#20998;&#24067;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;DAS&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;pythia-1b&#20013;&#20004;&#20010;&#22256;&#38590;&#30340;&#35821;&#35328;&#29616;&#35937;&#30340;&#23398;&#20064;&#36712;&#36857;&#65306;&#36127;&#26497;&#24615;&#39033;&#35768;&#21487;&#21644;&#22635;&#20805;-&#38388;&#38553;d&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12560v1 Announce Type: cross  Abstract: Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.12551</link><description>&lt;p&gt;
&#22312;GPS&#21463;&#38459;&#30340;&#25112;&#22330;&#29615;&#22659;&#20013;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20110;&#22320;&#26631;&#30340;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25112;&#22330;&#29615;&#22659;&#20013;&#36827;&#34892;&#23450;&#20301;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;GPS&#36830;&#25509;&#36890;&#24120;&#21463;&#38459;&#25110;&#19981;&#21487;&#38752;&#65292;&#32780;&#22312;&#24694;&#21155;&#30340;&#25112;&#22330;&#22320;&#24418;&#20013;&#37096;&#32626;&#29992;&#20110;&#23450;&#20301;&#30340;&#38170;&#33410;&#28857;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#26080;&#32447;&#32593;&#32476;&#23450;&#20301;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#26080;&#32447;&#30005;&#30340;&#38170;&#23450;&#33410;&#28857;&#21450;&#20854;&#24179;&#22343;&#36339;&#36291;&#36317;&#31163;&#65292;&#36825;&#22312;&#21160;&#24577;&#21644;&#31232;&#30095;&#30340;&#26080;&#32447;&#32593;&#32476;&#25299;&#25169;&#20013;&#23384;&#22312;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#22914;SLAM&#21644;&#35270;&#35273;&#37324;&#31243;&#35745;&#20351;&#29992;&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#36827;&#34892;&#22320;&#22270;&#29983;&#25104;&#21644;&#23039;&#24577;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;GPS&#25112;&#22330;&#29615;&#22659;&#20013;&#21482;&#21033;&#29992;&#34987;&#21160;&#25668;&#20687;&#22836;&#20256;&#24863;&#22120;&#24182;&#32771;&#34385;&#33258;&#28982;&#23384;&#22312;&#25110;&#20154;&#24037;&#35774;&#32622;&#30340;&#22320;&#26631;&#20316;&#20026;&#38170;&#30340;&#23450;&#20301;&#26032;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23450;&#21046;&#26657;&#20934;&#30340;&#31435;&#20307;&#35270;&#35273;&#25668;&#20687;&#22836;&#36827;&#34892;&#36317;&#31163;&#20272;&#35745;&#65292;&#24182;&#20351;&#29992;&#32463;&#25105;&#20204;&#23454;&#38469;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;YOLOv8s&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12551v1 Announce Type: cross  Abstract: Localization in a battlefield environment is increasingly challenging as GPS connectivity is often denied or unreliable, and physical deployment of anchor nodes across wireless networks for localization can be difficult in hostile battlefield terrain. Existing range-free localization methods rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in dynamic and sparse wireless network topology. Vision-based methods like SLAM and Visual Odometry use expensive sensor fusion techniques for map generation and pose estimation. This paper proposes a novel framework for localization in non-GPS battlefield environments using only the passive camera sensors and considering naturally existing or artificial landmarks as anchors. The proposed method utilizes a customcalibrated stereo vision camera for distance estimation and the YOLOv8s model, which is trained and fine-tuned with our real-world dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12530</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parallel Structures in Pre-training Data Yield In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24179;&#34892;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#20013;&#23398;&#20064;&#26469;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20855;&#22791;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#22312;&#21482;&#32473;&#20986;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#33021;&#21147;&#26469;&#33258;&#20309;&#22788;&#65292;&#22240;&#20026;&#39044;&#35757;&#32451;&#25991;&#26412;&#19982;ICL&#25552;&#31034;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21738;&#20123;&#27169;&#24335;&#26377;&#21161;&#20110;ICL&#12290;&#25105;&#20204;&#21457;&#29616;LMs&#30340;ICL&#33021;&#21147;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#8220;&#24179;&#34892;&#32467;&#26500;&#8221;&#8212;&#8212;&#22312;&#30456;&#21516;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#36981;&#24490;&#30456;&#20284;&#27169;&#26495;&#30340;&#30701;&#35821;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#19968;&#20010;&#30701;&#35821;&#26159;&#21542;&#25552;&#39640;&#20102;&#23545;&#21478;&#19968;&#20010;&#30701;&#35821;&#30340;&#39044;&#27979;&#26469;&#26816;&#27979;&#24179;&#34892;&#32467;&#26500;&#65292;&#24182;&#36827;&#34892;&#28040;&#34701;&#23454;&#39564;&#20197;&#30740;&#31350;&#20854;&#23545;ICL&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21435;&#38500;&#24179;&#34892;&#32467;&#26500;&#20250;&#23548;&#33268;LMs&#30340;ICL&#20934;&#30830;&#24230;&#19979;&#38477;51&#65285;&#65288;&#19982;&#38543;&#26426;&#20999;&#38500;&#30340;2&#65285;&#30456;&#27604;&#65289;&#12290;&#21363;&#20351;&#25490;&#38500;&#24120;&#35265;&#27169;&#24335;&#22914; n-gram
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12530v1 Announce Type: cross  Abstract: Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gr
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12527</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12527
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#65292;&#30001;&#27492;&#24102;&#26469;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21363;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#34892;&#20026;&#30340;&#20215;&#20540;&#12290;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#20013;&#36827;&#34892;&#23637;&#24320;&#36827;&#34892;&#25910;&#38598;&#39069;&#22806;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#23436;&#20840;&#22833;&#36133;&#12290;&#36825;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#35843;&#26597;&#21457;&#29616;&#65292;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;&#36807;&#31243;&#23548;&#33268;&#23384;&#22312;&#19968;&#32452;&#35302;&#21457;&#30149;&#24577;&#20540;&#36807;&#39640;&#30340;&#36793;&#32536;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12527v1 Announce Type: cross  Abstract: Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overes
&lt;/p&gt;</description></item><item><title>LangXAI&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#25552;&#20379;&#25991;&#26412;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#26368;&#32456;&#29992;&#25143;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30693;&#35782;&#30340;&#29702;&#35299;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12525</link><description>&lt;p&gt;
LangXAI&#65306;&#25972;&#21512;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#20197;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12525
&lt;/p&gt;
&lt;p&gt;
LangXAI&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#25552;&#20379;&#25991;&#26412;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#26368;&#32456;&#29992;&#25143;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30693;&#35782;&#30340;&#29702;&#35299;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LangXAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20026;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;&#23613;&#31649;XAI&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#30340;&#26368;&#32456;&#29992;&#25143;&#65292;&#20173;&#28982;&#23384;&#22312;&#29702;&#35299;&#24046;&#36317;&#12290;LangXAI&#36890;&#36807;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#36755;&#20986;&#30340;&#35299;&#37322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;LangXAI&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#65292;&#22312;&#21508;&#39033;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;BERTScore&#65292;&#20419;&#36827;&#20102;&#26356;&#21152;&#36879;&#26126;&#21644;&#21487;&#38752;&#30340;&#35270;&#35273;&#20219;&#21153;AI&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12525v1 Announce Type: cross  Abstract: LangXAI is a framework that integrates Explainable Artificial Intelligence (XAI) with advanced vision models to generate textual explanations for visual recognition tasks. Despite XAI advancements, an understanding gap persists for end-users with limited domain knowledge in artificial intelligence and computer vision. LangXAI addresses this by furnishing text-based explanations for classification, object detection, and semantic segmentation model outputs to end-users. Preliminary results demonstrate LangXAI's enhanced plausibility, with high BERTScore across tasks, fostering a more transparent and reliable AI framework on vision tasks for end-users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12518</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Neural Additive Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#19981;&#20122;&#20110;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#26377;&#26102;&#20063;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#21457;&#23637;&#20986;&#30340;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAMs&#65289;&#26159;&#22312;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26041;&#21521;&#19978;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NAM&#23376;&#31867;&#65292;&#23427;&#20351;&#29992;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#23545;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#36807;&#31243;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;GP-NAM&#65289;&#12290;GP-NAM&#20855;&#26377;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38543;&#29305;&#24449;&#32500;&#24230;&#32447;&#24615;&#22686;&#38271;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20248;&#21183;&#12290;&#19982;&#26356;&#28145;&#30340;NAM&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#25439;&#22833;&#65292;&#22240;&#20026;GPs&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#21442;&#25968;&#21333;&#21464;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;GP-NAM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12518v1 Announce Type: cross  Abstract: Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#24310;&#36831;&#25968;&#25454;&#28418;&#31227;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.12490</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Cross-Domain Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12490
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#24310;&#36831;&#25968;&#25454;&#28418;&#31227;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#28041;&#21450;&#35757;&#32451;&#23398;&#20064;&#20195;&#29702;&#20197;&#39034;&#24207;&#22320;&#25484;&#25569;&#19968;&#31995;&#21015;&#20219;&#21153;&#25110;&#31867;&#21035;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#22238;&#39038;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#25361;&#25112;&#22312;&#20110;&#21033;&#29992;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#19968;&#39046;&#22495;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#38382;&#39064;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#39046;&#22495;&#25345;&#32493;&#23398;&#20064;&#65288;CDCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#34987;&#38480;&#21046;&#22312;&#21333;&#19968;&#30417;&#30563;&#39046;&#22495;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32039;&#20945;&#30340;&#21367;&#31215;&#32593;&#32476;&#20013;&#32467;&#21512;&#20102;&#20219;&#21153;&#38388;&#21644;&#20219;&#21153;&#20869;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#12290;&#36825;&#31181;&#25972;&#21512;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#19982;&#20808;&#21069;&#20219;&#21153;&#30340;&#29305;&#24449;&#20445;&#25345;&#23545;&#40784;&#65292;&#20174;&#32780;&#24310;&#36831;&#21487;&#33021;&#21457;&#29983;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#28418;&#31227;&#65292;&#21516;&#26102;&#22312;&#30456;&#20851;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#36328;&#39046;&#22495;&#23398;&#20064;&#65288;UDA&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20869;&#20855;&#20307;&#30340;&#20266;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12490v1 Announce Type: cross  Abstract: Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.12479</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20462;&#21098;&#32593;&#32476;&#26159;&#19968;&#20010;&#22909;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In deep reinforcement learning, a pruned network is a good network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26377;&#25928;&#21033;&#29992;&#20854;&#32593;&#32476;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#20248;&#21183;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#24182;&#35777;&#26126;&#36880;&#28176;&#21098;&#26525;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#34920;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#65292;&#20165;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12451</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The (R)Evolution of Multimodal Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12451
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20026;&#29983;&#25104;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#22312;&#29983;&#25104;&#26234;&#33021;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#30446;&#21069;&#24050;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26080;&#32541;&#22320;&#25972;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#21516;&#26102;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#25552;&#20379;&#22522;&#20110;&#23545;&#35805;&#30340;&#25509;&#21475;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;MLLMs&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#36873;&#25321;&#12289;&#22810;&#27169;&#24577;&#23545;&#40784;&#31574;&#30053;&#21644;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#35270;&#35273;&#23450;&#20301;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#12289;&#35270;&#35273;&#29702;&#35299;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32534;&#21046;&#24182;&#25551;&#36848;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#29616;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.12426</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#23646;&#24615;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Attacks on Node Attributes in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#23646;&#24615;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;Projected Gradient Descent&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#20351;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32463;&#24120;&#29992;&#26469;&#27169;&#22411;&#21270;&#29616;&#20195;&#31038;&#20132;&#23186;&#20307;&#21644;&#25991;&#29486;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#37325;&#28857;&#20851;&#27880;&#20915;&#31574;&#26102;&#25915;&#20987;&#21644;&#27602;&#21270;&#25915;&#20987;&#65292;&#25506;&#31350;&#36825;&#20123;&#22270;&#30340;&#33030;&#24369;&#24615;&#12290;&#19982;Net Attack&#21644;Meta Attack&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#38376;&#38024;&#23545;&#33410;&#28857;&#23646;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;Hellaswag&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#22270;&#25968;&#25454;&#38598;Cora&#21644;CiteSeer&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#35780;&#20272;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20351;&#29992;Projected Gradient Descent (PGD)&#30340;&#20915;&#31574;&#26102;&#25915;&#20987;&#27604;&#37319;&#29992;Mean Node Embeddings&#21644;Graph Contrastive Learning&#31574;&#30053;&#30340;&#27602;&#21270;&#25915;&#20987;&#26356;&#20855;&#23041;&#21147;&#12290;&#36825;&#20026;&#22270;&#25968;&#25454;&#23433;&#20840;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#25351;&#20986;&#20102;&#22270;&#22522;&#27169;&#22411;&#26368;&#33030;&#24369;&#30340;&#22320;&#26041;&#65292;&#20174;&#32780;&#20026;&#24320;&#21457;&#24037;&#20316;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12426v1 Announce Type: cross  Abstract: Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the develo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#23581;&#35797;&#22238;&#31572;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;</title><link>https://arxiv.org/abs/2402.12422</link><description>&lt;p&gt;
&#27169;&#25311;&#20307;&#20316;&#20026;&#24847;&#35782;&#24322;&#22495;
&lt;/p&gt;
&lt;p&gt;
Simulacra as Conscious Exotica
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#23581;&#35797;&#22238;&#31572;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.12422v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38543;&#30528;&#36234;&#26469;&#36234;&#20687;&#20154;&#31867;&#34892;&#20026;&#30340;&#20250;&#35805;&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#21476;&#32769;&#30340;&#21746;&#23398;&#38382;&#39064;&#34987;&#37325;&#26032;&#23457;&#35270;&#12290;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#65292;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#21482;&#26159;&#20154;&#31867;&#34892;&#20026;&#30340;&#8220;&#32431;&#31929;&#8221;&#27169;&#25311;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34892;&#20026;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#20165;&#20165;&#8221;&#35282;&#33394;&#25198;&#28436;&#65311;&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12422v1 Announce Type: new  Abstract: The advent of conversational agents with increasingly human-like behaviour throws old philosophical questions into new light. Does it, or could it, ever make sense to speak of AI agents built out of generative language models in terms of consciousness, given that they are "mere" simulacra of human behaviour, and that what they do can be seen as "merely" role play? Drawing on the later writings of Wittgenstein, this paper attempts to tackle this question while avoiding the pitfalls of dualistic thinking.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22359;&#29366;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#24182;&#37319;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12419</link><description>&lt;p&gt;
EBFT&#65306;&#31232;&#30095;LLM&#30340;&#26377;&#25928;&#21644;&#22359;&#29366;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22359;&#29366;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#24182;&#37319;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31232;&#30095;LLM&#24494;&#35843;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35201;&#27714;&#21644;&#39640;&#26114;&#30340;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#24494;&#35843;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#24314;&#35823;&#24046;&#30340;&#39640;&#25928;&#24555;&#36895;&#24494;&#35843;&#31232;&#30095;LLM&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#37319;&#26679;&#20197;&#36827;&#34892;&#26657;&#20934;&#65292;&#24182;&#21033;&#29992;&#21453;&#21521;&#20256;&#25773;&#36880;&#22359;&#22320;&#20248;&#21270;&#22359;&#29366;&#37325;&#24314;&#35823;&#24046;&#65292;&#33268;&#21147;&#20110;&#23547;&#27714;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#32447;&#19978;&#22987;&#32456;&#34920;&#29616;&#21331;&#36234;&#12290;&#20363;&#22914;&#65292;&#22312;Wikitext2&#25968;&#25454;&#38598;&#19978;&#65292;LLamaV1-7B&#22312;70%&#31232;&#30095;&#24230;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;EBFT&#21462;&#24471;&#20102;16.88&#30340;&#22256;&#24785;&#24230;&#65292;&#36229;&#36807;&#20102;75.14&#30340;DSnoT&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12419v1 Announce Type: cross  Abstract: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#65292;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;&#27010;&#24565;&#65292;&#24182;&#22312;ImageNet100&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.12418</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#19968;&#32553;&#25918;&#65306;&#25506;&#32034;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Scaling: Exploring Depth Heterogeneity in Neural Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12418
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#65292;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26550;&#26500;&#20013;&#30340;&#28145;&#24230;&#24322;&#36136;&#24615;&#27010;&#24565;&#65292;&#24182;&#22312;ImageNet100&#19978;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#36890;&#24120;&#28041;&#21450;&#35774;&#35745;&#22522;&#26412;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#19968;&#20123;&#39044;&#23450;&#20041;&#30340;&#32553;&#25918;&#22240;&#23376;&#22686;&#21152;&#19981;&#21516;&#32500;&#24230;&#65288;&#22914;&#23485;&#24230;&#12289;&#28145;&#24230;&#31561;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#20108;&#38454;&#25439;&#22833;&#26223;&#35266;&#20449;&#24687;&#30340;&#33258;&#21160;&#32553;&#25918;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#29616;&#20195;&#35270;&#35273;transformers&#20013;&#30340;&#36339;&#36807;&#36830;&#25509;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#24863;&#30693;&#26041;&#27861;&#21516;&#26102;&#25193;&#23637;&#21644;&#35757;&#32451;transformers&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#36845;&#20195;&#12290;&#21463;&#21040;&#24182;&#38750;&#25152;&#26377;&#31070;&#32463;&#20803;&#37117;&#38656;&#35201;&#32479;&#19968;&#28145;&#24230;&#22797;&#26434;&#24615;&#30340;&#20551;&#35774;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#28145;&#24230;&#24322;&#36136;&#24615;&#12290;&#23545;DeiT-S&#22312;ImageNet100&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#27604;&#20256;&#32479;&#32553;&#25918;&#25552;&#39640;&#20102;2.5&#65285;&#30340;&#20934;&#30830;&#24615;&#24182;&#25552;&#39640;&#20102;10&#65285;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#65292;&#32553;&#25918;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;transformers&#30340;&#31532;&#19968;&#20010;&#23436;&#25972;&#32553;&#25918;&#26426;&#21046;&#65292;&#36825;&#26159;&#26397;&#21521;&#39640;&#25928;&#27169;&#22411;&#22330;&#26223;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12418v1 Announce Type: cross  Abstract: Conventional scaling of neural networks typically involves designing a base network and growing different dimensions like width, depth, etc. of the same by some predefined scaling factors. We introduce an automated scaling approach leveraging second-order loss landscape information. Our method is flexible towards skip connections a mainstay in modern vision transformers. Our training-aware method jointly scales and trains transformers without additional training iterations. Motivated by the hypothesis that not all neurons need uniform depth complexity, our approach embraces depth heterogeneity. Extensive evaluations on DeiT-S with ImageNet100 show a 2.5% accuracy gain and 10% parameter efficiency improvement over conventional scaling. Scaled networks demonstrate superior performance upon training small scale datasets from scratch. We introduce the first intact scaling mechanism for vision transformers, a step towards efficient model sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.12417</link><description>&lt;p&gt;
&#29992;&#36328;&#20844;&#21496;&#30340;&#21345;&#36710;&#21496;&#26426;&#23433;&#20840;&#27675;&#22260;&#24863;&#30693;&#26469;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#65306;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting trucking accidents with truck drivers 'safety climate perception across companies: A transfer learning approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12417
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21345;&#36710;&#20107;&#25925;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#23433;&#20840;&#20998;&#26512;&#22312;&#39044;&#27979;&#21345;&#36710;&#34892;&#19994;&#20107;&#25925;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20844;&#21496;&#21487;&#33021;&#38754;&#20020;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#21363;&#27809;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#24320;&#21457;&#33391;&#22909;&#30340;&#23433;&#20840;&#20998;&#26512;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#35757;&#32451;&#28982;&#21518;&#24494;&#35843;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#20219;&#20309;&#20844;&#21496;&#21033;&#29992;&#20854;&#20182;&#20844;&#21496;&#30340;&#25968;&#25454;&#24320;&#21457;AI&#27169;&#22411;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20107;&#25925;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;SafeNet&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20107;&#25925;&#39044;&#27979;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#12290;&#36890;&#36807;&#26469;&#33258;&#19971;&#23478;&#25968;&#25454;&#35268;&#27169;&#21508;&#19981;&#30456;&#21516;&#30340;&#21345;&#36710;&#20844;&#21496;&#30340;&#23433;&#20840;&#27675;&#22260;&#35843;&#26597;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12417v1 Announce Type: cross  Abstract: There is a rising interest in using artificial intelligence (AI)-powered safety analytics to predict accidents in the trucking industry. Companies may face the practical challenge, however, of not having enough data to develop good safety analytics models. Although pretrained models may offer a solution for such companies, existing safety research using transfer learning has mostly focused on computer vision and natural language processing, rather than accident analytics. To fill the above gap, we propose a pretrain-then-fine-tune transfer learning approach to help any company leverage other companies' data to develop AI models for a more accurate prediction of accident risk. We also develop SafeNet, a deep neural network algorithm for classification tasks suitable for accident prediction. Using the safety climate survey data from seven trucking companies with different data sizes, we show that our proposed approach results in better m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.12416</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aligning Individual and Collective Objectives in Multi-Agent Cooperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12416
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#28151;&#21512;&#21160;&#26426;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#30683;&#30462;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#22870;&#21169;&#25110;&#24341;&#20837;&#39069;&#22806;&#26426;&#21046;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#30528;&#25163;&#21160;&#35774;&#35745;&#25104;&#26412;&#21644;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#25910;&#25947;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#28151;&#21512;&#21160;&#26426;&#21338;&#24328;&#24314;&#27169;&#20026;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#20197;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Altruistic Gradient Adjustment (AgA)&#30340;&#26032;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#26032;&#39062;&#22320;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#26126;&#65292;AgA&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#23545;&#40784;&#26435;&#37325;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23186;&#20307;&#26381;&#21153;&#27169;&#22411;&#65292;&#22312;&#21160;&#24577;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#36229;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.12412</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;AI&#25512;&#21160;&#30340;&#21160;&#24577;&#21644;&#36229;&#20010;&#24615;&#21270;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#65306;&#19981;&#26029;&#21464;&#21270;&#30340;&#19981;&#37325;&#22797;&#21095;&#26412;
&lt;/p&gt;
&lt;p&gt;
Dynamic and Super-Personalized Media Ecosystem Driven by Generative AI: Unpredictable Plays Never Repeating The Same
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20154;&#24037;&#26234;&#33021;&#35270;&#39057;&#29983;&#25104;&#22120;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23186;&#20307;&#26381;&#21153;&#27169;&#22411;&#65292;&#22312;&#21160;&#24577;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#36229;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#25509;&#25910;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35270;&#39057;&#29983;&#25104;&#22120;&#30340;&#23186;&#20307;&#26381;&#21153;&#27169;&#22411;&#12290;&#35813;&#25552;&#35758;&#20559;&#31163;&#20102;&#20256;&#32479;&#30340;&#22810;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#65292;&#23436;&#20840;&#20381;&#36182;&#20110;&#20869;&#37096;&#21046;&#20316;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#20869;&#23481;&#21019;&#24314;&#36716;&#31227;&#21040;&#25509;&#25910;&#31471;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#22788;&#29702;&#24341;&#20837;&#26694;&#26550;&#65292;&#20801;&#35768;&#20998;&#21457;&#32593;&#32476;&#25552;&#20379;&#20419;&#20351;&#20869;&#23481;&#29983;&#25104;&#22120;&#30340;&#26381;&#21153;&#20803;&#32032;&#65292;&#32780;&#19981;&#26159;&#20998;&#21457;&#23436;&#20840;&#25104;&#21697;&#33410;&#30446;&#30340;&#32534;&#30721;&#25968;&#25454;&#12290;&#26381;&#21153;&#20803;&#32032;&#21253;&#25324;&#31934;&#24515;&#23450;&#21046;&#30340;&#25991;&#26412;&#25551;&#36848;&#12289;&#19968;&#20123;&#23545;&#35937;&#30340;&#36731;&#37327;&#32423;&#22270;&#20687;&#25968;&#25454;&#65292;&#25110;&#24212;&#29992;&#31243;&#24207;&#25509;&#21475;&#65292;&#32508;&#21512;&#31216;&#20043;&#20026;&#35821;&#20041;&#26469;&#28304;&#65292;&#29992;&#25143;&#32456;&#31471;&#23558;&#25509;&#25910;&#30340;&#35821;&#20041;&#25968;&#25454;&#36716;&#25442;&#20026;&#35270;&#39057;&#24103;&#12290;&#20511;&#21161;&#29983;&#25104;&#24335;AI&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;&#29992;&#25143;&#21487;&#20197;&#30456;&#24212;&#22320;&#20307;&#39564;&#36229;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35813;&#25552;&#35758;&#27010;&#25324;&#20102;&#20854;&#20013;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12412v1 Announce Type: cross  Abstract: This paper introduces a media service model that exploits artificial intelligence (AI) video generators at the receive end. This proposal deviates from the traditional multimedia ecosystem, completely relying on in-house production, by shifting part of the content creation onto the receiver. We bring a semantic process into the framework, allowing the distribution network to provide service elements that prompt the content generator, rather than distributing encoded data of fully finished programs. The service elements include fine-tailored text descriptions, lightweight image data of some objects, or application programming interfaces, comprehensively referred to as semantic sources, and the user terminal translates the received semantic data into video frames. Empowered by the random nature of generative AI, the users could then experience super-personalized services accordingly. The proposed idea incorporates the situations in which
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SKES&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.12411</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#20540;&#20272;&#35745;&#30340;&#28145;&#24230;&#32467;&#26500;&#30693;&#35782;&#21033;&#29992;&#19982;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12411
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SKES&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#37325;&#35201;&#24615;&#20272;&#35745;&#38382;&#39064;&#22312;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#21516;&#36136;&#32593;&#32476;&#25299;&#25169;&#20998;&#26512;&#26469;&#30740;&#31350;&#30340;&#12290;&#20026;&#20102;&#22788;&#29702;&#32593;&#32476;&#30340;&#24322;&#36136;&#24615;&#65292;&#26368;&#36817;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#22270;&#31070;&#32463;&#27169;&#22411;&#26469;&#33258;&#21160;&#23398;&#20064;&#22810;&#26679;&#30340;&#20449;&#24687;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#20840;&#33258;&#36866;&#24212;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#25506;&#32034;&#19981;&#36275;&#65292;&#20174;&#32780;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#23545;&#23396;&#31435;&#33410;&#28857;&#30340;&#20540;&#39044;&#27979;&#65292;&#34920;&#29616;&#19981;&#20339;&#19988;&#21487;&#35299;&#37322;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65306;SKES&#12290;&#19982;&#20197;&#21069;&#30340;&#33258;&#21160;&#23398;&#20064;&#35774;&#35745;&#19981;&#21516;&#65292;SKES&#21033;&#29992;&#24322;&#26500;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#20016;&#23500;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#12290;&#22522;&#20110;&#19968;&#20010;&#36275;&#22815;&#19981;&#20855;&#20449;&#24687;&#30340;&#21442;&#32771;&#65292;SKES&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#33410;&#28857;&#19982;&#21442;&#32771;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20272;&#35745;&#20219;&#20309;&#36755;&#20837;&#33410;&#28857;&#30340;&#37325;&#35201;&#24615;&#20540;&#12290;&#36825;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#33410;&#28857;&#37325;&#35201;&#24615;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12411v1 Announce Type: cross  Abstract: Node importance estimation problem has been studied conventionally with homogeneous network topology analysis. To deal with network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework: SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference. This establishes an interpretable node importance computation paradigm. F
&lt;/p&gt;</description></item><item><title>ModelGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#23450;&#21046;&#21270;&#30340;AI&#27169;&#22411;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#36895;&#21644;&#26041;&#20415;&#22320;&#20351;&#29992;AI&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12408</link><description>&lt;p&gt;
ModelGPT&#65306;&#37322;&#25918;LLM&#30340;&#33021;&#21147;&#65292;&#20026;&#23450;&#21046;&#27169;&#22411;&#29983;&#25104;&#38138;&#24179;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12408
&lt;/p&gt;
&lt;p&gt;
ModelGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#23450;&#21046;&#21270;&#30340;AI&#27169;&#22411;&#65292;&#35753;&#29992;&#25143;&#33021;&#22815;&#26356;&#24555;&#36895;&#21644;&#26041;&#20415;&#22320;&#20351;&#29992;AI&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#36890;&#36807;&#33258;&#21160;&#21270;&#20363;&#34892;&#20219;&#21153;&#65292;&#26631;&#24535;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#23454;&#29616;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#38761;&#26032;&#20102;&#21508;&#20010;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#21644;&#29305;&#23450;&#38656;&#27714;&#65292;&#20063;&#38590;&#20197;&#31616;&#21270;AI&#27169;&#22411;&#23545;&#26222;&#36890;&#29992;&#25143;&#30340;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModelGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#25110;&#20219;&#21153;&#25551;&#36848;&#26469;&#30830;&#23450;&#21644;&#29983;&#25104;&#29305;&#23450;&#23450;&#21046;&#30340;AI&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;LLM&#30340;&#33021;&#21147;&#12290;ModelGPT&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#27604;&#20043;&#21069;&#30340;&#33539;&#24335;&#65288;&#20363;&#22914;&#20840;&#21442;&#25968;&#25110;LoRA&#24494;&#35843;&#65289;&#24555;&#33267;&#22810;270&#20493;&#12290;&#22312;NLP&#12289;CV&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20351;AI&#27169;&#22411;&#26356;&#26131;&#35775;&#38382;&#21644;&#29992;&#25143;&#21451;&#22909;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/IshiKura-a/ModelGPT &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12408v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized various sectors by automating routine tasks, marking a step toward the realization of Artificial General Intelligence (AGI). However, they still struggle to accommodate the diverse and specific needs of users and simplify the utilization of AI models for the average user. In response, we propose ModelGPT, a novel framework designed to determine and generate AI models specifically tailored to the data or task descriptions provided by the user, leveraging the capabilities of LLMs. Given user requirements, ModelGPT is able to provide tailored models at most 270x faster than the previous paradigms (e.g. all-parameter or LoRA finetuning). Comprehensive experiments on NLP, CV, and Tabular datasets attest to the effectiveness of our framework in making AI models more accessible and user-friendly. Our code is available at https://github.com/IshiKura-a/ModelGPT.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#33021;&#20986;&#29616;&#36136;&#37327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12406</link><description>&lt;p&gt;
&#25945;&#24072;&#20316;&#20026;&#23485;&#23481;&#30340;&#19987;&#23478;&#65306;&#19981;&#20381;&#36182;&#20110;&#25945;&#24072;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#33021;&#20986;&#29616;&#36136;&#37327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26088;&#22312;&#22312;&#19981;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20511;&#21161;&#29983;&#25104;&#22120;&#23558;&#39044;&#35757;&#32451;&#30693;&#35782;&#33976;&#39311;&#32473;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#26080;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#39564;&#35777;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#23454;&#29616;DFKD&#30340;&#31283;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26412;&#25991;&#21457;&#29616;&#29616;&#26377;&#30340;DFKD&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#25945;&#24072;&#27169;&#22411;&#38750;&#24120;&#25935;&#24863;&#65292;&#26377;&#26102;&#21363;&#20351;&#20351;&#29992;&#35757;&#32451;&#33391;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#33976;&#39311;&#30340;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#26159;DFKD&#20013;&#30340;&#29983;&#25104;&#22120;&#24182;&#19981;&#24635;&#26159;&#20445;&#35777;&#20351;&#29992;&#29616;&#26377;&#30340;&#26088;&#22312;&#26368;&#23567;&#21270;&#31867;&#20808;&#39564;&#21644;&#23545;&#25239;&#25439;&#22833;&#30340;&#20195;&#34920;&#24615;&#31574;&#30053;&#20135;&#29983;&#31934;&#30830;&#32780;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#20107;&#23454;&#26159;&#31867;&#20808;&#39564;&#19981;&#20165;&#20943;&#23569;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#36824;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#26681;&#25454;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#24847;&#22806;&#20302;&#36136;&#37327;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12406v1 Announce Type: cross  Abstract: Data-free knowledge distillation (DFKD) aims to distill pretrained knowledge to a student model with the help of a generator without using original data. In such data-free scenarios, achieving stable performance of DFKD is essential due to the unavailability of validation data. Unfortunately, this paper has discovered that existing DFKD methods are quite sensitive to different teacher models, occasionally showing catastrophic failures of distillation, even when using well-trained teacher models. Our observation is that the generator in DFKD is not always guaranteed to produce precise yet diverse samples using the existing representative strategy of minimizing both class-prior and adversarial losses. Through our empirical study, we focus on the fact that class-prior not only decreases the diversity of generated samples, but also cannot completely address the problem of generating unexpectedly low-quality samples depending on teacher mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#21644;&#21306;&#20998;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#20013;&#30340;&#32454;&#32990;&#31867;&#22411;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#32454;&#32990;&#31867;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12405</link><description>&lt;p&gt;
scInterpreter: &#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322; scRNA-seq &#25968;&#25454;&#36827;&#34892;&#32454;&#32990;&#31867;&#22411;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#21644;&#21306;&#20998;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#20013;&#30340;&#32454;&#32990;&#31867;&#22411;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#32454;&#32990;&#31867;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#25509;&#38405;&#35835;&#21644;&#35299;&#37322;&#21333;&#32454;&#32990;&#32452;&#23398;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#22266;&#26377;&#38480;&#21046;&#65292;&#20294;&#23427;&#20204;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#35757;&#32451;&#21644;&#35843;&#25972;&#20855;&#26377;&#35299;&#37322;&#21644;&#21306;&#20998;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#32454;&#32990;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25581;&#31034;&#26032;&#29983;&#29289;&#23398;&#35265;&#35299;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12405v1 Announce Type: cross  Abstract: Despite the inherent limitations of existing Large Language Models in directly reading and interpreting single-cell omics data, they demonstrate significant potential and flexibility as the Foundation Model. This research focuses on how to train and adapt the Large Language Model with the capability to interpret and distinguish cell types in single-cell RNA sequencing data. Our preliminary research results indicate that these foundational models excel in accurately categorizing known cell types, demonstrating the potential of the Large Language Models as effective tools for uncovering new biological insights.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12399</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#24223;&#20026;&#23453;&#65306;&#30699;&#27491;MoE&#30340;Top-k&#36335;&#30001;&#22120;
&lt;/p&gt;
&lt;p&gt;
Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#30001;&#20110;&#19981;&#24179;&#34913;&#30340;&#36335;&#30001;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#20123;&#19987;&#23478;&#20250;&#28322;&#20986;&#65292;&#20854;&#20013;&#36229;&#20986;&#30340;&#20196;&#29260;&#20250;&#34987;&#20002;&#24323;&#12290;&#32780;&#19968;&#20123;&#19987;&#23478;&#26159;&#31354;&#38386;&#30340;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#22635;&#20805;&#20026;&#38646;&#65292;&#36127;&#38754;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20002;&#24323;&#20196;&#29260;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rectify-Router&#65292;&#21253;&#25324;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#12290;Intra-GPU&#30699;&#27491;&#22788;&#29702;&#20002;&#24323;&#30340;&#20196;&#29260;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36335;&#30001;&#21040;GPU&#20869;&#30340;&#19987;&#23478;&#65292;&#36991;&#20813;&#36328;GPU&#36890;&#20449;&#12290;Fill-in&#30699;&#27491;&#36890;&#36807;&#29992;&#20855;&#26377;&#39640;&#36335;&#30001;&#20998;&#25968;&#30340;&#20196;&#29260;&#26367;&#25442;&#22635;&#20805;&#20196;&#29260;&#26469;&#35299;&#20915;&#22635;&#20805;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#22825;&#20307;&#29289;&#29702;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#25104;&#21151;&#22320;&#32479;&#35745;&#22797;&#21046;&#20102;Maxwell-J\"uttner&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.12396</link><description>&lt;p&gt;
&#22312;&#22825;&#20307;&#29289;&#29702;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Toward using GANs in astrophysical Monte-Carlo simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12396
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#22825;&#20307;&#29289;&#29702;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#25104;&#21151;&#22320;&#32479;&#35745;&#22797;&#21046;&#20102;Maxwell-J\"uttner&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#30001;X&#23556;&#32447;&#28304;&#20135;&#29983;&#30340;&#20809;&#35889;&#38656;&#35201;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#12290;&#36825;&#20123;&#27169;&#25311;&#38656;&#35201;&#35780;&#20272;&#29289;&#29702;&#36807;&#31243;&#65292;&#20363;&#22914;&#22312;&#32039;&#20945;&#22825;&#20307;&#21608;&#22260;&#21457;&#29983;&#30340;&#21560;&#31215;&#36807;&#31243;&#65292;&#36890;&#36807;&#37319;&#26679;&#22810;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#36825;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#32791;&#26102;&#65292;&#22914;&#26524;&#29992;&#31070;&#32463;&#32593;&#32476;&#20195;&#26367;&#21487;&#33021;&#20250;&#21152;&#24555;&#36895;&#24230;&#12290;&#25105;&#20204;&#22312;&#25551;&#36848;&#30456;&#23545;&#35770;&#30005;&#23376;&#36895;&#24230;&#30340;Maxwell-J\"uttner&#20998;&#24067;&#31034;&#20363;&#19978;&#23637;&#31034;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#32479;&#35745;&#22797;&#21046;&#35813;&#20998;&#24067;&#12290;Kolmogorov-Smirnov&#27979;&#35797;&#30340;&#24179;&#22343;&#20540;&#20026;0.5&#65292;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#30340;&#26679;&#26412;&#19982;&#30495;&#23454;&#20998;&#24067;&#26080;&#27861;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12396v1 Announce Type: cross  Abstract: Accurate modelling of spectra produced by X-ray sources requires the use of Monte-Carlo simulations. These simulations need to evaluate physical processes, such as those occurring in accretion processes around compact objects by sampling a number of different probability distributions. This is computationally time-consuming and could be sped up if replaced by neural networks. We demonstrate, on an example of the Maxwell-J\"uttner distribution that describes the speed of relativistic electrons, that the generative adversarial network (GAN) is capable of statistically replicating the distribution. The average value of the Kolmogorov-Smirnov test is 0.5 for samples generated by the neural network, showing that the generated distribution cannot be distinguished from the true distribution.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20915;&#31574;&#26641;&#35299;&#37322;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#25552;&#39640;&#35782;&#21035;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12394</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#26631;&#24535;&#29289;&#25552;&#39640;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Model's Interpretability and Reliability using Biomarkers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12394
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20915;&#31574;&#26641;&#35299;&#37322;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35786;&#26029;&#27169;&#22411;&#65292;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#25552;&#39640;&#35782;&#21035;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#20855;&#26377;&#35299;&#37322;&#24615;&#30340;&#35786;&#26029;&#27169;&#22411;&#22312;&#21307;&#23398;&#36825;&#20010;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#32954;&#37096;&#36229;&#22768;&#35786;&#26029;&#27969;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#35786;&#26029;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#21033;&#29992;&#29983;&#29289;&#26631;&#24535;&#29289;&#25552;&#20379;&#30340;&#35299;&#37322;&#26159;&#21542;&#33021;&#22815;&#25913;&#21892;&#29992;&#25143;&#35782;&#21035;&#27169;&#22411;&#19981;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#65292;&#19982;&#20256;&#32479;&#30340;&#26174;&#33879;&#24615;&#22270;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#22522;&#20110;&#20020;&#24202;&#24314;&#31435;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20915;&#31574;&#26641;&#35299;&#37322;&#33021;&#22815;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26816;&#27979;&#21040;&#20551;&#38451;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12394v1 Announce Type: cross  Abstract: Accurate and interpretable diagnostic models are crucial in the safety-critical field of medicine. We investigate the interpretability of our proposed biomarker-based lung ultrasound diagnostic pipeline to enhance clinicians' diagnostic capabilities. The objective of this study is to assess whether explanations from a decision tree classifier, utilizing biomarkers, can improve users' ability to identify inaccurate model predictions compared to conventional saliency maps. Our findings demonstrate that decision tree explanations, based on clinically established biomarkers, can assist clinicians in detecting false positives, thus improving the reliability of diagnostic models in medicine.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#35268;&#21010;&#21644;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#35270;&#39057;&#28216;&#25103;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#24471;&#33258;&#21160;&#35268;&#21010;&#21464;&#24471;&#26356;&#23481;&#26131;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#12290;</title><link>https://arxiv.org/abs/2402.12393</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#35268;&#21010;&#21644;&#23398;&#20064;&#33258;&#21160;&#21270;&#35270;&#39057;&#28216;&#25103;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
On Automating Video Game Testing by Planning and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#35268;&#21010;&#21644;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#35270;&#39057;&#28216;&#25103;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#24471;&#33258;&#21160;&#35268;&#21010;&#21464;&#24471;&#26356;&#23481;&#26131;&#25509;&#35302;&#21040;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#35268;&#21010;&#21644;&#35268;&#21010;&#34892;&#20026;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#21270;&#27979;&#35797;&#29305;&#23450;&#35270;&#39057;&#28216;&#25103;&#26041;&#38754;&#30340;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#12290;&#22522;&#26412;&#24819;&#27861;&#26159;&#29983;&#25104;&#35814;&#32454;&#30340;&#28216;&#25103;&#26085;&#24535;&#65292;&#24182;&#24212;&#29992;&#34892;&#21160;&#27169;&#22411;&#23398;&#20064;&#26469;&#33719;&#24471;&#35268;&#21010;&#39046;&#22495;&#25551;&#36848;&#35821;&#35328;&#65288;PDDL&#65289;&#20013;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#20102;&#28216;&#25103;&#24320;&#21457;&#20154;&#21592;&#19982;&#20855;&#26377;PDDL&#24314;&#27169;&#32463;&#39564;&#20294;&#26080;&#28216;&#25103;&#24320;&#21457;&#25216;&#33021;&#30340;&#20154;&#21592;&#20043;&#38388;&#30340;&#39640;&#25928;&#21512;&#20316;&#65292;&#24182;&#19988;&#26080;&#38656;&#20219;&#20309;PDDL&#25110;&#20854;&#20182;&#27491;&#24335;&#31995;&#32479;&#32463;&#39564;&#12290;&#25105;&#20204;&#24635;&#20307;&#25551;&#36848;&#20102;&#35813;&#26041;&#27861;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#20855;&#20307;&#30340;&#27010;&#24565;&#35777;&#26126;&#31034;&#20363;&#19978;&#36827;&#34892;&#28436;&#31034; -- &#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20316;&#20026;&#27969;&#34892;&#28216;&#25103;&#24320;&#21457;&#24341;&#25806;Unity&#20013;&#30340;&#25945;&#31243;&#39033;&#30446;&#20043;&#19968;&#12290;&#26412;&#25991;&#26159;&#26397;&#30528;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#24037;&#20316;&#27969;&#31243;&#20013;&#23545;&#24314;&#27169;&#19987;&#23478;&#38656;&#27714;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#32780;&#20351;&#33258;&#21160;&#35268;&#21010;&#21487;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12393v1 Announce Type: cross  Abstract: In this paper, we propose a method and workflow for automating the testing of certain video game aspects using automated planning and planning action model learning techniques. The basic idea is to generate detailed gameplay logs and apply action model learning to obtain a formal model in the planning domain description language (PDDL). The workflow enables efficient cooperation of game developers without any experience with PDDL or other formal systems and a person experienced with PDDL modeling but no game development skills. We describe the method and workflow in general and then demonstrate it on a concrete proof-of-concept example -- a simple role-playing game provided as one of the tutorial projects in the popular game development engine Unity. This paper presents the first step towards minimizing or even eliminating the need for a modeling expert in the workflow, thus making automated planning accessible to a broader audience.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20844;&#20849;&#20132;&#36890;&#31449;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#26102;&#38388;&#27573;&#36827;&#34892;&#20998;&#21106;&#65292;&#24573;&#30053;&#20102;&#39069;&#22806;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102; Covid-19 &#22823;&#27969;&#34892;&#23545;&#38081;&#36335;&#20844;&#20849;&#20132;&#36890;&#20056;&#23458;&#37327;&#30340;&#24433;&#21709;&#21450;&#20854;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12392</link><description>&lt;p&gt;
&#36890;&#36807;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#29702;&#35299; Covid-19 &#22823;&#27969;&#34892;&#23545;&#20844;&#20849;&#20132;&#36890;&#20056;&#23458;&#37327;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Regression Mixture Model to understand the effect of the Covid-19 pandemic on Public Transport Ridership
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#20844;&#20849;&#20132;&#36890;&#31449;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#26102;&#38388;&#27573;&#36827;&#34892;&#20998;&#21106;&#65292;&#24573;&#30053;&#20102;&#39069;&#22806;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102; Covid-19 &#22823;&#27969;&#34892;&#23545;&#38081;&#36335;&#20844;&#20849;&#20132;&#36890;&#20056;&#23458;&#37327;&#30340;&#24433;&#21709;&#21450;&#20854;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Covid-19 &#22823;&#27969;&#34892;&#24443;&#24213;&#25913;&#21464;&#20102;&#22478;&#24066;&#20986;&#34892;&#26041;&#24335;&#65292;&#19981;&#20165;&#22312;&#30123;&#24773;&#39640;&#23792;&#26399;&#36890;&#36807;&#25919;&#24220;&#23553;&#38145;&#65292;&#32780;&#19988;&#22312;&#36739;&#38271;&#26102;&#38388;&#20869;&#37319;&#29992;&#23621;&#23478;&#21150;&#20844;&#25919;&#31574;&#12290;&#20026;&#20102;&#20102;&#35299;&#20854;&#23545;&#38081;&#36335;&#20844;&#20849;&#20132;&#36890;&#20056;&#23458;&#37327;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#20844;&#20849;&#20132;&#36890;&#31449;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#26102;&#38388;&#27573;&#36827;&#34892;&#20998;&#21106;&#65292;&#21516;&#26102;&#24573;&#30053;&#30001;&#20110;&#23448;&#26041;&#23553;&#38145;&#25110;&#38750;&#24037;&#20316;&#26085;&#31561;&#38468;&#21152;&#21464;&#37327;&#23548;&#33268;&#30340;&#21464;&#21270;&#12290;&#27599;&#20010;&#32676;&#38598;&#22240;&#27492;&#30001;&#19968;&#31995;&#21015;&#26102;&#38388;&#27573;&#23450;&#20041;&#65292;&#20854;&#20013;&#22806;&#29983;&#21464;&#37327;&#30340;&#24433;&#21709;&#26159;&#24658;&#23450;&#30340;&#12290;&#30001;&#20110;&#32676;&#38598;&#20869;&#30340;&#27599;&#20010;&#26102;&#38388;&#27573;&#20855;&#26377;&#33258;&#24049;&#30340;&#22238;&#24402;&#31995;&#25968;&#26469;&#24314;&#27169;&#21327;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#31995;&#25968;&#22914;&#20309;&#28436;&#21464;&#20197;&#29702;&#35299;&#32676;&#38598;&#20013;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22238;&#24402;&#28151;&#21512;&#27169;&#22411;&#21644;&#20351;&#29992; EM &#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#20854;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12392v1 Announce Type: cross  Abstract: The Covid-19 pandemic drastically changed urban mobility, both during the height of the pandemic with government lockdowns, but also in the longer term with the adoption of working-from-home policies. To understand its effects on rail public transport ridership, we propose a dedicated Regression Mixture Model able to perform both the clustering of public transport stations and the segmentation of time periods, while ignoring variations due to additional variables such as the official lockdowns or non-working days. Each cluster is thus defined by a series of segments in which the effect of the exogenous variables is constant. As each segment within a cluster has its own regression coefficients to model the impact of the covariates, we analyze how these coefficients evolve to understand the changes in the cluster. We present the regression mixture model and the parameter estimation using the EM algorithm, before demonstrating the benefit
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#37202;&#31934;&#28040;&#36153;&#30740;&#31350;&#20013;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;&#35821;&#20041;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#25551;&#36848;&#21644;&#30740;&#31350;&#20010;&#20307;&#20043;&#38388;&#24314;&#31435;&#30340;&#31038;&#20250;&#20851;&#31995;</title><link>https://arxiv.org/abs/2402.12390</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#37202;&#31934;&#28040;&#36153;&#30740;&#31350;&#20013;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;&#35821;&#20041;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
A Semantic Social Network Analysis Tool for Sensitivity Analysis and What-If Scenario Testing in Alcohol Consumption Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12390
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#37202;&#31934;&#28040;&#36153;&#30740;&#31350;&#20013;&#25935;&#24863;&#24615;&#20998;&#26512;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;&#35821;&#20041;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#25551;&#36848;&#21644;&#30740;&#31350;&#20010;&#20307;&#20043;&#38388;&#24314;&#31435;&#30340;&#31038;&#20250;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65288;SNA&#65289;&#26159;&#31038;&#20250;&#21644;&#34892;&#20026;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#19968;&#22871;&#25216;&#26415;&#65292;&#26088;&#22312;&#25551;&#36848;&#21644;&#30740;&#31350;&#19968;&#32452;&#20010;&#20307;&#20043;&#38388;&#24314;&#31435;&#30340;&#31038;&#20250;&#20851;&#31995;&#12290;&#22312;&#26500;&#24314;&#29992;&#20110;&#36827;&#34892;SNA&#20998;&#26512;&#30340;&#31038;&#20132;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#21021;&#22987;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#26469;&#25552;&#21462;&#20010;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#29305;&#24449;&#12290;&#36890;&#24120;&#36890;&#36807;&#22635;&#20889;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#30340;&#38382;&#21367;&#26469;&#23436;&#25104;&#36825;&#19968;&#36807;&#31243;&#65292;&#36825;&#20123;&#38382;&#39064;&#31245;&#21518;&#23558;&#29992;&#20110;&#33719;&#21462;&#25191;&#34892;&#30740;&#31350;&#25152;&#38656;&#30340;SNA&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#21487;&#33021;&#32593;&#32476;&#29983;&#25104;&#38382;&#39064;&#65292;&#20063;&#26377;&#35768;&#22810;&#23558;&#22238;&#31572;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#29305;&#24449;&#21644;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#20250;&#34987;&#24341;&#20837;&#35768;&#22810;&#21464;&#21270;&#65288;&#23427;&#20204;&#30340;&#25552;&#20986;&#26041;&#24335;&#65292;&#36171;&#20104;&#27599;&#20010;&#22238;&#31572;&#30340;&#26435;&#37325;&#31561;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12390v1 Announce Type: cross  Abstract: Social Network Analysis (SNA) is a set of techniques developed in the field of social and behavioral sciences research, in order to characterize and study the social relationships that are established among a set of individuals. When building a social network for performing an SNA analysis, an initial process of data gathering is achieved in order to extract the characteristics of the individuals and their relationships. This is usually done by completing a questionnaire containing different types of questions that will be later used to obtain the SNA measures needed to perform the study. There are, then, a great number of different possible network generating questions and also many possibilities for mapping the responses to the corresponding characteristics and relationships. Many variations may be introduced into these questions (the way they are posed, the weights given to each of the responses, etc.) that may have an effect on the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#22312;&#32447;&#25805;&#20316;&#21592;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#36827;&#21270;&#31639;&#27861;&#20013;&#25805;&#20316;&#21592;&#30340;&#36873;&#25321;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12381</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#25805;&#20316;&#21592;&#36873;&#25321;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Multi-objective Optimization with Deep Reinforcement Learning Assisted Operator Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12381
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#22312;&#32447;&#25805;&#20316;&#21592;&#36873;&#25321;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#36827;&#21270;&#31639;&#27861;&#20013;&#25805;&#20316;&#21592;&#30340;&#36873;&#25321;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#24102;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;&#31639;&#27861;&#31574;&#30053;&#12289;&#36827;&#21270;&#31639;&#23376;&#21644;&#32422;&#26463;&#22788;&#29702;&#25216;&#26415;&#30340;&#20351;&#29992;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#36827;&#21270;&#31639;&#27861;&#65288;CMOEAs&#65289;&#12290;CMOEAs&#30340;&#24615;&#33021;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#20351;&#29992;&#30340;&#25805;&#20316;&#21592;&#65292;&#28982;&#32780;&#65292;&#36890;&#24120;&#24456;&#38590;&#20026;&#25163;&#22836;&#30340;&#38382;&#39064;&#36873;&#25321;&#21512;&#36866;&#30340;&#25805;&#20316;&#21592;&#12290;&#22240;&#27492;&#65292;&#25913;&#36827;&#25805;&#20316;&#21592;&#30340;&#36873;&#25321;&#23545;CMOEAs&#26159;&#26377;&#21069;&#26223;&#30340;&#19988;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#22312;&#32447;&#25805;&#20316;&#21592;&#36873;&#25321;&#26694;&#26550;&#12290;&#20154;&#21475;&#30340;&#21160;&#24577;&#24615;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#34987;&#35270;&#20026;&#29366;&#24577;&#65307;&#20505;&#36873;&#25805;&#20316;&#21592;&#34987;&#35270;&#20026;&#34892;&#21160;&#65307;&#20154;&#21475;&#29366;&#24577;&#30340;&#25913;&#21892;&#34987;&#35270;&#20026;&#22870;&#21169;&#12290;&#36890;&#36807;&#20351;&#29992;Q&#32593;&#32476;&#26469;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#26469;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12381v1 Announce Type: new  Abstract: Solving constrained multi-objective optimization problems with evolutionary algorithms has attracted considerable attention. Various constrained multi-objective optimization evolutionary algorithms (CMOEAs) have been developed with the use of different algorithmic strategies, evolutionary operators, and constraint-handling techniques. The performance of CMOEAs may be heavily dependent on the operators used, however, it is usually difficult to select suitable operators for the problem at hand. Hence, improving operator selection is promising and necessary for CMOEAs. This work proposes an online operator selection framework assisted by Deep Reinforcement Learning. The dynamics of the population, including convergence, diversity, and feasibility, are regarded as the state; the candidate operators are considered as actions; and the improvement of the population state is treated as the reward. By using a Q-Network to learn a policy to estima
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;</title><link>https://arxiv.org/abs/2402.12161</link><description>&lt;p&gt;
&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Endowing Pre-trained Graph Models with Provable Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26088;&#22312;&#25429;&#25417;&#21487;&#36716;&#31227;&#30340;&#22266;&#26377;&#32467;&#26500;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#31867;&#20284;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;PGMs&#20063;&#20250;&#32487;&#25215;&#20154;&#31867;&#31038;&#20250;&#20013;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#20986;&#29616;&#27495;&#35270;&#34892;&#20026;&#12290;&#29616;&#26377;&#20844;&#24179;&#26041;&#27861;&#30340;&#21435;&#20559;&#35265;&#36807;&#31243;&#36890;&#24120;&#19982;GNNs&#30340;&#21442;&#25968;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#22312;&#29616;&#23454;&#20013;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#30452;&#25509;&#37319;&#29992;&#29616;&#26377;&#26041;&#27861;&#25913;&#21892;PGMs&#30340;&#20844;&#24179;&#24615;&#26159;&#19981;&#28789;&#27963;&#19988;&#20302;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#23545;&#27169;&#22411;&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#21487;&#35777;&#26126;&#19979;&#38480;&#65292;&#36825;&#30452;&#25509;&#25552;&#20379;&#20102;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#20445;&#35777;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#37197;&#22120;&#35843;&#20248;&#26694;&#26550;&#65292;&#36171;&#20104;&#39044;&#35757;&#32451;\textbf{&#22270;}&#27169;&#22411;&#20855;&#26377;\textbf{&#21487;&#35777;&#26126;}&#30340;\textbf{&#20844;}&#24179;\textbf{&#24615;}&#65288;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.11752</link><description>&lt;p&gt;
&#23545;&#35282;&#21270;SGD&#65306;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD
&lt;/p&gt;
&lt;p&gt;
Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11752
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Diagonalisation Stochastic Gradient Descent&#65288;&#23545;&#35282;&#21270;SGD&#65289;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#24179;&#28369;&#23454;&#29616;&#38750;&#21487;&#24494;&#27169;&#22411;&#30340;&#24555;&#36895;&#25910;&#25947;SGD&#65292;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#25968;&#37327;&#32423;&#30340;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#20110;&#38750;&#21487;&#24494;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#36739;&#20302;&#26041;&#24046;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#21487;&#33021;&#21361;&#21450;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;SGD&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35821;&#27861;&#26694;&#26550;&#26469;&#20998;&#22359;&#22320;&#23450;&#20041;&#38750;&#21487;&#24494;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20351;&#37325;&#26032;&#21442;&#25968;&#21270;&#26799;&#24230;&#20272;&#35745;&#22120;&#26080;&#20559;&#30340;&#24179;&#28369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;SGD&#21464;&#20307;&#65292;&#23545;&#35282;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#36880;&#27493;&#25552;&#39640;&#24179;&#28369;&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#21040;&#26410;&#24179;&#28369;&#65288;&#21407;&#22987;&#65289;&#30446;&#26631;&#30340;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#24037;&#20316;&#35268;&#33539;&#21270;&#26041;&#24046;&#19978;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
&lt;/p&gt;</description></item><item><title>&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10949</link><description>&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Eccentric Automatic Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10949
&lt;/p&gt;
&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#21046;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;&#23558;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#32435;&#20837;&#31995;&#32479;&#25552;&#31034;&#28040;&#24687;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31995;&#32479;&#21270;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;60&#31181;&#31995;&#32479;&#28040;&#24687;&#29255;&#27573;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Chain of Thought&#25552;&#31034;&#65292;&#36328;&#19977;&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;70&#20159;&#21040;70&#20159;&#20010;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#26524;&#24182;&#19981;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#26222;&#36941;&#36866;&#29992;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#31215;&#26497;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Llama2-70B&#22312;&#19981;&#20351;&#29992;Chain of Thought&#26102;&#26159;&#20010;&#20363;&#22806;&#65292;&#22240;&#20026;&#21457;&#29616;&#26368;&#20339;&#31995;&#32479;&#28040;&#24687;&#23454;&#38469;&#19978;&#26159;&#27809;&#26377;&#28040;&#24687;&#12290;&#32771;&#34385;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23548;&#33267;&#30340;&#21152;# Truncated due to exceeding character limit.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
&lt;/p&gt;</description></item><item><title>ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10930</link><description>&lt;p&gt;
ConSmax: &#20855;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10930
&lt;/p&gt;
&lt;p&gt;
ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#23558;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21306;&#20998;&#24320;&#26469;&#12290;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#30001;&#20110;&#33258;&#27880;&#24847;&#20013;&#24191;&#27867;&#20351;&#29992;Softmax&#65292;&#22312;&#30789;&#19978;&#23454;&#29616;&#23454;&#26102;LLM&#25512;&#26029;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Constant Softmax&#65288;ConSmax&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;&#35268;&#33539;&#21270;&#21442;&#25968;&#26469;&#28040;&#38500;Softmax&#20013;&#30340;&#26368;&#22823;&#25628;&#32034;&#21644;&#20998;&#27597;&#27714;&#21644;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#65292;&#20026;&#23556;&#30005;&#28304;&#30340;&#34920;&#24449;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10204</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23556;&#30005;&#22825;&#25991;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Radio-astronomical Image Reconstruction with Conditional Denoising Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#65292;&#20026;&#23556;&#30005;&#28304;&#30340;&#34920;&#24449;&#25552;&#20379;&#28508;&#22312;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19981;&#24178;&#20928;&#30340;&#23556;&#30005;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#65292;&#20197;&#20415;&#20934;&#30830;&#23450;&#20301;&#21644;&#27979;&#37327;&#27969;&#37327;&#23545;&#20110;&#30740;&#31350;&#39640;&#32418;&#31227;&#19979;&#30340;&#26143;&#31995;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;Atacama Large Millimetre Array (ALMA)&#31561;&#20202;&#22120;&#36827;&#34892;&#28145;&#24230;&#35266;&#27979;&#26102;&#12290;&#38543;&#30528;Square Kilometre Array (SKA)&#31561;&#26032;&#39033;&#30446;&#30340;&#21551;&#21160;&#65292;&#23545;&#26356;&#22909;&#30340;&#28304;&#25552;&#21462;&#26041;&#27861;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#30446;&#21069;&#30340;&#25216;&#26415;&#65292;&#22914;CLEAN&#21644;PyBDSF&#65292;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#21040;&#24494;&#24369;&#30340;&#28304;&#65292;&#20984;&#26174;&#20102;&#23545;&#26356;&#20934;&#30830;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#35758;&#20351;&#29992;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#19981;&#24178;&#20928;&#30340;&#22270;&#20687;&#20013;&#37325;&#24314;&#22825;&#31354;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31934;&#30830;&#23450;&#20301;&#23556;&#30005;&#28304;&#24182;&#27979;&#37327;&#20276;&#38543;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26631;&#24535;&#30528;&#23556;&#30005;&#28304;&#34920;&#24449;&#26041;&#38754;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#22522;&#20110;ALMA&#31532;5.3&#21608;&#26399;&#22825;&#32447;&#35774;&#32622;&#30340;CASA&#24037;&#20855;simalma&#27169;&#25311;&#30340;10164&#20010;&#22270;&#20687;&#19978;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10204v1 Announce Type: cross  Abstract: Reconstructing sky models from dirty radio images for accurate source localization and flux estimation is crucial for studying galaxy evolution at high redshift, especially in deep fields using instruments like the Atacama Large Millimetre Array (ALMA). With new projects like the Square Kilometre Array (SKA), there's a growing need for better source extraction methods. Current techniques, such as CLEAN and PyBDSF, often fail to detect faint sources, highlighting the need for more accurate methods. This study proposes using stochastic neural networks to rebuild sky models directly from dirty images. This method can pinpoint radio sources and measure their fluxes with related uncertainties, marking a potential improvement in radio source characterization. We tested this approach on 10164 images simulated with the CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied conditional Denoising Diffusion Probabilistic Models (D
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10011</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Simplicial Message Passing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#25299;&#25169;&#19978;&#26356;&#20026;&#22797;&#26434;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21333;&#20307;&#22797;&#21512;&#20307;&#19978;&#36827;&#34892;&#21487;&#25511;&#30340;E&#65288;n&#65289;-&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;Clifford&#32676;&#31561;&#21464;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30456;&#32467;&#21512;&#65292;&#21518;&#32773;&#22312;&#25299;&#25169;&#19978;&#27604;&#24120;&#35268;&#22270;&#28040;&#24687;&#20256;&#36882;&#26356;&#21152;&#22797;&#26434;&#12290;Clifford&#20195;&#25968;&#21253;&#25324;&#39640;&#38454;&#23545;&#35937;&#65292;&#22914;&#21452;&#21521;&#37327;&#21644;&#19977;&#21521;&#37327;&#65292;&#36825;&#20123;&#23545;&#35937;&#36890;&#36807;&#21521;&#37327;&#34893;&#29983;&#20986;&#20960;&#20309;&#29305;&#24449;&#65288;&#20363;&#22914;&#38754;&#31215;&#65292;&#20307;&#31215;&#65289;&#12290;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#39030;&#28857;&#30340;&#20960;&#20309;&#20056;&#31215;&#34920;&#31034;&#31616;&#21333;&#24418;&#24335;&#29305;&#24449;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#20849;&#20139;&#28040;&#24687;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26368;&#32456;&#30340;&#28040;&#24687;&#38480;&#21046;&#20026;&#26469;&#33258;&#19981;&#21516;&#32500;&#24230;&#30340;&#20256;&#20837;&#28040;&#24687;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#20849;&#20139;&#21333;&#20307;&#28040;&#24687;&#20256;&#36882;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36755;&#20986;&#36866;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10011v1 Announce Type: new  Abstract: We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing. Clifford algebras include higher-order objects such as bivectors and trivectors, which express geometric features (e.g., areas, volumes) derived from vectors. Using this knowledge, we represent simplex features through geometric products of their vertices. To achieve efficient simplicial message passing, we share the parameters of the message network across different dimensions. Additionally, we restrict the final message to an aggregation of the incoming messages from different dimensions, leading to what we term shared simplicial message passing. Experimental results show that our method is able to ou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09444</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multimodal Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#26159;&#35780;&#20272;&#21160;&#20316;&#25191;&#34892;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20165;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24573;&#35270;&#20102;&#38899;&#39057;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;AQA&#39640;&#24230;&#20381;&#36182;&#35270;&#35273;&#20449;&#24687;&#65292;&#20294;&#38899;&#39057;&#20063;&#26159;&#25552;&#39640;&#35780;&#20998;&#22238;&#24402;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#34917;&#20805;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#32972;&#26223;&#38899;&#20048;&#30340;&#36816;&#21160;&#39033;&#30446;&#20013;&#65292;&#22914;&#33457;&#26679;&#28369;&#20912;&#21644;&#38901;&#24459;&#20307;&#25805;&#12290;&#20026;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;AQA&#65292;&#21363;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;PAMFN&#65289;&#65292;&#23427;&#20998;&#21035;&#23545;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#21644;&#19968;&#20010;&#28151;&#21512;&#27169;&#24577;&#20998;&#25903;&#32452;&#25104;&#65292;&#29420;&#31435;&#22320;&#25506;&#32034;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#28176;&#36827;&#22320;&#32858;&#21512;&#26469;&#33258;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09444v1 Announce Type: cross  Abstract: Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08491</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#30340;&#24067;&#23572;&#27169;&#22411;&#21560;&#24341;&#23376;&#26223;&#35266;&#20013;&#30340;&#25511;&#21046;&#36941;&#21382;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#21487;&#29992;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#19981;&#21516;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#28287;&#23454;&#39564;&#21457;&#29616;&#37325;&#32534;&#31243;&#31574;&#30053;&#30340;&#25928;&#29575;&#21463;&#21040;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#20415;&#24110;&#21161;&#35782;&#21035;&#37325;&#32534;&#31243;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#26694;&#26550;&#30340;BNs&#21644;PBNs&#20197;&#21450;&#24322;&#27493;&#26356;&#26032;&#27169;&#24335;&#19979;&#21046;&#23450;&#20102;&#19968;&#20010;&#25511;&#21046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20266;&#21560;&#24341;&#23376;&#29366;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07859</link><description>&lt;p&gt;
Lissard&#65306;&#38271;&#32780;&#31616;&#21333;&#30340;&#39034;&#24207;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Lissard: Long and Simple Sequential Reasoning Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07859
&lt;/p&gt;
&lt;p&gt;
Lissard&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#33021;&#22815;&#35299;&#20915;&#38656;&#35201;&#22788;&#29702;&#25968;&#21313;&#19975;&#20010;&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#37325;&#22797;&#20351;&#29992;&#31616;&#21333;&#35268;&#21017;&#30340;&#20219;&#21153;&#19978;&#24120;&#24120;&#22833;&#36133;&#65292;&#29978;&#33267;&#22312;&#27604;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#24207;&#21015;&#35201;&#30701;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20363;&#22914;&#65292;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22312;&#20004;&#20010;&#21015;&#34920;&#20013;&#25214;&#21040;&#20849;&#21516;&#39033;&#65292;&#21015;&#34920;&#20013;&#30340;&#39033;&#26368;&#22810;&#21487;&#36798;20&#20010;&#65292;&#20294;&#26159;&#24403;&#21015;&#34920;&#20013;&#30340;&#39033;&#36798;&#21040;80&#20010;&#26102;&#65292;&#23427;&#20204;&#20250;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Lissard&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#21644;&#29983;&#25104;&#21508;&#31181;&#24207;&#21015;&#38271;&#24230;&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#37325;&#22797;&#30340;&#36807;&#31243;&#25191;&#34892;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24320;&#28304;&#27169;&#22411;&#65288;Mistral-7B&#21644;Mixtral-8x7B&#65289;&#21644;&#19987;&#26377;&#27169;&#22411;&#65288;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#38543;&#30528;&#24207;&#21015;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#37117;&#21576;&#19968;&#33268;&#19979;&#38477;&#36235;&#21183;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/unicamp-dl/Lissard&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are now capable of solving tasks that require dealing with long sequences consisting of hundreds of thousands of tokens. However, they often fail on tasks that require repetitive use of simple rules, even on sequences that are much shorter than those seen during training. For example, state-of-the-art LLMs can find common items in two lists with up to 20 items but fail when lists have 80 items. In this paper, we introduce Lissard, a benchmark comprising seven tasks whose goal is to assess the ability of models to process and generate wide-range sequence lengths, requiring repetitive procedural execution. Our evaluation of open-source (Mistral-7B and Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent decline in performance across all models as the complexity of the sequence increases. The datasets and code are available at https://github.com/unicamp-dl/Lissard
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07845</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36830;&#25509;&#20449;&#24687;&#30340;&#20108;&#20803;&#24615;&#26469;&#35757;&#32451;&#20197;&#26816;&#27979;&#22270;&#20013;&#30340;&#31038;&#21306;&#12290;&#30446;&#21069;&#65292;&#20248;&#21270;GNN&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#22522;&#20934;&#20540;&#30340;&#27604;&#36739;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#20248;&#21270;&#27169;&#22359;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;GNN&#23558;&#33410;&#28857;&#32858;&#31867;&#25104;&#31038;&#21306;&#65292;&#32780;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#27169;&#22359;&#24615;&#26159;&#19968;&#31181;&#22270;&#20998;&#21306;&#36136;&#37327;&#24230;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20063;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#21516;&#26102;&#32534;&#30721;&#29305;&#24449;&#30340;GNN&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#26080;&#30417;&#30563;&#24230;&#37327;&#24615;&#33021;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#22522;&#20934;&#20540;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25506;&#31350;&#20026;&#20160;&#20040;&#21487;&#20197;&#20351;&#29992;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#21512;&#25104;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21512;&#25104;&#22270;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#12289;&#38543;&#26426;&#21644;&#38646;&#20449;&#24687;&#31354;&#38388;&#20998;&#21306;&#20013;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#35299;&#20915;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#12289;&#20809;&#32420;&#38464;&#34746;&#20202;&#21644;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#36710;&#36742;&#36816;&#21160;&#20272;&#35745;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.07429</link><description>&lt;p&gt;
&#29992;&#20110;&#36710;&#36742;&#23450;&#20301;&#30340;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Particle Filter SLAM for Vehicle Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#35299;&#20915;&#20102;&#36710;&#36742;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#12289;&#20809;&#32420;&#38464;&#34746;&#20202;&#21644;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#36710;&#36742;&#36816;&#21160;&#20272;&#35745;&#21644;&#29615;&#22659;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#21160;&#24577;&#26500;&#24314;&#22320;&#22270;&#30340;&#21516;&#26102;&#30830;&#23450;&#26426;&#22120;&#20154;&#23450;&#20301;&#30340;&#31934;&#30830;&#20301;&#32622;&#12290;&#36825;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#21463;&#21040;&#20102;&#8220;&#20808;&#26377;&#40481;&#36824;&#26159;&#20808;&#26377;&#34507;&#8221;&#22256;&#22659;&#30340;&#24433;&#21709;&#65292;&#20934;&#30830;&#30340;&#24314;&#22270;&#20381;&#36182;&#20110;&#21487;&#38752;&#30340;&#26426;&#22120;&#20154;&#23450;&#20301;&#20272;&#35745;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#27492;&#22806;&#65292;SLAM&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#35813;&#39046;&#22495;&#20013;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31890;&#23376;&#28388;&#27874;SLAM&#26041;&#27861;&#26469;&#35299;&#20915;SLAM&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#25968;&#25454;&#21644;&#20809;&#32420;&#38464;&#34746;&#20202;&#65288;FOG&#65289;&#20449;&#24687;&#65292;&#23454;&#29616;&#23545;&#36710;&#36742;&#36816;&#21160;&#30340;&#31934;&#30830;&#20272;&#35745;&#65292;&#32780;&#28608;&#20809;&#38647;&#36798;&#25216;&#26415;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#21608;&#22260;&#38556;&#30861;&#29289;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#23545;&#29615;&#22659;&#24863;&#30693;&#20316;&#20986;&#36129;&#29486;&#12290;&#36825;&#20123;&#25968;&#25454;&#27969;&#30340;&#38598;&#25104;&#26368;&#32456;&#24314;&#31435;&#20102;&#19968;&#20010;&#31890;&#23376;&#28388;&#27874;SLAM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Localization and Mapping (SLAM) presents a formidable challenge in robotics, involving the dynamic construction of a map while concurrently determining the precise location of the robotic agent within an unfamiliar environment. This intricate task is further compounded by the inherent "chicken-and-egg" dilemma, where accurate mapping relies on a dependable estimation of the robot's location, and vice versa. Moreover, the computational intensity of SLAM adds an additional layer of complexity, making it a crucial yet demanding topic in the field. In our research, we address the challenges of SLAM by adopting the Particle Filter SLAM method. Our approach leverages encoded data and fiber optic gyro (FOG) information to enable precise estimation of vehicle motion, while lidar technology contributes to environmental perception by providing detailed insights into surrounding obstacles. The integration of these data streams culminates in the establishment of a Particle Filter SLAM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;NRAM&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#20010;&#24615;&#21270;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07422</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
News Recommendation with Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;NRAM&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#20010;&#24615;&#21270;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26032;&#38395;&#25512;&#33616;&#39046;&#22495;&#65292;&#36825;&#26159;&#22312;&#32447;&#20449;&#24687;&#20998;&#20139;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#26032;&#38395;&#25512;&#33616;&#36827;&#34892;&#20102;&#28165;&#26224;&#30340;&#20171;&#32461;&#65292;&#23450;&#20041;&#20102;&#26680;&#24515;&#38382;&#39064;&#24182;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#21644;&#36817;&#26399;&#20540;&#24471;&#27880;&#24847;&#30340;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NRAM&#65288;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#65289;&#30340;&#23454;&#29616;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;NRAM&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#25968;&#23383;&#26032;&#38395;&#24179;&#21488;&#19978;&#38024;&#23545;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#20869;&#23481;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the area of news recommendation, a key component of online information sharing. Initially, we provide a clear introduction to news recommendation, defining the core problem and summarizing current methods and notable recent algorithms. We then present our work on implementing the NRAM (News Recommendation with Attention Mechanism), an attention-based approach for news recommendation, and assess its effectiveness. Our evaluation shows that NRAM has the potential to significantly improve how news content is personalized for users on digital news platforms.
&lt;/p&gt;</description></item><item><title>"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07197</link><description>&lt;p&gt;
GraphTranslator&#65306;&#23558;&#22270;&#27169;&#22411;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#29992;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07197
&lt;/p&gt;
&lt;p&gt;
"GraphTranslator"&#26159;&#19968;&#20010;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#32763;&#35793;&#22120;&#65292;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#36825;&#20004;&#31181;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#23454;&#29616;&#26356;&#20855;&#21019;&#26032;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#22270;&#39046;&#22495;&#20013;&#36825;&#20010;&#24819;&#27861;&#36739;&#23569;&#34987;&#25506;&#32034;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#24378;&#22823;&#30340;&#22270;&#27169;&#22411;&#65288;GMs&#65289;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#39044;&#23450;&#20041;&#24418;&#24335;&#30340;&#20219;&#21153;&#20013;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21516;&#26102;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#21644;&#24320;&#25918;&#24335;&#20219;&#21153;&#65292;&#26080;&#35770;&#26159;&#23558;LLMs&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#22120;&#36824;&#26159;&#20316;&#20026;&#29420;&#31435;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GraphTranslator&#30340;&#32763;&#35793;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;GM&#21644;LLMs&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#39044;&#23450;&#20041;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;LLMs&#30340;&#25193;&#23637;&#25509;&#21475;&#20026;GM&#25552;&#20379;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#32763;&#35793;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Producer&#30340;&#26500;&#24314;&#22270;&#25991;&#23545;&#40784;&#25968;&#25454;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and instruction-following capabilities, have catalyzed a revolutionary transformation across diverse research fields of artificial intelligence, especially for open-ended tasks. While the idea is less explored in the graph domain, despite the availability of numerous powerful graph models (GMs), they are restricted to tasks in a pre-defined form. Although several methods applying LLMs to graphs have been proposed, they fail to simultaneously handle the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as a standalone predictor. To break this dilemma, we propose to bridge the pretrained GM and LLM by a Translator, named GraphTranslator, aiming to leverage GM to handle the pre-defined tasks effectively and utilize the extended interface of LLMs to offer various open-ended tasks for GM. To train such Translator, we propose a Producer capable of constructing the graph-text alignment data along node
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07140</link><description>&lt;p&gt;
&#25991;&#23383;&#25551;&#36848;&#20013;&#30340;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07140
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#20013;&#30340;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#25991;&#26412;&#39034;&#24207;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19982;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#26159;&#21333;&#35843;&#36882;&#20943;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#65292;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22270;&#25512;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25991;&#26412;&#39034;&#24207;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#31354;&#38388;&#29702;&#35299;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#26174;&#33879;&#24433;&#21709;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#25913;&#21464;&#22270;&#25551;&#36848;&#30340;&#25991;&#26412;&#39034;&#24207;&#65292;&#25105;&#20204;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#20174;42.22&#65285;&#25552;&#39640;&#21040;70&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#22270;&#22823;&#23567;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#24615;&#33021;&#19981;&#38543;&#22270;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35268;&#27169;&#21270;&#22270;&#25512;&#29702;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22270;&#22823;&#23567;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.06196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06196
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#33258;2022&#24180;11&#26376;ChatGPT&#21457;&#24067;&#20197;&#26469;&#12290;LLMs&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#21313;&#20159;&#21442;&#25968;&#26469;&#33719;&#24471;&#24191;&#27867;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#31526;&#21512;&#32553;&#25918;&#23450;&#24459;&#30340;&#39044;&#27979;&#12290;LLMs&#30340;&#30740;&#31350;&#39046;&#22495;&#23613;&#31649;&#38750;&#24120;&#26032;&#65292;&#20294;&#22312;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#26368;&#33879;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;&#19977;&#20010;&#27969;&#34892;&#30340;LLM&#31995;&#21015;&#65288;GPT&#12289;LLaMA&#12289;PaLM&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#36129;&#29486;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;LLM&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#20934;&#22791;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#65292;&#23457;&#26597;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#27969;&#34892;LLM&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03843</link><description>&lt;p&gt;
&#20809;&#23398;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method for optical steel rope non-destructive damage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#28023;&#25300;&#29615;&#22659;&#65288;&#31354;&#20013;&#21514;&#32034;&#36947;&#65289;&#20013;&#30340;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RGBD-UNet&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#38050;&#19997;&#32499;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#30340;CMA&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#21644;&#32467;&#21512;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;VovNetV3.5&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#38050;&#19997;&#32499;&#12290;&#23427;&#23558;VovNet&#26550;&#26500;&#19982;DBB&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32972;&#26223;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#21516;&#22330;&#26223;&#20013;&#38050;&#19997;&#32499;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#27492;&#31639;&#27861;&#30340;&#20256;&#24863;&#22120;&#35782;&#21035;&#24615;&#33021;&#65288;h&#65289;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07453</link><description>&lt;p&gt;
&#35268;&#27169;&#21270;&#27169;&#22411;&#32534;&#36753;&#20250;&#23548;&#33268;&#28176;&#36827;&#24615;&#21644;&#31361;&#21457;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Editing at Scale leads to Gradual and Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07453
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#35268;&#27169;&#21270;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#20250;&#36880;&#28176;&#36951;&#24536;&#20808;&#21069;&#30340;&#20107;&#23454;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#36753;&#30693;&#35782;&#26159;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#33021;&#21147;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#32416;&#27491;&#38169;&#35823;&#23398;&#20064;&#30340;&#20107;&#23454;&#65292;&#21516;&#26102;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#30340;&#26032;&#20107;&#23454;&#21015;&#34920;&#26356;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#20102;&#20351;&#27169;&#22411;&#32534;&#36753;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#65292;&#25105;&#20204;&#24517;&#39035;&#33021;&#22815;&#23545;&#21516;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#32534;&#36753;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65306;ROME &#21644; MEMIT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#34987;&#39034;&#24207;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#23427;&#19981;&#26029;&#22320;&#36951;&#24536;&#20808;&#21069;&#32534;&#36753;&#36807;&#30340;&#20107;&#23454;&#20197;&#21450;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#36951;&#24536;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;--&#21021;&#22987;&#30340;&#28176;&#36827;&#24615;&#36951;&#24536;&#38454;&#27573;&#65292;&#38543;&#21518;&#26159;&#31361;&#28982;&#25110;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07453v2 Announce Type: replace-cross  Abstract: Editing knowledge in large language models is an attractive capability to have which allows us to correct incorrectly learnt facts during pre-training, as well as update the model with an ever-growing list of new facts. While existing model editing techniques have shown promise, they are usually evaluated using metrics for reliability, specificity and generalization over one or few edits. We argue that for model editing to have practical utility, we must be able to make multiple edits to the same model. With this in mind, we evaluate the current model editing methods at scale, focusing on two state of the art methods: ROME and MEMIT. We find that as the model is edited sequentially with multiple facts, it continually forgets previously edited facts and the ability to perform downstream tasks. This forgetting happens in two phases -- an initial gradual but progressive forgetting phase followed by abrupt or catastrophic forgettin
&lt;/p&gt;</description></item><item><title>EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.07128</link><description>&lt;p&gt;
EHRAgent&#65306;&#20195;&#30721;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07128
&lt;/p&gt;
&lt;p&gt;
EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23578;&#26410;&#26377;&#22826;&#22810;&#24320;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;EHRAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;LLM&#20195;&#29702;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#30340;&#20195;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;EHR&#38382;&#31572;&#20219;&#21153;&#21046;&#23450;&#20026;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#36807;&#31243;&#65292;&#23558;&#19968;&#20010;&#22797;&#26434;&#20219;&#21153;&#39640;&#25928;&#22320;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21487;&#31649;&#29702;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#38598;&#25104;&#20132;&#20114;&#24335;&#32534;&#30721;&#21644;&#25191;&#34892;&#21453;&#39304;&#65292;EHRAgent&#20174;&#38169;&#35823;&#28040;&#24687;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26368;&#21021;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#26469;&#22686;&#24378;LLM&#20195;&#29702;&#65292;&#20351;EHRAgent&#33021;&#22815;&#26377;&#25928;&#22320;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#26368;&#30456;&#20851;&#30340;&#25104;&#21151;&#26696;&#20363;&#19978;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#34920;&#26684;EHR&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07128v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#23545;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#36827;&#34892;&#19978;&#38480;&#32422;&#26463;&#65292;&#24182;&#25581;&#31034;&#20102;&#26680;&#20998;&#35299;&#20013;&#23384;&#22312;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.15995</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#30340;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization in Kernel Regression Under Realistic Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#23545;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#30340;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#36827;&#34892;&#19978;&#38480;&#32422;&#26463;&#65292;&#24182;&#25581;&#31034;&#20102;&#26680;&#20998;&#35299;&#20013;&#23384;&#22312;&#30340;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#24050;&#32463;&#30830;&#31435;&#30340;&#20107;&#23454;&#26159;&#65292;&#29616;&#20195;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20284;&#20046;&#33021;&#22815;&#36867;&#36991;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#22312;&#36807;&#24230;&#25311;&#21512;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#33391;&#22909;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#22312;&#26680;&#22238;&#24402;&#30456;&#23545;&#26131;&#22788;&#29702;&#30340;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#36807;&#21435;&#30340;&#30740;&#31350;&#35201;&#20040;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#19968;&#20010;&#29421;&#31364;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26469;&#38480;&#21046;&#20960;&#20046;&#25152;&#26377;&#24120;&#35265;&#21644;&#29616;&#23454;&#35774;&#32622;&#19979;&#26680;&#22238;&#24402;&#30340;&#36229;&#20986;&#39118;&#38505;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#24120;&#35265;&#26680;&#20989;&#25968;&#20197;&#21450;&#20219;&#24847;&#30340;&#27491;&#21017;&#21270;&#37327;&#12289;&#22122;&#22768;&#12289;&#20219;&#24847;&#36755;&#20837;&#32500;&#24230;&#21644;&#20219;&#24847;&#26679;&#26412;&#25968;&#37117;&#25104;&#31435;&#30340;&#20005;&#26684;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25552;&#20379;&#20102;&#30456;&#23545;&#25200;&#21160;&#30028;&#38480;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#19968;&#31181;&#33258;&#25105;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21363;&#26680;&#20998;&#35299;&#30340;&#29305;&#24449;&#20540;&#20013;&#23384;&#22312;&#37325;&#23614;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15995v2 Announce Type: replace-cross  Abstract: It is by now well-established that modern over-parameterized models seem to elude the bias-variance tradeoff and generalize well despite overfitting noise. Many recent works attempt to analyze this phenomenon in the relatively tractable setting of kernel regression. However, as we argue in detail, most past works on this topic either make unrealistic assumptions, or focus on a narrow problem setup. This work aims to provide a unified theory to upper bound the excess risk of kernel regression for nearly all common and realistic settings. Specifically, we provide rigorous bounds that hold for common kernels and for any amount of regularization, noise, any input dimension, and any number of samples. Furthermore, we provide relative perturbation bounds for the eigenvalues of kernel matrices, which may be of independent interest. These reveal a self-regularization phenomenon, whereby a heavy tail in the eigendecomposition of the ker
&lt;/p&gt;</description></item><item><title>&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;</title><link>https://arxiv.org/abs/2312.15006</link><description>&lt;p&gt;
&#35780;&#20272;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15006
&lt;/p&gt;
&lt;p&gt;
&#19977;&#31181;&#25552;&#31034;&#26041;&#27861;&#23545;ChatGPT&#30340;&#25968;&#23398;&#33021;&#21147;&#24182;&#26410;&#20135;&#29983;&#19968;&#36143;&#24615;&#25913;&#36827;&#25928;&#26524;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#25552;&#31034;&#26041;&#27861;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#19977;&#31181;&#35268;&#23450;&#24615;&#25552;&#31034;&#26041;&#27861; - &#31616;&#21333;&#25552;&#31034;&#12289;&#20010;&#20154;&#25552;&#31034;&#21644;&#23545;&#35805;&#25552;&#31034; - &#36825;&#20123;&#26041;&#27861;&#20197;&#25552;&#21319;LLMs&#35821;&#35328;&#20219;&#21153;&#25928;&#26524;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#22312;OpenAI&#30340;LLM&#38386;&#32842;&#26426;&#22120;&#20154;ChatGPT-3.5&#19978;&#36827;&#34892;&#27492;&#20998;&#26512;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;MATH&#12289;GSM8K&#21644;MMLU&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38382;&#39064;&#38598;&#21512;&#65292;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;&#20102;&#21508;&#31181;&#25968;&#23398;&#25361;&#25112;&#12290;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#35843;&#25972;&#30340;&#35780;&#20998;&#33050;&#26412;&#29992;&#20110;&#30830;&#23450;&#36825;&#20123;&#25552;&#31034;&#24178;&#39044;&#22312;&#22686;&#24378;&#27169;&#22411;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#19982;&#39044;&#26399;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#26174;&#31034;&#65292;&#25152;&#26816;&#39564;&#30340;&#26041;&#27861;&#22343;&#26410;&#22312;&#25345;&#32493;&#25913;&#36827;ChatGPT-3.5&#22522;&#20934;&#34920;&#29616;&#19978;&#65292;&#37096;&#20998;&#26041;&#27861;&#29978;&#33267;&#23548;&#33268;&#26126;&#26174;&#30340;&#36864;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25552;&#31034;&#31574;&#30053;&#26410;&#24517;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15006v2 Announce Type: replace  Abstract: This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not nece
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LANS&#30340;&#24067;&#23616;&#24863;&#30693;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#24067;&#23616;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22359;(MLA-PLM)&#21644;&#24067;&#23616;&#24863;&#30693;&#34701;&#21512;&#27880;&#24847;&#21147;(LA-FA)&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#20960;&#20309;&#22270;&#34920;&#24067;&#23616;&#20449;&#24687;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.16476</link><description>&lt;p&gt;
LANS: &#29992;&#20110;&#24179;&#38754;&#20960;&#20309;&#38382;&#39064;&#30340;&#24067;&#23616;&#24863;&#30693;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
LANS: A Layout-Aware Neural Solver for Plane Geometry Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LANS&#30340;&#24067;&#23616;&#24863;&#30693;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#24067;&#23616;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22359;(MLA-PLM)&#21644;&#24067;&#23616;&#24863;&#30693;&#34701;&#21512;&#27880;&#24847;&#21147;(LA-FA)&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#20960;&#20309;&#22270;&#34920;&#24067;&#23616;&#20449;&#24687;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;(GPS)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#65292;&#38656;&#35201;&#22810;&#27169;&#24577;&#29702;&#35299;&#12289;&#34701;&#21512;&#21644;&#25512;&#29702;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#23558;GPS&#35270;&#20026;&#19968;&#39033;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#65292;&#20294;&#22312;&#20195;&#34920;&#25658;&#24102;&#20016;&#23500;&#21644;&#22797;&#26434;&#24067;&#23616;&#20449;&#24687;&#30340;&#20960;&#20309;&#22270;&#34920;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LANS&#30340;&#24067;&#23616;&#24863;&#30693;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#38598;&#25104;&#20102;&#20004;&#20010;&#26032;&#27169;&#22359;&#65306;&#22810;&#27169;&#24577;&#24067;&#23616;&#24863;&#30693;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22359;(MLA-PLM)&#21644;&#24067;&#23616;&#24863;&#30693;&#34701;&#21512;&#27880;&#24847;&#21147;(LA-FA)&#12290;MLA-PLM&#37319;&#29992;&#32467;&#26500;-&#35821;&#20041;&#39044;&#35757;&#32451;(SSP)&#26469;&#23454;&#29616;&#20840;&#23616;&#20851;&#31995;&#24314;&#27169;&#65292;&#28857;&#21305;&#37197;&#39044;&#35757;&#32451;(PMP)&#26469;&#23454;&#29616;&#35270;&#35273;&#28857;&#21644;&#25991;&#26412;&#28857;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290; LA-FA&#20351;&#29992;&#24067;&#23616;&#24863;&#30693;&#27880;&#24847;&#25513;&#27169;&#65292;&#23454;&#29616;&#28857;&#24341;&#23548;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LANS&#30340;&#24067;&#23616;&#24863;&#30693;&#24615;&#33021;&#12290;&#23545;Geometry3K&#21644;PGPS9K&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#24067;&#23616;&#24863;&#30693;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16476v2 Announce Type: replace-cross  Abstract: Geometry problem solving (GPS) is a challenging mathematical reasoning task requiring multi-modal understanding, fusion, and reasoning. Existing neural solvers take GPS as a vision-language task but are short in the representation of geometry diagrams that carry rich and complex layout information. In this paper, we propose a layout-aware neural solver named LANS, integrated with two new modules: multimodal layout-aware pre-trained language module (MLA-PLM) and layout-aware fusion attention (LA-FA). MLA-PLM adopts structural-semantic pre-training (SSP) to implement global relationship modeling, and point-match pre-training (PMP) to achieve alignment between visual points and textual points. LA-FA employs a layout-aware attention mask to realize point-guided cross-modal fusion for further boosting layout awareness of LANS. Extensive experiments on datasets Geometry3K and PGPS9K validate the effectiveness of the layout-aware modu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;</title><link>https://arxiv.org/abs/2311.14688</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#26469;&#23454;&#29616;&#31243;&#24207;&#20844;&#24179;
&lt;/p&gt;
&lt;p&gt;
Procedural Fairness Through Decoupling Objectionable Data Generating Components
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#24182;&#24378;&#35843;&#20102;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#35201;&#27714;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#24182;&#35299;&#20915;&#20102;&#32463;&#24120;&#34987;&#24573;&#35270;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21363;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#65292;&#21363;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20013;&#31435;&#65288;&#21363;&#19981;&#25104;&#38382;&#39064;&#30340;&#65289;&#26041;&#38754;&#30340;&#21487;&#33021;&#26080;&#24847;&#30340;&#25913;&#21464;&#65292;&#21644;/&#25110;&#23545;&#26368;&#19981;&#21033;&#21033;&#30410;&#20010;&#20307;&#30340;&#23454;&#29616;&#27809;&#26377;&#31243;&#24207;&#20445;&#35777;&#12290;&#21463;&#32422;&#32752;&#183;&#32599;&#23572;&#26031;&#23545;&#32431;&#31243;&#24207;&#20844;&#27491;&#30340;&#20513;&#23548;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#20915;&#31574;&#35270;&#20026;&#31038;&#20250;&#21046;&#24230;&#30340;&#32553;&#24433;&#65292;&#24182;&#32771;&#34385;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26412;&#36523;&#22914;&#20309;&#28385;&#36275;&#31243;&#24207;&#20844;&#24179;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#32771;&#28857;&#21644;&#30456;&#20851;&#30340;&#20215;&#20540;&#23454;&#20363;&#21270;&#35268;&#21017;&#65292;&#23558;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#19982;&#20013;&#31435;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#35299;&#32806;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#38450;&#27490;&#20266;&#35013;&#30340;&#31243;&#24207;&#19981;&#20844;&#24179;&#30340;&#24517;&#35201;&#24615;&#65292;&#19981;&#20165;&#24341;&#36215;&#20102;&#25105;&#20204;&#21147;&#22270;&#32531;&#35299;&#30340;&#21487;&#25239;&#35758;&#30340;&#25968;&#25454;&#29983;&#25104;&#32452;&#20214;&#30340;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14688v2 Announce Type: replace-cross  Abstract: We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals. Inspired by John Rawls's advocacy for pure procedural justice, we view automated decision-making as a microcosm of social institutions, and consider how the data generating process itself can satisfy the requirements of procedural fairness. We propose a framework that decouples the objectionable data generating components from the neutral ones by utilizing reference points and the associated value instantiation rule. Our findings highlight the necessity of preventing disguised procedural unfairness, drawing attention not only to the objectionable data generating components that we aim to mitiga
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#24110;&#21161;&#20811;&#26381;&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#25512;&#29702;&#26102;&#38388;&#38271;&#21644;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13845</link><description>&lt;p&gt;
&#20351;&#29992;&#25512;&#21069;&#26144;&#23556;&#36827;&#34892;&#21508;&#22320;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Touring sampling with pushforward maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13845
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#23545;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#24110;&#21161;&#20811;&#26381;&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#25512;&#29702;&#26102;&#38388;&#38271;&#21644;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#24076;&#26395;&#23558;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#30340;&#20174;&#19994;&#32773;&#26469;&#35828;&#65292;&#37319;&#26679;&#26041;&#27861;&#30340;&#25968;&#37327;&#21487;&#33021;&#20196;&#20154;&#29983;&#30031;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22312;&#8220;&#29983;&#25104;&#24314;&#27169;&#8221;&#35774;&#32622;&#20013;&#35768;&#22810;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20102;&#23457;&#35270;&#21644;&#32452;&#32455;&#65292;&#20854;&#20013;&#24076;&#26395;&#29983;&#25104;&#19982;&#19968;&#20123;&#35757;&#32451;&#26679;&#26412;&#31867;&#20284;&#30340;&#26032;&#25968;&#25454;&#12290;&#36890;&#36807;&#25581;&#31034;&#29616;&#26377;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#20811;&#26381;&#19982;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30456;&#20851;&#30340;&#19968;&#20123;&#24403;&#21069;&#25361;&#25112;&#65292;&#27604;&#22914;&#30001;&#20110;&#25193;&#25955;&#27169;&#25311;&#32780;&#23548;&#33268;&#30340;&#38271;&#25512;&#29702;&#26102;&#38388;&#65292;&#25110;&#32773;&#29983;&#25104;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13845v2 Announce Type: replace-cross  Abstract: The number of sampling methods could be daunting for a practitioner looking to cast powerful machine learning methods to their specific problem. This paper takes a theoretical stance to review and organize many sampling approaches in the ``generative modeling'' setting, where one wants to generate new data that are similar to some training examples. By revealing links between existing methods, it might prove useful to overcome some of the current challenges in sampling with diffusion models, such as long inference time due to diffusion simulation, or the lack of diversity in generated samples.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.10537</link><description>&lt;p&gt;
MedAgents: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#21307;&#23398;&#25512;&#29702;&#30340;&#21512;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23613;&#31649;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#21253;&#25324;&#20116;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#25910;&#38598;&#39046;&#22495;&#19987;&#23478;&#12289;&#25552;&#20986;&#20010;&#21035;&#20998;&#26512;&#12289;&#23558;&#36825;&#20123;&#20998;&#26512;&#24635;&#32467;&#25104;&#25253;&#21578;&#12289;&#22312;&#35752;&#35770;&#20013;&#21453;&#22797;&#36845;&#20195;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#65292;&#26368;&#32456;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#38646;-shot&#24773;&#26223;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.06555</link><description>&lt;p&gt;
&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06555
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21551;&#21457;&#24335;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#20419;&#36827;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#65292;&#24182;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22788;&#29702;&#26032;&#24773;&#20917;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25991;&#26723;&#32423;&#20107;&#20214;&#35770;&#35777;&#25552;&#21462;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20197;&#20943;&#36731;&#36825;&#19968;&#20219;&#21153;&#23545;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21551;&#21457;&#39537;&#21160;&#30340;&#31867;&#27604;&#38142;&#25509;&#65288;HD-LoA&#65289;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#31034;&#20363;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20026;EAE&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#24182;&#39564;&#35777;&#20102;LLMs&#36890;&#36807;ICL&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#21551;&#21457;&#24335;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;&#21551;&#21457;&#24335;&#39537;&#21160;&#31034;&#33539;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#26434;&#20081;&#30340;&#31034;&#20363;&#36873;&#25321;&#36807;&#31243;&#36716;&#21270;&#20026;&#24378;&#35843;&#20219;&#21153;&#21551;&#21457;&#24335;&#30340;&#26377;&#26465;&#19981;&#32010;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#21463;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#27604;&#38142;&#25509;&#25552;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23558;&#26032;&#24773;&#20917;&#31867;&#27604;&#20110;&#24050;&#30693;&#24773;&#20917;&#26469;&#22788;&#29702;&#26032;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#26377;&#38480;ICL&#31034;&#20363;&#20197;&#22806;&#30340;&#26410;&#35265;&#31867;&#21035;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06555v2 Announce Type: replace-cross  Abstract: In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL exa
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;</title><link>https://arxiv.org/abs/2310.18940</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#35821;&#35328;&#20195;&#29702;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#36827;&#34892;&#25112;&#30053;&#23545;&#25112;
&lt;/p&gt;
&lt;p&gt;
Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18940
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29436;&#20154;&#26432;&#28216;&#25103;&#20013;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#30340;&#20195;&#29702;&#22312;&#21508;&#39046;&#22495;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#32431;LLM&#20195;&#29702;&#24448;&#24448;&#34920;&#29616;&#20986;&#22266;&#26377;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#26469;&#28304;&#20110;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#28789;&#27963;&#35821;&#35328;&#34892;&#20026;&#21644;&#24378;&#22823;&#20915;&#31574;&#33021;&#21147;&#30340;&#25112;&#30053;&#35821;&#35328;&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#21319;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36873;&#25321;&#29436;&#20154;&#26432;&#20316;&#20026;&#20855;&#26377;&#22810;&#26679;&#27807;&#36890;&#21644;&#25112;&#30053;&#28216;&#25103;&#29609;&#27861;&#30340;&#25361;&#25112;&#27979;&#35797;&#24179;&#21488;&#12290;&#20026;&#20102;&#20943;&#36731;&#35821;&#35328;&#34892;&#20026;&#20013;&#30340;&#22266;&#26377;&#20559;&#35265;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#20351;&#29992;LLM&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#24182;&#29983;&#25104;&#22810;&#26679;&#34892;&#20026;&#20505;&#36873;&#38598;&#12290;&#28982;&#21518;&#65292;&#32463;&#36807;&#35757;&#32451;&#20197;&#20248;&#21270;&#20915;&#31574;&#33021;&#21147;&#30340;RL&#31574;&#30053;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18940v3 Announce Type: replace  Abstract: Agents built with large language models (LLMs) have shown great potential across a wide range of domains. However, in complex decision-making tasks, pure LLM-based agents tend to exhibit intrinsic bias in their choice of actions, which is inherited from the model's training data and results in suboptimal performance. To develop strategic language agents, i.e., agents that generate flexible language actions and possess strong decision-making abilities, we propose a novel framework that powers LLM-based agents with reinforcement learning (RL). We consider Werewolf, a popular social deduction game, as a challenging testbed that emphasizes versatile communication and strategic gameplay. To mitigate the intrinsic bias in language actions, our agents use an LLM to perform deductive reasoning and generate a diverse set of action candidates. Then an RL policy trained to optimize the decision-making ability chooses an action from the candidat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; Drag&amp;Drop &#30340;&#26032;&#26631;&#27880;&#31574;&#30053;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#24369;&#26631;&#27880;&#65292;&#29305;&#21035;&#36866;&#21512;&#26102;&#38388;&#21644;&#23481;&#31215;&#25104;&#20687;&#65292;&#21487;&#31616;&#21270;&#32959;&#30244;&#23450;&#20301;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2310.15098</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#21644;&#23481;&#31215;&#25968;&#25454;&#20013;&#33719;&#21462;&#32959;&#30244;&#23450;&#20301;&#30340;&#24369;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Acquiring Weak Annotations for Tumor Localization in Temporal and Volumetric Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026; Drag&amp;Drop &#30340;&#26032;&#26631;&#27880;&#31574;&#30053;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#24369;&#26631;&#27880;&#65292;&#29305;&#21035;&#36866;&#21512;&#26102;&#38388;&#21644;&#23481;&#31215;&#25104;&#20687;&#65292;&#21487;&#31616;&#21270;&#32959;&#30244;&#23450;&#20301;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#22823;&#35268;&#27169;&#21644;&#33391;&#22909;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#23545;&#20110;&#33258;&#21160;&#21270;&#32959;&#30244;&#26816;&#27979;&#21644;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#36164;&#28304;&#26377;&#38480;&#26102;&#65292;&#22312;&#26631;&#35760;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#26102;&#30830;&#23450;&#26368;&#20339;&#31867;&#22411;&#26631;&#27880;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32467;&#32928;&#38236;&#26816;&#26597;&#35270;&#39057;&#20013;&#30340;&#24687;&#32905;&#21644;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;&#33008;&#33146;&#32959;&#30244;&#65307;&#36825;&#20004;&#31181;&#24212;&#29992;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#65292;&#28041;&#21450;&#20020;&#26102;&#25110;&#31354;&#38388;&#32500;&#24230;&#65292;&#22240;&#27492;&#23545;&#27599;&#20687;&#32032;&#30340;&#26631;&#27880;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#27880;&#31574;&#30053;&#65292;&#31216;&#20026;Drag&amp;Drop&#65292;&#23427;&#31616;&#21270;&#20102;&#26631;&#27880;&#36807;&#31243;&#65292;&#20351;&#20043;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#21644;&#23481;&#31215;&#25104;&#20687;&#65292;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#24369;&#26631;&#27880;&#65288;&#20363;&#22914;&#27599;&#20687;&#32032;&#12289;&#36793;&#30028;&#26694;&#12289;&#28034;&#40486;&#12289;&#26925;&#22278;&#21644;&#28857;&#65289;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#30340;Drag&amp;Drop&#26631;&#27880;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15098v2 Announce Type: replace-cross  Abstract: Creating large-scale and well-annotated datasets to train AI algorithms is crucial for automated tumor detection and localization. However, with limited resources, it is challenging to determine the best type of annotations when annotating massive amounts of unlabeled data. To address this issue, we focus on polyps in colonoscopy videos and pancreatic tumors in abdominal CT scans; both applications require significant effort and time for pixel-wise annotation due to the high dimensional nature of the data, involving either temporary or spatial dimensions. In this paper, we develop a new annotation strategy, termed Drag&amp;Drop, which simplifies the annotation process to drag and drop. This annotation strategy is more efficient, particularly for temporal and volumetric imaging, than other types of weak annotations, such as per-pixel, bounding boxes, scribbles, ellipses, and points. Furthermore, to exploit our Drag&amp;Drop annotations,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;EX-FEVER&#65292;&#21253;&#21547;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#32463;&#36807;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#20855;&#26377;&#35814;&#32454;&#30340;&#35299;&#37322;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.09754</link><description>&lt;p&gt;
EX-FEVER&#65306;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;EX-FEVER&#65292;&#21253;&#21547;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#32463;&#36807;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#20855;&#26377;&#35814;&#32454;&#30340;&#35299;&#37322;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#26681;&#25454;&#22810;&#20010;&#35777;&#25454;&#33258;&#21160;&#25506;&#31350;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#22987;&#32456;&#33268;&#21147;&#20110;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#26356;&#19981;&#29992;&#35828;&#35299;&#37322;&#24615;&#20102;&#65292;&#36825;&#26159;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#22330;&#26223;&#20013;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#19968;&#30452;&#21463;&#21046;&#20110;&#32570;&#20047;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EX-FEVER&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#12290;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#28041;&#21450;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#26159;&#36890;&#36807;&#24635;&#32467;&#21644;&#20462;&#25913;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#25991;&#26723;&#30340;&#20449;&#24687;&#32780;&#21019;&#24314;&#30340;&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#38468;&#24102;&#19968;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#21644;&#19968;&#20010;&#35828;&#26126;&#65292;&#27010;&#36848;&#25903;&#25345;&#30495;&#23454;&#24615;&#20998;&#31867;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09754v2 Announce Type: replace  Abstract: Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in accuracy improvement, let alone explainability, a critical capability of fact verification systems. Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant, high-quality dataset. Previous datasets either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EXFEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionall
&lt;/p&gt;</description></item><item><title>&#36830;&#25509;&#32467;&#26500;&#23545;&#31070;&#32463;&#22238;&#36335;&#23398;&#20064;&#21160;&#24577;&#26377;&#20851;&#38190;&#24433;&#21709;&#65292;&#39640;&#31209;&#21021;&#22987;&#26435;&#37325;&#36890;&#24120;&#23548;&#33268;&#24816;&#24615;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2310.08513</link><description>&lt;p&gt;
&#36830;&#25509;&#32467;&#26500;&#22914;&#20309;&#22609;&#36896;&#31070;&#32463;&#22238;&#36335;&#20013;&#30340;&#23500;&#38598;&#21644;&#24816;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
How connectivity structure shapes rich and lazy learning in neural circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08513
&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#32467;&#26500;&#23545;&#31070;&#32463;&#22238;&#36335;&#23398;&#20064;&#21160;&#24577;&#26377;&#20851;&#38190;&#24433;&#21709;&#65292;&#39640;&#31209;&#21021;&#22987;&#26435;&#37325;&#36890;&#24120;&#23548;&#33268;&#24816;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#25506;&#35752;&#20102;&#19968;&#20123;&#32593;&#32476;&#23646;&#24615;&#22914;&#20309;&#20851;&#38190;&#24433;&#21709;&#20854;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#29305;&#21035;&#22320;&#65292;&#20855;&#26377;&#23567;&#65288;&#22823;&#65289;&#26041;&#24046;&#30340;&#21021;&#22987;&#26435;&#37325;&#20998;&#24067;&#21487;&#33021;&#20135;&#29983;&#23500;&#38598;&#65288;&#24816;&#24615;&#65289;&#27169;&#24335;&#65292;&#20854;&#20013;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#32593;&#32476;&#29366;&#24577;&#21644;&#34920;&#31034;&#30340;&#26174;&#30528;&#65288;&#24494;&#23567;&#65289;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#65292;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#21487;&#33021;&#23637;&#29616;&#20986;&#20302;&#31209;&#32467;&#26500;&#65292;&#22240;&#27492;&#19982;&#36890;&#24120;&#29992;&#20110;&#36825;&#20123;&#30740;&#31350;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#26377;&#26174;&#33879;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#26435;&#37325;&#32467;&#26500;&#65288;&#29305;&#21035;&#26159;&#23427;&#20204;&#30340;&#26377;&#25928;&#31209;&#65289;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23398;&#20064;&#27169;&#24335;&#12290;&#36890;&#36807;&#32463;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#31209;&#21021;&#22987;&#21270;&#36890;&#24120;&#20135;&#29983;&#34920;&#26126;&#24816;&#24615;&#23398;&#20064;&#30340;&#36739;&#23567;&#32593;&#32476;&#21464;&#21270;&#65292;&#36825;&#19968;&#21457;&#29616;&#20063;&#19982;&#23454;&#39564;&#39537;&#21160;&#30340;&#21021;&#22987; co &#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08513v2 Announce Type: replace-cross  Abstract: In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity could exhibit a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights -- in particular their effective rank -- influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWAP&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#37319;&#29992;&#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#30340;&#26799;&#24230;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;SWAP&#22312;&#22122;&#22768;&#25233;&#21046;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04918</link><description>&lt;p&gt;
SWAP: &#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#29992;&#20110;&#40065;&#26834;&#32593;&#32476;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
SWAP: Sparse Entropic Wasserstein Regression for Robust Network Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWAP&#30340;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#65292;&#37319;&#29992;&#31232;&#30095;&#29109;&#24335;Wasserstein&#22238;&#24402;&#26469;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#30340;&#26799;&#24230;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;SWAP&#22312;&#22122;&#22768;&#25233;&#21046;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#35745;&#31639;&#32463;&#39564;Fisher&#20449;&#24687;&#30697;&#38453;(FIM)&#26102;&#23384;&#22312;&#19981;&#20934;&#30830;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SWAP&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#29109;&#24335;Wasserstein&#22238;&#24402;(EWR)&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#20248;&#21270;&#20013;&#23558;&#24120;&#29992;&#30340;&#26631;&#20934;&#32447;&#24615;&#22238;&#24402;(LR)&#21644;EWR&#20132;&#25442;&#23637;&#31034;&#65292;SWAP&#22312;&#37319;&#29992;&#37051;&#36817;&#25554;&#20540;&#36328;&#25968;&#25454;&#28857;&#26102;&#22312;&#22122;&#22768;&#25233;&#21046;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#36739;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#12290;SWAP&#30340;&#29420;&#29305;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#22312;&#22122;&#22768;&#20943;&#23569;&#21644;&#21327;&#26041;&#24046;&#20449;&#24687;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;SWAP&#19982;&#26368;&#20808;&#36827;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#24403;&#32593;&#32476;&#35268;&#27169;&#25110;&#30446;&#26631;&#31232;&#30095;&#24230;&#36739;&#22823;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#19988;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study tackles the issue of neural network pruning that inaccurate gradients exist when computing the empirical Fisher Information Matrix (FIM). We introduce SWAP, an Entropic Wasserstein regression (EWR) network pruning formulation, capitalizing on the geometric attributes of the optimal transport (OT) problem. The "swap" of a commonly used standard linear regression (LR) with the EWR in optimization is analytically showcased to excel in noise mitigation by adopting neighborhood interpolation across data points, yet incurs marginal extra computational cost. The unique strength of SWAP is its intrinsic ability to strike a balance between noise reduction and covariance information preservation. Extensive experiments performed on various networks show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy 
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2310.03976</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#33258;&#25105;&#65306;&#29992;&#25143;&#23545;&#20154;&#24037;&#26234;&#33021;&#22312;&#20154;&#38469;&#20132;&#27969;&#21644;&#33258;&#25105;&#26041;&#38754;&#28508;&#21147;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
From Text to Self: Users' Perceptions of Potential of AI on Interpersonal Communication and Self
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.03976
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24037;&#20855;&#22312;&#20154;&#38469;&#20132;&#27969;&#26041;&#38754;&#30340;&#33021;&#21147;&#25345;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#21487;&#20197;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#12289;&#24110;&#21161;&#34920;&#36798;&#24819;&#27861;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#65292;&#20294;&#20063;&#25581;&#31034;&#20986;&#24037;&#20855;&#23384;&#22312;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21644;&#29992;&#25143;&#20851;&#20110;&#25216;&#26415;&#19981;&#30495;&#23454;&#24615;&#21644;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#20013;&#20171;&#20132;&#27969;&#65288;AIMC&#65289;&#39046;&#22495;&#20013;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#24037;&#20855;&#27491;&#25104;&#20026;&#20154;&#38469;&#20132;&#27969;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#19968;&#21608;&#30340;&#26085;&#35760;&#21644;&#35775;&#35848;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#29992;&#25143;&#23545;&#36825;&#20123;&#24037;&#20855;&#22312;&#30701;&#26399;&#20869;&#25903;&#25345;&#20154;&#38469;&#20132;&#27969;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#23548;&#33268;&#30340;&#38271;&#26399;&#25928;&#26524;&#30340;&#30475;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21442;&#19982;&#32773;&#23545;AIMC&#25903;&#25345;&#25345;&#26377;&#31215;&#26497;&#30475;&#27861;&#65292;&#35748;&#20026;&#20854;&#33021;&#22815;&#22686;&#21152;&#27807;&#36890;&#33258;&#20449;&#65292;&#24110;&#21161;&#25214;&#21040;&#20934;&#30830;&#30340;&#35821;&#35328;&#34920;&#36798;&#24819;&#27861;&#65292;&#20197;&#21450;&#20811;&#26381;&#35821;&#35328;&#21644;&#25991;&#21270;&#38556;&#30861;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;AIMC&#24037;&#20855;&#30446;&#21069;&#23384;&#22312;&#30340;&#23616;&#38480;&#65292;&#21253;&#25324;&#21872;&#21990;&#30340;&#22238;&#22797;&#12289;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#20197;&#21450;&#36807;&#24230;&#24773;&#32490;&#21270;&#12290;&#36825;&#20123;&#32570;&#38519;&#36827;&#19968;&#27493;&#21463;&#21040;&#29992;&#25143;&#23545;&#19981;&#30495;&#23454;&#24615;&#21644;&#23545;&#25216;&#26415;&#36807;&#24230;&#20381;&#36182;&#30340;&#25285;&#24551;&#25152;&#21152;&#21095;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.03976v2 Announce Type: cross  Abstract: In the rapidly evolving landscape of AI-mediated communication (AIMC), tools powered by Large Language Models (LLMs) are becoming integral to interpersonal communication. Employing a mixed-methods approach, we conducted a one-week diary and interview study to explore users' perceptions of these tools' ability to: 1) support interpersonal communication in the short-term, and 2) lead to potential long-term effects. Our findings indicate that participants view AIMC support favorably, citing benefits such as increased communication confidence, and finding precise language to express their thoughts, navigating linguistic and cultural barriers. However, the study also uncovers current limitations of AIMC tools, including verbosity, unnatural responses, and excessive emotional intensity. These shortcomings are further exacerbated by user concerns about inauthenticity and potential overreliance on the technology. Furthermore, we identified fou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30913;&#21270;&#21160;&#21147;&#23398;&#21407;&#29702;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20197;&#32039;&#23494;&#21453;&#26144;&#20154;&#31867;&#20915;&#31574;&#21160;&#21147;&#23398;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#20010;&#20307;&#36873;&#25321;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.00267</link><description>&lt;p&gt;
&#20559;&#22909;&#30340;&#29289;&#29702;&#23398;&#65306;&#36890;&#36807;&#30913;&#21270;&#21160;&#21147;&#23398;&#25581;&#31034;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Physics of Preference: Unravelling Imprecision of Human Preferences through Magnetisation Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30913;&#21270;&#21160;&#21147;&#23398;&#21407;&#29702;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20197;&#32039;&#23494;&#21453;&#26144;&#20154;&#31867;&#20915;&#31574;&#21160;&#21147;&#23398;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#20010;&#20307;&#36873;&#25321;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;&#20559;&#22909;&#36870;&#36716;&#31561;&#24726;&#35770;&#24615;&#20915;&#31574;&#34892;&#20026;&#36890;&#24120;&#28304;&#33258;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#25110;&#26434;&#20081;&#24773;&#20917;&#12290;&#36890;&#36807;&#21033;&#29992;&#38081;&#30913;&#32435;&#31859;&#32467;&#26500;&#20013;&#30340;&#30913;&#21270;&#36870;&#36716;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#32039;&#23494;&#21453;&#26144;&#20102;&#20154;&#31867;&#20915;&#31574;&#21160;&#21147;&#23398;&#12290;&#32463;&#36807;&#19968;&#31995;&#21015;&#24515;&#29702;&#23398;&#25968;&#25454;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29087;&#32451;&#22320;&#25429;&#25417;&#20102;&#20010;&#20307;&#36873;&#25321;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#29289;&#29702;&#23398;&#21644;&#24515;&#29702;&#23398;&#30340;&#34701;&#21512;&#20026;&#29702;&#35299;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#31934;&#30830;&#24615;&#24320;&#36767;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#32463;&#20856;&#21644;&#37327;&#23376;&#29289;&#29702;&#27169;&#22411;&#23545;&#20154;&#31867;&#34892;&#20026;&#21644;&#20915;&#31574;&#30340;&#25506;&#32034;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00267v2 Announce Type: replace-cross  Abstract: Paradoxical decision-making behaviours such as preference reversal often arise from imprecise or noisy human preferences. Harnessing the physical principle of magnetisation reversal in ferromagnetic nanostructures, we developed a model that closely reflects human decision-making dynamics. Tested against a spectrum of psychological data, our model adeptly captures the complexities inherent in individual choices. This blend of physics and psychology paves the way for fresh perspectives on understanding the imprecision of human decision-making processes, extending the reach of the current classical and quantum physical models of human behaviour and decision-making.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#32593;&#32476;&#24182;&#23454;&#26102;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#19982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2302.02181</link><description>&lt;p&gt;
&#27169;&#22411;&#25340;&#25509;&#19982;&#21487;&#35270;&#21270;&#65306;&#22914;&#20309;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#23454;&#26102;&#21453;&#36716;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02181
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#32593;&#32476;&#24182;&#23454;&#26102;&#21453;&#36716;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#19982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#19982;&#21033;&#29992;1x1&#21367;&#31215;&#30340;GAN&#29983;&#25104;&#22120;&#25340;&#25509;&#36215;&#26469;&#37325;&#24314;&#28608;&#27963;&#12290;&#25105;&#20204;&#22312;AFHQ&#37326;&#29983;&#25968;&#25454;&#38598;&#12289;ImageNet1K&#30340;&#21160;&#29289;&#22270;&#20687;&#20197;&#21450;&#26579;&#33394;&#32452;&#32455;&#26679;&#26412;&#30340;&#30495;&#23454;&#25968;&#23383;&#30149;&#29702;&#25195;&#25551;&#22270;&#20687;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#22788;&#29702;&#26102;&#38388;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02181v2 Announce Type: replace-cross  Abstract: In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, making this approach promising for practical applications.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;BFAR&#21644;BFRR&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.13664</link><description>&lt;p&gt;
&#20351;&#29992;von Mises-Fisher&#28151;&#21512;&#27169;&#22411;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Gender Bias in Face Recognition Using the von Mises-Fisher Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.13664
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;BFAR&#21644;BFRR&#65292;&#24182;&#36890;&#36807;&#21518;&#22788;&#29702;&#26041;&#27861;&#20943;&#36731;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26085;&#24120;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#35768;&#22810;&#35843;&#26597;&#26174;&#31034;&#35768;&#22810;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#65292;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#23376;&#32452;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#65292;&#36825;&#20419;&#20351;&#20174;&#19994;&#32773;&#24320;&#21457;&#20855;&#26377;&#19968;&#33268;/&#21487;&#27604;&#24615;&#33021;&#30340;&#20844;&#24179;&#31995;&#32479;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#38754;&#37096;&#35782;&#21035;&#32593;&#32476;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;BFAR&#21644;BFRR&#65292;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#22266;&#26377;&#30340;&#37096;&#32626;&#38656;&#27714;&#12290;&#21463;&#20960;&#20309;&#32771;&#34385;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#65292;&#20854;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#23545;&#21463;&#27495;&#35270;&#20122;&#32452;&#26377;&#26356;&#22810;&#34920;&#31034;&#33021;&#21147;&#12290;&#23427;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;Fa
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.13664v2 Announce Type: replace-cross  Abstract: In spite of the high performance and reliability of deep learning algorithms in a wide range of everyday applications, many investigations tend to show that a lot of models exhibit biases, discriminating against specific subgroups of the population (e.g. gender, ethnicity). This urges the practitioner to develop fair systems with a uniform/comparable performance across sensitive groups. In this work, we investigate the gender bias of deep Face Recognition networks. In order to measure this bias, we introduce two new metrics, $\mathrm{BFAR}$ and $\mathrm{BFRR}$, that better reflect the inherent deployment needs of Face Recognition systems. Motivated by geometric considerations, we mitigate gender bias through a new post-processing methodology which transforms the deep embeddings of a pre-trained model to give more representation power to discriminated subgroups. It consists in training a shallow neural network by minimizing a Fa
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2205.12944</link><description>&lt;p&gt;
&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learning in Mean Field Games: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.12944
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#22330;&#21338;&#24328;&#30340;&#32467;&#21512;&#26377;&#26395;&#22312;&#24456;&#22823;&#35268;&#27169;&#19978;&#35299;&#20915;&#28216;&#25103;&#30340;&#22343;&#34913;&#21644;&#31038;&#20250;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21512;&#20316;&#21644;&#21512;&#20316;&#28216;&#25103;&#22312;&#25317;&#26377;&#22823;&#37327;&#29609;&#23478;&#26102;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#24120;&#21464;&#24471;&#38590;&#20197;&#35299;&#20915;&#12290;&#22343;&#22330;&#21338;&#24328;(Mean Field Games, MFGs)&#30001;Lasry&#21644;Lions&#20197;&#21450;Huang&#65292;Caines&#21644;Malham\'e&#24341;&#20837;&#65292;&#20381;&#38752;&#22343;&#22330;&#36817;&#20284;&#20801;&#35768;&#29609;&#23478;&#25968;&#37327;&#22686;&#38271;&#21040;&#26080;&#31351;&#22823;&#12290;&#20256;&#32479;&#35299;&#20915;&#36825;&#20123;&#28216;&#25103;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35299;&#20915;&#24102;&#26377;&#23545;&#27169;&#22411;&#30340;&#23436;&#20840;&#20102;&#35299;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning, RL)&#20986;&#29616;&#22312;&#35299;&#20915;&#35268;&#27169;&#22797;&#26434;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;RL&#21644;MFGs&#30340;&#32467;&#21512;&#26377;&#26395;&#35299;&#20915;&#22312;&#20154;&#21475;&#35268;&#27169;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#24222;&#22823;&#30340;&#28216;&#25103;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#36817;&#36805;&#36895;&#22686;&#38271;&#30340;&#20851;&#20110;RL&#26041;&#27861;&#22312;MFGs&#20013;&#23398;&#20064;&#22343;&#34913;&#21644;&#31038;&#20132;&#26368;&#20248;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;M&#20013;&#26368;&#24120;&#35265;&#30340;&#35774;&#32622;(&#38745;&#24577;&#12289;&#31283;&#24577;&#21644;&#36827;&#21270;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;</title><link>https://arxiv.org/abs/2203.00156</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#19982;&#26426;&#22120;&#20154;&#38388;&#25509;&#25918;&#32622;&#20132;&#25509;&#30340;&#20027;&#21160;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Preemptive Motion Planning for Human-to-Robot Indirect Placement Handovers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.00156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#30340;&#20154;&#26426;&#22242;&#38431;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#26368;&#22522;&#26412;&#30340;&#21327;&#20316;&#20219;&#21153;&#20043;&#19968;&#26159;&#29289;&#20307;&#20132;&#25509;&#12290;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20132;&#25509;&#21487;&#20197;&#37319;&#29992;&#20004;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#30452;&#25509;&#25163;&#23545;&#25163;&#25110;&#65288;2&#65289;&#38388;&#25509;&#25163;&#23545;&#25918;&#32622;&#20877;&#25235;&#21462;&#12290;&#21518;&#19968;&#31181;&#26041;&#24335;&#30830;&#20445;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#26368;&#23567;&#25509;&#35302;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#38656;&#35201;&#31561;&#24453;&#29289;&#20307;&#39318;&#20808;&#25918;&#32622;&#22312;&#34920;&#38754;&#19978;&#32780;&#22686;&#21152;&#38386;&#32622;&#26102;&#38388;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#36825;&#31181;&#38386;&#32622;&#26102;&#38388;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#21363;&#29289;&#20307;&#23558;&#34987;&#25918;&#32622;&#22312;&#20309;&#22788;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#39044;&#20808;&#20197;&#20219;&#20309;&#19968;&#31181;&#26377;&#29983;&#20135;&#21147;&#30340;&#26041;&#24335;&#34892;&#21160;&#65292;&#39044;&#27979;&#21644;&#36816;&#21160;&#35268;&#21010;&#24517;&#39035;&#23454;&#26102;&#21457;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#27979;&#35268;&#21010;&#27969;&#31243;&#65292;&#20801;&#35768;&#26426;&#22120;&#20154;&#20027;&#21160;&#26397;&#30528;&#20154;&#31867;&#20195;&#29702;&#30340;&#39044;&#23450;&#25918;&#32622;&#20301;&#32622;&#31227;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.00156v3 Announce Type: replace-cross  Abstract: As technology advances, the need for safe, efficient, and collaborative human-robot-teams has become increasingly important. One of the most fundamental collaborative tasks in any setting is the object handover. Human-to-robot handovers can take either of two approaches: (1) direct hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach ensures minimal contact between the human and robot but can also result in increased idle time due to having to wait for the object to first be placed down on a surface. To minimize such idle time, the robot must preemptively predict the human intent of where the object will be placed. Furthermore, for the robot to preemptively act in any sort of productive manner, predictions and motion planning must occur in real-time. We introduce a novel prediction-planning pipeline that allows the robot to preemptively move towards the human agent's intended placement location using g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2201.11653</link><description>&lt;p&gt;
&#12298;SGD&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#35268;&#21017;&#23398;&#21040;&#30340;&#34920;&#31034;&#65306;&#21464;&#21270;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26465;&#20214;&#12299;
&lt;/p&gt;
&lt;p&gt;
Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#32780;&#20114;&#19981;&#24178;&#25200;&#12290;&#20943;&#23569;&#20114;&#30456;&#24178;&#25200;&#30340;&#26377;&#25928;&#26041;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#20013;&#25214;&#21040;&#12290;&#26681;&#25454;Aljundi&#31561;&#20154;&#21644;Hadsell&#31561;&#20154;&#30340;&#35266;&#28857;&#65292;&#22312;&#34920;&#31034;&#27700;&#24179;&#26045;&#21152;&#31232;&#30095;&#24615;&#23545;&#36830;&#32493;&#23398;&#20064;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#31232;&#30095;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#40723;&#21169;&#21442;&#25968;&#20043;&#38388;&#30340;&#23569;&#37325;&#21472;&#65292;&#23548;&#33268;&#26356;&#23569;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#24341;&#36215;&#36739;&#23569;&#30340;&#24178;&#25200;&#65292;&#22240;&#20026;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#23450;&#21709;&#24212;&#23558;&#20943;&#23569;&#19982;&#20854;&#20182;&#21442;&#25968;&#30340;&#37325;&#21472;&#26426;&#20250;&#12290;&#32771;&#34385;&#21040;&#20154;&#33041;&#22312;&#19968;&#29983;&#20013;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#28982;&#22686;&#21152;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#26465;&#20214;&#21487;&#33021;&#20026;&#20102;&#35299;&#22823;&#33041;&#21151;&#33021;&#25552;&#20379;&#35265;&#35299;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#39044;&#27979;&#12289;&#35268;&#21010;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#20197;&#21450;&#20854;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#20013;&#30340;&#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.09491</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#31354;&#38388;&#21644;&#35268;&#21010;: &#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Memory, Space, and Planning: Multiscale Predictive Representations. (arXiv:2401.09491v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32508;&#21512;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#35760;&#24518;&#19982;&#39044;&#27979;&#12289;&#35268;&#21010;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#20197;&#21450;&#20854;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#20013;&#30340;&#22810;&#23610;&#24230;&#39044;&#27979;&#34920;&#31034;&#65292;&#20026;&#25105;&#20204;&#29702;&#35299;&#22823;&#33041;&#20013;&#30340;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#19982;&#39044;&#27979;&#21644;&#35268;&#21010;&#23494;&#19981;&#21487;&#20998;&#12290;&#29983;&#29289;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#28789;&#27963;&#34892;&#20026;&#21462;&#20915;&#20110;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20174;&#36807;&#21435;&#20013;&#23398;&#20064;&#21644;&#39044;&#27979;&#26410;&#26469;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#31456;&#22238;&#39038;&#20102;&#35745;&#31639;&#12289;&#34892;&#20026;&#21644;&#31070;&#32463;&#35777;&#25454;&#65292;&#34920;&#26126;&#36825;&#20123;&#36807;&#31243;&#20381;&#36182;&#20110;&#23398;&#20064;&#32463;&#39564;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#21363;&#35748;&#30693;&#22320;&#22270;&#65292;&#24182;&#24471;&#20986;&#20004;&#20010;&#20851;&#38190;&#35201;&#28857;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#35760;&#24518;&#32467;&#26500;&#22312;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#36136;&#65288;PFC&#65289;&#23618;&#27425;&#32467;&#26500;&#20013;&#32452;&#32455;&#20026;&#22810;&#23610;&#24230;&#12289;&#32039;&#20945;&#30340;&#39044;&#27979;&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#39044;&#27979;&#24615;&#35760;&#24518;&#32467;&#26500;&#23545;&#28023;&#39532;&#20307;&#21644;PFC&#30340;&#20114;&#34917;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#26082;&#33021;&#20351;&#36807;&#21435;&#30340;&#35814;&#32454;&#21644;&#36830;&#36143;&#30340;&#20107;&#20214;&#22238;&#24518;&#36215;&#26469;&#65292;&#20063;&#33021;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25512;&#24191;&#32463;&#39564;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#12290;&#36825;&#20123;&#35265;&#35299;&#25512;&#21160;&#20102;&#25105;&#20204;&#23545;&#22823;&#33041;&#20013;&#35760;&#24518;&#21644;&#35268;&#21010;&#26426;&#21046;&#30340;&#29702;&#35299;&#65292;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.04812</link><description>&lt;p&gt;
&#37319;&#26679;&#19982;&#26463;&#32538;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-and-Bound for Non-Convex Optimization. (arXiv:2401.04812v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Monte Carlo Tree Search (MCTS)&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#25968;&#20540;&#19978;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#21644;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#65292;&#36991;&#20813;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#31215;&#26497;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#26631;&#20934;&#26041;&#27861;&#65292;&#22914;&#20998;&#25903;&#21644;&#26463;&#32538;&#65292;&#32500;&#25252;&#21487;&#29992;&#20110;&#31995;&#32479;&#21098;&#26525;&#30340;&#20998;&#21306;&#26641;&#12290;&#26641;&#30340;&#22823;&#23567;&#38543;&#32500;&#24230;&#30340;&#22686;&#21152;&#32780;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#25913;&#36827;&#20102;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;(MCTS)&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#26631;&#20934;&#30340;&#35775;&#38382;&#35745;&#25968;&#26469;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#32780;&#26159;&#21033;&#29992;&#30446;&#26631;&#30340;&#25968;&#20540;&#19978;&#20272;&#35745;&#20316;&#20026;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#65292;&#24182;&#32771;&#34385;&#37319;&#26679;&#20272;&#35745;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#36991;&#20813;&#20102;&#36890;&#24120;&#22266;&#23450;&#32452;&#21512;&#27169;&#24335;&#30340;&#26641;&#29983;&#38271;&#65292;&#24182;&#31215;&#26497;&#22320;&#32553;&#23567;&#21040;&#26377;&#24076;&#26395;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#22312;&#39640;&#32500;&#38750;&#20984;&#20248;&#21270;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard approaches for global optimization of non-convex functions, such as branch-and-bound, maintain partition trees to systematically prune the domain. The tree size grows exponentially in the number of dimensions. We propose new sampling-based methods for non-convex optimization that adapts Monte Carlo Tree Search (MCTS) to improve efficiency. Instead of the standard use of visitation count in Upper Confidence Bounds, we utilize numerical overapproximations of the objective as an uncertainty metric, and also take into account of sampled estimates of first-order and second-order information. The Monte Carlo tree in our approach avoids the usual fixed combinatorial patterns in growing the tree, and aggressively zooms into the promising regions, while still balancing exploration and exploitation. We evaluate the proposed algorithms on high-dimensional non-convex optimization benchmarks against competitive baselines and analyze the effects of the hyper parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04620</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#30340;Agent&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Agent&#36234;&#26469;&#36234;&#22810;&#22320;&#28183;&#36879;&#21040;&#20154;&#31867;&#29983;&#20135;&#21644;&#29983;&#27963;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20984;&#26174;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#21069;AI&#31995;&#32479;&#30340;&#23545;&#40784;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#20154;&#20026;&#24178;&#39044;&#23545;LLM&#36827;&#34892;&#34987;&#21160;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;Agent&#20855;&#26377;&#25509;&#21463;&#29615;&#22659;&#21453;&#39304;&#21644;&#33258;&#25105;&#36827;&#21270;&#31561;&#29305;&#24615;&#65292;&#20351;&#24471;LLM&#23545;&#40784;&#26041;&#27861;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EvolutionaryAgent&#30340;Agent&#36827;&#21270;&#21644;&#23545;&#40784;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#23558;Agent&#23545;&#40784;&#36716;&#21270;&#20026;&#36866;&#32773;&#29983;&#23384;&#30340;&#28436;&#21270;&#21644;&#36873;&#25321;&#36807;&#31243;&#12290;&#22312;&#31038;&#20250;&#35268;&#33539;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#19982;&#24403;&#21069;&#31038;&#20250;&#35268;&#33539;&#26356;&#22909;&#36866;&#24212;&#30340;Agent&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#29983;&#23384;&#21644;&#20256;&#25773;&#27010;&#29575;&#65292;&#32780;&#23545;&#40784;&#19981;&#36275;&#30340;Agent&#21017;&#36880;&#28176;&#20943;&#23569;&#12290;&#36890;&#36807;&#22810;&#20010;&#35282;&#24230;&#23545;&#19982;&#31038;&#20250;&#35268;&#33539;&#30456;&#23545;&#40784;&#30340;Agent&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11456</link><description>&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#36845;&#20195;&#20559;&#22909;&#23398;&#20064;&#65306;&#22312;KL&#32422;&#26463;&#19979;&#23558;&#29702;&#35770;&#19982;&#23454;&#36341;&#32852;&#31995;&#36215;&#26469;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint. (arXiv:2312.11456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;KL&#32422;&#26463;&#19979;&#30340;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#23454;&#36341;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#23545;&#40784;&#36807;&#31243;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#21363;&#21453;&#21521;KL&#27491;&#21017;&#21270;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;RLHF&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#23545;&#36825;&#20010;&#20844;&#24335;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#20173;&#28982;&#24456;&#24320;&#25918;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#31163;&#32447;&#12289;&#22312;&#32447;&#21644;&#28151;&#21512;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#26397;&#30528;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#23545;&#20449;&#24687;&#29702;&#35770;&#31574;&#30053;&#25913;&#36827;&#39044;&#35328;&#30340;&#31283;&#20581;&#36817;&#20284;&#65292;&#33258;&#28982;&#22320;&#20135;&#29983;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;RLHF&#31639;&#27861;&#12290;&#36825;&#21253;&#25324;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#36845;&#20195;&#29256;&#26412;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#31639;&#27861;&#65292;&#20197;&#21450;&#31163;&#32447;&#24773;&#26223;&#19979;&#30340;&#22810;&#27493;&#25298;&#32477;&#25277;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#23545;&#40784;&#23454;&#39564;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate t
&lt;/p&gt;</description></item><item><title>LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2312.09007</link><description>&lt;p&gt;
LLMind: &#20026;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#19982;AI&#21644;&#29289;&#32852;&#32593;&#36827;&#34892;&#21327;&#35843;&#30340;LLM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09007
&lt;/p&gt;
&lt;p&gt;
LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLMind&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;LLMs&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#65292;&#25552;&#20986;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#35745;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22797;&#26434;&#20219;&#21153;&#30340;&#25191;&#34892;&#26159;&#36890;&#36807;&#25511;&#21046;&#33050;&#26412;&#23454;&#29616;&#30340;&#65292;&#36825;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#21327;&#20316;&#12290;LLMs&#20351;&#29992;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#65288;FSMs&#65289;&#30340;&#35821;&#35328;&#32534;&#30721;&#36716;&#25442;&#26041;&#27861;&#29983;&#25104;&#25511;&#21046;&#33050;&#26412;&#12290;&#35813;&#26694;&#26550;&#36824;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#26512;&#21644;&#21709;&#24212;&#20248;&#21270;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#21644;&#25928;&#26524;&#12290;&#26368;&#32456;&#65292;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#19981;&#20165;&#26088;&#22312;&#21019;&#26032;&#29289;&#32852;&#32593;&#35774;&#22791;&#25511;&#21046;&#21644;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#65292;&#36824;&#20419;&#36827;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLMind, an AI framework that utilizes large language models (LLMs) as a central orchestrator. The framework integrates LLMs with domain-specific AI modules, enabling IoT devices to collaborate effectively in executing complex tasks. The LLM engages in natural conversations with human users via a user-friendly social media platform to come up with a plan to execute complex tasks. In particular, the execution of a complex task, which may involve the collaborations of multiple domain-specific AI modules and IoT devices, is realized through a control script. The LLM generates the control script using a Language-Code transformation approach based on finite-state machines (FSMs). The framework also incorporates semantic analysis and response optimization techniques to enhance speed and effectiveness. Ultimately, this framework is designed not only to innovate IoT device control and enrich user experiences but also to foster an intelligent and integrated IoT device
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;</title><link>http://arxiv.org/abs/2312.06717</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#39318;&#20010;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;AI&#30740;&#31350;&#39046;&#22495;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#32418;&#38431;&#27169;&#22411;&#20197;&#31361;&#20986;&#38544;&#31169;&#39118;&#38505;&#30340;&#24037;&#20316;&#65292;&#23581;&#35797;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#65292;&#20351;&#24471;&#25968;&#25454;&#21487;&#20197;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#39640;&#25928;&#21024;&#38500;&#20197;&#31526;&#21512;&#29616;&#26377;&#30340;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#35797;&#22270;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#24635;&#32467;&#24320;&#21457;&#31639;&#27861;&#12289;&#35777;&#26126;&#23450;&#29702;&#21644;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#30340;&#25216;&#26415;&#30740;&#31350;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#19981;&#26159;&#25105;&#20204;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20316;&#21697;&#20197;&#21450;&#26368;&#36817;&#30340;&#27861;&#24459;&#36827;&#23637;&#30830;&#23454;&#24433;&#21709;&#20102;&#36825;&#20123;&#25216;&#26415;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#22788;&#29702;&#26041;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#31532;&#19968;&#33410;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#23613;&#21147;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#28431;&#25481;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h
&lt;/p&gt;</description></item><item><title>INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.09868</link><description>&lt;p&gt;
INTERVENOR: &#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09868
&lt;/p&gt;
&lt;p&gt;
INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INTERVENOR&#30340;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#65288;INTERactiVE chaiN Of Repairing&#65289;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65288;&#36845;&#20195;&#21028;&#26029;&#12289;&#37325;&#26032;&#24605;&#32771;&#21644;&#20462;&#22797;&#65289;&#65292;&#24182;&#20419;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;INTERVENOR&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#21363;Code Learner&#21644;Code Teacher&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#20462;&#22797;&#20013;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#20114;&#21160;&#26469;&#20462;&#22797;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;Code Learner&#26681;&#25454;Code Teacher&#30340;&#25351;&#23548;&#29983;&#25104;&#21644;&#20462;&#22797;&#20195;&#30721;&#65292;&#32780;Code Teacher&#26681;&#25454;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#38169;&#35823;&#65292;&#24182;&#36845;&#20195;&#29983;&#25104;&#20462;&#22797;&#38142;&#26465;&#65288;CoR&#65289;&#20197;&#24341;&#23548;Code Learner&#30340;&#20195;&#30721;&#20462;&#22797;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;INTERVENOR&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#36716;&#25442;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;GPT-3.5&#27169;&#22411;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;13%&#21644;4.5%&#30340;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;CoR&#33021;&#22815;&#25581;&#31034;bug&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2311.00967</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#35270;&#35273;&#35821;&#35328;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Interpreter for Robot Task Planning. (arXiv:2311.00967v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#21152;&#36895;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#35268;&#21010;&#22120;&#30340;&#21457;&#23637;&#12290;&#21516;&#26102;&#65292;&#31526;&#21495;&#35268;&#21010;&#22120;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#23558;&#36825;&#20004;&#31181;&#36235;&#21183;&#30456;&#32467;&#21512;&#65292;&#21363;&#22810;&#27169;&#24577;&#35268;&#21010;&#38382;&#39064;&#35268;&#33539;&#12290;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#38382;&#39064;&#25551;&#36848;&#65288;PD&#65289;&#65292;&#36825;&#26159;&#35268;&#21010;&#22120;&#29992;&#26469;&#26597;&#25214;&#35745;&#21010;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#20214;&#12290;&#36890;&#36807;&#20174;&#35821;&#35328;&#25351;&#20196;&#21644;&#22330;&#26223;&#35266;&#27979;&#20013;&#29983;&#25104;PD&#65292;&#25105;&#20204;&#21487;&#20197;&#39537;&#21160;&#31526;&#21495;&#35268;&#21010;&#22120;&#22312;&#35821;&#35328;&#24341;&#23548;&#26694;&#26550;&#19979;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vision-Language Interpreter&#65288;ViLaIn&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#36827;&#30340;LLM&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;PD&#12290;ViLaIn&#21487;&#20197;&#36890;&#36807;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#21453;&#39304;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;PD&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;ViLaIn&#21644;&#31526;&#21495;&#35268;&#21010;&#22120;&#33021;&#22815;&#20934;&#30830;&#22320;&#29983;&#25104;&#26377;&#25928;&#30340;&#26426;&#22120;&#20154;&#35745;&#21010;&#21527;&#65311;&#20026;&#20102;&#35780;&#20272;ViLaIn&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#38382;&#39064;&#25551;&#36848;&#29983;&#25104;&#65288;ProDG&#65289;&#25968;&#25454;&#38598;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;&#35813;&#26694;&#26550;&#23558;&#22312;&#35780;&#20272;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are accelerating the development of language-guided robot planners. Meanwhile, symbolic planners offer the advantage of interpretability. This paper proposes a new task that bridges these two trends, namely, multimodal planning problem specification. The aim is to generate a problem description (PD), a machine-readable file used by the planners to find a plan. By generating PDs from language instruction and scene observation, we can drive symbolic planners in a language-guided framework. We propose a Vision-Language Interpreter (ViLaIn), a new framework that generates PDs using state-of-the-art LLM and vision-language models. ViLaIn can refine generated PDs via error message feedback from the symbolic planner. Our aim is to answer the question: How accurately can ViLaIn and the symbolic planner generate valid robot plans? To evaluate ViLaIn, we introduce a novel dataset called the problem description generation (ProDG) dataset. The framework is evaluated wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.19906</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Interpretable Prototype-based Graph Information Bottleneck. (arXiv:2310.19906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#20013;&#23558;&#21407;&#22411;&#23398;&#20064;&#19982;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25104;&#21151;&#23548;&#33268;&#20102;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#21644;&#23545;&#20854;&#39044;&#27979;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#36825;&#20652;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#20026;&#40657;&#30418;&#27169;&#22411;&#25552;&#20379;&#36879;&#26126;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;&#21407;&#22411;&#30340;&#20351;&#29992;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#21407;&#22411;&#26469;&#26263;&#31034;&#24433;&#21709;&#39044;&#27979;&#30340;&#35757;&#32451;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#32473;&#21407;&#22411;&#25552;&#20379;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#36807;&#22810;&#20449;&#24687;&#65292;&#23548;&#33268;&#20851;&#38190;&#23376;&#32467;&#26500;&#30340;&#25490;&#38500;&#25110;&#26080;&#20851;&#23376;&#32467;&#26500;&#30340;&#21253;&#21547;&#65292;&#36825;&#21487;&#20197;&#38480;&#21046;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#31216;&#20026;&#35299;&#37322;&#24615;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#22270;&#20449;&#24687;&#29942;&#39048; (PGIB)&#65292;&#23558;&#21407;&#22411;&#23398;&#20064;&#32435;&#20837;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65292;&#20026;&#21407;&#22411;&#25552;&#20379;&#36755;&#20837;&#22270;&#30340;&#20851;&#38190;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNNs) has led to a need for understanding their decision-making process and providing explanations for their predictions, which has given rise to explainable AI (XAI) that offers transparent explanations for black-box models. Recently, the use of prototypes has successfully improved the explainability of models by learning prototypes to imply training graphs that affect the prediction. However, these approaches tend to provide prototypes with excessive information from the entire graph, leading to the exclusion of key substructures or the inclusion of irrelevant substructures, which can limit both the interpretability and the performance of the model in downstream tasks. In this work, we propose a novel framework of explainable GNNs, called interpretable Prototype-based Graph Information Bottleneck (PGIB) that incorporates prototype learning within the information bottleneck framework to provide prototypes with the key subgraph from the input graph
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.08949</link><description>&lt;p&gt;
Easier Multimodal Generation: Diffusion Models Meet LLMs
&lt;/p&gt;
&lt;p&gt;
Making Multimodal Generation Easier: When Diffusion Models Meet LLMs. (arXiv:2310.08949v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08949
&lt;/p&gt;
&lt;p&gt;
EasyGen&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#26356;&#23481;&#26131;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;&#30456;&#27604;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;EasyGen&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292; &#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#65292;&#24182;&#19988;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#65292;&#36824;&#33021;&#22815;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EasyGen&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20027;&#35201;&#20381;&#36182;&#20110;&#32534;&#30721;&#22120;&#22914;CLIP&#25110;ImageBind&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#26469;&#26725;&#25509;&#27169;&#24577;&#20043;&#38388;&#24046;&#36317;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;EasyGen&#22522;&#20110;&#19968;&#20010;&#21517;&#20026;BiDiffuser&#30340;&#21452;&#21521;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#65292;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#30340;&#27169;&#24577;&#20132;&#20114;&#12290;EasyGen&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#23618;&#23558;BiDiffuser&#21644;LLM&#36827;&#34892;&#38598;&#25104;&#65292;&#22788;&#29702;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#29983;&#25104;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38480;&#20110;&#29983;&#25104;&#25991;&#26412;&#22238;&#22797;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#19981;&#21516;&#65292;EasyGen&#36824;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;LLM&#21019;&#24314;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#30001;BiDiffuser&#35299;&#37322;&#29983;&#25104;&#36866;&#24403;&#30340;&#35270;&#35273;&#22238;&#22797;&#26469;&#20419;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#20102;EasyGen&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#35757;&#32451;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual responses. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07138</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#33258;&#28982;&#22320;&#20307;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21407;&#29702;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#21644;MTL&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#36830;&#25509;&#65292;&#20294;&#22312;&#35774;&#35745;&#26126;&#30830;&#23558;MTL&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#30340;&#31070;&#32463;&#32467;&#26500;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#65288;DTR&#65289;&#65292;&#19968;&#31181;&#23545;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#31616;&#21333;&#38468;&#21152;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#28608;&#27963;&#27169;&#22411;&#20013;&#30340;&#23376;&#36890;&#36947;&#26469;&#20026;&#21333;&#20010;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#12290;DTR&#30340;&#29305;&#21035;&#21560;&#24341;&#20154;&#20043;&#22788;&#22312;&#20110;&#23427;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65306;&#65288;1&#65289;&#20219;&#21153;&#20146;&#21644;&#24615;&#65306;DTR&#20026;&#30456;&#37051;&#26102;&#38388;&#27493;&#30340;&#20219;&#21153;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#65292;&#24182;&#23558;&#28608;&#27963;&#30340;&#36890;&#36947;&#20316;&#20026;&#28369;&#21160;&#31383;&#21475;&#36890;&#36807;&#26102;&#38388;&#27493;&#36827;&#34892;&#31227;&#21160;&#65292;&#21033;&#29992;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#22266;&#26377;&#30340;&#24378;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep
&lt;/p&gt;</description></item><item><title>Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06692</link><description>&lt;p&gt;
Meta-CoT:&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#30340;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models. (arXiv:2310.06692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06692
&lt;/p&gt;
&lt;p&gt;
Meta-CoT&#26159;&#19968;&#31181;&#22312;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#33021;&#22815;&#36890;&#29992;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21033;&#29992;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#31181;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#38142;&#20197;&#20316;&#20026;&#24471;&#20986;&#31572;&#26696;&#30340;&#22522;&#26412;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;CoT&#26041;&#27861;&#35201;&#20040;&#20165;&#20165;&#20351;&#29992;&#31867;&#20284;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#30340;&#36890;&#29992;&#25552;&#31034;&#65292;&#35201;&#20040;&#36807;&#20110;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#28436;&#31034;&#26469;&#36798;&#21040;&#29702;&#24819;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#40511;&#27807;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-CoT&#65292;&#19968;&#31181;&#22312;&#26410;&#30693;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#30340;&#28151;&#21512;&#20219;&#21153;&#22330;&#26223;&#20013;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;CoT&#25552;&#31034;&#26041;&#27861;&#12290;Meta-CoT&#39318;&#20808;&#26681;&#25454;&#36755;&#20837;&#38382;&#39064;&#23545;&#22330;&#26223;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#20197;&#33258;&#21160;&#27169;&#24335;&#20174;&#30456;&#24212;&#30340;&#25968;&#25454;&#27744;&#20013;&#26500;&#24314;&#22810;&#26679;&#30340;&#28436;&#31034;&#12290;Meta-CoT&#22312;&#21313;&#20010;&#20844;&#20849;&#22522;&#20934;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#20248;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Meta-CoT&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on handcrafted task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose Meta-CoT, a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. Meta-CoT firstly categorizes the scenario based on the input question and subsequently constructs diverse demonstrations from the corresponding data pool in an automatic pattern. Meta-CoT simultaneously enjoys remarkable performances on ten public benchmark reasoning tasks and superior generalization capabilities. Notably, Meta-CoT achieves the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04270</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks. (arXiv:2310.04270v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#38646;&#26679;&#26412;LLMs&#22312;&#23567;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#29978;&#33267;&#36229;&#36807;&#20102;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23427;&#20204;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;6&#20010;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;26&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#28909;&#38376;LLMs&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#21644;&#27604;&#36739;&#30340;&#30740;&#31350;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#38646;&#26679;&#26412;LLMs&#29978;&#33267;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20351;LLMs&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#22791;&#20102;&#24456;&#24378;&#30340;&#19987;&#19994;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#27809;&#26377;&#19968;&#20010;LLM&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with 
&lt;/p&gt;</description></item><item><title>SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03013</link><description>&lt;p&gt;
SemiReward: &#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SemiReward: A General Reward Model for Semi-supervised Learning. (arXiv:2310.03013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03013
&lt;/p&gt;
&lt;p&gt;
SemiReward&#26159;&#19968;&#20010;&#36890;&#29992;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#26469;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#35757;&#32451;&#26694;&#26550;&#21644;&#20266;&#26631;&#31614;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#21306;&#20998;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#36991;&#20813;&#30830;&#35777;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#38480;&#21046;&#20110;&#39044;&#23450;&#20041;&#30340;&#26041;&#26696;&#25110;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#31574;&#30053;&#65292;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#26631;&#31614;&#12289;&#24555;&#36895;&#25910;&#25947;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#22870;&#21169;&#26694;&#26550;&#65288;SemiReward&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#22870;&#21169;&#20998;&#25968;&#20197;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#31867;&#22411;&#21644;&#22330;&#26223;&#19979;&#19982;&#20027;&#27969;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20351;&#29992;&#12290;&#20026;&#20102;&#20943;&#23569;&#30830;&#35777;&#20559;&#35265;&#65292;&#22312;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#31574;&#30053;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#12290;&#36890;&#36807;&#22312;&#19977;&#31181;&#27169;&#24577;&#30340;13&#20010;&#26631;&#20934;&#21322;&#30417;&#30563;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;SemiReward&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves sig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11087</link><description>&lt;p&gt;
Embed-Search-Align: &#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;DNA&#24207;&#21015;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Embed-Search-Align: DNA Sequence Alignment using Transformer Models. (arXiv:2309.11087v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11087
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#24207;&#21015;&#23545;&#40784;&#28041;&#21450;&#23558;&#30701;DNA&#35835;&#21462;&#20998;&#37197;&#21040;&#24191;&#27867;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19978;&#30340;&#26368;&#21487;&#33021;&#20301;&#32622;&#12290;&#36825;&#20010;&#36807;&#31243;&#23545;&#20110;&#21508;&#31181;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21464;&#24322;&#35843;&#29992;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34920;&#35266;&#22522;&#22240;&#32452;&#23398;&#12290;&#20256;&#32479;&#26041;&#27861;&#32463;&#36807;&#25968;&#21313;&#24180;&#30340;&#25913;&#36827;&#65292;&#20197;&#20004;&#20010;&#27493;&#39588;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20808;&#36827;&#34892;&#22522;&#22240;&#32452;&#32034;&#24341;&#65292;&#28982;&#21518;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#20197;&#30830;&#23450;&#32473;&#23450;&#35835;&#21462;&#30340;&#21487;&#33021;&#20301;&#32622;&#12290;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;Transformer&#26550;&#26500;&#20026;DNA&#24207;&#21015;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#22312;&#28041;&#21450;&#20998;&#31867;&#30701;DNA&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26089;&#26399;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#26816;&#27979;&#32534;&#30721;&#21644;&#38750;&#32534;&#30721;&#21306;&#22495;&#20197;&#21450;&#35782;&#21035;&#22686;&#24378;&#23376;&#21644;&#21551;&#21160;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24207;&#21015;&#23545;&#40784;&#20219;&#21153;&#65292;&#23545;&#40784;&#20219;&#21153;&#30340;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23545;&#24212;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16071</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Synthesis via Class-Adaptive Cross-Attention. (arXiv:2308.16071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20840;&#23616;&#22270;&#20687;&#32479;&#35745;&#20449;&#24687;&#65292;&#23548;&#33268;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#65292;&#24182;&#24341;&#36215;&#35832;&#22914;&#33394;&#24425;&#25110;&#20809;&#29031;&#20998;&#24067;&#20559;&#31227;&#31561;&#20840;&#23616;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22120;&#38656;&#35201;&#35821;&#20041;&#24067;&#23616;&#26469;&#26144;&#23556;&#26679;&#24335;&#65292;&#23545;&#29305;&#24449;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#23545;&#40784;&#32422;&#26463;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20195;&#26367;&#21453;&#24402;&#19968;&#21270;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26679;&#24335;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In semantic image synthesis, the state of the art is dominated by methods that use spatially-adaptive normalization layers, which allow for excellent visual generation quality and editing versatility. Granted their efficacy, recent research efforts have focused toward finer-grained local style control and multi-modal generation. By construction though, such layers tend to overlook global image statistics leading to unconvincing local style editing and causing global inconsistencies such as color or illumination distribution shifts. Also, the semantic layout is required for mapping styles in the generator, putting a strict alignment constraint over the features. In response, we designed a novel architecture where cross-attention layers are used in place of de-normalization ones for conditioning the image generation. Our model inherits the advantages of both solutions, retaining state-of-the-art reconstruction quality, as well as improved global and local style transfer. Code and models 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.09842</link><description>&lt;p&gt;
&#29992;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26522;&#20030;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees. (arXiv:2308.09842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;epsilon-ProVe&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36817;&#20284;&#30340;&#26041;&#27861;&#26469;&#26522;&#20030;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23433;&#20840;&#21306;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23433;&#20840;&#21306;&#22495;&#26159;&#20445;&#35777;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#31995;&#32479;&#30340;&#20449;&#20219;&#30340;&#20851;&#38190;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AllDNN-Verification&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#23433;&#20840;&#23646;&#24615;&#21644;&#19968;&#20010;DNN&#65292;&#26522;&#20030;&#23646;&#24615;&#36755;&#20837;&#22495;&#30340;&#25152;&#26377;&#23433;&#20840;&#21306;&#22495;&#65292;&#21363;&#23646;&#24615;&#25104;&#31435;&#30340;&#21306;&#22495;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;#P&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#21483;&#20570;epsilon-ProVe&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32479;&#35745;&#39044;&#27979;&#23481;&#38480;&#38480;&#21046;&#33719;&#24471;&#21487;&#25511;&#20302;&#20272;&#30340;&#36755;&#20986;&#21487;&#36798;&#38598;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#27010;&#29575;&#20445;&#35777;&#30340;&#23433;&#20840;&#21306;&#22495;&#30340;&#32039;&#23494;&#19979;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#36825;&#31181;&#26032;&#22411;&#30340;DNN&#39564;&#35777;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.06770</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#20998;&#26512;&#25552;&#39640; LLM &#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis. (arXiv:2306.06770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#20294;&#26159;&#21333;&#29420;&#30340;&#25552;&#31034;&#24037;&#31243;&#24182;&#19981;&#33021;&#20026;&#26426;&#22120;&#20154;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#24773;&#22659;&#30456;&#20851;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#25193;&#23637;&#21644;&#34917;&#20805;&#25552;&#31034;&#24037;&#31243;&#65292;&#32531;&#35299;&#20854;&#23616;&#38480;&#24615;&#65292;&#20197;&#27492;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#23545; LLMs &#20135;&#29983;&#30340;&#20505;&#36873;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#12289;&#20462;&#22797;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#20154;&#20174; LLM &#20013;&#26816;&#32034;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#19981;&#21516;&#21709;&#24212;&#21518;&#21487;&#20197;&#36798;&#21040;&gt;75% &#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#65292;&#26080;&#38656;&#29992;&#25143;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) offer significant promise as a knowledge source for robotic task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM but alone is insufficient for acquiring relevant, situationally grounded knowledge for an embodied robotic agent learning novel tasks. We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous robot, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how a robot, by retrieving and evaluating a breadth of responses from the LLM, can achieve &gt;75% task completion in one-shot learning without user oversight. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;AutoML&#65289;&#30340;&#21457;&#23637;&#21152;&#36895;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#20250;&#24341;&#21457;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#22240;&#27492;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#32852;&#21512;&#20248;&#21270;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;AutoML&#31995;&#32479;&#65292;&#26412;&#25991;&#21628;&#21505;AutoML&#31995;&#32479;&#24320;&#21457;&#32773;&#24212;&#35813;&#35748;&#35782;&#21040;&#20844;&#24179;&#24615;&#22788;&#29702;&#26410;&#24517;&#26159;&#32431;&#31929;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#37266;&#27492;&#31867;&#31639;&#27861;&#20063;&#20855;&#22791;&#25104;&#20026;&#20844;&#24179;&#30740;&#31350;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08485</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65306;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;AutoML&#30340;&#25351;&#21335;&#19982;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Can Fairness be Automated? Guidelines and Opportunities for Fairness-aware AutoML. (arXiv:2303.08485v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08485
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;AutoML&#65289;&#30340;&#21457;&#23637;&#21152;&#36895;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#20250;&#24341;&#21457;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#22240;&#27492;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#32852;&#21512;&#20248;&#21270;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;AutoML&#31995;&#32479;&#65292;&#26412;&#25991;&#21628;&#21505;AutoML&#31995;&#32479;&#24320;&#21457;&#32773;&#24212;&#35813;&#35748;&#35782;&#21040;&#20844;&#24179;&#24615;&#22788;&#29702;&#26410;&#24517;&#26159;&#32431;&#31929;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#37266;&#27492;&#31867;&#31639;&#27861;&#20063;&#20855;&#22791;&#25104;&#20026;&#20844;&#24179;&#30740;&#31350;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#24341;&#20837;&#20102;&#19968;&#20123;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31995;&#32479;&#24320;&#21457;&#30340;&#25216;&#26415;&#65292;&#21152;&#36895;&#20102;&#35813;&#36807;&#31243;&#65292;&#38477;&#20302;&#20102;&#26032;&#25163;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;ML&#27169;&#22411;&#30340;&#20915;&#31574;&#21487;&#33021;&#20250;&#22312;&#25105;&#20204;&#30340;&#31038;&#20250;&#20013;&#22797;&#21046;&#12289;&#25918;&#22823;&#25110;&#29978;&#33267;&#24341;&#20837;&#19981;&#20844;&#24179;&#24615;&#65292;&#23545;&#65288;&#32676;&#20307;&#20013;&#30340;&#65289;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#25552;&#20986;&#32852;&#21512;&#20248;&#21270;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#30340;AutoML&#31995;&#32479;&#65292;&#20197;&#20943;&#36731;&#19982;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#25439;&#23475;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#22266;&#26377;&#30340;&#36328;&#23398;&#31185;&#20027;&#39064;&#65292;&#20165;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#38382;&#39064;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;AutoML&#31995;&#32479;&#24320;&#21457;&#32773;&#23545;&#20844;&#24179;&#24615;&#24863;&#30693;AutoML&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#21516;&#26102;&#24341;&#36215;AutoML&#20316;&#20026;&#20844;&#24179;&#30740;&#31350;&#24037;&#20855;&#30340;&#28508;&#21147;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19981;&#21516;&#26041;&#24335;&#30340;&#20844;&#24179;&#30456;&#20851;&#21361;&#23475;&#21450;&#20854;&#38543;&#20043;&#32780;&#26469;&#30340;&#24433;&#21709;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of automated machine learning (AutoML) introduces techniques that automate parts of the development of machine learning (ML) systems, accelerating the process and reducing barriers for novices. However, decisions derived from ML models can reproduce, amplify, or even introduce unfairness in our societies, causing harm to (groups of) individuals. In response, researchers have started to propose AutoML systems that jointly optimize fairness and predictive performance to mitigate fairness-related harm. However, fairness is a complex and inherently interdisciplinary subject, and solely posing it as an optimization problem can have adverse side effects. With this work, we aim to raise awareness among developers of AutoML systems about such limitations of fairness-aware AutoML, while also calling attention to the potential of AutoML as a tool for fairness research. We present a comprehensive overview of different ways in which fairness-related harm can arise and the ensuing implica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.03445</link><description>&lt;p&gt;
&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#24615;&#21450;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#20197;&#20351;&#29992;&#36739;&#24369;&#20551;&#35774;&#35777;&#26126;&#25910;&#25947;&#30340;&#19968;&#33324;&#26041;&#27861;&#65307;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#26041;&#27861;&#35777;&#26126;&#20102;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20165;&#21487;&#29992;&#20989;&#25968;&#30340;&#26377;&#22122;&#27979;&#37327;&#24773;&#20917;&#19979;&#25214;&#21040;&#38646;&#28857;&#25110;&#22266;&#23450;&#28857;&#12290;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21306;&#20998;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#21644;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#65292;&#22312;&#8220;&#21516;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#27599;&#20010;&#29468;&#27979;&#30340;&#32452;&#20214;&#37117;&#20250;&#22312;&#27599;&#20010;&#26102;&#38388;&#26356;&#26032;&#65292;&#32780;&#22312;&#8220;&#24322;&#27493;&#8221;&#26356;&#26032;&#20013;&#65292;&#20165;&#26356;&#26032;&#19968;&#20010;&#32452;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20013;&#38388;&#24773;&#20917;&#65292;&#31216;&#20026;&#8220;&#25209;&#37327;&#24322;&#27493;&#38543;&#26426;&#36924;&#36817;&#8221;&#65288;BASA&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26102;&#38388;&#28857;&#20165;&#26356;&#26032;&#8220;&#24403;&#21069;&#20272;&#35745;&#35299;&#8221;&#30340;&#19968;&#20123;&#20294;&#19981;&#26159;&#20840;&#37096;&#30340;&#32452;&#20214;&#12290;BASA&#20801;&#35768;&#29992;&#25143;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#35777;&#26126;&#27492;&#31867;&#31639;&#27861;&#25910;&#25947;&#20110;&#25152;&#30740;&#31350;&#26144;&#23556;&#30340;&#22266;&#23450;&#28857;&#12290;&#36825;&#20123;&#25910;&#25947;&#35777;&#26126;&#20351;&#29992;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#24369;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29616;&#26377;&#30340;&#25910;&#25947;&#35777;&#26126;&#35201;&#27714;&#27493;&#38271;&#21442;&#25968;&#20197;&#36866;&#24403;&#30340;&#36895;&#29575;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20165;&#35201;&#27714;&#27599;&#20010;&#32452;&#20214;&#20855;&#26377;&#36275;&#22815;&#30340;&#26356;&#26032;&#39057;&#29575;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#35777;&#26126;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;SARSA&#31639;&#27861;&#30340;&#25209;&#37327;&#24322;&#27493;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic approximation (SA) algorithm is a widely used probabilistic method for finding a zero or a fixed point of a vector-valued funtion, when only noisy measurements of the function are available. In the literature to date, one makes a distinction between ``synchronous'' updating, whereby every component of the current guess is updated at each time, and ``asynchronous'' updating, whereby only one component is updated. In this paper, we study an intermediate situation that we call ``batch asynchronous stochastic approximation'' (BASA), in which, at each time instant, \textit{some but not all} components of the current estimated solution are updated. BASA allows the user to trade off memory requirements against time complexity. We develop a general methodology for proving that such algorithms converge to the fixed point of the map under study. These convergence proofs make use of weaker hypotheses than existing results. Specifically, existing convergence proofs require that the 
&lt;/p&gt;</description></item></channel></rss>