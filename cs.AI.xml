<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12103</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity through Human Feedback. (arXiv:2310.12103v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#22312;&#25552;&#39640;&#23450;&#24615;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24403;&#20165;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#26368;&#22823;&#21270;&#24179;&#22343;&#20154;&#31867;&#20559;&#22909;&#30340;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26426;&#21046;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#35201;&#27714;&#22810;&#26679;&#21270;&#27169;&#22411;&#21709;&#24212;&#30340;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#65292;&#20854;&#25928;&#26524;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33268;&#21147;&#20110;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#23545;&#25163;&#21160;&#23450;&#20041;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#20381;&#36182;&#32422;&#26463;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#20004;&#32773;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#20811;&#26381;RLHF&#21644;QD&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QDHF&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;QD&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;QD&#26041;&#27861;&#30456;&#27604;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19982;QD&#30340;&#25628;&#32034;&#33021;&#21147;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.02299</link><description>&lt;p&gt;
3D&#29289;&#29702;&#31995;&#32479;&#20013;&#23398;&#20064;&#23545;&#31216;&#24615;&#30772;&#32570;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems. (arXiv:2310.02299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31561;&#20215;&#27169;&#22411;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#23436;&#32654;&#23545;&#31216;&#24615;&#30340;&#20551;&#35774;&#26377;&#26102;&#21487;&#33021;&#20250;&#38480;&#21046;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#19982;&#36825;&#20123;&#23545;&#31216;&#24615;&#19981;&#23436;&#20840;&#19968;&#33268;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#20102;&#29992;&#20110;&#24314;&#27169;3D&#29289;&#29702;&#31995;&#32479;&#30340;&#26494;&#24347;&#20843;&#38754;&#20307;&#32676;&#21367;&#31215;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#21367;&#31215;&#25216;&#26415;&#33021;&#22815;&#22312;&#20445;&#25345;&#19982;&#25968;&#25454;&#19968;&#33268;&#30340;&#26368;&#39640;&#31561;&#21464;&#24615;&#27700;&#24179;&#30340;&#21516;&#26102;&#65292;&#21457;&#29616;&#29289;&#29702;&#31995;&#32479;&#20013;&#24494;&#22937;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#12290;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25581;&#31034;&#30456;&#21464;&#20013;&#30340;&#23545;&#31216;&#24615;&#30772;&#32570;&#22240;&#32032;&#65292;&#36824;&#21487;&#20197;&#22312;&#27969;&#20307;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06157</link><description>&lt;p&gt;
Robust-MBDL:&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#30340;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#31561;&#32452;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#23427;&#22312;&#36724;&#25215;&#26426;&#22120;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26059;&#36716;&#26426;&#22120;&#21097;&#20313;&#23551;&#21629;(RUL)&#39044;&#27979;&#21644;&#36816;&#34892;&#29366;&#24577;(CO)&#37492;&#21035;&#30340;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#21253;&#25324;&#20027;&#35201;&#32452;&#20214;&#65306;(1)&#37319;&#29992;LSTM-Autoencoder&#23545;&#25391;&#21160;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65307;(2)&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#20174;&#21435;&#22122;&#25968;&#25454;&#20013;&#29983;&#25104;&#26102;&#22495;&#12289;&#39057;&#22495;&#21644;&#26102;&#39057;&#22495;&#29305;&#24449;&#65307;(3)&#37319;&#29992;&#26032;&#39062;&#32780;&#31283;&#20581;&#30340;&#22810;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#32467;&#26500;&#26469;&#21033;&#29992;&#22810;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;XJTU-SY&#21644;PRONOSTIA&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#20855;&#26377;&#22312;&#36724;&#25215;&#26426;&#22120;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a Robust Multi-branch Deep learning-based system for remaining useful life (RUL) prediction and condition operations (CO) identification of rotating machines is proposed. In particular, the proposed system comprises main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a feature extraction to generate time-domain, frequency-domain, and time-frequency based features from the denoised data; (3) a novel and robust multi-branch deep learning network architecture to exploit the multiple features. The performance of our proposed system was evaluated and compared to the state-of-the-art systems on two benchmark datasets of XJTU-SY and PRONOSTIA. The experimental results prove that our proposed system outperforms the state-of-the-art systems and presents potential for real-life applications on bearing machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09300</link><description>&lt;p&gt;
V2A-Mapper&#65306;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#36731;&#37327;&#32423;&#30340;&#35270;&#21548;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#23398;&#20064;&#24471;&#21040;&#30340;&#20195;&#34920;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21644;&#36801;&#31227;&#33267;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20174;&#22836;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#38899;&#39057;&#27169;&#24577;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#20013;&#65292;&#21033;&#29992;FMs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22768;&#38899;&#26159;&#36328;&#27169;&#24577;&#29983;&#25104;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35270;&#21548;&#29983;&#25104;&#65288;V2A&#65289;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#20174;&#22836;&#35774;&#35745;&#21644;&#26500;&#24314;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;CLIP&#27169;&#22411;&#21644;&#21548;&#35273;CLAP&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;</title><link>http://arxiv.org/abs/2308.06385</link><description>&lt;p&gt;
ZYN&#65306;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#19982;&#26159;&#38750;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
ZYN: Zero-Shot Reward Models with Yes-No Questions. (arXiv:2308.06385v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ZYN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26159;&#38750;&#38382;&#39064;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#22686;&#24378;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#24773;&#24863;&#20248;&#21270;&#21644;&#20010;&#24615;&#21270;&#25552;&#31034;&#29983;&#25104;&#22120;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#23450;&#21521;&#20110;&#26399;&#26395;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25209;&#35780;&#32773;&#65292;&#36890;&#36807;&#19968;&#20010;&#34920;&#31034;&#29992;&#25143;&#20559;&#22909;&#30340;&#26159;&#38750;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#20197;&#38646;&#24335;&#26041;&#24335;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#38646;&#24335;&#22870;&#21169;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#24494;&#35843;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#23398;&#20064;&#20449;&#21495;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#65292;&#23601;&#20687;&#22312;RLAIF&#20013;&#19968;&#26679;&#65307;&#28982;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#20182;&#19978;&#19979;&#25991;&#20013;&#20063;&#26159;&#20860;&#23481;&#30340;&#65292;&#20363;&#22914;&#36136;&#37327;&#22810;&#26679;&#24615;&#25628;&#32034;&#12290;&#36890;&#36807;&#22312;&#19982;&#25991;&#26412;&#29983;&#25104;&#30456;&#20851;&#30340;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#35299;&#27602;&#12289;&#20248;&#21270;&#30005;&#24433;&#35780;&#35770;&#30340;&#24773;&#24863;&#25110;&#20219;&#20309;&#20854;&#20182;&#23646;&#24615;&#12289;&#24341;&#23548;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#30340;&#20851;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#35266;&#28857;&#65292;&#20197;&#21450;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20219;&#21153;&#30340;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;ZYN&#26694;&#26550;&#33021;&#21147;&#30340;&#22823;&#37327;&#35777;&#25454;&#12290;&#20195;&#30721;&#23558;&#22312;\url&#22788;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we address the problem of directing the text generations of a LLM towards a desired behavior, aligning the generated text with the preferences of the human operator. We propose using another language model as a critic, reward model in a zero-shot way thanks to the prompt of a Yes-No question that represents the user preferences, without requiring further labeled data. This zero-shot reward model provides the learning signal to further fine-tune the base LLM using reinforcement learning, as in RLAIF; yet our approach is also compatible in other contexts such as quality-diversity search. Extensive evidence of the capabilities of the proposed ZYN framework is provided through experiments in different domains related to text generation, including detoxification; optimizing sentiment of movie reviews, or any other attribute; steering the opinion about a particular topic the model may have; and personalizing prompt generators for text-to-image tasks. Code to be released at \url
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06221</link><description>&lt;p&gt;
&#20351;&#29992;&#20108;&#38454;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#21644;&#35757;&#32451;&#39640;&#25928;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#20462;&#21098;&#21644;&#22686;&#38271;&#26041;&#27861;&#20197;&#21450;&#20248;&#21270;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22238;&#24402;&#25214;&#21040;&#19968;&#20010;&#21021;&#22987;&#30340;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#36890;&#36807;&#20462;&#21098;&#19981;&#24517;&#35201;&#30340;&#36755;&#20837;&#26469;&#26368;&#23567;&#21270;&#39564;&#35777;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#31867;&#20284;&#20110;Ho-Kashyap&#35268;&#21017;&#30340;&#26041;&#27861;&#25913;&#21892;&#26399;&#26395;&#36755;&#20986;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#36755;&#20986;&#21028;&#21035;&#24335;&#32553;&#25918;&#20026;&#24191;&#20041;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#25209;&#37327;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#35757;&#32451;&#26102;&#38271;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#20462;&#21098;&#19982;&#22686;&#38271;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#28982;&#21518;&#65292;&#23558;&#36755;&#20837;&#21333;&#20803;&#32553;&#25918;&#20026;S&#22411;&#36755;&#20986;&#21333;&#20803;&#30340;&#32593;&#32476;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#39304;&#36865;&#21040;MLP&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20013;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#26550;&#26500;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#20110;d&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#21407;&#21017;&#21644;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03669</link><description>&lt;p&gt;
&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#19979;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;BDCM&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#22312;Pearl&#30340;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#25429;&#25417;&#22240;&#26524;&#24178;&#39044;&#30340;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411;&#65288;DCM&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#20551;&#35774;&#25152;&#26377;&#28151;&#28102;&#22240;&#32032;&#37117;&#26159;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20351;&#24471;DCM&#26080;&#27861;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;DCM&#30340;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#20110;&#21453;&#38376;&#20934;&#21017;&#30340;DCM&#65288;BDCM&#65289;&#65292;&#20854;&#24605;&#24819;&#26681;&#26893;&#20110;&#22312;DAG&#20013;&#25214;&#21040;&#35201;&#21253;&#25324;&#22312;&#25193;&#25955;&#27169;&#22411;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#21464;&#37327;&#30340;&#21453;&#38376;&#20934;&#21017;&#65292;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#23558;DCM&#25193;&#23637;&#21040;&#23384;&#22312;&#26080;&#27861;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#12290;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#26080;&#27861;&#27979;&#37327;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#26356;&#31934;&#30830;&#22320;&#25429;&#25417;&#21040;&#20102;&#21453;&#20107;&#23454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03443</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65288;MDR&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;MDR&#33021;&#22815;&#22312;&#20943;&#23567;&#26041;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#32972;&#26223;&#19979;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20272;&#35745;&#22120;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25240;&#34935;&#38382;&#39064;&#12290;&#21442;&#25968;&#21270;&#26041;&#27861;&#30001;&#20110;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#27169;&#22411;&#32780;&#23548;&#33268;&#20559;&#24046;&#65292;&#32780;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#30001;&#20110;&#26041;&#24046;&#32780;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21028;&#21035;&#24335;&#30340;&#19981;&#33391;&#34892;&#20026;&#25233;&#21046;&#22120;&#65288;MIPS&#65289;&#26469;&#36890;&#36807;&#23545;&#21160;&#20316;&#30340;&#23884;&#20837;&#26469;&#20943;&#23567;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20351;&#20272;&#35745;&#22120;&#26356;&#20934;&#30830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIPS&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#8212;&#8212;&#36793;&#38469;&#21270;&#21452;&#37325;&#31283;&#20581;&#65288;MDR&#65289;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#22312;&#27604;MIPS&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#26159;&#26080;&#20559;&#30340;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;IPS&#30340;&#26041;&#24046;&#20943;&#23567;&#65292;&#36825;&#26159;MIPS&#30340;&#20027;&#35201;&#20248;&#21183;&#12290;&#32463;&#39564;&#23454;&#39564;&#35777;&#23454;&#20102;MDR&#30456;&#23545;&#20110;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.03358</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;POMDP&#20013;&#22522;&#20110;&#22312;&#32447;&#32858;&#31867;&#26631;&#31614;&#30340;&#31163;&#25955;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Discrete Message via Online Clustering Labels in Decentralized POMDP. (arXiv:2308.03358v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#22238;&#25253;&#24046;&#36317;&#19978;&#30028;&#65292;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#38382;&#39064;&#36716;&#21270;&#20026;&#31163;&#25955;&#28040;&#24687;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#37327;&#21270;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#20449;&#24320;&#38144;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22909;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#36890;&#20449;&#23545;&#20110;&#35299;&#20915;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#23558;&#26412;&#22320;&#20449;&#24687;/&#29305;&#24449;&#32534;&#30721;&#25104;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#28040;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#40657;&#30418;&#26041;&#27861;&#26080;&#27861;&#23545;&#26399;&#26395;&#22238;&#25253;&#25552;&#20379;&#20219;&#20309;&#37327;&#21270;&#20445;&#35777;&#65292;&#24120;&#24120;&#23548;&#33268;&#29983;&#25104;&#36890;&#20449;&#24320;&#38144;&#39640;&#12289;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#36830;&#32493;&#28040;&#24687;&#12290;&#26412;&#25991;&#22312;&#29702;&#24819;&#31574;&#30053;&#19982;&#26368;&#20248;&#37096;&#20998;&#21487;&#35266;&#23519;&#31574;&#30053;&#20043;&#38388;&#24314;&#31435;&#20102;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#12290;&#35813;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#37325;&#26032;&#23450;&#20041;&#20026;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#26412;&#22320;&#35266;&#23519;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20316;&#20026;&#32858;&#31867;&#26631;&#31614;&#65292;&#24182;&#19988;&#22238;&#25253;&#24046;&#36317;&#30340;&#19978;&#30028;&#20316;&#20026;&#32858;&#31867;&#25439;&#22833;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#19978;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#22320;&#31616;&#21333;&#30340;&#28040;&#24687;&#29983;&#25104;&#20989;&#25968;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is crucial for solving cooperative Multi-Agent Reinforcement Learning tasks in Partially-Observable Markov Decision Processes. Existing works often rely on black-box methods to encode local information/features into messages shared with other agents. However, such black-box approaches are unable to provide any quantitative guarantees on the expected return and often lead to the generation of continuous messages with high communication overhead and poor interpretability. In this paper, we establish an upper bound on the return gap between an ideal policy with full observability and an optimal partially-observable policy with discrete communication. This result enables us to recast multi-agent communication into a novel online clustering problem over the local observations at each agent, with messages as cluster labels and the upper bound on the return gap as clustering loss. By minimizing the upper bound, we propose a surprisingly simple design of message generation functi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02668</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20027;&#23548;&#33539;&#24335;&#26159;&#20381;&#36182;&#20110;&#23436;&#20840;&#24102;&#27880;&#37322;&#30340;&#35757;&#32451;&#22270;&#20687;&#65292;&#36825;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20381;&#36182;&#24182;&#25552;&#39640;&#32467;&#26524;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20197;&#38480;&#21046;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#39062;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#26174;&#33879;&#25913;&#36827;&#24072;&#29983;&#33976;&#39311;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;(i)&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#25913;&#36827;&#20102;&#33976;&#39311;&#26041;&#27861;&#65292;(ii)&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23454;&#20363;&#20998;&#21106;&#26550;&#26500;&#12289;&#20027;&#24178;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#20043;&#21069;&#21482;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#26469;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#39044;&#28903;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#23548;&#24072;&#27169;&#22411;&#30340;&#25351;&#23548;&#22312;&#39044;&#28903;&#38454;&#27573;&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#30340;&#33976;&#39311;&#26041;&#27861;&#22312;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on 
&lt;/p&gt;</description></item><item><title>GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.03480</link><description>&lt;p&gt;
GSHOT: &#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03480
&lt;/p&gt;
&lt;p&gt;
GSHOT&#26159;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#65292;&#20174;&#32780;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#29983;&#25104;&#24314;&#27169;&#22240;&#20854;&#30452;&#25509;&#23398;&#20064;&#28508;&#22312;&#38544;&#34255;&#22270;&#20998;&#24067;&#30340;&#24778;&#20154;&#33021;&#21147;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26368;&#21021;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#19968;&#26679;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#21457;&#29616;&#31561;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#19981;&#24635;&#26159;&#26377;&#36275;&#22815;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23569;&#26679;&#26412;&#22270;&#29983;&#25104;&#24314;&#27169;&#36825;&#19968;&#36804;&#20170;&#26410;&#26366;&#25506;&#32034;&#30340;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GSHOT&#65292;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#26631;&#35760;&#22270;&#29983;&#25104;&#24314;&#27169;&#12290;GSHOT&#23398;&#20064;&#20174;&#31867;&#20284;&#30340;&#36741;&#21161;&#22270;&#25968;&#25454;&#38598;&#20013;&#36716;&#31227;&#20803;&#30693;&#35782;&#12290;&#21033;&#29992;&#36825;&#20123;&#20808;&#21069;&#30340;&#32463;&#39564;&#65292;GSHOT&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#33258;&#25105;&#35843;&#25972;&#24555;&#36895;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph generative modeling has gained enormous attraction in recent years due to its impressive ability to directly learn the underlying hidden graph distribution. Despite their initial success, these techniques, like much of the existing deep generative methods, require a large number of training samples to learn a good model. Unfortunately, large number of training samples may not always be available in scenarios such as drug discovery for rare diseases. At the same time, recent advances in few-shot learning have opened door to applications where available training data is limited. In this work, we introduce the hitherto unexplored paradigm of few-shot graph generative modeling. Towards this, we develop GSHOT, a meta-learning based framework for few-shot labeled graph generative modeling. GSHOT learns to transfer meta-knowledge from similar auxiliary graph datasets. Utilizing these prior experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced fine-tuning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13721</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#24341;&#23548;&#38382;&#31572;&#30340;&#25345;&#32493;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#37325;&#26500;&#20026;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#36890;&#36807;&#32467;&#21512;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#36866;&#24212;&#26032;&#26381;&#21153;&#65292;&#20294;&#26159;&#31616;&#21333;&#22320;&#20351;&#29992;&#26032;&#26381;&#21153;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#38477;&#20302;&#20808;&#21069;&#23398;&#20064;&#30340;&#26381;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#37325;&#26500;&#20026;&#19968;&#32452;&#30001;&#20363;&#23376;&#24341;&#23548;&#30340;&#31890;&#24230;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#36716;&#31227;&#65292;&#20174;&#32780;&#33719;&#24471;&#25345;&#32493;&#30340;&#23398;&#20064;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#29305;&#23450;&#26381;&#21153;&#30340;&#35760;&#24518;&#36127;&#25285;&#65292;&#24182;&#25945;&#20250;&#27169;&#22411;&#23558;&#25152;&#32473;&#38382;&#39064;&#21644;&#31034;&#20363;&#29992;&#20110;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#24517;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20010;&#21482;&#26377;6000&#19975;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20174;&#26816;&#32034;&#22120;&#33719;&#21462;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#33719;&#24471;&#24040;&#22823;&#30340;&#25552;&#21319;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#31616;&#21333;&#30340;&#25345;&#32493;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user's goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.01141</link><description>&lt;p&gt;
DeepSaDe: &#23398;&#20064;&#30830;&#20445;&#28385;&#36275;&#39046;&#22495;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#34892;&#20026;&#24517;&#39035;&#26159;&#23433;&#20840;&#30340;&#12290;&#24403;&#21069;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32422;&#26463;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#65288;&#21363;&#20351;&#22312;&#26410;&#30475;&#36807;&#30340;&#25968;&#25454;&#19978;&#65289;&#65292;&#25110;&#32773;&#23427;&#20204;&#23545;&#21487;&#24378;&#21046;&#25191;&#34892;&#30340;&#32422;&#26463;&#31867;&#22411;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#32422;&#26463;&#24182;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#24448;&#23558;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#35270;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#22686;&#21152;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#26032;&#20803;&#32032;&#65306;&#32593;&#32476;&#23618;&#19978;&#30340;&#32422;&#26463;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.04723</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#32463;&#39564;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#24674;&#22797;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#38750;AI&#19987;&#23478;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#21644;&#26080;&#27861;&#35299;&#37322;&#65292;&#36825;&#20351;&#24471;&#38750;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#38590;&#20197;&#29702;&#35299;&#25110;&#24178;&#39044;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#35299;&#37322;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26049;&#36793;&#35757;&#32451;&#24433;&#21709;&#39044;&#27979;&#22120;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#24433;&#21709;&#39044;&#27979;&#22120;&#26159;&#23398;&#20064;&#22870;&#21169;&#26469;&#28304;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#24674;&#22797;&#26377;&#20851;&#31574;&#30053;&#22914;&#20309;&#21453;&#26144;&#29615;&#22659;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.00640</link><description>&lt;p&gt;
AmbiFC: &#29992;&#35777;&#25454;&#26816;&#39564;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24517;&#39035;&#23558;&#22768;&#26126;&#19982;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#27604;&#36739;&#20197;&#39044;&#27979;&#30495;&#23454;&#24615;&#12290;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21487;&#33021;&#26080;&#27861;&#26126;&#30830;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#65292;&#24182;&#20135;&#29983;&#21508;&#31181;&#26377;&#25928;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#38656;&#35201;&#27169;&#22411;&#20026;&#27599;&#20010;&#22768;&#26126;&#39044;&#27979;&#21333;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#31649;&#29702;&#27492;&#31867;&#27169;&#31946;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#33719;&#21462;&#30340;&#32463;&#36807;&#32454;&#31890;&#24230;&#35777;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#29616;&#23454;&#22768;&#26126;&#12290;&#25105;&#20204;&#24443;&#24213;&#20998;&#26512;&#20102;AmbiFC&#20013;&#28041;&#21450;&#21547;&#31946;&#22768;&#26126;&#24341;&#36215;&#30340;&#20105;&#35758;&#65292;&#35266;&#23519;&#21040;&#19982;&#27880;&#37322;&#20154;&#21592;&#30340;&#33258;&#25105;&#35780;&#20272;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#35821;&#35328;&#29616;&#35937;&#24378;&#28872;&#30456;&#20851;&#30340;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#35777;&#25454;&#30340;&#21547;&#31946;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#26680;&#26597;&#20219;&#21153;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#20449;&#21495;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
&lt;/p&gt;</description></item></channel></rss>