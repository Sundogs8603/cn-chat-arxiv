<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#36861;&#28335;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#21382;&#21490;&#36712;&#36857;&#65292;&#24378;&#35843;&#20102;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#21644;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;LLMs&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.02049</link><description>&lt;p&gt;
&#21518;&#22270;&#28789;&#26102;&#20195;: LLM&#35780;&#20272;&#30340;&#22320;&#22270;&#32472;&#21046;
&lt;/p&gt;
&lt;p&gt;
Post Turing: Mapping the landscape of LLM Evaluation. (arXiv:2311.02049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36861;&#28335;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#21382;&#21490;&#36712;&#36857;&#65292;&#24378;&#35843;&#20102;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#21644;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;LLMs&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#20013;&#65292;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#21644;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#36861;&#28335;&#20102;LLM&#35780;&#20272;&#30340;&#21382;&#21490;&#36712;&#36857;&#65292;&#20174;Alan Turing&#25552;&#20986;&#30340;&#22522;&#26412;&#38382;&#39064;&#21040;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#26102;&#20195;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#28436;&#21464;&#20998;&#20026;&#19981;&#21516;&#30340;&#26102;&#26399;&#65292;&#27599;&#20010;&#26102;&#26399;&#37117;&#20197;&#20854;&#29420;&#29305;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#26631;&#20934;&#20026;&#29305;&#28857;&#12290;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#20687;&#20154;&#31867;&#34892;&#20026;&#65292;&#20256;&#32479;&#30340;&#35780;&#20272;&#20195;&#29702;&#65292;&#22914;&#22270;&#28789;&#27979;&#35797;&#65292;&#21464;&#24471;&#19981;&#22826;&#21487;&#38752;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24314;&#31435;&#32479;&#19968;&#35780;&#20272;&#31995;&#32479;&#30340;&#32039;&#36843;&#24615;&#65292;&#32771;&#34385;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#24120;&#35265;&#35780;&#20272;&#26041;&#27861;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#35780;&#20272;&#26041;&#27861;&#19978;&#36827;&#34892;&#23450;&#24615;&#36716;&#21464;&#65292;&#24378;&#35843;&#26631;&#20934;&#21270;&#21644;&#23458;&#35266;&#26631;&#20934;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#21628;&#21505;AI&#31038;&#21306;&#20849;&#21516;&#24212;&#23545;LLM&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), introduction of well-defined and standardized evaluation methodologies remains a crucial challenge. This paper traces the historical trajectory of LLM evaluations, from the foundational questions posed by Alan Turing to the modern era of AI research. We categorize the evolution of LLMs into distinct periods, each characterized by its unique benchmarks and evaluation criteria. As LLMs increasingly mimic human-like behaviors, traditional evaluation proxies, such as the Turing test, have become less reliable. We emphasize the pressing need for a unified evaluation system, given the broader societal implications of these models. Through an analysis of common evaluation methodologies, we advocate for a qualitative shift in assessment approaches, underscoring the importance of standardization and objective criteria. This work serves as a call for the AI community to collaboratively address the challenges of LLM evaluation, en
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.02041</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Quantum circuit synthesis with diffusion models. (arXiv:2311.02041v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#37327;&#23376;&#30005;&#36335;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#65292;&#32780;&#19988;&#33021;&#22815;&#32469;&#36807;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#25903;&#25345;&#25193;&#23637;&#21151;&#33021;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#37327;&#23376;&#35774;&#22791;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26368;&#36817;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#25152;&#25215;&#35834;&#30340;&#20248;&#21183;&#20381;&#36182;&#20110;&#23558;&#37327;&#23376;&#25805;&#20316;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#29289;&#29702;&#23454;&#29616;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#20197;&#20419;&#36827;&#36825;&#31181;&#36716;&#21270;&#12290;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#22312;&#22522;&#20110;&#38376;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#20135;&#29983;&#25152;&#38656;&#30340;&#37327;&#23376;&#25805;&#20316;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DMs&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#32463;&#20856;&#27169;&#25311;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#22266;&#26377;&#30340;&#25351;&#25968;&#32423;&#24320;&#38144;&#65292;&#36825;&#26159;&#20808;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#19968;&#30452;&#23384;&#22312;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#65306;&#32416;&#32544;&#29983;&#25104;&#21644;&#37193;&#32534;&#35793;&#12290;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30005;&#36335;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25903;&#25345;&#20856;&#22411;&#30340;DM&#25193;&#23637;&#65292;&#20363;&#22914;&#25513;&#30721;&#21644;&#32534;&#36753;&#65292;&#20197;&#20351;&#30005;&#36335;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#37327;&#23376;&#35774;&#22791;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#37327;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has recently emerged as a transformative technology. Yet, its promised advantages rely on efficiently translating quantum operations into viable physical realizations. In this work, we use generative machine learning models, specifically denoising diffusion models (DMs), to facilitate this transformation. Leveraging text-conditioning, we steer the model to produce desired quantum operations within gate-based quantum circuits. Notably, DMs allow to sidestep during training the exponential overhead inherent in the classical simulation of quantum dynamics -- a consistent bottleneck in preceding ML techniques. We demonstrate the model's capabilities across two tasks: entanglement generation and unitary compilation. The model excels at generating new circuits and supports typical DM extensions such as masking and editing to, for instance, align the circuit generation to the constraints of the targeted quantum device. Given their flexibility and generalization abilities, we
&lt;/p&gt;</description></item><item><title>APRICOT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;ICU&#24739;&#32773;&#20013;&#23454;&#26102;&#39044;&#27979;&#25935;&#24863;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.02026</link><description>&lt;p&gt;
APRICOT: &#37325;&#30151;&#30417;&#25252;&#30149;&#25151;(ICU)&#20013;&#30340;&#25935;&#24863;&#24230;&#39044;&#27979;&#65306;&#39044;&#27979;&#31283;&#23450;&#24615;&#12289;&#36716;&#21464;&#21644;&#32500;&#25345;&#29983;&#21629;&#30340;&#27835;&#30103;
&lt;/p&gt;
&lt;p&gt;
APRICOT: Acuity Prediction in Intensive Care Unit (ICU): Predicting Stability, Transitions, and Life-Sustaining Therapies. (arXiv:2311.02026v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02026
&lt;/p&gt;
&lt;p&gt;
APRICOT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;ICU&#24739;&#32773;&#20013;&#23454;&#26102;&#39044;&#27979;&#25935;&#24863;&#24230;&#29366;&#24577;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICU&#20013;&#30340;&#24739;&#32773;&#20005;&#37325;&#31243;&#24230;&#29366;&#24577;&#21487;&#33021;&#20250;&#22312;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#20043;&#38388;&#36805;&#36895;&#21464;&#21270;&#65292;&#26377;&#26102;&#20250;&#23548;&#33268;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#12290;&#26089;&#26399;&#26816;&#27979;&#21040;&#24694;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21450;&#26102;&#30340;&#24178;&#39044;&#21644;&#26356;&#22909;&#30340;&#29983;&#23384;&#29575;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#30340;&#27599;&#26085;&#35780;&#20272;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27515;&#20129;&#29575;&#20316;&#20026;ICU&#20013;&#25935;&#24863;&#24230;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#26410;&#25972;&#21512;&#25935;&#24863;&#24230;&#29366;&#24577;&#20197;&#30830;&#23450;&#24739;&#32773;&#30340;&#31283;&#23450;&#24615;&#25110;&#23545;&#32500;&#25345;&#29983;&#21629;&#27835;&#30103;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;APRICOT&#65288;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#30340;&#25935;&#24863;&#24230;&#39044;&#27979;&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;ICU&#24739;&#32773;&#30340;&#25935;&#24863;&#24230;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#22806;&#37096;&#12289;&#26102;&#38388;&#19978;&#21644;&#21069;&#30651;&#24615;&#22320;&#24320;&#21457;&#21644;&#24191;&#27867;&#39564;&#35777;&#20102;APRICOT&#27169;&#22411;&#65306;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#65288;UFH&#65289;&#12289;eICU&#21512;&#20316;&#30740;&#31350;&#25968;&#25454;&#24211;&#65288;eICU&#65289;&#21644;&#37325;&#30151;&#30417;&#25252;&#21307;&#30103;&#20449;&#24687;&#24066;&#22330;&#65288;MIMIC&#65289;-IV&#12290;
&lt;/p&gt;
&lt;p&gt;
The acuity state of patients in the intensive care unit (ICU) can quickly change from stable to unstable, sometimes leading to life-threatening conditions. Early detection of deteriorating conditions can result in providing more timely interventions and improved survival rates. Current approaches rely on manual daily assessments. Some data-driven approaches have been developed, that use mortality as a proxy of acuity in the ICU. However, these methods do not integrate acuity states to determine the stability of a patient or the need for life-sustaining therapies. In this study, we propose APRICOT (Acuity Prediction in Intensive Care Unit), a Transformer-based neural network to predict acuity state in real-time in ICU patients. We develop and extensively validate externally, temporally, and prospectively the APRICOT model on three large datasets: University of Florida Health (UFH), eICU Collaborative Research Database (eICU), and Medical Information Mart for Intensive Care (MIMIC)-IV. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;$Conan$&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#21644;&#22810;&#36718;&#39044;&#27979;&#25512;&#29702;&#65292;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#26032;&#21457;&#29616;&#21644;&#29616;&#26377;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.02018</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20027;&#21160;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Reasoning in an Open-World Environment. (arXiv:2311.02018v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;$Conan$&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#20027;&#21160;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#21644;&#22810;&#36718;&#39044;&#27979;&#25512;&#29702;&#65292;&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#26032;&#21457;&#29616;&#21644;&#29616;&#26377;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#25972;&#21512;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#22312;&#23436;&#25972;&#20449;&#24687;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#27169;&#22411;&#26159;&#34987;&#21160;&#30340;&#65292;&#26681;&#25454;&#39044;&#23384;&#30340;&#30693;&#35782;&#22238;&#31572;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20855;&#26377;&#20027;&#21160;&#25506;&#32034;&#12289;&#31215;&#32047;&#21644;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#26032;&#21457;&#29616;&#21644;&#29616;&#26377;&#20449;&#24687;&#35299;&#20915;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$Conan$&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#65292;&#29992;&#20110;&#35780;&#20272;&#20027;&#21160;&#25512;&#29702;&#12290;$Conan$&#20419;&#36827;&#20027;&#21160;&#25506;&#32034;&#65292;&#24182;&#20419;&#36827;&#22810;&#36718;&#39044;&#27979;&#25512;&#29702;&#65292;&#31867;&#20284;&#20110;&#23500;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#22914;Minecraft&#12290;&#19982;&#20027;&#35201;&#20381;&#36182;&#21333;&#36718;&#25512;&#26029;&#30340;&#25351;&#20196;&#36319;&#38543;&#30340;&#20043;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;$Conan$&#36843;&#20351;&#20195;&#29702;&#19982;&#21608;&#22260;&#29615;&#22659;&#36827;&#34892;&#20027;&#21160;&#20114;&#21160;&#65292;&#23558;&#26032;&#30340;&#35777;&#25454;&#19982;&#20808;&#21069;&#30340;&#30693;&#35782;&#34701;&#21512;&#65292;&#20174;&#19981;&#23436;&#20840;&#30340;&#35266;&#23519;&#20013;&#38416;&#26126;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in vision-language learning have achieved notable success on complete-information question-answering datasets through the integration of extensive world knowledge. Yet, most models operate passively, responding to questions based on pre-stored knowledge. In stark contrast, humans possess the ability to actively explore, accumulate, and reason using both newfound and existing information to tackle incomplete-information questions. In response to this gap, we introduce $Conan$, an interactive open-world environment devised for the assessment of active reasoning. $Conan$ facilitates active exploration and promotes multi-round abductive inference, reminiscent of rich, open-world settings like Minecraft. Diverging from previous works that lean primarily on single-round deduction via instruction following, $Conan$ compels agents to actively interact with their surroundings, amalgamating new evidence with prior knowledge to elucidate events from incomplete observations. Our an
&lt;/p&gt;</description></item><item><title>DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.02017</link><description>&lt;p&gt;
DeliverAI: &#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39135;&#21697;&#37197;&#36865;
&lt;/p&gt;
&lt;p&gt;
DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02017
&lt;/p&gt;
&lt;p&gt;
DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20174;&#29983;&#20135;&#32773;&#21040;&#28040;&#36153;&#32773;&#30340;&#29289;&#21697;&#37197;&#36865;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#27969;&#34892;&#30149;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#22686;&#38271;&#12290;&#20122;&#39532;&#36874;&#29983;&#40092;&#12289;Shopify&#12289;UberEats&#12289;InstaCart&#21644;DoorDash&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#28040;&#36153;&#21697;&#25110;&#39135;&#21697;&#37197;&#36865;&#19994;&#21153;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#39135;&#21697;&#37197;&#36865;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#27599;&#27425;&#37197;&#36865;&#37117;&#26159;&#22312;&#26368;&#30701;&#26102;&#38388;&#36335;&#24452;&#19978;&#20174;&#29983;&#20135;&#32773;&#30452;&#25509;&#21040;&#28040;&#36153;&#32773;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#27169;&#22411;&#19979;&#65292;&#26377;&#24456;&#22823;&#30340;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#39135;&#21697;&#37197;&#36865;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#21644;&#37197;&#36865;&#25104;&#26412;&#37117;&#38656;&#35201;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;&#20986;&#31199;&#36710;&#34892;&#19994;&#20013;&#25340;&#36710;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeliverAI - &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#20849;&#20139;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#36335;&#24452;&#20849;&#20139;&#23581;&#35797;&#19981;&#21516;&#65292;DeliverAI&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#12289;&#26102;&#38388;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.02013</link><description>&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Score Models for Offline Goal-Conditioned Reinforcement Learning. (arXiv:2311.02013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;SMORe&#65292;&#23427;&#23558;&#21344;&#26377;&#21305;&#37197;&#30340;&#35270;&#35282;&#19982;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;GCRL&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#29992;&#31232;&#30095;&#22870;&#21169;&#20989;&#25968;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#22312;&#29615;&#22659;&#20013;&#23454;&#29616;&#22810;&#20010;&#30446;&#26631;&#12290;&#31163;&#32447;GCRL&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#21033;&#29992;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#38598;&#23398;&#20064;&#22810;&#26679;&#21270;&#21644;&#21487;&#22797;&#29992;&#25216;&#33021;&#30340;&#36890;&#29992;&#22411;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#20195;GCRL&#26041;&#27861;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;GCRL&#30340;&#21478;&#19968;&#31181;&#35266;&#28857;&#26159;&#20248;&#21270;&#21344;&#26377;&#21305;&#37197;&#65292;&#20294;&#38656;&#35201;&#23398;&#20064;&#37492;&#21035;&#22120;&#65292;&#38543;&#21518;&#35813;&#37492;&#21035;&#22120;&#20316;&#20026;&#19979;&#28216;&#24378;&#21270;&#23398;&#20064;&#30340;&#20266;&#22870;&#21169;&#12290;&#23398;&#20064;&#21040;&#30340;&#37492;&#21035;&#22120;&#30340;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#29983;&#25104;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GCRL&#26041;&#27861;&#65292;&#22522;&#20110;&#28151;&#21512;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#35270;&#35282;&#65292;&#37319;&#29992;&#26080;&#37492;&#21035;&#22120;&#30340;&#26041;&#27861;&#65306;SMORe&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;GCRL&#30340;&#21344;&#26377;&#21305;&#37197;&#35270;&#35282;&#19982;&#19968;&#20010;&#26377;&#25928;&#30340;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#26080;&#37492;&#21035;&#22120;&#26041;&#27861;&#65306;SMORe&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a conve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#26469;&#21516;&#26102;&#35299;&#20915;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.01994</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Obtaining Explainable Classification Models using Distributionally Robust Optimization. (arXiv:2311.01994v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#26469;&#21516;&#26102;&#35299;&#20915;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#20445;&#35777;&#27867;&#21270;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#29702;&#35299;&#25552;&#35758;&#20998;&#31867;&#22120;&#22914;&#20309;&#26681;&#25454;&#29305;&#24449;&#20540;&#32473;&#25968;&#25454;&#20998;&#37197;&#26631;&#31614;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20351;&#29992;&#29305;&#24449;&#20540;&#35268;&#21017;&#38598;&#26500;&#24314;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#20132;&#20114;&#20316;&#29992;&#12290;&#35268;&#21017;&#38598;&#30340;&#31232;&#30095;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#26435;&#34913;&#12290;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#26469;&#25214;&#21040;&#21512;&#36866;&#30340;&#31232;&#30095;&#24230;&#36873;&#25321;&#65288;&#20363;&#22914;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#65289;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#26469;&#23398;&#20064;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#31454;&#20105;&#22240;&#32032;&#30340;&#35268;&#21017;&#38598;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#26469;&#30830;&#20445;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#35813;&#20844;&#24335;&#21033;&#29992;&#21015;&#29983;&#25104;&#26377;&#25928;&#22320;&#25628;&#32034;&#35268;&#21017;&#38598;&#21512;&#30340;&#31354;&#38388;&#24182;&#26500;&#24314;&#31232;&#30095;&#30340;&#35268;&#21017;&#38598;&#21512;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#25110;Boosting&#21450;&#20854;&#21464;&#20307;&#31561;&#25216;&#26415;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29702;&#35770;&#32467;&#26524;&#26469;&#25512;&#21160;&#36825;&#19968;&#20844;&#24335;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model explainability is crucial for human users to be able to interpret how a proposed classifier assigns labels to data based on its feature values. We study generalized linear models constructed using sets of feature value rules, which can capture nonlinear dependencies and interactions. An inherent trade-off exists between rule set sparsity and its prediction accuracy. It is computationally expensive to find the right choice of sparsity -- e.g., via cross-validation -- with existing methods. We propose a new formulation to learn an ensemble of rule sets that simultaneously addresses these competing factors. Good generalization is ensured while keeping computational costs low by utilizing distributionally robust optimization. The formulation utilizes column generation to efficiently search the space of rule sets and constructs a sparse ensemble of rule sets, in contrast with techniques like random forests or boosting and their variants. We present theoretical results that motivate an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RT-Trajectory&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#22312;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2311.01977</link><description>&lt;p&gt;
RT-Trajectory: &#36890;&#36807;&#22238;&#39038;&#36712;&#36857;&#33609;&#22270;&#23454;&#29616;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches. (arXiv:2311.01977v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RT-Trajectory&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#22312;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#30340;&#26368;&#37325;&#35201;&#24895;&#26223;&#20043;&#19968;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#12289;&#35821;&#20041;&#27010;&#24565;&#25110;&#35270;&#35273;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#27867;&#21270;&#21040;&#26032;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#22312;&#25342;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23558;&#26080;&#27861;&#27867;&#21270;&#21040;&#25240;&#21472;&#20219;&#21153;&#65292;&#21363;&#20351;&#25240;&#21472;&#30340;&#33218;&#37096;&#36712;&#36857;&#19982;&#25342;&#21462;&#21644;&#25918;&#32622;&#31867;&#20284;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#36890;&#36807;&#31895;&#31961;&#30340;&#36712;&#36857;&#33609;&#22270;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;&#36825;&#31181;&#27867;&#21270;&#23558;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31895;&#31961;&#36712;&#36857;&#33609;&#22270;&#30340;&#31574;&#30053;&#26465;&#20214;&#26041;&#27861;&#65292;&#31216;&#20026;RT-Trajectory&#65292;&#23427;&#26159;&#23454;&#29992;&#30340;&#12289;&#26131;&#20110;&#25351;&#23450;&#30340;&#65292;&#21487;&#20197;&#20351;&#31574;&#30053;&#26377;&#25928;&#22320;&#25191;&#34892;&#21407;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call RT-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01967</link><description>&lt;p&gt;
&#25552;&#31034;&#30340;&#35821;&#35328;&#65306;&#20160;&#20040;&#35821;&#35328;&#23646;&#24615;&#20351;&#24471;&#25552;&#31034;&#25104;&#21151;&#65311;
&lt;/p&gt;
&lt;p&gt;
The language of prompting: What linguistic properties make a prompt successful?. (arXiv:2311.01967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#26377;&#36739;&#39640;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#19968;&#20195;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#26469;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24615;&#33021;&#23545;&#25552;&#31034;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#36827;&#34892;&#20247;&#21253;&#25552;&#31034;&#25110;&#35774;&#35745;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#23545;&#25552;&#31034;&#30340;&#35821;&#35328;&#23646;&#24615;&#19982;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#20041;&#19978;&#31561;&#25928;&#20294;&#22312;&#35821;&#35328;&#32467;&#26500;&#19978;&#26377;&#25152;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#35821;&#27861;&#23646;&#24615;&#65288;&#22914;&#24773;&#24577;&#12289;&#26102;&#24577;&#12289;&#35821;&#24577;&#21644;&#35821;&#27668;&#65289;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#21516;&#20041;&#35789;&#24341;&#20837;&#35789;&#27719;-&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#24120;&#35265;&#20551;&#35774;&#30456;&#30683;&#30462;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#22256;&#24785;&#24230;&#30340;&#25552;&#31034;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#36825;&#20123;&#25552;&#31034;&#21453;&#26144;&#20102;&#39044;&#35757;&#32451;&#25110;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;&#25552;&#31034;&#22312;&#25968;&#25454;&#38598;&#25110;&#27169;&#22411;&#20043;&#38388;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#24615;&#33021;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20934;&#27844;&#28431;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.01964</link><description>&lt;p&gt;
&#19981;&#35201;&#35753;&#20320;&#30340;LLM&#25104;&#20026;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#27450;&#39575;&#32773;
&lt;/p&gt;
&lt;p&gt;
Don't Make Your LLM an Evaluation Benchmark Cheater. (arXiv:2311.01964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#22522;&#20934;&#27844;&#28431;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#21069;&#27839;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#26500;&#24314;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#27979;&#37327;LLMs&#22312;&#19981;&#21516;&#26041;&#38754;&#30340;&#33021;&#21147;&#27700;&#24179;&#12290;&#23613;&#31649;&#24050;&#32463;&#21457;&#24067;&#20102;&#35768;&#22810;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#22522;&#20934;&#30340;&#21512;&#29702;&#20351;&#29992;&#21644;&#19981;&#21516;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#37492;&#20110;&#36825;&#20123;&#20851;&#27880;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#19981;&#24688;&#24403;&#20351;&#29992;&#35780;&#20272;&#22522;&#20934;&#21644;&#35823;&#23548;&#24615;&#35299;&#37322;&#35780;&#20272;&#32467;&#26524;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#19968;&#20010;&#29305;&#27530;&#38382;&#39064;&#65292;&#21363;&#23548;&#33268;&#19981;&#24688;&#24403;&#35780;&#20272;&#30340;\emph{&#22522;&#20934;&#27844;&#28431;}&#65292;&#21363;&#35780;&#20272;&#38598;&#30456;&#20851;&#30340;&#25968;&#25454;&#20598;&#23572;&#34987;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#22312;&#27169;&#22411;&#27979;&#35797;&#20043;&#21069;&#20934;&#22791;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#29616;&#35937;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \ie \emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#38752;&#30340;&#35299;&#37322;&#30495;&#30456;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#23458;&#35266;&#30340;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#30340;XAI&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#20250;&#29983;&#25104;&#26356;&#22810;&#30340;&#22122;&#22768;&#21796;&#37266;&#22270;&#12290;</title><link>http://arxiv.org/abs/2311.01961</link><description>&lt;p&gt;
&#23545;XAI&#21518;&#32622;&#25216;&#26415;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#65306;&#19982;Ground Truth&#35299;&#37322;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#21487;&#38752;&#30340;&#35299;&#37322;&#30495;&#30456;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#23458;&#35266;&#30340;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#30340;XAI&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#20250;&#29983;&#25104;&#26356;&#22810;&#30340;&#22122;&#22768;&#21796;&#37266;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19982;&#20854;&#22522;&#30784;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#32570;&#20047;&#35299;&#37322;&#30340;&#30495;&#23454;&#24615;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#24544;&#23454;&#24230;&#26159;&#30830;&#20445;&#27491;&#30830;&#30340;XAI&#26041;&#27861;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#20855;&#26377;&#21487;&#38752;&#30340;&#35299;&#37322;&#30495;&#30456;&#30340;&#26032;&#39062;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#36827;&#34892;&#20102;&#20844;&#24179;&#23458;&#35266;&#30340;&#27604;&#36739;&#12290;&#26412;&#27604;&#36739;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#37492;&#23450;&#20302;&#24544;&#23454;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#25490;&#38500;&#22312;&#36827;&#19968;&#27493;&#30740;&#31350;&#20043;&#22806;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21487;&#20449;&#21644;&#26377;&#25928;&#30340;XAI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36755;&#20986;&#20449;&#24687;&#21521;&#36755;&#20837;&#30340;&#21453;&#21521;&#20256;&#25773;&#30340;XAI&#26041;&#27861;&#30456;&#23545;&#20110;&#20381;&#36182;&#28789;&#25935;&#24230;&#20998;&#26512;&#25110;&#31867;&#28608;&#27963;&#22270;&#65288;CAM&#65289;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#26356;&#22810;&#30340;&#22122;&#22768;&#21796;&#37266;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI) methods to their underlying models is a challenging task, primarily due to the absence of a ground truth for explanations. However, assessing fidelity is a necessary step for ensuring a correct XAI methodology. In this study, we conduct a fair and objective comparison of the current state-of-the-art XAI methods by introducing three novel image datasets with reliable ground truth for explanations. The primary objective of this comparison is to identify methods with low fidelity and eliminate them from further research, thereby promoting the development of more trustworthy and effective XAI techniques. Our results demonstrate that XAI methods based on the backpropagation of output information to input yield higher accuracy and reliability compared to methods relying on sensitivity analysis or Class Activation Maps (CAM). However, the backpropagation method tends to generate more noisy saliency maps. These finding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26234;&#33021;&#35777;&#20070;&#8221;&#35774;&#35745;&#65292;&#20197;&#24212;&#23545;&#37329;&#34701;&#34892;&#19994;&#38754;&#20020;&#30340;&#32593;&#32476;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#35777;&#20070;&#65292;&#20225;&#19994;&#21487;&#20197;&#20197;&#31243;&#24207;&#21270;&#30340;&#26041;&#24335;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#30830;&#20445;&#20854;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2311.01956</link><description>&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#34892;&#19994;&#32593;&#32476;&#23041;&#32961;&#30340;&#26234;&#33021;&#35777;&#20070;Web3&#24212;&#29992;&#26550;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Architecture of Smart Certificates for Web3 Applications Against Cyberthreats in Financial Industry. (arXiv:2311.01956v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26234;&#33021;&#35777;&#20070;&#8221;&#35774;&#35745;&#65292;&#20197;&#24212;&#23545;&#37329;&#34701;&#34892;&#19994;&#38754;&#20020;&#30340;&#32593;&#32476;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#35777;&#20070;&#65292;&#20225;&#19994;&#21487;&#20197;&#20197;&#31243;&#24207;&#21270;&#30340;&#26041;&#24335;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#30830;&#20445;&#20854;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19982;&#24403;&#21069;&#20114;&#32852;&#32593;&#21464;&#38761;&#30456;&#20851;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#21306;&#22359;&#38142;&#21644;&#20998;&#24067;&#24335;&#23384;&#20648;&#31561;&#26032;&#20852;&#25216;&#26415;&#12290;&#23427;&#36824;&#30740;&#31350;&#20102;Web3&#24212;&#29992;&#22312;&#22609;&#36896;&#20114;&#32852;&#32593;&#26410;&#26469;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26234;&#33021;&#35777;&#20070;&#8221;&#35774;&#35745;&#65292;&#36825;&#20123;&#25968;&#23383;&#35777;&#20070;&#21487;&#20197;&#20197;&#31243;&#24207;&#21270;&#30340;&#26041;&#24335;&#25191;&#34892;&#12290;&#21033;&#29992;&#36825;&#20123;&#35777;&#20070;&#65292;&#20225;&#19994;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#32593;&#32476;&#25915;&#20987;&#65292;&#24182;&#30830;&#20445;&#20854;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#36817;&#26399;Web3&#24212;&#29992;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Certik&#12289;Forta&#12289;Slither&#21644;Securify&#20225;&#19994;&#21644;&#39033;&#30446;&#30340;&#30456;&#24403;&#20110;&#26368;&#21021;&#20026;Web1&#21644;Web2&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#30340;&#20195;&#30721;&#25195;&#25551;&#24037;&#20855;&#65292;&#32477;&#38750;&#20687;&#35777;&#20070;&#19968;&#26679;&#33021;&#24110;&#21161;&#20225;&#19994;&#25269;&#24481;&#32593;&#32476;&#23041;&#32961;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;Web3&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#19978;&#24314;&#31435;&#24182;&#37319;&#29992;&#19968;&#22871;&#26041;&#27861;&#26469;&#25552;&#39640;&#20225;&#19994;&#25968;&#23383;&#22522;&#30784;&#35774;&#26045;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the security challenges associated with the current internet transformations, specifically focusing on emerging technologies such as blockchain and decentralized storage. It also investigates the role of Web3 applications in shaping the future of the internet. The primary objective is to propose a novel design for 'smart certificates,' which are digital certificates that can be programmatically enforced. Utilizing such certificates, an enterprise can better protect itself from cyberattacks and ensure the security of its data and systems. Web3 recent security solutions by companies and projects like Certik, Forta, Slither, and Securify are the equivalent of code scanning tool that were originally developed for Web1 and Web2 applications, and definitely not like certificates to help enterprises feel safe against cyberthreats. We aim to improve the resilience of enterprises' digital infrastructure by building on top of Web3 application and put methodologies in place f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#33258;&#20027;&#24230;&#30340;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#33258;&#20027;&#24230;&#20998;&#20026;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#20004;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.01939</link><description>&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#37327;&#21270;&#33258;&#20027;&#24230;&#35780;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Quantitative Autonomy Quantification Framework for Fully Autonomous Robotic Systems. (arXiv:2311.01939v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#33258;&#20027;&#24230;&#30340;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#33258;&#20027;&#24230;&#20998;&#20026;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#20004;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#20027;&#21151;&#33021;&#20419;&#36827;&#20102;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#26377;&#38480;&#20154;&#31867;&#30417;&#30563;&#30340;&#39046;&#22495;&#21450;&#26356;&#36828;&#22788;&#30340;&#37096;&#32626;&#65292;&#20294;&#22312;&#20219;&#21153;&#38656;&#27714;&#21644;&#33258;&#20027;&#33021;&#21147;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#37327;&#21270;&#33258;&#20027;&#24230;&#30340;&#26041;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#27809;&#26377;&#36776;&#21035;&#33258;&#20027;&#24230;&#21464;&#21270;&#30340;&#23376;&#27169;&#24335;&#29305;&#24449;&#65292;&#24182;&#19988;&#19968;&#20123;&#26041;&#27861;&#22522;&#20110;&#36829;&#21453;Goodhart&#23450;&#24459;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#23436;&#20840;&#33258;&#20027;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#38656;&#27714;&#30340;&#33258;&#20027;&#24230;&#35780;&#20272;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#30830;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#29305;&#24615;&#65292;&#25512;&#23548;&#20986;&#19977;&#20010;&#33258;&#20027;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#25152;&#38656;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#21644;&#21709;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#30830;&#23450;&#33258;&#20027;&#24230;&#27700;&#24179;&#21644;&#33258;&#20027;&#24230;&#31243;&#24230;&#30340;&#20989;&#25968;&#65292;&#23558;&#33258;&#20027;&#24230;&#20316;&#20026;&#20004;&#37096;&#20998;&#24230;&#37327;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#29305;&#24615;&#22522;&#20110;&#26426;&#22120;&#20154;&#26368;&#32456;&#21462;&#20195;&#20154;&#31867;&#30340;&#35266;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although autonomous functioning facilitates deployment of robotic systems in domains that admit limited human oversight on our planet and beyond, finding correspondence between task requirements and autonomous capability is still an open challenge. Consequently, a number of methods for quantifying autonomy have been proposed over the last three decades, but to our knowledge all these have no discernment of sub-mode features of variation of autonomy and some are based on metrics that violet the Goodhart's law. This paper focuses on the full autonomous mode and proposes a task-requirements based autonomy assessment framework. The framework starts by establishing robot task characteristics from which three autonomy metrics, namely requisite capability, reliability and responsiveness, and functions for determining autonomy as a two-part measure, namely of level of autonomy and degree of autonomy are derived. These characteristics are founded on the realization that robots ultimately replac
&lt;/p&gt;</description></item><item><title>Supermind Ideator&#26159;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#21019;&#24847;&#24819;&#27861;&#65292;&#24182;&#24110;&#21161;&#29992;&#25143;&#20351;&#29992;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#36866;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;&#35774;&#35745;&#20154;&#32452;&#25110;&#20154;&#26426;&#32452;&#21512;&#30340;&#21019;&#26032;&#24819;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01937</link><description>&lt;p&gt;
&#36229;&#32423;&#24515;&#26234;&#29702;&#24565;&#32773;&#65306;&#25506;&#32034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20197;&#25903;&#25345;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Supermind Ideator: Exploring generative AI to support creative problem-solving. (arXiv:2311.01937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01937
&lt;/p&gt;
&lt;p&gt;
Supermind Ideator&#26159;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#25903;&#25345;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#33021;&#22815;&#25552;&#20379;&#26032;&#30340;&#21019;&#24847;&#24819;&#27861;&#65292;&#24182;&#24110;&#21161;&#29992;&#25143;&#20351;&#29992;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#24182;&#29305;&#21035;&#36866;&#29992;&#20110;&#29983;&#25104;&#20851;&#20110;&#35774;&#35745;&#20154;&#32452;&#25110;&#20154;&#26426;&#32452;&#21512;&#30340;&#21019;&#26032;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#25903;&#25345;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#21162;&#21147;&#21253;&#25324;&#21050;&#28608;&#21019;&#24847;&#24605;&#32500;&#30340;&#25216;&#26415;&#65288;&#22914;&#22836;&#33041;&#39118;&#26292;&#21644;&#35774;&#35745;&#24605;&#32500;&#65289;&#20197;&#21450;&#35760;&#24405;&#21644;&#20998;&#20139;&#36825;&#20123;&#24819;&#27861;&#30340;&#36719;&#20214;&#24037;&#20855;&#12290;&#29616;&#22312;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#29992;&#25143;&#20174;&#26410;&#24819;&#21040;&#30340;&#26032;&#24819;&#27861;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#36825;&#20123;&#24819;&#27861;&#20013;&#36873;&#25321;&#65292;&#25110;&#32773;&#29992;&#23427;&#20204;&#26469;&#21050;&#28608;&#26356;&#22810;&#30340;&#24819;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#65292;Supermind Ideator&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT 3.5&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#25552;&#31034;&#12289;&#24494;&#35843;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#20351;&#29992;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#25216;&#26415;&#12290;&#20854;&#20013;&#19968;&#20123;&#25216;&#26415;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#38382;&#39064;&#65307;&#21478;&#19968;&#20123;&#21017;&#19987;&#38376;&#29992;&#20110;&#24110;&#21161;&#29983;&#25104;&#20851;&#20110;&#22914;&#20309;&#35774;&#35745;&#20154;&#32452;&#25110;&#20154;&#26426;&#32452;&#21512;&#65288;&#8220;&#36229;&#32423;&#24515;&#26234;&#8221;&#65289;&#30340;&#21019;&#26032;&#24819;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31995;&#32479;&#30340;&#21021;&#27493;&#32463;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#31995;&#32479;&#20197;&#25903;&#25345;&#20854;&#20182;&#29305;&#23450;&#38382;&#39064;&#35299;&#20915;&#39046;&#22495;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous efforts to support creative problem-solving have included (a) techniques (such as brainstorming and design thinking) to stimulate creative ideas, and (b) software tools to record and share these ideas. Now, generative AI technologies can suggest new ideas that might never have occurred to the users, and users can then select from these ideas or use them to stimulate even more ideas. Here, we describe such a system, Supermind Ideator. The system uses a large language model (GPT 3.5) and adds prompting, fine tuning, and a user interface specifically designed to help people use creative problem-solving techniques. Some of these techniques can be applied to any problem; others are specifically intended to help generate innovative ideas about how to design groups of people and/or computers ("superminds"). We also describe our early experiences with using this system and suggest ways it could be extended to support additional techniques for other specific problem-solving domains.
&lt;/p&gt;</description></item><item><title>GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;</title><link>http://arxiv.org/abs/2311.01927</link><description>&lt;p&gt;
GateLoop: &#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling. (arXiv:2311.01927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01927
&lt;/p&gt;
&lt;p&gt;
GateLoop&#26159;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#25511;&#21046;&#30340;&#32447;&#24615;&#36882;&#24402;&#24207;&#21015;&#27169;&#22411;&#65292;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#32473;Attention&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#36882;&#24402;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#24314;&#27169;&#38271;&#24207;&#21015;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#29616;&#26377;&#27169;&#22411;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20854;&#28508;&#21147;&#12290;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GateLoop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#30784;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#25511;&#21046;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25512;&#24191;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#65292;&#22914;S4&#12289;S5&#12289;LRU&#21644;RetNet&#12290;&#21033;&#29992;&#36825;&#19968;&#29702;&#35770;&#36827;&#27493;&#65292;GateLoop&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#22312;&#23454;&#35777;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;$O(l)$&#36882;&#24402;&#27169;&#24335;&#21644;&#39640;&#24230;&#20248;&#21270;&#30340;&#20851;&#32852;&#25195;&#25551;&#23454;&#29616;&#30340;&#39640;&#25928;$O(l \log_{2} l)$&#24182;&#34892;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;$O(l^2)$&#30340;&#20195;&#29702;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23545;Transformer&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26550;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#21521;Attention&#25552;&#20379;&#25968;&#25454;&#25511;&#21046;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;&#32780;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#26080;&#20851;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on da
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01918</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38416;&#26126;&#20102;&#20154;&#24037;&#21307;&#30103;&#21161;&#25163;&#30340;&#36827;&#23637;&#36335;&#24452;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review. (arXiv:2311.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25991;&#31456;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24847;&#20041;&#12290;LLMs&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#27169;&#25311;&#20154;&#31867;&#32423;&#21035;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#22686;&#24378;&#21307;&#30103;&#21508;&#20010;&#26041;&#38754;&#30340;&#37325;&#35201;&#20852;&#36259;&#65292;&#33539;&#22260;&#20174;&#21307;&#23398;&#25945;&#32946;&#21040;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#28041;&#21450;&#22810;&#26041;&#38754;&#30340;&#25968;&#25454;&#27169;&#24577;&#21644;&#24494;&#22937;&#30340;&#25512;&#29702;&#25216;&#33021;&#65292;&#36825;&#32473;LLMs&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#21644;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#32771;&#23519;&#20102;&#36890;&#29992;&#22411;&#21644;&#19987;&#38376;&#21270;LLMs&#30340;&#22522;&#26412;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30693;&#35782;&#26816;&#32034;&#12289;&#30740;&#31350;&#25903;&#25345;&#12289;&#20020;&#24202;&#24037;&#20316;&#27969;&#33258;&#21160;&#21270;&#21644;&#35786;&#26029;&#36741;&#21161;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#37492;&#20110;&#21307;&#23398;&#30340;&#22266;&#26377;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#35813;&#32508;&#36848;&#36827;&#19968;&#27493;&#20851;&#27880;&#22810;&#27169;&#24577;LLMs&#65292;&#30740;&#31350;&#20854;&#22788;&#29702;&#21307;&#23398;&#24433;&#20687;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31561;&#22810;&#26679;&#21270;&#25968;&#25454;&#31867;&#22411;&#20197;&#22686;&#24378;&#35786;&#26029;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#35777;&#26126;&#20102;SNNs&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01875</link><description>&lt;p&gt;
&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65306;&#20248;&#21183;&#21644;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Functional Data Analysis with Sequential Neural Networks: Advantages and Comparative Study. (arXiv:2311.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22686;&#24378;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#35777;&#26126;&#20102;SNNs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#65288;FDA&#65289;&#26159;&#19968;&#31181;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#29305;&#24449;&#30340;&#32479;&#35745;&#39046;&#22495;&#12290;&#39034;&#24207;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#65292;&#36825;&#26159;&#21151;&#33021;&#25968;&#25454;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#23613;&#31649;SNNs&#22312;&#24314;&#27169;&#21151;&#33021;&#25968;&#25454;&#26041;&#38754;&#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#22312;FDA&#31038;&#21306;&#20013;&#20351;&#29992;&#19981;&#22810;&#12290;SNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#21183;&#26159;&#26131;&#20110;&#23454;&#26045;&#65292;&#21487;&#20197;&#35753;&#24191;&#22823;&#38750;&#23398;&#26415;&#30028;&#30340;&#20154;&#32676;&#33021;&#22815;&#20351;&#29992;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;FDA&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#26469;&#35828;&#65292;&#22240;&#20026;&#23427;&#20204;&#36807;&#20110;&#22797;&#26434;&#12290;&#22522;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#35758;&#22312;FDA&#24212;&#29992;&#20013;&#21033;&#29992;SNNs&#65292;&#24182;&#36890;&#36807;&#19982;&#24120;&#35265;FDA&#22238;&#24402;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#21644;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;SNNs&#30340;&#26550;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;FDA&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional Data Analysis (FDA) is a statistical domain developed to handle functional data characterized by high dimensionality and complex data structures. Sequential Neural Networks (SNNs) are specialized neural networks capable of processing sequence data, a fundamental aspect of functional data. Despite their great flexibility in modeling functional data, SNNs have been inadequately employed in the FDA community. One notable advantage of SNNs is the ease of implementation, making them accessible to a broad audience beyond academia. Conversely, FDA-based methodologies present challenges, particularly for practitioners outside the field, due to their intricate complexity. In light of this, we propose utilizing SNNs in FDA applications and demonstrate their effectiveness through comparative analyses against popular FDA regression models based on numerical experiments and real-world data analysis. SNN architectures allow us to surpass the limitations of traditional FDA methods, offerin
&lt;/p&gt;</description></item><item><title>Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2311.01870</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#27431;&#27954;&#35758;&#20250;&#25968;&#25454;&#38598;&#29992;&#20110;&#20998;&#26512;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01870
&lt;/p&gt;
&lt;p&gt;
Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Multi-EuP&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;24&#31181;&#35821;&#35328;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#20998;&#26512;&#22312;&#25490;&#21517;&#19978;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25317;&#26377;&#19968;&#20010;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#30340;&#20027;&#39064;&#34987;&#32763;&#35793;&#25104;&#20102;&#25152;&#26377;24&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20379;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#20415;&#20110;&#30740;&#31350;&#20154;&#21475;&#20559;&#35265;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;Multi-EuP&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26631;&#35760;&#21270;&#31574;&#30053;&#36873;&#25321;&#24341;&#36215;&#30340;&#35821;&#35328;&#20559;&#35265;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;LLMs&#25110;&#20351;&#29992;&#29616;&#26377;LLMs&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01866</link><description>&lt;p&gt;
&#26397;&#30528;&#27010;&#24565;&#24863;&#30693;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Concept-Aware Large Language Models. (arXiv:2311.01866v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;LLMs&#25110;&#20351;&#29992;&#29616;&#26377;LLMs&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#30452;&#35273;&#24182;&#25913;&#21892;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22312;&#21508;&#31181;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#36171;&#20104;&#26426;&#22120;&#24418;&#25104;&#21644;&#25512;&#29702;&#27010;&#24565;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#23588;&#20854;&#26159;&#65292;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20027;&#35201;&#22312;&#35789;&#20803;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#32780;&#19981;&#26159;&#27010;&#24565;&#32423;&#21035;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#20195;LLMs&#23545;&#20154;&#31867;&#27010;&#24565;&#21450;&#20854;&#32467;&#26500;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#38454;&#27573;&#20013;&#24320;&#21457;&#27010;&#24565;&#24863;&#30693;LLMs&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;LLMs&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#26377;LLMs&#36755;&#20986;&#30340;&#26356;&#31616;&#21333;&#26041;&#27861;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#27010;&#24565;&#39564;&#35777;&#35777;&#26126;&#20102;&#26356;&#22909;&#22320;&#21305;&#37197;&#20154;&#31867;&#30452;&#35273;&#65292;&#24182;&#25552;&#21319;&#20102;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#20102;&#27010;&#24565;&#24863;&#30693;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts play a pivotal role in various human cognitive functions, including learning, reasoning and communication. However, there is very little work on endowing machines with the ability to form and reason with concepts. In particular, state-of-the-art large language models (LLMs) work at the level of tokens, not concepts.  In this work, we analyze how well contemporary LLMs capture human concepts and their structure. We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline. We sketch a method for pretraining LLMs using concepts, and also explore the simpler approach that uses the output of existing LLMs. Despite its simplicity, our proof-of-concept is shown to better match human intuition, as well as improve the robustness of predictions. These preliminary results underscore the promise of concept-aware LLMs.
&lt;/p&gt;</description></item><item><title>SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2311.01864</link><description>&lt;p&gt;
SortNet: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25490;&#24207;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
SortNet: Learning To Rank By a Neural-Based Sorting Algorithm. (arXiv:2311.01864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01864
&lt;/p&gt;
&lt;p&gt;
SortNet&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#35757;&#32451;&#38598;&#65292;&#26681;&#25454;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#30340;&#25490;&#24207;&#31034;&#20363;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#30456;&#20851;&#24615;&#25490;&#21517;&#30340;&#38382;&#39064;&#65292;&#21363;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#20934;&#23545;&#19968;&#32452;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#29992;&#25143;&#21487;&#33021;&#20559;&#22909;&#19981;&#21516;&#30340;&#30456;&#20851;&#24615;&#26631;&#20934;&#65292;&#22240;&#27492;&#25490;&#24207;&#31639;&#27861;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#23398;&#20064;&#25490;&#24207;&#30340;&#20219;&#21153;&#22312;&#25991;&#29486;&#20013;&#23384;&#22312;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;1&#65289;&#36890;&#36807;&#31034;&#20363;&#23398;&#20064;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#35780;&#20272;&#27599;&#20010;&#23545;&#35937;&#30340;&#23646;&#24615;&#65292;&#29983;&#25104;&#21487;&#29992;&#20110;&#23545;&#23545;&#35937;&#36827;&#34892;&#25490;&#24207;&#30340;&#32477;&#23545;&#30456;&#20851;&#24615;&#20540;&#65307;2&#65289;&#19968;&#31181;&#25104;&#23545;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#35937;&#23545;&#26469;&#23398;&#20064;&#8220;&#20559;&#22909;&#20989;&#25968;&#8221;&#65292;&#23450;&#20041;&#21738;&#19968;&#20010;&#23545;&#35937;&#24212;&#35813;&#39318;&#20808;&#25490;&#21517;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SortNet&#65292;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#27604;&#36739;&#22120;&#26469;&#23545;&#23545;&#35937;&#36827;&#34892;&#33258;&#36866;&#24212;&#25490;&#24207;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38598;&#25552;&#20379;&#20102;&#23545;&#20110;&#25104;&#23545;&#39033;&#30446;&#20043;&#38388;&#25152;&#38656;&#25490;&#24207;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#26500;&#24314;&#65292;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#28155;&#21152;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#27604;&#36739;&#22120;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#20307;&#31995;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of relevance ranking consists of sorting a set of objects with respect to a given criterion. Since users may prefer different relevance criteria, the ranking algorithms should be adaptable to the user needs. Two main approaches exist in literature for the task of learning to rank: 1) a score function, learned by examples, which evaluates the properties of each object yielding an absolute relevance value that can be used to order the objects or 2) a pairwise approach, where a "preference function" is learned using pairs of objects to define which one has to be ranked first. In this paper, we present SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. The neural network training set provides examples of the desired ordering between pairs of items and it is constructed by an iterative procedure which, at each iteration, adds the most informative training examples. Moreover, the comparator adopts a connectionist architecture that 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#24341;&#25806;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#25552;&#20379;&#26032;&#23454;&#20307;&#30340;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;81.2&#65285;&#65292;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#20026;77.8&#65285;&#65292;&#27492;&#22806;&#65292;&#35813;&#24341;&#25806;&#36824;&#20248;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2311.01860</link><description>&lt;p&gt;
FAME&#65306;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
FAME: Flexible, Scalable Analogy Mappings Engine. (arXiv:2311.01860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#21487;&#25193;&#23637;&#30340;&#31867;&#27604;&#26144;&#23556;&#24341;&#25806;&#65292;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#24341;&#25806;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#25552;&#20379;&#26032;&#23454;&#20307;&#30340;&#24314;&#35758;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;81.2&#65285;&#65292;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#20026;77.8&#65285;&#65292;&#27492;&#22806;&#65292;&#35813;&#24341;&#25806;&#36824;&#20248;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#65307;&#22312;&#38754;&#23545;&#26032;&#24773;&#22659;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#20013;&#36716;&#31227;&#20808;&#21069;&#30340;&#32463;&#39564;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#35745;&#31639;&#31867;&#27604;&#30340;&#24037;&#20316;&#37117;&#20005;&#37325;&#20381;&#36182;&#22797;&#26434;&#30340;&#25163;&#24037;&#21046;&#20316;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#36755;&#20837;&#35201;&#27714;&#65292;&#21482;&#38656;&#23545;&#23454;&#20307;&#36827;&#34892;&#26144;&#23556;&#12290;&#25105;&#20204;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#30830;&#23450;&#23454;&#20307;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#37096;&#20998;&#31867;&#27604;&#65292;&#24182;&#24314;&#35758;&#28155;&#21152;&#26032;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36755;&#20986;&#26131;&#20110;&#35299;&#37322;&#65292;&#29992;&#25143;&#21487;&#20197;&#29702;&#35299;&#20026;&#20160;&#20040;&#36873;&#25321;&#20102;&#29305;&#23450;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27491;&#30830;&#22320;&#26144;&#23556;&#20102;81.2&#65285;&#30340;&#32463;&#20856;2x2&#31867;&#27604;&#38382;&#39064;&#65288;&#29468;&#27979;&#27700;&#24179;=50&#65285;&#65289;&#12290;&#22312;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#65292;&#23427;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;77.8&#65285;&#65288;&#24179;&#22343;&#29468;&#27979;&#27700;&#24179;=13.1&#65285;&#65289;&#12290;&#22312;&#21478;&#19968;&#20010;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#32988;&#36807;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#33258;&#21160;&#24314;&#35758;&#30340;&#26032;&#23454;&#20307;&#31867;&#20284;&#20110;&#20154;&#31867;&#24314;&#35758;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method's output is easily interpretable, allowing for users to understand why a specific mapping was chosen.  Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26234;&#33021;&#22810;&#23618;&#35270;&#22270;&#21512;&#25104;&#26550;&#26500;&#65292;&#20026;&#31227;&#21160;&#33258;&#32452;&#32593;&#20013;&#30340;&#25968;&#25454;&#21253;&#36335;&#30001;&#38382;&#39064;&#25552;&#20379;&#20102;&#32418;&#28779;&#34433;&#20248;&#21270;&#36335;&#30001;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.01842</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26234;&#33021;&#22810;&#23618;&#35270;&#22270;&#21512;&#25104;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Neural Radiance Field-Based Architecture for Intelligent Multilayered View Synthesis. (arXiv:2311.01842v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#26234;&#33021;&#22810;&#23618;&#35270;&#22270;&#21512;&#25104;&#26550;&#26500;&#65292;&#20026;&#31227;&#21160;&#33258;&#32452;&#32593;&#20013;&#30340;&#25968;&#25454;&#21253;&#36335;&#30001;&#38382;&#39064;&#25552;&#20379;&#20102;&#32418;&#28779;&#34433;&#20248;&#21270;&#36335;&#30001;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#32452;&#32593;&#30001;&#19968;&#20123;&#26080;&#32447;&#20415;&#25658;&#33410;&#28857;&#32452;&#25104;&#65292;&#23427;&#20204;&#22312;&#36884;&#20013;&#33258;&#21457;&#32858;&#38598;&#65292;&#24314;&#31435;&#19968;&#20010;&#20020;&#26102;&#32593;&#32476;&#65292;&#26080;&#38656;&#20219;&#20309;&#20013;&#22830;&#31649;&#29702;&#12290;&#31227;&#21160;&#33258;&#32452;&#32593;&#30001;&#19968;&#32676;&#25968;&#37327;&#21487;&#35266;&#19988;&#23494;&#38598;&#30340;&#31227;&#21160;&#33410;&#28857;&#32452;&#25104;&#65292;&#23427;&#20204;&#31359;&#36234;&#21508;&#31181;&#22320;&#24418;&#65292;&#20165;&#20381;&#38752;&#26080;&#32447;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20219;&#20309;&#38598;&#20013;&#24335;&#31649;&#29702;&#12290;&#27492;&#22806;&#65292;&#36335;&#30001;&#24212;&#25552;&#20379;&#19968;&#31181;&#22312;&#20219;&#24847;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#21363;&#26102;&#20256;&#36755;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#26368;&#20339;&#30340;&#25968;&#25454;&#21253;&#36335;&#30001;&#26159;&#20027;&#35201;&#38382;&#39064;&#12290;&#25152;&#25552;&#35758;&#30340;&#21327;&#35758;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25214;&#21040;&#36153;&#29992;&#26368;&#20302;&#30340;&#26368;&#23567;&#23481;&#37327;&#33719;&#21462;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#22312;&#20219;&#19968;&#33410;&#28857;&#25925;&#38556;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#35777;&#21487;&#38752;&#30340;&#20256;&#36755;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#32418;&#28779;&#34433;&#20248;&#21270;&#36335;&#30001;&#36873;&#25321;&#31574;&#30053;&#26469;&#25913;&#36827;&#25353;&#38656;&#28304;&#36335;&#30001;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A mobile ad hoc network is made up of a number of wireless portable nodes that spontaneously come together en route for establish a transitory network with no need for any central management. A mobile ad hoc network (MANET) is made up of a sizable and reasonably dense community of mobile nodes that travel across any terrain and rely solely on wireless interfaces for communication, not on any well before centralized management. Furthermore, routing be supposed to offer a method for instantly delivering data across a network between any two nodes. Finding the best packet routing from across infrastructure is the major issue, though. The proposed protocol's major goal is to identify the least-expensive nominal capacity acquisition that assures the transportation of realistic transport that ensures its durability in the event of any node failure. This study suggests the Optimized Route Selection via Red Imported Fire Ants (RIFA) Strategy as a way to improve on-demand source routing systems
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffDub&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#65292;&#22312;&#20445;&#30041;&#20854;&#20182;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#32534;&#30721;&#21644;&#38754;&#37096;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#37197;&#38899;&#30340;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01811</link><description>&lt;p&gt;
DiffDub: &#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#30340;&#36890;&#29992;&#28436;&#21592;&#35270;&#35273;&#37197;&#38899;
&lt;/p&gt;
&lt;p&gt;
DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffDub&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#65292;&#22312;&#20445;&#30041;&#20854;&#20182;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#32534;&#30721;&#21644;&#38754;&#37096;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#37197;&#38899;&#30340;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#36890;&#29992;&#30340;&#35270;&#35273;&#37197;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#21019;&#26032;&#30475;&#21040;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#33539;&#20363;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#35299;&#32806;&#28210;&#26579;&#21644;&#21475;&#22411;&#21516;&#27493;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31895;&#31961;&#30340;&#26631;&#35760;&#28857;&#25110;&#23616;&#38480;&#20110;&#21333;&#19968;&#30340;&#35828;&#35805;&#20154;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffDub&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20462;&#22797;&#28210;&#26579;&#22120;&#21644;&#19968;&#20010;&#33945;&#29256;&#26469;&#32472;&#21046;Diffusion&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#30028;&#23450;&#21487;&#32534;&#36753;&#21306;&#22495;&#21644;&#26410;&#26356;&#25913;&#21306;&#22495;&#12290;&#36825;&#26679;&#21487;&#20197;&#22312;&#20445;&#30041;&#20854;&#20313;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35821;&#20041;&#32534;&#30721;&#22120;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#25429;&#25417;&#39640;&#32423;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#24573;&#30053;&#20102;&#38754;&#37096;&#23450;&#20301;&#65292;&#23548;&#33268;&#24103;&#38388;&#21475;&#25110;&#40763;&#23376;&#25238;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#21151;&#33021;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, inc
&lt;/p&gt;</description></item><item><title>AFPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#19981;&#21516;&#30340;&#27604;&#20363;&#23610;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#39069;&#22806;&#23384;&#20648;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2311.01792</link><description>&lt;p&gt;
AFPQ&#65306;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
AFPQ: Asymmetric Floating Point Quantization for LLMs. (arXiv:2311.01792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01792
&lt;/p&gt;
&lt;p&gt;
AFPQ&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#38750;&#23545;&#31216;&#28014;&#28857;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#19981;&#21516;&#30340;&#27604;&#20363;&#23610;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#26080;&#38656;&#39069;&#22806;&#23384;&#20648;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38754;&#20020;&#26377;&#38480;&#30340;&#20869;&#23384;&#23481;&#37327;&#21644;&#24102;&#23485;&#30340;&#37096;&#32626;&#25361;&#25112;&#12290;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#24182;&#21152;&#36895;&#25512;&#26029;&#12290;&#23613;&#31649;&#28014;&#28857;&#65288;FP&#65289;&#26684;&#24335;&#22312;LLM&#37327;&#21270;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23567;&#32452;&#22823;&#23567;&#25110;&#23376;4&#20301;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;FP&#37327;&#21270;&#32570;&#20047;&#19981;&#23545;&#31216;&#24615;&#65292;&#19981;&#36866;&#21512;&#22788;&#29702;LLM&#26435;&#37325;&#24352;&#37327;&#30340;&#19981;&#23545;&#31216;&#20540;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#23545;&#31216;FP&#37327;&#21270;&#65288;AFPQ&#65289;&#65292;&#20026;&#27491;&#20540;&#21644;&#36127;&#20540;&#35774;&#32622;&#20102;&#20998;&#21035;&#30340;&#27604;&#20363;&#23610;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#22320;&#25554;&#20837;&#20854;&#20182;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;GPTQ&#21644;AWQ&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38750;&#23545;&#31216;&#25972;&#25968;&#65288;INT&#65289;&#37327;&#21270;&#30456;&#27604;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/zhangsichengsjtu/AFPQ&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show great performance in various tasks, but face deployment challenges from limited memory capacity and bandwidth. Low-bit weight quantization can save memory and accelerate inference. Although floating-point (FP) formats show good performance in LLM quantization, they tend to perform poorly with small group sizes or sub-4 bits. We find the reason is that the absence of asymmetry in previous FP quantization makes it unsuitable for handling asymmetric value distribution of LLM weight tensors. In this work, we propose asymmetric FP quantization (AFPQ), which sets separate scales for positive and negative values. Our method leads to large accuracy improvements and can be easily plugged into other quantization methods, including GPTQ and AWQ, for better performance. Besides, no additional storage is needed compared with asymmetric integer (INT) quantization. The code is available at https://github.com/zhangsichengsjtu/AFPQ.
&lt;/p&gt;</description></item><item><title>TCM-GPT&#26159;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20256;&#32479;&#20013;&#21307;&#19987;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01786</link><description>&lt;p&gt;
TCM-GPT:&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine. (arXiv:2311.01786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01786
&lt;/p&gt;
&lt;p&gt;
TCM-GPT&#26159;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20256;&#32479;&#20013;&#21307;&#19987;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24182;&#20855;&#26377;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#24448;&#24448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#12289;&#29420;&#29305;&#30446;&#26631;&#21644;&#35745;&#31639;&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#20256;&#32479;&#20013;&#21307;&#65289;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;TCMDA&#65288;&#20256;&#32479;&#20013;&#21307;&#39046;&#22495;&#36866;&#24212;&#65289;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35782;&#21035;&#39046;&#22495;&#20851;&#38190;&#35789;&#24182;&#20174;&#36890;&#29992;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#20256;&#32479;&#20013;&#21307;&#29305;&#23450;&#35821;&#26009;&#24211;TCM-Corpus-1B&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;TCMDA&#26041;&#27861;&#20351;&#29992;&#36825;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training and fine-tuning have emerged as a promising paradigm across various natural language processing (NLP) tasks. The effectiveness of pretrained large language models (LLM) has witnessed further enhancement, holding potential for applications in the field of medicine, particularly in the context of Traditional Chinese Medicine (TCM). However, the application of these general models to specific domains often yields suboptimal results, primarily due to challenges like lack of domain knowledge, unique objectives, and computational efficiency. Furthermore, their effectiveness in specialized domains, such as Traditional Chinese Medicine, requires comprehensive evaluation. To address the above issues, we propose a novel domain specific TCMDA (TCM Domain Adaptation) approach, efficient pre-training with domain-specific corpus. Specifically, we first construct a large TCM-specific corpus, TCM-Corpus-1B, by identifying domain keywords and retreving from general corpus. Then, our TCMDA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21322;&#30417;&#30563;&#23039;&#24577;&#20272;&#35745;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#20174;&#24314;&#27169;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20004;&#20010;&#26368;&#22823;&#24046;&#24322;&#23398;&#29983;&#26469;&#25512;&#21160;&#32769;&#24072;&#29983;&#25104;&#19981;&#21516;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01770</link><description>&lt;p&gt;
&#29992;&#26368;&#22823;&#24046;&#24322;&#27169;&#22411;&#26469;&#24314;&#27169;&#21322;&#30417;&#30563;&#20108;&#32500;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation. (arXiv:2311.01770v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21322;&#30417;&#30563;&#23039;&#24577;&#20272;&#35745;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#20174;&#24314;&#27169;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20004;&#20010;&#26368;&#22823;&#24046;&#24322;&#23398;&#29983;&#26469;&#25512;&#21160;&#32769;&#24072;&#29983;&#25104;&#19981;&#21516;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23039;&#24577;&#20272;&#35745;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20855;&#26377;&#23454;&#38469;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#35768;&#22810;&#20248;&#31168;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#32622;&#20449;&#24230;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#22312;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#20013;&#24456;&#38590;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#22312;&#23039;&#24577;&#20272;&#35745;&#20013;&#65292;&#32622;&#20449;&#24230;&#20165;&#34920;&#31034;&#28909;&#22270;&#20301;&#32622;&#26159;&#20851;&#38190;&#28857;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#26159;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#20174;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#35282;&#24230;&#26469;&#20272;&#35745;&#21322;&#30417;&#30563;&#23039;&#24577;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#21452;&#24179;&#22343;&#24072;&#20613;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#26368;&#22823;&#24046;&#24322;&#23398;&#29983;&#65292;&#20197;&#26377;&#25928;&#22320;&#25512;&#21160;&#20004;&#20010;&#32769;&#24072;&#20026;&#21516;&#19968;&#26679;&#26412;&#29983;&#25104;&#19981;&#21516;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#22810;&#20010;&#19981;&#30830;&#23450;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised pose estimation is a practically challenging task for computer vision. Although numerous excellent semi-supervised classification methods have emerged, these methods typically use confidence to evaluate the quality of pseudo-labels, which is difficult to achieve in pose estimation tasks. For example, in pose estimation, confidence represents only the possibility that a position of the heatmap is a keypoint, not the quality of that prediction. In this paper, we propose a simple yet efficient framework to estimate the quality of pseudo-labels in semi-supervised pose estimation tasks from the perspective of modeling the uncertainty of the pseudo-labels. Concretely, under the dual mean-teacher framework, we construct the two maximum discrepant students (MDSs) to effectively push two teachers to generate different decision boundaries for the same sample. Moreover, we create multiple uncertainties to assess the quality of the pseudo-labels. Experimental results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.01757</link><description>&lt;p&gt;
Indo LEGO-ABSA&#65306;&#19968;&#31181;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#30340;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#38024;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#20219;&#21153;&#29983;&#25104;&#24335;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#29702;&#35299;&#19982;&#23454;&#20307;&#30340;&#29305;&#23450;&#26041;&#38754;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#21069;&#26399;&#30740;&#31350;&#24050;&#32463;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#12290;LEGO-ABSA&#26159;&#19968;&#20010;&#25104;&#21151;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#22312;&#33521;&#25991;&#20013;&#12290;LEGO-ABSA&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#25552;&#31034;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#29615;&#22659;&#20013;&#23578;&#26410;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#30340;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#20013;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#25552;&#31034;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#20102;Indo LEGO-ABSA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Previous research has utilized generative pre-trained language models to perform aspect-based sentiment analysis. LEGO-ABSA is one framework that has successfully employed generative pre-trained language models in aspect-based sentiment analysis, particularly in English. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Bahasa Indonesia. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Bahasa Indonesia using generative pre-trained language models. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sentiment analy
&lt;/p&gt;</description></item><item><title>RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.01753</link><description>&lt;p&gt;
RiskQ: &#39118;&#38505;&#25935;&#24863;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01753
&lt;/p&gt;
&lt;p&gt;
RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#29305;&#28857;&#26159;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12289;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#21327;&#35843;&#21644;&#20998;&#25955;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;MARL&#20013;&#21046;&#23450;&#21327;&#35843;&#35201;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#29702;&#65292;&#20316;&#20026;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;IGM&#65289;&#21644;&#20998;&#24067;&#24335;IGM&#65288;DIGM&#65289;&#21407;&#29702;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#35813;&#21407;&#29702;&#35201;&#27714;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#38598;&#21512;&#24212;&#19982;&#20013;&#22830;&#31574;&#30053;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#31561;&#20215;&#12290;&#24403;&#21069;&#30340;MARL&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#23545;&#20110;&#24120;&#35265;&#30340;&#39118;&#38505;&#24230;&#37327;&#65288;&#20363;&#22914;&#39118;&#38505;&#20215;&#20540;&#65288;VaR&#65289;&#24230;&#37327;&#25110;&#25197;&#26354;&#30340;&#39118;&#38505;&#24230;&#37327;&#65289;&#19981;&#28385;&#36275;RIGM&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RiskQ&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#26469;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#20998;&#37197;&#25193;&#39057;&#22240;&#23376;&#65292;&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#24182;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;</title><link>http://arxiv.org/abs/2311.01743</link><description>&lt;p&gt;
&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;&#30452;&#25509;&#21355;&#26143;&#36830;&#25509;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario. (arXiv:2311.01743v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22320;&#19979;LoRaWAN&#32593;&#32476;&#33021;&#25928;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#20998;&#37197;&#25193;&#39057;&#22240;&#23376;&#65292;&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#24182;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20559;&#36828;&#20892;&#19994;&#21644;&#28798;&#23475;&#25937;&#25588;&#34892;&#21160;&#20013;&#65292;&#22320;&#19979;LoRaWAN&#19982;&#38750;&#22320;&#29699;&#32593;&#32476;&#65288;NTN&#65289;&#30340;&#38598;&#25104;&#20026;&#32463;&#27982;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#12290;LoRa&#35843;&#21046;&#21033;&#29992;&#20934;&#27491;&#20132;&#25193;&#39057;&#22240;&#23376;&#65288;SFs&#65289;&#26469;&#20248;&#21270;&#25968;&#25454;&#36895;&#29575;&#12289;&#31354;&#38386;&#26102;&#38388;&#12289;&#35206;&#30422;&#33539;&#22260;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#22320;&#19979;LoRaWAN NTN&#20013;&#65292;&#26377;&#25928;&#22320;&#20026;&#32456;&#31471;&#35774;&#22791;&#20998;&#37197;SFs&#20197;&#26368;&#23567;&#21270;&#20849;&#20139;&#25193;&#39057;&#22240;&#23376;&#24178;&#25200;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;SFs&#20998;&#37197;&#26041;&#26696;&#26469;&#20248;&#21270;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;&#20026;&#20102;&#26377;&#25928;&#25429;&#25417;&#23494;&#38598;&#32593;&#32476;&#20013;&#35774;&#22791;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20998;&#26512;&#22870;&#21169;&#26426;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;dueling double deep Q-network&#65288;MAD3QN&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;MAA2C&#65289;&#31639;&#27861;&#30340;SFs&#20998;&#37197;&#25216;&#26415;&#12290;&#19982;&#22235;&#20010;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;SFs&#20998;&#37197;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01727</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#23545;&#37327;&#23376;&#36807;&#31243;&#36827;&#34892;&#28789;&#27963;&#30340;&#35823;&#24046;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Flexible Error Mitigation of Quantum Processes with Data Augmentation Empowered Neural Model. (arXiv:2311.01727v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#22320;&#32531;&#35299;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#21508;&#31181;&#22122;&#22768;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#37327;&#23376;&#36807;&#31243;&#20013;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#35745;&#31639;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37327;&#23376;&#35823;&#24046;&#32531;&#35299;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#23545;&#26080;&#22122;&#22768;&#32479;&#35745;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#36825;&#26159;&#23454;&#29616;&#23454;&#38469;&#37327;&#23376;&#36827;&#23637;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#24378;&#21270;&#30340;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#35823;&#24046;&#32531;&#35299;&#65288;DAEM&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29305;&#23450;&#22122;&#22768;&#31867;&#22411;&#21644;&#27979;&#37327;&#35774;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#20165;&#26681;&#25454;&#30446;&#26631;&#37327;&#23376;&#36807;&#31243;&#30340;&#22122;&#22768;&#27979;&#37327;&#32467;&#26524;&#20272;&#35745;&#26080;&#22122;&#22768;&#32479;&#35745;&#20540;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#23454;&#38469;&#23454;&#26045;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#32531;&#35299;&#21508;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#65288;&#21253;&#25324;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#65289;&#26041;&#38754;&#19982;&#20808;&#21069;&#30340;&#35823;&#24046;&#32531;&#35299;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#26469;&#32531;&#35299;&#22810;&#31181;&#31867;&#22411;&#30340;&#37327;&#23376;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#26469;&#23637;&#31034;&#20854;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown their effectiveness in various tasks in the realm of quantum computing. However, their application in quantum error mitigation, a crucial step towards realizing practical quantum advancements, has been restricted by reliance on noise-free statistics. To tackle this critical challenge, we propose a data augmentation empowered neural model for error mitigation (DAEM). Our model does not require any prior knowledge about the specific noise type and measurement settings and can estimate noise-free statistics solely from the noisy measurement results of the target quantum process, rendering it highly suitable for practical implementation. In numerical experiments, we show the model's superior performance in mitigating various types of noise, including Markovian noise and Non-Markovian noise, compared with previous error mitigation methods. We further demonstrate its versatility by employing the model to mitigate errors in diverse types of quantum processes, includ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01723</link><description>&lt;p&gt;
&#23454;&#29616;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#21487;&#20197;&#37322;&#25918;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#65292;&#20294;&#20250;&#24433;&#21709;&#27169;&#22411;&#23545;&#20110;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#40065;&#26834;&#24494;&#35843;&#26088;&#22312;&#30830;&#20445;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#20197;&#21450;&#24494;&#35843;&#30340;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#32622;&#20449;&#24230;&#26657;&#20934;&#36825;&#19968;&#26631;&#20934;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#38656;&#27714;&#26085;&#30410;&#22686;&#21152;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#32454;&#35843;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#26657;&#20934;&#30340;&#25285;&#24551;&#65292;&#24182;&#36890;&#36807;&#26174;&#31034;&#26222;&#36890;&#24494;&#35843;&#29978;&#33267;&#26368;&#20808;&#36827;&#30340;&#40065;&#26834;&#24494;&#35843;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#36896;&#25104;&#20102;&#25439;&#23475;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#26657;&#20934;&#40065;&#26834;&#24494;&#35843;&#65288;CaRot&#65289;&#65292;&#23427;&#22312;&#26657;&#20934;&#21644;&#40065;&#26834;&#24615;&#19978;&#25552;&#20379;&#20102;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fine-tuning unleashes the potential of a pre-trained model to a specific task, it trades off the model's generalization capability on out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims to ensure performance on OOD datasets as well as an in-distribution (ID) dataset for which the model is being tuned. However, another criterion for reliable machine learning (ML), confidence calibration, has been overlooked despite its increasing demand for real-world high-stakes ML applications (e.g., autonomous driving and medical diagnosis). For the first time, we raise concerns about the calibration of fine-tuned vision-language models (VLMs) under distribution shift by showing that naive fine-tuning and even state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained VLMs, especially on OOD datasets. To address this, we provide a simple approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the calibration and robustness on bot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#23545;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;ASQP&#25216;&#26415;&#21644;&#25552;&#39640;GPT&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01713</link><description>&lt;p&gt;
&#23545;&#20013;&#22269;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#23545;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25913;&#36827;ASQP&#25216;&#26415;&#21644;&#25552;&#39640;GPT&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#26159;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;ASQP&#25968;&#25454;&#38598;&#29305;&#28857;&#26159;&#35268;&#27169;&#23567;&#19988;&#22235;&#20803;&#32452;&#23494;&#24230;&#20302;&#65292;&#36825;&#38459;&#30861;&#20102;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#25193;&#22823;&#23481;&#37327;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#22269;ASQP&#25968;&#25454;&#38598;&#65292;&#20174;&#22810;&#20010;&#22312;&#32447;&#24179;&#21488;&#25910;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#20960;&#20010;&#26174;&#33879;&#30340;&#29305;&#28857;&#65306;&#26356;&#22823;&#30340;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;10,000+&#20010;&#26679;&#26412;&#65289;&#65292;&#20016;&#23500;&#30340;&#26041;&#38754;&#31867;&#21035;&#65292;&#27599;&#20010;&#21477;&#23376;&#26356;&#22810;&#30340;&#35789;&#25968;&#20197;&#21450;&#27604;&#29616;&#26377;ASQP&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#23494;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#27169;&#22411;&#22312;ASQP&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;ASQP&#22522;&#20934;&#32447;&#23454;&#39564;&#24378;&#35843;&#20102;&#38656;&#35201;&#25506;&#32034;&#39069;&#22806;&#25216;&#26415;&#26469;&#35299;&#20915;ASQP&#30340;&#38656;&#27714;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#25913;&#36827;GPT&#24615;&#33021;&#30340;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level sentiment analysis. Current ASQP datasets are characterized by their small size and low quadruple density, which hinders technical development. To expand capacity, we construct two large Chinese ASQP datasets crawled from multiple online platforms. The datasets hold several significant characteristics: larger size (each with 10,000+ samples) and rich aspect categories, more words per sentence, and higher density than existing ASQP datasets. Moreover, we are the first to evaluate the performance of Generative Pre-trained Transformer (GPT) series models on ASQP and exhibit potential issues. The experiments with state-of-the-art ASQP baselines underscore the need to explore additional techniques to address ASQP, as well as the importance of further investigation into methods to improve the performance of GPTs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26694;&#26550;&#65292;&#21363;DFKD-T$^{3}$&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01689</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#65306;&#36890;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#26694;&#26550;&#65292;&#21363;DFKD-T$^{3}$&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#65288;DFKD&#65289;&#22312;&#21387;&#32553;&#27169;&#22411;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20808;&#21069;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23545;DFKD&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31867;&#21035;&#20219;&#21153;&#36827;&#34892;&#33976;&#39311;&#30340;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DFKD&#26694;&#26550;&#65292;&#21517;&#20026;DFKD-T$^{3}$&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21487;&#25511;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#12290;&#36825;&#20010;&#26032;&#39062;&#30340;DFKD-T$^{3}$&#26694;&#26550;&#23548;&#33268;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#26694;&#26550;&#65292;&#23558;&#36890;&#29992;&#39046;&#22495;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#21387;&#32553;&#21451;&#22909;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#29305;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21319;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33976;&#39311;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the model when original training data is unavailable. Previous works for DFKD in NLP mainly focus on distilling encoder-only structures like BERT on classification tasks, which overlook the notable progress of generative language modeling. In this work, we propose a novel DFKD framework, namely DFKD-T$^{3}$, where the pretrained generative language model can also serve as a controllable data generator for model compression. This novel framework DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to transform the general domain corpus to compression-friendly task data, targeting to improve both the \textit{specificity} and \textit{diversity}. Extensive experiments show that our method can boost the distillation performance in various downstream tasks such as sentiment analysis, linguistic acceptability, and information extraction. Furthermore, we show that the generated texts can be directly used 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#21518;&#20998;&#23618;&#21305;&#37197;&#26694;&#26550;&#65292;&#23558;&#35266;&#23519;&#25968;&#25454;&#36716;&#21270;&#20026;&#31867;&#20284;&#20110;&#38543;&#26426;&#35797;&#39564;&#65292;&#20026;&#31934;&#20934;&#21307;&#23398;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#32416;&#27491;&#24739;&#32773;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20020;&#24202;&#30452;&#35266;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#32963;&#32928;&#38388;&#36136;&#30244;&#65288;GIST&#65289;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24314;&#35758;&#30340;&#20248;&#20110;&#19987;&#23478;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2311.01681</link><description>&lt;p&gt;
&#36890;&#21521;&#31934;&#20934;&#21307;&#23398;&#30340;R.O.A.D.
&lt;/p&gt;
&lt;p&gt;
The R.O.A.D. to precision medicine. (arXiv:2311.01681v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01681
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#21518;&#20998;&#23618;&#21305;&#37197;&#26694;&#26550;&#65292;&#23558;&#35266;&#23519;&#25968;&#25454;&#36716;&#21270;&#20026;&#31867;&#20284;&#20110;&#38543;&#26426;&#35797;&#39564;&#65292;&#20026;&#31934;&#20934;&#21307;&#23398;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#32416;&#27491;&#24739;&#32773;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20020;&#24202;&#30452;&#35266;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#32963;&#32928;&#38388;&#36136;&#30244;&#65288;GIST&#65289;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24314;&#35758;&#30340;&#20248;&#20110;&#19987;&#23478;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#21518;&#20998;&#23618;&#21305;&#37197;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#38543;&#26426;&#35797;&#39564;&#25968;&#25454;&#20122;&#32452;&#20998;&#26512;&#30340;&#19981;&#36275;&#65292;&#24182;&#23558;&#35266;&#23519;&#25968;&#25454;&#36716;&#21270;&#20026;&#31867;&#20284;&#20110;&#38543;&#26426;&#35797;&#39564;&#65292;&#20026;&#31934;&#20934;&#21307;&#23398;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#36807;&#31243;&#32416;&#27491;&#20102;&#27835;&#30103;&#19979;&#39044;&#27979;&#32467;&#26524;&#30340;&#28508;&#22312;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#26469;&#35757;&#32451;&#26368;&#20248;&#31574;&#30053;&#26641;&#65288;OPTs&#65289;&#65292;OPTs&#26159;&#20915;&#31574;&#26641;&#65292;&#26681;&#25454;&#24739;&#32773;&#29305;&#24449;&#23558;&#27835;&#30103;&#26368;&#20248;&#22320;&#20998;&#37197;&#32473;&#20122;&#32452;&#12290;&#36825;&#26377;&#21161;&#20110;&#21019;&#36896;&#20020;&#24202;&#30452;&#35266;&#30340;&#27835;&#30103;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;&#20102;&#20855;&#26377;&#32963;&#32928;&#38388;&#36136;&#30244;&#65288;GIST&#65289;&#30340;&#24739;&#32773;&#30340;&#35266;&#23519;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#25351;&#26631;&#22312;&#22806;&#37096;&#38431;&#21015;&#20013;&#39564;&#35777;&#20102;OPTs&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20123;&#24314;&#35758;&#20248;&#20110;GIST&#39046;&#22495;&#30340;&#19987;&#23478;&#24314;&#35758;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#20102;&#30456;&#21516;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
We propose a prognostic stratum matching framework that addresses the deficiencies of Randomized trial data subgroup analysis and transforms ObservAtional Data to be used as if they were randomized, thus paving the road for precision medicine. Our approach counters the effects of unobserved confounding in observational data by correcting the estimated probabilities of the outcome under a treatment through a novel two-step process. These probabilities are then used to train Optimal Policy Trees (OPTs), which are decision trees that optimally assign treatments to subgroups of patients based on their characteristics. This facilitates the creation of clinically intuitive treatment recommendations. We applied our framework to observational data of patients with gastrointestinal stromal tumors (GIST) and validated the OPTs in an external cohort using the sensitivity and specificity metrics. We show that these recommendations outperformed those of experts in GIST. We further applied the same 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2311.01677</link><description>&lt;p&gt;
DialogBench: &#23558;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26032;&#30340;&#23545;&#35805;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#21047;&#26032;&#20102;&#20154;&#20204;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21360;&#35937;&#12290;&#23545;&#35805;&#31995;&#32479;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#36275;&#22815;&#20687;&#20154;&#31867;&#65292;&#20197;&#20415;&#36890;&#36807;&#28385;&#36275;&#20132;&#27969;&#12289;&#24773;&#24863;&#21644;&#31038;&#20132;&#24402;&#23646;&#30340;&#38656;&#35201;&#19982;&#29992;&#25143;&#24314;&#31435;&#38271;&#26399;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#21547;12&#20010;&#23545;&#35805;&#20219;&#21153;&#65292;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#24212;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#27599;&#20010;&#20219;&#21153;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24191;&#27867;&#20351;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#36731;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#23545;28&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65288;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#25351;&#23548;&#35843;&#20248;&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#25928;&#30410;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MineSegSAT&#65292;&#19968;&#20010;&#20351;&#29992;Sentinel-2&#25968;&#25454;&#21644;SegFormer&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30719;&#20135;&#24320;&#37319;&#29616;&#22330;&#21463;&#21040;&#24433;&#21709;&#30340;&#21306;&#22495;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20808;&#36827;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#20934;&#30830;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#25928;&#26524;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01676</link><description>&lt;p&gt;
MineSegSAT&#65306;&#19968;&#20010;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;Sentinel-2&#24433;&#20687;&#20013;&#30340;&#37319;&#30719;&#24178;&#25200;&#21306;&#22495;&#33539;&#22260;
&lt;/p&gt;
&lt;p&gt;
MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery. (arXiv:2311.01676v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MineSegSAT&#65292;&#19968;&#20010;&#20351;&#29992;Sentinel-2&#25968;&#25454;&#21644;SegFormer&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#30719;&#20135;&#24320;&#37319;&#29616;&#22330;&#21463;&#21040;&#24433;&#21709;&#30340;&#21306;&#22495;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20808;&#36827;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#20934;&#30830;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#30340;&#25928;&#26524;&#65292;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30719;&#20135;&#24320;&#37319;&#34892;&#19994;&#30340;&#29615;&#22659;&#24433;&#21709;&#22312;&#29702;&#35299;&#21644;&#20943;&#36731;&#24320;&#37319;&#27963;&#21160;&#30340;&#29983;&#24577;&#21518;&#26524;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MineSegSAT&#65292;&#19968;&#20010;&#21033;&#29992;&#22312;Sentinel-2&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;SegFormer&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#26550;&#26500;&#26469;&#39044;&#27979;&#30719;&#20135;&#24320;&#37319;&#29616;&#22330;&#29615;&#22659;&#21463;&#21040;&#24433;&#21709;&#30340;&#21306;&#22495;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#26159;&#20174;2021&#24180;&#21152;&#25343;&#22823;&#35199;&#37096;&#30340;&#38750;&#37325;&#21472;&#22320;&#21306;&#25910;&#38598;&#30340;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#36890;&#36807;2021&#24180;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#30830;&#23450;&#30340;&#21463;&#30719;&#19994;&#27963;&#21160;&#29615;&#22659;&#24433;&#21709;&#30340;&#22303;&#22320;&#21306;&#22495;&#12290;&#37319;&#29992;SegFormer&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#35821;&#20041;&#20998;&#21106;&#26694;&#26550;&#65292;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#20934;&#30830;&#30340;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21253;&#25324;Dice&#12289;Tversky&#21644;Lovasz&#25439;&#22833;&#22312;&#20869;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25928;&#26524;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#23545;&#27979;&#35797;&#21306;&#22495;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the environmental impact of the mineral extraction industry plays a critical role in understanding and mitigating the ecological consequences of extractive activities. This paper presents MineSegSAT, a model that presents a novel approach to predicting environmentally impacted areas of mineral extraction sites using the SegFormer deep learning segmentation architecture trained on Sentinel-2 data. The data was collected from non-overlapping regions over Western Canada in 2021 containing areas of land that have been environmentally impacted by mining activities that were identified from high-resolution satellite imagery in 2021. The SegFormer architecture, a state-of-the-art semantic segmentation framework, is employed to leverage its advanced spatial understanding capabilities for accurate land cover classification. We investigate the efficacy of loss functions including Dice, Tversky, and Lovasz loss respectively. The trained model was utilized for inference over the test reg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24322;&#36136;&#29305;&#24449;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#31038;&#21306;&#38887;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.01661</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#65306;&#22522;&#20110;&#30456;&#20114;&#20851;&#32852;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-driven Community Resilience Rating based on Intertwined Socio-Technical Systems Features. (arXiv:2311.01661v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24322;&#36136;&#29305;&#24449;&#21644;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#23545;&#31038;&#21306;&#38887;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38887;&#24615;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#22810;&#32500;&#30340;&#29616;&#35937;&#65292;&#20854;&#28304;&#20110;&#19981;&#21516;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#20854;&#38887;&#24615;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#31038;&#21306;&#38887;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33030;&#24369;&#24615;&#35780;&#20272;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#20854;&#23545;&#20110;&#25429;&#25417;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20869;&#37096;&#30340;&#24322;&#36136;&#29305;&#24449;&#21450;&#20854;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;&#38887;&#24615;&#30340;&#31283;&#20581;&#24615;&#12289;&#20887;&#20313;&#24615;&#21644;&#36164;&#28304;&#24615;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19977;&#23618;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31038;&#21306;&#38887;&#24615;&#35780;&#20272;&#26041;&#27861;&#65288;&#31216;&#20026;Resili-Net&#65289;&#12290;&#22312;&#31038;&#21306;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#65288;&#21363;&#35774;&#26045;&#12289;&#22522;&#30784;&#35774;&#26045;&#21644;&#31038;&#20250;&#65289;&#20013;&#65292;&#30830;&#23450;&#21644;&#35745;&#31639;&#20102;&#20855;&#20307;&#30340;12&#20010;&#21487;&#27979;&#37327;&#30340;&#38887;&#24615;&#29305;&#24449;&#65292;&#19982;&#38887;&#24615;&#30340;&#31283;&#20581;&#24615;&#12289;&#20887;&#20313;&#24615;&#21644;&#36164;&#28304;&#24615;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#30456;&#20851;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#32654;&#22269;&#22823;&#37117;&#24066;&#32479;&#35745;&#21306;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community resilience is a complex and muti-faceted phenomenon that emerges from complex and nonlinear interactions among different socio-technical systems and their resilience properties. However, present studies on community resilience focus primarily on vulnerability assessment and utilize index-based approaches, with limited ability to capture heterogeneous features within community socio-technical systems and their nonlinear interactions in shaping robustness, redundancy, and resourcefulness components of resilience. To address this gap, this paper presents an integrated three-layer deep learning model for community resilience rating (called Resili-Net). Twelve measurable resilience features are specified and computed within community socio-technical systems (i.e., facilities, infrastructures, and society) related to three resilience components of robustness, redundancy, and resourcefulness. Using publicly accessible data from multiple metropolitan statistical areas in the United S
&lt;/p&gt;</description></item><item><title>MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2311.01650</link><description>&lt;p&gt;
MARRS: &#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MARRS: Multimodal Reference Resolution System. (arXiv:2311.01650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01650
&lt;/p&gt;
&lt;p&gt;
MARRS&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;&#36825;&#20010;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22788;&#29702;&#19978;&#19979;&#25991;&#23545;&#20110;&#20219;&#20309;&#23545;&#35805;&#29702;&#35299;&#20219;&#21153;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#20010;&#19978;&#19979;&#25991;&#21487;&#33021;&#26159;&#23545;&#35805;&#24335;&#30340;&#65288;&#20381;&#36182;&#20110;&#20043;&#21069;&#30340;&#29992;&#25143;&#26597;&#35810;&#25110;&#31995;&#32479;&#22238;&#31572;&#65289;&#65292;&#20063;&#21487;&#33021;&#26159;&#35270;&#35273;&#30340;&#65288;&#20381;&#36182;&#20110;&#29992;&#25143;&#30475;&#21040;&#30340;&#19996;&#35199;&#65292;&#20363;&#22914;&#20182;&#20204;&#30340;&#23631;&#24149;&#19978;&#65289;&#65292;&#25110;&#32773;&#26159;&#32972;&#26223;&#30340;&#65288;&#22522;&#20110;&#19968;&#20123;&#20449;&#21495;&#65292;&#27604;&#22914;&#21709;&#36215;&#30340;&#38393;&#38047;&#25110;&#32773;&#27491;&#22312;&#25773;&#25918;&#30340;&#38899;&#20048;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MARRS&#65288;&#22810;&#27169;&#24577;&#21442;&#32771;&#35299;&#26512;&#31995;&#32479;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#22312;&#22788;&#29702;&#23545;&#35805;&#24335;&#12289;&#35270;&#35273;&#21644;&#32972;&#26223;&#19978;&#19979;&#25991;&#26041;&#38754;&#36127;&#36131;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23454;&#29616;&#19978;&#19979;&#25991;&#26597;&#35810;&#30340;&#22788;&#29702;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#32771;&#35299;&#26512;&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26597;&#35810;&#37325;&#20889;&#22788;&#29702;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#30456;&#20114;&#34917;&#20805;&#65292;&#24418;&#25104;&#19968;&#20010;&#32479;&#19968;&#12289;&#36830;&#36143;&#12289;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#29702;&#35299;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successfully handling context is essential for any dialog understanding task. This context maybe be conversational (relying on previous user queries or system responses), visual (relying on what the user sees, for example, on their screen), or background (based on signals such as a ringing alarm or playing music). In this work, we present an overview of MARRS, or Multimodal Reference Resolution System, an on-device framework within a Natural Language Understanding system, responsible for handling conversational, visual and background context. In particular, we present different machine learning models to enable handing contextual queries; specifically, one to enable reference resolution, and one to handle context via query rewriting. We also describe how these models complement each other to form a unified, coherent, lightweight system that can understand context while preserving user privacy.
&lt;/p&gt;</description></item><item><title>RTP&#26159;&#19968;&#31181;&#20197;&#20869;&#23384;&#21435;&#37325;&#20026;&#37325;&#28857;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#29615;&#22659;&#20013;&#20248;&#21270;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#19982;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#24615;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01635</link><description>&lt;p&gt;
RTP: &#29992;&#20869;&#23384;&#21435;&#37325;&#24605;&#32771;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
RTP: Rethinking Tensor Parallelism with Memory Deduplication. (arXiv:2311.01635v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01635
&lt;/p&gt;
&lt;p&gt;
RTP&#26159;&#19968;&#31181;&#20197;&#20869;&#23384;&#21435;&#37325;&#20026;&#37325;&#28857;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#29615;&#22659;&#20013;&#20248;&#21270;&#20869;&#23384;&#28040;&#32791;&#24182;&#23454;&#29616;&#19982;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#24615;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#26159;&#19982;&#35757;&#32451;&#24222;&#22823;&#27169;&#22411;&#30456;&#20851;&#30340;&#26174;&#33879;&#20869;&#23384;&#24320;&#38144;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#28145;&#20837;&#30740;&#31350;&#20102;&#26059;&#36716;&#24352;&#37327;&#24182;&#34892;&#24615;&#65288;RTP&#65289;&#12290;RTP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#24067;&#24335;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#20869;&#23384;&#21435;&#37325;&#12290;&#23427;&#20855;&#26377;&#23450;&#21046;&#30340;&#36890;&#20449;&#21407;&#35821;&#21644;Flyweight Pattern&#21021;&#22987;&#21270;&#31561;&#29420;&#29305;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;RTP&#30830;&#20445;&#20102;&#20998;&#21306;&#35745;&#31639;&#21644;&#20998;&#21306;&#26435;&#37325;&#36890;&#20449;&#20043;&#38388;&#30340;&#26080;&#32541;&#37325;&#21472;&#65292;&#20248;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#24378;&#35843;&#20102;RTP&#30340;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20854;&#20869;&#23384;&#28040;&#32791;&#19982;&#26368;&#20339;&#20540;&#38750;&#24120;&#25509;&#36817; - &#22312;&#22810;&#21488;&#26426;&#22120;&#20043;&#38388;&#20844;&#24179;&#22320;&#20998;&#37197;&#21333;&#21488;&#26426;&#22120;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RTP&#33021;&#22815;&#23454;&#29616;&#19982;&#20998;&#24067;&#24335;&#25968;&#25454;&#24182;&#34892;&#24615;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Par
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;ChatGPT&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#24418;&#25104;&#24615;&#21453;&#39304;&#65292;&#21463;&#35775;&#32773;&#23545;&#21453;&#39304;&#25345;&#31215;&#26497;&#35780;&#20215;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#23567;&#32452;&#20351;&#29992;&#21453;&#39304;&#26469;&#25913;&#36827;&#35780;&#20272;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2311.01634</link><description>&lt;p&gt;
&#8220;&#25509;&#36817;&#8230;&#8230;&#20294;&#19981;&#21450;&#25945;&#32946;&#32773;&#8221;--&#20351;&#29992;ChatGPT&#20026;&#22823;&#22411;&#21327;&#20316;&#23398;&#20064;&#20013;&#25552;&#20379;&#24418;&#25104;&#24615;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
"Close...but not as good as an educator." -- Using ChatGPT to provide formative feedback in large-class collaborative learning. (arXiv:2311.01634v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01634
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;ChatGPT&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#24418;&#25104;&#24615;&#21453;&#39304;&#65292;&#21463;&#35775;&#32773;&#23545;&#21453;&#39304;&#25345;&#31215;&#26497;&#35780;&#20215;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#23567;&#32452;&#20351;&#29992;&#21453;&#39304;&#26469;&#25913;&#36827;&#35780;&#20272;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30701;&#26102;&#38388;&#20869;&#21521;&#22810;&#20010;&#38382;&#39064;&#23548;&#21521;&#23398;&#20064;&#23567;&#32452;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24418;&#25104;&#24615;&#21453;&#39304;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#24418;&#25104;&#24615;&#21453;&#39304;&#65292;&#22312;&#19968;&#20010;&#23567;&#26102;&#30340;Zoom&#20998;&#32452;&#27963;&#21160;&#20013;&#25945;&#25480;&#20174;&#19994;&#21355;&#29983;&#19987;&#19994;&#20154;&#21592;&#22914;&#20309;&#20026;&#25968;&#23383;&#21355;&#29983;&#35745;&#21010;&#21046;&#23450;&#35780;&#20272;&#35745;&#21010;&#12290;&#23398;&#20064;&#32773;&#23436;&#25104;&#20102;&#21253;&#25324;Likert&#37327;&#34920;&#21644;&#24320;&#25918;&#24335;&#38382;&#39064;&#30340;&#35780;&#20272;&#35843;&#26597;&#65292;&#24182;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;44&#21517;&#35843;&#26597;&#21463;&#35775;&#32773;&#20013;&#26377;&#19968;&#21322;&#20174;&#26410;&#20351;&#29992;&#36807;ChatGPT&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#21463;&#35775;&#32773;&#23545;&#21453;&#39304;&#25345;&#31215;&#26497;&#35780;&#20215;&#65292;&#25551;&#36848;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#23567;&#32452;&#21160;&#24577;&#65292;&#24182;&#23545;&#21453;&#39304;&#20570;&#20986;&#20102;&#36866;&#24212;&#24615;&#22238;&#24212;&#65292;&#20294;&#21482;&#26377;&#19977;&#20010;&#23567;&#32452;&#20351;&#29992;&#21453;&#39304;&#24490;&#29615;&#26469;&#25913;&#36827;&#20182;&#20204;&#30340;&#35780;&#20272;&#35745;&#21010;&#12290;&#26410;&#26469;&#30340;&#25945;&#32946;&#32773;&#21487;&#20197;&#20174;&#25105;&#20204;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#35774;&#35745;&#25552;&#31034;&#12289;&#25552;&#20379;&#22914;&#20309;&#20351;&#29992;ChatGPT&#30340;&#35828;&#26126;&#65292;&#20197;&#21450;&#20026;ChatGPT&#30340;&#26368;&#20339;&#23567;&#32452;&#20114;&#21160;&#25552;&#20379;&#25903;&#25345;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#32773;&#24212;&#35813;&#25506;&#32034;ChatGPT&#23545;&#23567;&#32452;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delivering personalised, formative feedback to multiple problem-based learning groups in a short time period can be almost impossible. We employed ChatGPT to provide personalised formative feedback in a one-hour Zoom break-out room activity that taught practicing health professionals how to formulate evaluation plans for digital health initiatives. Learners completed an evaluation survey that included Likert scales and open-ended questions that were analysed. Half of the 44 survey respondents had never used ChatGPT before. Overall, respondents found the feedback favourable, described a wide range of group dynamics, and had adaptive responses to the feedback, yet only three groups used the feedback loop to improve their evaluation plans. Future educators can learn from our experience including engineering prompts, providing instructions on how to use ChatGPT, and scaffolding optimal group interactions with ChatGPT. Future researchers should explore the influence of ChatGPT on group dyna
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#35266;&#23519;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#35782;&#21035;&#24182;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#36793;&#30028;&#26102;&#30340;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01617</link><description>&lt;p&gt;
&#25552;&#21069;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26426;&#21046;&#65292;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#35266;&#23519;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#35782;&#21035;&#24182;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#22312;&#20219;&#21153;&#36793;&#30028;&#26102;&#30340;&#36873;&#25321;&#24615;&#21487;&#22609;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#23398;&#20064;&#21040;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#19988;&#23545;&#26410;&#26469;&#30340;&#20219;&#21153;&#26377;&#24456;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#34920;&#31034;&#12290;&#20197;&#22823;&#33041;&#20013;&#21019;&#24314;&#21644;&#26356;&#26032;&#30340;&#20107;&#20214;&#27169;&#22411;&#20026;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21457;&#29983;&#22312;&#20219;&#21153;&#36793;&#30028;&#65292;&#21363;&#19968;&#20010;&#20219;&#21153;&#32467;&#26463;&#24182;&#21478;&#19968;&#20010;&#20219;&#21153;&#24320;&#22987;&#26102;&#12290;&#36890;&#36807;&#35266;&#23519;&#23545;&#27604;&#25439;&#22833;&#23545;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#20887;&#20313;&#35825;&#23548;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26032;&#20219;&#21153;&#30340;&#21069;&#20960;&#20010;&#26679;&#26412;&#65292;&#35782;&#21035;&#21644;&#20445;&#30041;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#31227;&#33021;&#21147;&#26368;&#26377;&#36129;&#29486;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#37322;&#25918;&#32593;&#32476;&#30340;&#20854;&#20313;&#37096;&#20998;&#26469;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#35832;&#22914;CIFAR10&#21644;TinyImagenet&#31561;&#22522;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20219;&#21153;&#22686;&#37327;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning has emerged as a promising technique for continual learning as it can learn representations that are robust to catastrophic forgetting and generalize well to unseen future tasks. Previous work in continual learning has addressed forgetting by using previous task data and trained models. Inspired by event models created and updated in the brain, we propose a new mechanism that takes place during task boundaries, i.e., when one task finishes and another starts. By observing the redundancy-inducing ability of contrastive loss on the output of a neural network, our method leverages the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. We evaluate the proposed methods on benchmark computer vision datasets including CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the task-incrementa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#30340;&#26032;&#20852;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#25506;&#35752;&#20102;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#26694;&#26550;&#19979;&#22914;&#20309;&#29702;&#35299;&#21644;&#22609;&#36896;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#22810;&#26234;&#33021;&#20307;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01609</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#26032;&#20852;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Responsible Emergent Multi-Agent Behavior. (arXiv:2311.01609v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01609
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#36127;&#36131;&#20219;&#30340;&#26032;&#20852;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#65292;&#25506;&#35752;&#20102;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#26694;&#26550;&#19979;&#22914;&#20309;&#29702;&#35299;&#21644;&#22609;&#36896;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#22810;&#26234;&#33021;&#20307;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#31038;&#21306;&#30340;&#28966;&#28857;&#12290;&#38543;&#30528;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#20445;&#25345;&#39640;&#24230;&#20154;&#31867;&#20860;&#23481;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#26377;&#25216;&#26415;&#21364;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#28857;&#65306;&#20154;&#31867;&#38382;&#39064;&#26159;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;&#20027;&#27969;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#21333;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#23396;&#31435;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#20154;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;&#20174;&#39550;&#36710;&#21040;&#35848;&#21028;&#32463;&#27982;&#25919;&#31574;&#65292;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#28041;&#21450;&#22810;&#20010;&#20010;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#34892;&#20026;&#21160;&#26426;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#35770;&#25991;&#24320;&#23637;&#20102;&#36127;&#36131;&#20219;&#30340;&#26032;&#20852;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#22914;&#20309;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22609;&#36896;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#20010;&#25903;&#26609;&#65306;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#27491;&#24615;&#12289;&#20195;&#29702;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Responsible AI has risen to the forefront of the AI research community. As neural network-based learning algorithms continue to permeate real-world applications, the field of Responsible AI has played a large role in ensuring that such systems maintain a high-level of human-compatibility. Despite this progress, the state of the art in Responsible AI has ignored one crucial point: human problems are multi-agent problems. Predominant approaches largely consider the performance of a single AI system in isolation, but human problems are, by their very nature, multi-agent. From driving in traffic to negotiating economic policy, human problem-solving involves interaction and the interplay of the actions and motives of multiple individuals.  This dissertation develops the study of responsible emergent multi-agent behavior, illustrating how researchers and practitioners can better understand and shape multi-agent learning with respect to three pillars of Responsible AI: interpretability, fairn
&lt;/p&gt;</description></item><item><title>DRNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21040;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.01602</link><description>&lt;p&gt;
DRNet&#65306;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#36710;&#36947;&#21464;&#25442;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep Reinforcement Learning. (arXiv:2311.01602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01602
&lt;/p&gt;
&lt;p&gt;
DRNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21040;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#39550;&#39542;&#22330;&#26223;&#21644;&#21608;&#22260;&#36710;&#36742;&#30340;&#22810;&#21464;&#31038;&#20132;&#34892;&#20026;&#65292;&#36710;&#36947;&#21464;&#25442;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20840;&#26032;&#12289;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#39640;&#36895;&#20844;&#36335;&#19978;&#25191;&#34892;&#21512;&#29702;&#30340;&#36710;&#36947;&#21464;&#25442;&#65292;&#24182;&#32771;&#34385;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#65292;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#23398;&#20250;&#39550;&#39542;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#23433;&#20840;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;DRNet&#32467;&#21512;&#20102;&#23433;&#20840;&#39564;&#35777;&#30340;&#24605;&#24819;&#65292;&#36825;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#26368;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30830;&#20445;&#22312;&#20219;&#20309;&#26102;&#21051;&#21482;&#36873;&#25321;&#23433;&#20840;&#30340;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have outperformed numerous rule-based methods for decision-making in autonomous vehicles. Despite recent efforts, lane changing remains a major challenge, due to the complex driving scenarios and changeable social behaviors of surrounding vehicles. To help improve the state of the art, we propose to leveraging the emerging \underline{D}eep \underline{R}einforcement learning (DRL) approach for la\underline{NE} changing at the \underline{T}actical level. To this end, we present "DRNet", a novel and highly efficient DRL-based framework that enables a DRL agent to learn to drive by executing reasonable lane changing on simulated highways with an arbitrary number of lanes, and considering driving style of surrounding vehicles to make better decisions. Furthermore, to achieve a safe policy for decision-making, DRNet incorporates ideas from safety verification, the most important component of autonomous driving, to ensure that only safe actions are chosen at any ti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>MetaReVision&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#26469;&#24555;&#36895;&#23398;&#20064;&#21644;&#35782;&#21035;&#26032;&#30340;&#22270;&#20687;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2311.01580</link><description>&lt;p&gt;
MetaReVision: &#20351;&#29992;&#26816;&#32034;&#36827;&#34892;&#20803;&#23398;&#20064;&#30340;&#35270;&#35273;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
MetaReVision: Meta-Learning with Retrieval for Visually Grounded Compositional Concept Acquisition. (arXiv:2311.01580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01580
&lt;/p&gt;
&lt;p&gt;
MetaReVision&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#26469;&#24555;&#36895;&#23398;&#20064;&#21644;&#35782;&#21035;&#26032;&#30340;&#22270;&#20687;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#22238;&#24518;&#21644;&#25512;&#24191;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#33719;&#21462;&#30340;&#22522;&#26412;&#27010;&#24565;&#26469;&#23398;&#20064;&#26032;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411; - MetaReVision&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;&#22522;&#30784;&#32452;&#21512;&#27010;&#24565;&#23398;&#20064;&#38382;&#39064;&#12290;MetaReVision&#30001;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#21644;&#19968;&#20010;&#20803;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#65292;&#26088;&#22312;&#23558;&#26816;&#32034;&#21040;&#30340;&#22522;&#26412;&#27010;&#24565;&#20316;&#20026;&#25903;&#25345;&#38598;&#21512;&#24182;&#29992;&#20110;&#20803;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26469;&#35782;&#21035;&#22522;&#20110;&#22270;&#20687;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#36890;&#36807;&#20174;&#26816;&#32034;&#22120;&#26500;&#24314;&#30340;episode&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;MetaReVision&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32452;&#21512;&#34920;&#31034;&#65292;&#21487;&#20197;&#24555;&#36895;&#26356;&#26032;&#20197;&#35782;&#21035;&#26032;&#30340;&#32452;&#21512;&#27010;&#24565;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;CompCOCO&#21644;CompFlickr&#26469;&#35780;&#20272;&#22522;&#20110;&#22270;&#20687;&#30340;&#32452;&#21512;&#27010;&#24565;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MetaReVision&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#26816;&#32034;&#27169;&#22359;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have the ability to learn novel compositional concepts by recalling and generalizing primitive concepts acquired from past experiences. Inspired by this observation, in this paper, we propose MetaReVision, a retrieval-enhanced meta-learning model to address the visually grounded compositional concept learning problem. The proposed MetaReVision consists of a retrieval module and a meta-learning module which are designed to incorporate retrieved primitive concepts as a supporting set to meta-train vision-anguage models for grounded compositional concept recognition. Through meta-learning from episodes constructed by the retriever, MetaReVision learns a generic compositional representation that can be fast updated to recognize novel compositional concepts. We create CompCOCO and CompFlickr to benchmark the grounded compositional concept learning. Our experimental results show that MetaReVision outperforms other competitive baselines and the retrieval module plays an important role 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#26469;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#20943;&#36731;&#20102;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01573</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#25913;&#21892;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39537;&#21160;&#30340;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#26469;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#20943;&#36731;&#20102;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#37492;&#21035;&#27169;&#22411;&#26102;&#65292;&#20844;&#24179;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#37096;&#39046;&#22495;&#12290;&#27169;&#22411;&#24448;&#24448;&#23558;&#29305;&#23450;&#29305;&#24449;&#65288;&#22914;&#24180;&#40836;&#21644;&#32932;&#33394;&#65289;&#19982;&#26080;&#20851;&#23646;&#24615;&#65288;&#19979;&#28216;&#20219;&#21153;&#65289;&#30456;&#20851;&#32852;&#65292;&#23548;&#33268;&#20559;&#35265;&#19982;&#29616;&#23454;&#19981;&#31526;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#20123;&#30456;&#20851;&#24615;&#23384;&#22312;&#20110;&#25968;&#25454;&#20013;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20256;&#36882;&#32473;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#22312;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65288;DiffAE&#65289;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#20301;&#20110;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#30001;&#23545;&#27604;&#24615;&#25991;&#26412;&#20108;&#26497;&#20307;&#36827;&#34892;&#30417;&#30563;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#32534;&#36753;&#21463;&#20445;&#25252;&#29305;&#24449;&#65288;&#24180;&#40836;&#21644;&#32932;&#33394;&#65289;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#36335;&#24452;&#24212;&#29992;&#20110;&#22686;&#24378;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#22312;CelebA-HQ&#21644;UTKFace&#19978;&#23545;&#24180;&#40836;&#21644;&#32932;&#33394;&#20316;&#20026;&#21463;&#20445;&#25252;&#29305;&#24449;&#30340;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#35813;&#26041;&#27861;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness is crucial when training a deep-learning discriminative model, especially in the facial domain. Models tend to correlate specific characteristics (such as age and skin color) with unrelated attributes (downstream tasks), resulting in biases which do not correspond to reality. It is common knowledge that these correlations are present in the data and are then transferred to the models during training. This paper proposes a method to mitigate these correlations to improve fairness. To do so, we learn interpretable and meaningful paths lying in the semantic space of a pre-trained diffusion model (DiffAE) -- such paths being supervised by contrastive text dipoles. That is, we learn to edit protected characteristics (age and skin color). These paths are then applied to augment images to improve the fairness of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on several downstream tasks with age and skin color as protected characteristics. As a proxy for fairnes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01571</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Preserving the knowledge of long clinical texts using aggregated ensembles of large language models. (arXiv:2311.01571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#20004;&#20010;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#65292;&#22914;&#20837;&#38498;&#35760;&#24405;&#12289;&#20986;&#38498;&#23567;&#32467;&#21644;&#36827;&#23637;&#35760;&#24405;&#65292;&#21253;&#21547;&#20016;&#23500;&#32780;&#23453;&#36149;&#30340;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#20020;&#24202;&#25991;&#26412;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#36755;&#20837;&#38271;&#24230;&#30340;&#38480;&#21046;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32858;&#21512;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#20445;&#30041;&#38271;&#31687;&#20020;&#24202;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#21333;&#29420;&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#25110;&#25991;&#26412;&#32858;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#23398;&#20064;&#19982;&#25991;&#26412;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#22312;&#20004;&#20010;&#20020;&#24202;&#32467;&#26524;&#39044;&#27979;&#20219;&#21153;&#65288;&#27515;&#20129;&#39044;&#27979;&#21644;&#20303;&#38498;&#22825;&#25968;&#39044;&#27979;&#65289;&#19978;&#35757;&#32451;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#22522;&#32447;&#12289;&#29420;&#31435;&#30340;&#38598;&#25104;&#21644;&#32858;&#21512;&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#26102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical texts, such as admission notes, discharge summaries, and progress notes, contain rich and valuable information that can be used for various clinical outcome prediction tasks. However, applying large language models, such as BERT-based models, to clinical texts poses two major challenges: the limitation of input length and the diversity of data sources. This paper proposes a novel method to preserve the knowledge of long clinical texts using aggregated ensembles of large language models. Unlike previous studies which use model ensembling or text aggregation methods separately, we combine ensemble learning with text aggregation and train multiple large language models on two clinical outcome tasks: mortality prediction and length of stay prediction. We show that our method can achieve better results than baselines, ensembling, and aggregation individually, and can improve the performance of large language models while handling long inputs and diverse datasets. We conduct extensi
&lt;/p&gt;</description></item><item><title>&#23545;&#22522;&#30784;&#27169;&#22411;&#24066;&#22330;&#30340;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36235;&#21521;&#33258;&#28982;&#22404;&#26029;&#65292;&#38656;&#35201;&#21452;&#31649;&#40784;&#19979;&#30340;&#30417;&#31649;&#21709;&#24212;&#26469;&#30830;&#20445;&#24066;&#22330;&#31454;&#20105;&#21644;&#27169;&#22411;&#36136;&#37327;&#26631;&#20934;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#23545;&#31038;&#20250;&#31119;&#21033;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2311.01550</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#24066;&#22330;&#38598;&#20013;&#24230;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Market Concentration Implications of Foundation Models. (arXiv:2311.01550v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01550
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22522;&#30784;&#27169;&#22411;&#24066;&#22330;&#30340;&#32467;&#26500;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36235;&#21521;&#33258;&#28982;&#22404;&#26029;&#65292;&#38656;&#35201;&#21452;&#31649;&#40784;&#19979;&#30340;&#30417;&#31649;&#21709;&#24212;&#26469;&#30830;&#20445;&#24066;&#22330;&#31454;&#20105;&#21644;&#27169;&#22411;&#36136;&#37327;&#26631;&#20934;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#23545;&#31038;&#20250;&#31119;&#21033;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#30784;&#27169;&#22411;&#24066;&#22330;&#30340;&#32467;&#26500;&#65292;&#21363;&#37027;&#20123;&#39537;&#21160;ChatGPT&#31561;&#24212;&#29992;&#24182;&#36866;&#29992;&#20110;&#19979;&#28216;&#29992;&#36884;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#31454;&#20105;&#25919;&#31574;&#21644;&#30417;&#31649;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#33021;&#21147;&#26368;&#24378;&#30340;&#27169;&#22411;&#26377;&#19968;&#31181;&#21521;&#33258;&#28982;&#22404;&#26029;&#30340;&#36235;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#38754;&#23545;&#24191;&#38420;&#30340;&#24066;&#22330;&#12290;&#36825;&#38656;&#35201;&#21452;&#31649;&#40784;&#19979;&#30340;&#30417;&#31649;&#21709;&#24212;&#65306;&#65288;&#19968;&#65289;&#21453;&#22404;&#26029;&#26426;&#26500;&#38656;&#35201;&#36890;&#36807;&#35299;&#20915;&#25112;&#30053;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#30830;&#20445;&#22404;&#26029;&#19981;&#20250;&#22402;&#30452;&#20256;&#25773;&#21040;&#19979;&#28216;&#29992;&#36884;&#65292;&#26469;&#30830;&#20445;&#24066;&#22330;&#30340;&#21487;&#31454;&#20105;&#24615;&#65307;&#65288;&#20108;&#65289;&#32771;&#34385;&#21040;&#24066;&#22330;&#32422;&#26463;&#30340;&#38477;&#20302;&#65292;&#30417;&#31649;&#26426;&#26500;&#24212;&#30830;&#20445;&#26368;&#20855;&#33021;&#21147;&#30340;&#27169;&#22411;&#31526;&#21512;&#36275;&#22815;&#30340;&#36136;&#37327;&#26631;&#20934;&#65288;&#21253;&#25324;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#38750;&#27495;&#35270;&#12289;&#21487;&#38752;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#65289;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#23545;&#31038;&#20250;&#31119;&#21033;&#20570;&#20986;&#36129;&#29486;&#12290;&#30417;&#31649;&#26426;&#26500;&#36824;&#24212;&#30830;&#20445;AI&#21644;&#38750;AI&#24212;&#29992;&#20043;&#38388;&#26377;&#19968;&#20010;&#20844;&#24179;&#30340;&#30417;&#31649;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the structure of the market for foundation models, i.e., large AI models such as those that power ChatGPT and that are adaptable to downstream uses, and we examine the implications for competition policy and regulation. We observe that the most capable models will have a tendency towards natural monopoly and may have potentially vast markets. This calls for a two-pronged regulatory response: (i) Antitrust authorities need to ensure the contestability of the market by tackling strategic behavior, in particular by ensuring that monopolies do not propagate vertically to downstream uses, and (ii) given the diminished potential for market discipline, there is a role for regulators to ensure that the most capable models meet sufficient quality standards (including safety, privacy, non-discrimination, reliability and interoperability standards) to maximally contribute to social welfare. Regulators should also ensure a level regulatory playing field between AI and non-AI application
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#26800;&#24615;&#33021;&#36827;&#34892;&#24320;&#25918;&#24335;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#23545;&#35937;&#30340;&#30693;&#35782;&#26469;&#20272;&#35745;&#32858;&#31867;&#20013;&#24515;&#21644;&#22823;&#23567;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#23545;&#35937;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32858;&#31867;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2311.01540</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#26800;&#24615;&#33021;&#36827;&#34892;&#24320;&#25918;&#24335;&#23545;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open-Set Object Recognition Using Mechanical Properties During Interaction. (arXiv:2311.01540v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#26800;&#24615;&#33021;&#36827;&#34892;&#24320;&#25918;&#24335;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#23545;&#35937;&#30340;&#30693;&#35782;&#26469;&#20272;&#35745;&#32858;&#31867;&#20013;&#24515;&#21644;&#22823;&#23567;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#35782;&#21035;&#23545;&#35937;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32858;&#31867;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22810;&#25968;&#35302;&#35273;&#26426;&#22120;&#20154;&#22312;&#23553;&#38381;&#38598;&#26465;&#20214;&#19979;&#36816;&#34892;&#65292;&#20294;&#23427;&#20204;&#22312;&#36229;&#20986;&#26426;&#22120;&#20154;&#30693;&#35782;&#33539;&#22260;&#30340;&#24320;&#25918;&#26465;&#20214;&#19979;&#25805;&#20316;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26426;&#26800;&#24615;&#33021;&#36827;&#34892;&#24320;&#25918;&#24335;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#20197;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#24182;&#36880;&#27493;&#26631;&#35760;&#26032;&#23545;&#35937;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#21033;&#29992;&#24050;&#30693;&#23545;&#35937;&#30340;&#30693;&#35782;&#26469;&#20272;&#35745;&#32858;&#31867;&#20013;&#24515;&#21644;&#22823;&#23567;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;&#31639;&#27861;&#37027;&#26679;&#38543;&#26426;&#36873;&#25321;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#23454;&#38469;&#23545;&#35937;&#30340;&#20132;&#20114;&#20013;&#20272;&#35745;&#30340;&#26426;&#26800;&#24615;&#33021;&#36827;&#34892;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#30001;&#26032;&#39062;&#24615;&#26816;&#27979;&#22120;&#36129;&#29486;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#23545;&#35937;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32858;&#31867;&#31639;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36229;&#21442;&#25968;&#30740;&#31350;&#26174;&#31034;&#65292;&#32858;&#31867;&#32467;&#26524;&#19982;&#32858;&#31867;&#22823;&#23567;&#23494;&#20999;&#30456;&#20851;&#65292;&#38656;&#35201;&#36866;&#24403;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
while most of the tactile robots are operated in close-set conditions, it is challenging for them to operate in open-set conditions where test objects are beyond the robots' knowledge. We proposed an open-set recognition framework using mechanical properties to recongise known objects and incrementally label novel objects. The main contribution is a clustering algorithm that exploits knowledge of known objects to estimate cluster centre and sizes, unlike a typical algorithm that randomly selects them. The framework is validated with the mechanical properties estimated from a real object during interaction. The results show that the framework could recognise objects better than alternative methods contributed by the novelty detector. Importantly, our clustering algorithm yields better clustering performance than other methods. Furthermore, the hyperparameters studies show that cluster size is important to clustering results and needed to be tuned properly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.01534</link><description>&lt;p&gt;
&#22823;&#22411;&#22320;&#22270;&#19978;&#30340;&#25353;&#38656;&#22478;&#24066;&#20986;&#34892;&#38382;&#39064;&#30340;&#36817;&#20284;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version). (arXiv:2311.01534v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#22810;&#26234;&#33021;&#20307;&#20986;&#31199;&#36710;&#36335;&#24452;&#38382;&#39064;&#65292;&#26410;&#26469;&#20056;&#36710;&#35831;&#27714;&#30340;&#20301;&#32622;&#21644;&#25968;&#37327;&#20107;&#20808;&#26410;&#30693;&#65292;&#20294;&#36981;&#24490;&#20272;&#35745;&#30340;&#32463;&#39564;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22914;&#26524;&#22522;&#30784;&#31574;&#30053;&#26159;&#31283;&#23450;&#30340;&#65292;&#37027;&#20040;&#22522;&#20110;&#28378;&#21160;&#30340;&#31639;&#27861;&#19982;&#36825;&#26679;&#30340;&#22522;&#30784;&#31574;&#30053;&#20135;&#29983;&#25509;&#36817;&#26368;&#20248;&#30340;&#31283;&#23450;&#31574;&#30053;&#12290;&#23613;&#31649;&#22522;&#20110;&#28378;&#21160;&#30340;&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#23398;&#20064;&#20855;&#26377;&#23545;&#26410;&#26469;&#38656;&#27714;&#32771;&#34385;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#22823;&#22411;&#22478;&#24066;&#29615;&#22659;&#21487;&#33021;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#12290;&#22823;&#22411;&#29615;&#22659;&#24448;&#24448;&#26377;&#22823;&#37327;&#35831;&#27714;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#22411;&#30340;&#20986;&#31199;&#36710;&#38431;&#20445;&#35777;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#65288;&#36880;&#19968;&#65289;&#28378;&#21160;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#65292;&#20854;&#20013;&#35745;&#31639;&#22797;&#26434;&#24615;&#38543;&#20195;&#29702;&#25968;&#37327;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#36880;&#19968;&#28378;&#21160;&#20026;&#22522;&#30784;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but follow an estimated empirical distribution. Recent theory has shown that if a base policy is stable then a rollout-based algorithm with such a base policy produces a near-optimal stable policy. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive. Large environments tend to have a large volume of requests, and hence require a large fleet of taxis to guarantee stability. In this paper, we aim to address the computational bottleneck of multiagent (one-at-a-time) rollout, where the computational complexity grows linearly in the number of agents. We propose an approximate one-at-a-time rollout-based two-phase algorithm that reduces the computatio
&lt;/p&gt;</description></item><item><title>NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01530</link><description>&lt;p&gt;
NOD-TAMP:&#22810;&#27493;&#39588;&#25805;&#32437;&#35268;&#21010;&#20013;&#30340;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01530
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#23621;&#21644;&#24037;&#21378;&#29615;&#22659;&#20013;&#24320;&#21457;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#26102;&#31243;&#20219;&#21153;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20197;&#21450;&#38656;&#35201;&#22312;&#21508;&#31181;&#29289;&#20307;&#24418;&#29366;&#21644;&#22330;&#26223;&#24067;&#23616;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#30340;&#20551;&#35774;&#65292;&#22914;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26032;&#39062;&#32972;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;NODs&#65289;&#22312;&#29289;&#20307;&#21644;&#22330;&#26223;&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;NOD-TAMP&#20174;&#23569;&#25968;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#30701;&#30340;&#25805;&#32437;&#36712;&#36857;&#65292;&#20351;&#29992;NOD&#29305;&#24449;&#26469;&#35843;&#25972;&#36825;&#20123;&#36712;&#36857;&#65292;&#24182;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#24191;&#27867;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;NOD-TAMP&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25805;&#32437;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;Java&#21644;Python&#30340;&#35299;&#37322;&#22312;&#21487;&#35835;&#24615;&#21644;&#35789;&#27719;&#23494;&#24230;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2311.01490</link><description>&lt;p&gt;
&#24403;&#34987;&#35201;&#27714;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Behavior of Large Language Models When Prompted to Generate Code Explanations. (arXiv:2311.01490v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;Java&#21644;Python&#30340;&#35299;&#37322;&#22312;&#21487;&#35835;&#24615;&#21644;&#35789;&#27719;&#23494;&#24230;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20171;&#32461;&#32534;&#31243;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#20195;&#30721;&#31034;&#20363;&#35299;&#37322;&#26102;&#30340;&#34892;&#20026;&#12290;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#35299;&#37322;&#30340;&#24615;&#36136;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#30340;&#25514;&#36766;&#12289;&#34987;&#35299;&#37322;&#30340;&#30446;&#26631;&#20195;&#30721;&#31034;&#20363;&#12289;&#32534;&#31243;&#35821;&#35328;&#12289;&#28201;&#24230;&#21442;&#25968;&#21644;LLM&#30340;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;Java&#21644;Python&#32780;&#35328;&#65292;&#23427;&#20204;&#22312;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#20445;&#25345;&#19968;&#33268;&#65306;&#21487;&#35835;&#24615;&#27700;&#24179;&#22823;&#32422;&#22312;7-8&#24180;&#32423;&#65292;&#20197;&#21450;&#35789;&#27719;&#23494;&#24230;&#65292;&#21363;&#19982;&#24635;&#35299;&#37322;&#22823;&#23567;&#30456;&#23545;&#30340;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#30340;&#30456;&#23545;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35299;&#37322;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#24471;&#20998;&#24456;&#39640;&#65292;&#20294;&#22312;&#23436;&#25972;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#24471;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper systematically explores how Large Language Models (LLMs) generate explanations of code examples of the type used in intro-to-programming courses. As we show, the nature of code explanations generated by LLMs varies considerably based on the wording of the prompt, the target code examples being explained, the programming language, the temperature parameter, and the version of the LLM. Nevertheless, they are consistent in two major respects for Java and Python: the readability level, which hovers around 7-8 grade, and lexical density, i.e., the relative size of the meaningful words with respect to the total explanation size. Furthermore, the explanations score very high in correctness but less on three other metrics: completeness, conciseness, and contextualization.
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>RENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#20851;&#31995;&#25552;&#21462;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#23454;&#20307;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#23454;&#26102;&#35299;&#26512;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01472</link><description>&lt;p&gt;
&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#20851;&#31995;&#65288;RENA&#65289;&#65306;&#29992;&#20110;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance. (arXiv:2311.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01472
&lt;/p&gt;
&lt;p&gt;
RENA&#26159;&#19968;&#31181;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#20851;&#31995;&#25552;&#21462;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#23454;&#20307;&#21644;&#35821;&#20041;&#20851;&#31995;&#65292;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#23454;&#26102;&#35299;&#26512;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#20174;&#26032;&#38395;&#25991;&#31456;&#65288;RENA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#21450;&#20854;&#35821;&#20041;&#20851;&#31995;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;React&#26694;&#26550;&#26500;&#24314;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36755;&#20837;&#26032;&#38395;&#25991;&#31456;&#24182;&#20174;&#20004;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#65292;&#20197;&#29983;&#25104;&#25152;&#25552;&#20379;&#25991;&#26412;&#20013;&#30340;&#20851;&#31995;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#22240;&#27492;&#65292;RENA&#20801;&#35768;&#23454;&#26102;&#35299;&#26512;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20026;&#24320;&#28304;&#24773;&#25253;&#39537;&#21160;&#30340;&#27969;&#34892;&#30149;&#39044;&#35686;&#31995;&#32479;EPIWATCH&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction from News Articles (RENA) is a browser-based tool designed to extract key entities and their semantic relationships in English language news articles related to infectious diseases. Constructed using the React framework, this system presents users with an elegant and user-friendly interface. It enables users to input a news article and select from a choice of two models to generate a comprehensive list of relations within the provided text. As a result, RENA allows real-time parsing of news articles to extract key information for epidemic surveillance, contributing to EPIWATCH, an open-source intelligence-based epidemic warning system.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.01463</link><description>&lt;p&gt;
&#25903;&#25345;&#21487;&#20449;&#24230;&#30340;LLM&#21019;&#24314;&#36807;&#31243;&#65306;&#22788;&#29702;&#21307;&#30103;AI&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01463
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#30340;LLM&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#30528;&#37325;&#20110;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30701;&#26102;&#38388;&#20869;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36805;&#36895;&#22686;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#21307;&#30103;&#39046;&#22495;&#23545;&#20854;&#37319;&#29992;&#23384;&#22312;&#29369;&#35947;&#12290;&#37492;&#20110;&#21307;&#30103;&#20107;&#20851;&#37325;&#22823;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#29978;&#33267;&#25552;&#20986;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#20043;&#21069;&#19981;&#24212;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21019;&#24314;&#21487;&#38752;&#12289;&#21487;&#20449;&#21644;&#26080;&#20559;&#32622;&#27169;&#22411;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#36825;&#26159;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#22312;&#21307;&#30103;&#32972;&#26223;&#19979;&#23545;&#24187;&#35273;&#36827;&#34892;&#37327;&#21270;&#12289;&#39564;&#35777;&#21644;&#32531;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#21307;&#30103;&#39046;&#22495;&#26410;&#26469;&#30340;&#21487;&#33021;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proliferated across multiple domains in as short period of time. There is however hesitation in the medical and healthcare domain towards their adoption because of issues like factuality, coherence, and hallucinations. Give the high stakes nature of healthcare, many researchers have even cautioned against its usage until these issues are resolved. The key to the implementation and deployment of LLMs in healthcare is to make these models trustworthy, transparent (as much possible) and explainable. In this paper we describe the key elements in creating reliable, trustworthy, and unbiased models as a necessary condition for their adoption in healthcare. Specifically we focus on the quantification, validation, and mitigation of hallucinations in the context in healthcare. Lastly, we discuss how the future of LLMs in healthcare may look like.
&lt;/p&gt;</description></item><item><title>FacadeNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35282;&#30456;&#20851;&#20803;&#32032;&#36827;&#34892;&#31934;&#30830;&#20462;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.01240</link><description>&lt;p&gt;
FacadeNet: &#36890;&#36807;&#36873;&#25321;&#24615;&#32534;&#36753;&#36827;&#34892;&#26465;&#20214;&#31435;&#38754;&#32508;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01240
&lt;/p&gt;
&lt;p&gt;
FacadeNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23454;&#29616;&#20102;&#20174;&#19981;&#21516;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35282;&#30456;&#20851;&#20803;&#32032;&#36827;&#34892;&#31934;&#30830;&#20462;&#25913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FacadeNet&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#19981;&#21516;&#30340;&#35270;&#35282;&#21512;&#25104;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23558;&#19968;&#20010;&#31435;&#38754;&#30340;&#21333;&#19968;&#35270;&#22270;&#21644;&#25152;&#38656;&#30340;&#35270;&#35282;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#19968;&#20010;&#20174;&#19981;&#21516;&#35270;&#35282;&#30475;&#21040;&#30340;&#31435;&#38754;&#22270;&#20687;&#12290;&#20026;&#20102;&#31934;&#30830;&#20462;&#25913;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#20803;&#32032;&#65288;&#22914;&#31383;&#25143;&#21644;&#38376;&#65289;&#65292;&#21516;&#26102;&#20445;&#30041;&#19982;&#35270;&#35282;&#26080;&#20851;&#30340;&#32467;&#26500;&#65288;&#22914;&#22681;&#22721;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36873;&#25321;&#24615;&#32534;&#36753;&#27169;&#22359;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24314;&#31569;&#31435;&#38754;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce FacadeNet, a deep learning approach for synthesizing building facade images from diverse viewpoints. Our method employs a conditional GAN, taking a single view of a facade along with the desired viewpoint information and generates an image of the facade from the distinct viewpoint. To precisely modify view-dependent elements like windows and doors while preserving the structure of view-independent components such as walls, we introduce a selective editing module. This module leverages image embeddings extracted from a pre-trained vision transformer. Our experiments demonstrated state-of-the-art performance on building facade generation, surpassing alternative methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20415;&#22312;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2311.01057</link><description>&lt;p&gt;
&#24102;&#26377;TinyissimoYOLO&#30340;AI&#38598;&#25104;&#26234;&#33021;&#30524;&#38236;&#19978;&#30340;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#36229;&#39640;&#25928;&#35774;&#22791;&#20869;&#30446;&#26631;&#26816;&#27979;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20415;&#22312;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#19978;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30524;&#38236;&#20511;&#21161;&#23574;&#31471;&#35745;&#31639;&#25216;&#26415;&#12289;&#21152;&#36895;&#30828;&#20214;&#26550;&#26500;&#21644;&#23567;&#22411;AI&#31639;&#27861;&#65292;&#27491;&#36805;&#36895;&#33719;&#24471;&#20808;&#36827;&#21151;&#33021;&#12290;&#22312;&#38754;&#21521;&#20840;&#22825;&#20351;&#29992;&#20197;&#23454;&#29616;&#28385;&#24847;&#29992;&#25143;&#20307;&#39564;&#26102;&#65292;&#23558;AI&#38598;&#25104;&#21040;&#20855;&#26377;&#23567;&#23610;&#23544;&#21644;&#26377;&#38480;&#30005;&#27744;&#23481;&#37327;&#30340;&#26234;&#33021;&#30524;&#38236;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#21033;&#29992;&#26032;&#22411;&#20302;&#21151;&#32791;&#22788;&#29702;&#22120;&#23454;&#29616;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#20197;&#22312;&#26234;&#33021;&#30524;&#38236;&#20013;&#23454;&#29616;&#38271;&#26102;&#38388;&#36830;&#32493;&#36816;&#34892;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26234;&#33021;&#30524;&#38236;&#22312;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#24773;&#20917;&#19979;&#30340;&#33021;&#37327;&#21644;&#26102;&#24310;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#30524;&#38236;&#21407;&#22411;&#20316;&#20026;&#30740;&#31350;&#24179;&#21488;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#24494;&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;AI&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#26032;&#22411;&#27627;&#29926;&#32423;&#21151;&#29575;RISC-V&#24182;&#34892;&#22788;&#29702;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#36890;&#20449;&#30340;&#20302;&#21151;&#32791;&#34013;&#29273;&#27169;&#22359;&#12290;&#26234;&#33021;&#30524;&#38236;&#38598;&#25104;&#20102;&#22270;&#20687;&#21644;&#38899;&#39057;&#24863;&#24212;&#25509;&#21475;&#31561;&#30005;&#28304;&#24490;&#29615;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart glasses are rapidly gaining advanced functionality thanks to cutting-edge computing technologies, accelerated hardware architectures, and tiny AI algorithms. Integrating AI into smart glasses featuring a small form factor and limited battery capacity is still challenging when targeting full-day usage for a satisfactory user experience. This paper illustrates the design and implementation of tiny machine-learning algorithms exploiting novel low-power processors to enable prolonged continuous operation in smart glasses. We explore the energy- and latency-efficient of smart glasses in the case of real-time object detection. To this goal, we designed a smart glasses prototype as a research platform featuring two microcontrollers, including a novel milliwatt-power RISC-V parallel processor with a hardware accelerator for visual AI, and a Bluetooth low-power module for communication. The smart glasses integrate power cycling mechanisms, including image and audio sensing interfaces. Fur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.20550</link><description>&lt;p&gt;
CapsFusion: &#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20550
&lt;/p&gt;
&lt;p&gt;
CapsFusion&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#25191;&#34892;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#35268;&#27169;&#22522;&#20110;&#32593;&#32476;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#22312;&#36825;&#19968;&#25104;&#21151;&#20013;&#36215;&#30528;&#26681;&#26412;&#24615;&#30340;&#36129;&#29486;&#65292;&#20294;&#23384;&#22312;&#30528;&#36807;&#22810;&#30340;&#22122;&#22768;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#30001;&#29983;&#25104;&#24335;&#23383;&#24149;&#27169;&#22411;&#21512;&#25104;&#30340;&#26367;&#20195;&#23383;&#24149;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#21512;&#25104;&#23383;&#24149;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#21487;&#25193;&#23637;&#24615;&#19981;&#36275;&#21644;&#19990;&#30028;&#30693;&#35782;&#20007;&#22833;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#20854;&#21021;&#22987;&#22522;&#20934;&#25104;&#21151;&#20013;&#22823;&#37096;&#20998;&#34987;&#25513;&#30422;&#20102;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#26159;&#29616;&#26377;&#21512;&#25104;&#23383;&#24149;&#20013;&#36807;&#20110;&#31616;&#21270;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#32570;&#20047;&#30693;&#35782;&#32454;&#33410;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#12289;&#26356;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CapsFusion&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25972;&#21512;&#21644;&#32454;&#21270;&#26469;&#33258;&#32593;&#32476;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#21512;&#25104;&#23383;&#24149;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#24555;&#36895;&#36866;&#24212;&#22810;&#35270;&#22270;&#36816;&#21160;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#26032;&#35757;&#32451;&#21608;&#26399;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.20414</link><description>&lt;p&gt;
&#22810;&#35270;&#35273;&#36816;&#21160;&#31995;&#32479;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta Learning for Multi-View Visuomotor Systems. (arXiv:2310.20414v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#24555;&#36895;&#36866;&#24212;&#22810;&#35270;&#22270;&#36816;&#21160;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#26032;&#35757;&#32451;&#21608;&#26399;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#26426;&#22120;&#20154;&#30340;&#22810;&#35270;&#35273;&#36816;&#21160;&#31995;&#32479;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#30456;&#26426;&#37197;&#32622;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#26469;&#24494;&#35843;&#24863;&#30693;&#32593;&#32476;&#65292;&#21516;&#26102;&#20445;&#25345;&#31574;&#30053;&#32593;&#32476;&#19981;&#21464;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#38656;&#30340;&#26032;&#35757;&#32451;&#21608;&#26399;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569;&#65292;&#20197;&#23454;&#29616;&#22522;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach for quickly adapting a multi-view visuomotor system for robots to varying camera configurations from the baseline setup. It utilises meta-learning to fine-tune the perceptual network while keeping the policy network fixed. Experimental results demonstrate a significant reduction in the number of new training episodes needed to attain baseline performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.20381</link><description>&lt;p&gt;
GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#21644;&#21307;&#23398;VQA&#26041;&#38754;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;&#19978;&#20173;&#38656;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GPT-4V&#22312;&#19981;&#21516;&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;&#35270;&#35273;&#23450;&#20301;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;GPT-4V&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#39318;&#20010;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;&#22522;&#20934;&#30340;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#32473;&#20986;&#32467;&#26500;&#33391;&#22909;&#30340;&#25552;&#31034;&#26102;&#65292;GPT-4V&#22312;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#30340;&#29983;&#25104;&#25551;&#36848;&#24615;&#25253;&#21578;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;MIMIC-CXR&#25968;&#25454;&#38598;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#25581;&#31034;&#20102;&#26576;&#20123;&#35780;&#20272;&#25351;&#26631;(&#22914;CIDEr)&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#22312;&#21307;&#23398;VQA&#39046;&#22495;&#65292;GPT-4V&#22312;&#21306;&#20998;&#38382;&#39064;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#65292;&#20294;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#19981;&#21450;&#29616;&#26377;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#24120;&#35268;&#35780;&#20272;&#25351;&#26631;&#22914;BLEU&#20998;&#25968;&#30340;&#23616;&#38480;&#24615;&#65292;&#21628;&#21505;&#24320;&#21457;&#26356;&#22909;&#30340;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20033</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#24635;&#32467;&#20013;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization. (arXiv:2310.20033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#25968;&#25454;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;LLaMA&#31995;&#21015;&#22312;&#25429;&#25417;&#21644;&#27987;&#32553;&#20851;&#38190;&#19978;&#19979;&#25991;&#20449;&#24687;&#21450;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31038;&#21306;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#34394;&#26500;&#38382;&#39064;&#30340;&#25285;&#24551;&#20173;&#22312;&#19981;&#26029;&#19978;&#21319;&#12290;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#34394;&#26500;&#30340;&#25688;&#35201;&#65292;&#36825;&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#65289;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#38169;&#35823;&#30340;&#35786;&#26029;&#12290;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23454;&#29616;&#20107;&#23454;&#19968;&#33268;&#24615;&#30340;&#25215;&#35834;&#65292;&#20294;&#36825;&#31181;&#35757;&#32451;&#36807;&#31243;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#65292;&#32780;&#22312;&#20020;&#24202;&#39046;&#22495;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31649;&#36947;&#65292;&#20351;&#29992;ChatGPT&#20195;&#26367;&#20154;&#31867;&#19987;&#23478;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#20197;&#25913;&#21892;&#20020;&#24202;&#31508;&#35760;&#24635;&#32467;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summari
&lt;/p&gt;</description></item><item><title>JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.19180</link><description>&lt;p&gt;
JEN-1 Composer: &#19968;&#20010;&#29992;&#20110;&#39640;&#20445;&#30495;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19180
&lt;/p&gt;
&lt;p&gt;
JEN-1 Composer&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#39640;&#20445;&#30495;&#12289;&#28789;&#27963;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#38899;&#36712;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#38899;&#20048;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#21512;&#25104;&#20219;&#21153;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#38899;&#36712;&#29983;&#25104;&#30340;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#21407;&#22987;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#21333;&#29420;&#32452;&#25104;&#21644;&#32452;&#21512;&#22810;&#38899;&#36712;&#30340;&#28789;&#27963;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#20316;&#26354;&#23478;&#30340;&#20856;&#22411;&#24037;&#20316;&#27969;&#31243;&#19981;&#21516;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEN-1 Composer&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#39640;&#25928;&#22320;&#24314;&#27169;&#22810;&#38899;&#36712;&#38899;&#20048;&#30340;&#36793;&#32536;&#12289;&#26465;&#20214;&#21644;&#32852;&#21512;&#20998;&#24067;&#12290;JEN-1 Composer&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#22320;&#25972;&#21512;&#20219;&#20309;&#22522;&#20110;&#25193;&#25955;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#20363;&#22914;Jen-1&#65292;&#22686;&#24378;&#20854;&#22810;&#21151;&#33021;&#22810;&#38899;&#36712;&#38899;&#20048;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35838;&#31243;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#36880;&#27493;&#25351;&#23548;&#27169;&#22411;&#20174;&#21333;&#38899;&#36712;&#29983;&#25104;&#21040;&#28789;&#27963;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rapid advances in generative artificial intelligence, the text-to-music synthesis task has emerged as a promising direction for music generation from scratch. However, finer-grained control over multi-track generation remains an open challenge. Existing models exhibit strong raw generation capability but lack the flexibility to compose separate tracks and combine them in a controllable manner, differing from typical workflows of human composers. To address this issue, we propose JEN-1 Composer, a unified framework to efficiently model marginal, conditional, and joint distributions over multi-track music via a single model. JEN-1 Composer framework exhibits the capacity to seamlessly incorporate any diffusion-based music generation system, \textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music generation. We introduce a curriculum training strategy aimed at incrementally instructing the model in the transition from single-track generation to the flexible genera
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.09219</link><description>&lt;p&gt;
"&#20975;&#21033;&#26159;&#19968;&#20010;&#28201;&#26262;&#30340;&#20154;&#65292;&#32422;&#29791;&#22827;&#26159;&#19968;&#20010;&#27036;&#26679;": LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"Kelly is a Warm Person, Joseph is a Role Model": Gender Biases in LLM-Generated Reference Letters. (arXiv:2310.09219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#30740;&#31350;&#65292;&#24182;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#26469;&#23637;&#29616;&#36890;&#36807;&#35821;&#35328;&#39118;&#26684;&#21644;&#35789;&#27719;&#20869;&#23481;&#26469;&#20307;&#29616;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#29992;&#25143;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21327;&#21161;&#25776;&#20889;&#21508;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#65292;&#21253;&#25324;&#25512;&#33616;&#20449;&#31561;&#32844;&#19994;&#25991;&#20214;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#26041;&#20415;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#24341;&#20837;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#30001;&#20110;&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#21487;&#33021;&#34987;&#29992;&#25143;&#30452;&#25509;&#22312;&#32844;&#19994;&#25110;&#23398;&#26415;&#22330;&#26223;&#20013;&#20351;&#29992;&#65292;&#23427;&#20204;&#26377;&#21487;&#33021;&#36896;&#25104;&#30452;&#25509;&#30340;&#31038;&#20250;&#20260;&#23475;&#65292;&#22914;&#38477;&#20302;&#22899;&#24615;&#30003;&#35831;&#32773;&#30340;&#25104;&#21151;&#29575;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#26469;&#30340;&#32531;&#35299;&#21644;&#30417;&#25511;&#65292;&#20840;&#38754;&#30740;&#31350;&#27492;&#31867;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#21644;&#30456;&#20851;&#20260;&#23475;&#21183;&#22312;&#24517;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#25512;&#33616;&#20449;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#30740;&#31350;&#12290;&#21463;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#26469;&#23637;&#29616;LLM&#29983;&#25104;&#30340;&#20449;&#20214;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#35821;&#35328;&#39118;&#26684;&#30340;&#20559;&#35265;&#21644;&#35789;&#27719;&#20869;&#23481;&#30340;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25512;&#33616;&#20449;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models advance, users have started to utilize Large Language Models (LLMs) to assist in writing various types of content, including professional documents such as recommendation letters. Despite their convenience, these applications introduce unprecedented fairness concerns. As generated reference letters might be directly utilized by users in professional or academic scenarios, they have the potential to cause direct social harms, such as lowering success rates for female applicants. Therefore, it is imminent and necessary to comprehensively study fairness issues and associated harms in such real-world use cases for future mitigation and monitoring. In this paper, we critically examine gender bias in LLM-generated reference letters. Inspired by findings in social science, we design evaluation methods to manifest gender biases in LLM-generated letters through 2 dimensions: biases in language style and biases in lexical content. Furthermore, we investigate the ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#65292;&#20026;&#20154;&#31867;&#35299;&#37322;&#25552;&#20379;&#20102;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#24402;&#22240;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.05619</link><description>&lt;p&gt;
&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Dynamic Top-k Estimation Consolidates Disagreement between Feature Attribution Methods. (arXiv:2310.05619v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#21160;&#24577; Top-k &#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#25972;&#21512;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#65292;&#20026;&#20154;&#31867;&#35299;&#37322;&#25552;&#20379;&#20102;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#24402;&#22240;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#20998;&#25968;&#29992;&#20110;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034; k &#20010;&#26631;&#35760;&#26469;&#21521;&#29992;&#25143;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#26469;&#30830;&#23450;&#24212;&#26174;&#31034;&#30340;&#26368;&#20339; k &#20010;&#26631;&#35760;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21477;&#23376;&#20043;&#38388;&#26159;&#21160;&#24577;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#21477;&#23376;&#38271;&#24230;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312; NLI &#20219;&#21153;&#20013;&#27604;&#36739;&#22810;&#31181;&#26041;&#27861;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#29992;&#22266;&#23450;&#30340; k &#21644;&#21160;&#24577;&#30340; k&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#38745;&#24577;&#30340; k &#26102;&#65292;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#21644; Vanilla Gradient &#22312;&#22823;&#22810;&#25968;&#26041;&#27861;&#20043;&#38388;&#21644;&#26041;&#27861;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25351;&#26631;&#19978;&#34920;&#29616;&#24471;&#26368;&#22909;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20351;&#29992;&#21160;&#24577; k &#26102;&#30340;&#20248;&#21183;&#28040;&#22833;&#20102;&#65292;&#32780;&#21160;&#24577; k &#20027;&#35201;&#25913;&#36827;&#20102;&#38598;&#25104;&#26799;&#24230;&#21644; GradientXInput &#30340;&#34920;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#35777;&#26126;&#36890;&#36807;&#20998;&#26512;&#24402;&#22240;&#20998;&#25968;&#30340;&#36830;&#32493;&#23646;&#24615;&#23545;&#20110;&#25972;&#21512;&#20154;&#31867;&#35299;&#37322;&#30340;&#24402;&#22240;&#20449;&#21495;&#26159;&#20855;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution scores are used for explaining the prediction of a text classifier to users by highlighting a k number of tokens. In this work, we propose a way to determine the number of optimal k tokens that should be displayed from sequential properties of the attribution scores. Our approach is dynamic across sentences, method-agnostic, and deals with sentence length bias. We compare agreement between multiple methods and humans on an NLI task, using fixed k and dynamic k. We find that perturbation-based methods and Vanilla Gradient exhibit highest agreement on most method--method and method--human agreement metrics with a static k. Their advantage over other methods disappears with dynamic ks which mainly improve Integrated Gradient and GradientXInput. To our knowledge, this is the first evidence that sequential properties of attribution scores are informative for consolidating attribution signals for human interpretation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#23545;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#19981;&#21516;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#21767;&#35835;&#29305;&#24449;&#65292;&#25552;&#39640;&#21767;&#35835;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05058</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading. (arXiv:2310.05058v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#20998;&#31163;&#30340;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#65292;&#23454;&#29616;&#20102;&#23545;&#27973;&#23618;&#21644;&#28145;&#23618;&#30340;&#19981;&#21516;&#22788;&#29702;&#65292;&#24182;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#21767;&#35835;&#29305;&#24449;&#65292;&#25552;&#39640;&#21767;&#35835;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#20010;&#20307;&#30340;&#21767;&#35835;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#20010;&#20307;&#30340;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#20182;/&#22905;&#30340;&#20960;&#24352;&#38754;&#37096;&#22270;&#20687;&#65292;&#29978;&#33267;&#26159;&#19968;&#24352;&#27973;&#23618;&#32593;&#32476;&#30340;&#21333;&#19968;&#22270;&#20687;&#26469;&#20934;&#30830;&#25551;&#32472;&#65292;&#32780;&#19982;&#35762;&#35805;&#38754;&#37096;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#21160;&#24577;&#29305;&#24449;&#21017;&#38656;&#35201;&#28145;&#23618;&#24207;&#21015;&#32593;&#32476;&#26469;&#20934;&#30830;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27973;&#23618;&#21644;&#28145;&#23618;&#20998;&#21035;&#22788;&#29702;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#21767;&#35835;&#24773;&#22659;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#20010;&#20307;&#29420;&#29305;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#31361;&#20986;&#30340;&#21475;&#33108;&#21644;&#19979;&#39052;&#65289;&#23545;&#20110;&#19981;&#21516;&#30340;&#21333;&#35789;&#21644;&#21457;&#38899;&#30340;&#21767;&#35835;&#34920;&#29616;&#20855;&#26377;&#19981;&#21516;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#33258;&#36866;&#24212;&#22320;&#22686;&#24378;&#25110;&#25233;&#21046;&#29305;&#24449;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#21767;&#35835;&#12290;&#22522;&#20110;&#36825;&#20004;&#28857;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20010;&#20307;&#30340;&#29305;&#24449;&#26469;&#33258;&#21160;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#21487;&#20998;&#31163;&#38544;&#34255;&#21333;&#20803;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for speaker adaptation in lip reading, motivated by two observations. Firstly, a speaker's own characteristics can always be portrayed well by his/her few facial images or even a single image with shallow networks, while the fine-grained dynamic features associated with speech content expressed by the talking face always need deep sequential networks to represent accurately. Therefore, we treat the shallow and deep layers differently for speaker adaptive lip reading. Secondly, we observe that a speaker's unique characteristics ( e.g. prominent oral cavity and mandible) have varied effects on lip reading performance for different words and pronunciations, necessitating adaptive enhancement or suppression of the features for robust lip reading. Based on these two observations, we propose to take advantage of the speaker's own characteristics to automatically learn separable hidden unit contributions with different targets for shallow layers and de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03940</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#38590;&#35270;&#22270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#23545;&#22270;&#20687;&#36755;&#20837;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#32780;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#23545;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#22686;&#24378;&#27969;&#31243;&#20013;&#30340;&#25805;&#20316;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#22914;&#38543;&#26426;&#35009;&#21098;&#25110;&#39068;&#33394;&#25197;&#26354;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#35270;&#22270;&#29983;&#25104;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22312;&#30446;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#20294;&#24378;&#22823;&#30340;&#8220;&#38590;&#35270;&#22270;&#36873;&#25321;&#8221;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#35270;&#22270;&#29983;&#25104;&#25193;&#23637;&#21040;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31574;&#30053;&#21253;&#25324;&#20197;&#19979;&#36845;&#20195;&#27493;&#39588;&#65306;1&#65289;&#38543;&#26426;&#36873;&#25321;&#22810;&#20010;&#35270;&#22270;&#24182;&#21019;&#24314;&#20004;&#20010;&#35270;&#22270;&#30340;&#37197;&#23545;&#65292;2&#65289;&#36827;&#34892;&#21521;&#21069;&#20256;&#36882;...
&lt;/p&gt;
&lt;p&gt;
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04579</link><description>&lt;p&gt;
EGOFALLS:&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65288;arXiv:2309.04579v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33030;&#24369;&#20154;&#32676;&#65292;&#22914;&#32769;&#24180;&#20154;&#65292;&#25684;&#20498;&#24448;&#24448;&#26159;&#20005;&#37325;&#19988;&#24120;&#23548;&#33268;&#27515;&#20129;&#30340;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#20381;&#36182;&#21333;&#20010;&#20256;&#24863;&#22120;&#65288;&#22270;&#20687;&#25110;&#21152;&#36895;&#24230;&#35745;&#65289;&#25429;&#25417;&#25968;&#25454;&#26469;&#35299;&#20915;&#25684;&#20498;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#20174;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#25429;&#25417;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22312;&#25552;&#21462;&#30340;&#25551;&#36848;&#31526;&#20043;&#19978;&#26500;&#24314;&#30340;&#36831;&#20915;&#31574;&#34701;&#21512;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#25105;&#20204;&#35748;&#20026;&#30340;&#31532;&#19968;&#20010;&#20844;&#20849;&#21516;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;14&#20010;&#21463;&#35797;&#32773;&#30340;10,948&#20010;&#35270;&#39057;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#20197;&#35780;&#20272;&#21333;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24615;&#33021;&#65292;&#35270;&#35273;&#20449;&#24687;&#34701;&#21512;&#20197;&#21450;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.02553</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#32763;&#35793;&#30340;&#34892;&#20026;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24212;&#29992;&#35813;&#26041;&#27861;&#65292;&#21457;&#29616;&#22312;&#27979;&#35797;&#32467;&#26524;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#23384;&#22312;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#21487;&#35266;&#23519;&#21040;&#19968;&#33268;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#20013;&#30340;&#34892;&#20026;&#27979;&#35797;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#34892;&#20026;&#26469;&#32454;&#31890;&#24230;&#35780;&#20272;&#31995;&#32479;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#34892;&#20026;&#27979;&#35797;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#27979;&#35797;&#33539;&#22260;&#26377;&#38480;&#12289;&#28085;&#30422;&#30340;&#35821;&#35328;&#31181;&#31867;&#20063;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28304;&#21477;&#23376;&#65292;&#20197;&#27979;&#35797;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22791;&#36873;&#38598;&#65292;&#20197;&#39564;&#35777;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#21542;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20351;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#34892;&#20026;&#27979;&#35797;&#23454;&#38469;&#21487;&#34892;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;&#22810;&#20010;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#24635;&#20307;&#19978;&#36890;&#36807;&#29575;&#19982;&#20256;&#32479;&#20934;&#30830;&#29575;&#24230;&#37327;&#21487;&#35266;&#23519;&#21040;&#30340;&#36235;&#21183;&#30456;&#31526;&#65292;&#20294;&#20173;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metri
&lt;/p&gt;</description></item><item><title>BigFUSE&#26159;&#19968;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20809;&#23376;&#20256;&#25773;&#30340;&#20840;&#23616;&#24433;&#21709;&#21644;&#23616;&#37096;&#22270;&#20687;&#36136;&#37327;&#65292;&#31283;&#23450;&#20102;&#21452;&#35270;&#22270;&#20809;&#29255;&#33639;&#20809;&#26174;&#24494;&#38236;&#20013;&#30340;&#22270;&#20687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.01865</link><description>&lt;p&gt;
BigFUSE: &#22312;&#20855;&#26377;&#22270;&#20687;&#24418;&#25104;&#20808;&#39564;&#30340;&#21452;&#35270;&#22270;&#20809;&#29255;&#33639;&#20809;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#22270;&#20687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior. (arXiv:2309.01865v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01865
&lt;/p&gt;
&lt;p&gt;
BigFUSE&#26159;&#19968;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#20809;&#23376;&#20256;&#25773;&#30340;&#20840;&#23616;&#24433;&#21709;&#21644;&#23616;&#37096;&#22270;&#20687;&#36136;&#37327;&#65292;&#31283;&#23450;&#20102;&#21452;&#35270;&#22270;&#20809;&#29255;&#33639;&#20809;&#26174;&#24494;&#38236;&#20013;&#30340;&#22270;&#20687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#29255;&#33639;&#20809;&#26174;&#24494;&#38236;&#65288;LSFM&#65289;&#26159;&#19968;&#31181;&#24179;&#38754;&#29031;&#26126;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#26679;&#26412;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#65292;&#20294;&#22312;&#20809;&#23376;&#36890;&#36807;&#21402;&#32452;&#32455;&#26102;&#20250;&#20986;&#29616;&#20809;&#25955;&#23556;&#24341;&#36215;&#30340;&#22270;&#20687;&#27169;&#31946;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21452;&#35270;&#22270;&#25104;&#20687;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#36890;&#36807;&#20174;&#30456;&#23545;&#26041;&#21521;&#35266;&#23519;&#26679;&#26412;&#65292;&#21487;&#20197;&#29702;&#24819;&#22320;&#25195;&#25551;&#26679;&#26412;&#30340;&#21508;&#20010;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#22312;&#27604;&#36739;&#20004;&#20010;&#35270;&#22270;&#30340;&#22270;&#20687;&#36136;&#37327;&#26469;&#30830;&#23450;&#28966;&#28857;&#26102;&#65292;&#20250;&#20135;&#29983;&#31354;&#38388;&#19981;&#19968;&#33268;&#30340;&#28966;&#28857;&#24230;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35270;&#37326;&#26377;&#38480;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BigFUSE&#65292;&#19968;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22270;&#20687;&#34701;&#21512;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#32452;&#32455;&#20013;&#20809;&#23376;&#20256;&#25773;&#30340;&#20840;&#23616;&#24433;&#21709;&#65292;&#24182;&#26681;&#25454;&#23616;&#37096;&#22270;&#20687;&#36136;&#37327;&#26469;&#30830;&#23450;&#28966;&#28857;&#21644;&#27169;&#31946;&#65292;&#20174;&#32780;&#31283;&#23450;&#20102;LSFM&#20013;&#30340;&#22270;&#20687;&#34701;&#21512;&#12290;&#21463;&#21040;&#21452;&#35270;&#22270;LSFM&#20013;&#22270;&#20687;&#24418;&#25104;&#20808;&#39564;&#30340;&#21551;&#21457;&#65292;&#22270;&#20687;&#34701;&#21512;&#34987;&#35270;&#20026;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#20272;&#35745;&#28966;&#28857;&#21644;&#27169;&#31946;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples, experiences defocused image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dualview imaging is helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes Theorem, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15457</link><description>&lt;p&gt;
&#20174;SMOTE&#21040;Mixup&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From SMOTE to Mixup for Deep Imbalanced Classification. (arXiv:2308.15457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;SMOTE&#21040;Mixup&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#19981;&#24179;&#34913;&#20998;&#31867;&#12290;&#36890;&#36807;&#23545;SMOTE&#36827;&#34892;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;Mixup&#25216;&#26415;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#25216;&#26415;&#36890;&#36807;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#22909;&#30340;&#20998;&#31867;&#22120;&#22240;&#20026;&#23569;&#25968;&#31867;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#32780;&#22256;&#38590;&#37325;&#37325;&#12290;&#20256;&#32479;&#19978;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#21517;&#23569;&#25968;&#31867;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#65292;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#34987;&#29992;&#26469;&#25913;&#21892;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;SMOTE&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#21542;&#20063;&#26377;&#30410;&#22788;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#21407;&#22987;&#30340;SMOTE&#23545;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#65292;&#24182;&#20351;&#29992;&#36719;&#26631;&#31614;&#22686;&#24378;&#20102;SMOTE&#12290;&#23558;&#24471;&#21040;&#30340;&#36719;SMOTE&#19982;Mixup&#65292;&#19968;&#31181;&#29616;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;&#20256;&#32479;&#21644;&#29616;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#32435;&#20837;&#21516;&#19968;&#20010;&#33539;&#30068;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;Mixup&#36890;&#36807;&#38544;&#24335;&#22320;&#23454;&#29616;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38388;&#38553;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36793;&#30028;&#30340;Mixup&#25216;&#26415;&#65292;&#26356;&#26126;&#30830;&#22320;&#23454;&#29616;&#20102;&#19981;&#24179;&#34913;&#38388;&#38553;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#37117;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01923</link><description>&lt;p&gt;
&#22810;&#37325;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#30740;&#31350;&#65292;&#21457;&#29616;&#23545;&#20110;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#20250;&#22823;&#22823;&#38477;&#20302;&#23545;&#26410;&#32771;&#34385;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36719;&#20214;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#65292;&#20294;&#32771;&#34385;&#21040;&#35768;&#22810;&#29992;&#25143;&#20855;&#26377;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23545;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#21892;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#32771;&#34385;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;ML&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25913;&#21892;&#21333;&#20010;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#22823;&#22823;&#38477;&#20302;&#20102;&#26410;&#32771;&#34385;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;88.3&#65285;&#30340;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#36825;&#31181;&#38477;&#20302;&#65288;&#24179;&#22343;&#20026;57.5&#65285;&#65289;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32771;&#34385;&#21333;&#20010;&#21644;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#20934;&#30830;&#29575;&#25439;&#22833;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#22810;&#23646;&#24615;&#27169;&#24335;&#19979;&#21487;&#20197;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#20445;&#25252;&#23646;&#24615;&#26102;&#65292;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.14283</link><description>&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#65306;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#12289;&#24320;&#25918;&#25361;&#25112;&#21644;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#30340;&#24615;&#36136;&#12289;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#24320;&#25918;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#37117;&#35774;&#35745;&#29992;&#20110;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#22330;&#26223;&#38656;&#35201;&#26356;&#36890;&#29992;&#30340;AI&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#32780;&#19981;&#38656;&#35201;&#19987;&#38376;&#20026;&#23427;&#20204;&#35774;&#35745;&#12290;&#36890;&#29992;&#30446;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;GPAIS&#65289;&#36825;&#20010;&#26415;&#35821;&#34987;&#23450;&#20041;&#20026;&#25351;&#20195;&#36825;&#20123;AI&#31995;&#32479;&#12290;&#23613;&#31649;&#36804;&#20170;&#20026;&#27490;&#65292;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#36275;&#22815;&#24378;&#22823;&#20197;&#27169;&#25311;&#20154;&#31867;&#24182;&#25913;&#36827;&#21508;&#31181;&#26234;&#21147;&#20219;&#21153;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#24895;&#26395;&#12289;&#34394;&#26500;&#30340;&#27010;&#24565;&#65292;&#24182;&#34987;&#35748;&#20026;&#23545;&#25105;&#20204;&#31038;&#20250;&#26500;&#25104;&#39118;&#38505;&#12290;&#34429;&#28982;&#25105;&#20204;&#31163;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#21487;&#33021;&#36824;&#24456;&#36965;&#36828;&#65292;&#20294;GPAIS&#26159;&#29616;&#23454;&#23384;&#22312;&#24182;&#20301;&#23621;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29616;&#26377;GPAIS&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#65292;&#20801;&#35768;&#26681;&#25454;&#20854;&#24615;&#36136;&#21644;&#38480;&#21046;&#36880;&#27493;&#21306;&#20998;GPAIS&#30340;&#31867;&#22411;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#23553;&#38381;&#19990;&#30028;&#21644;&#24320;&#25918;&#19990;&#30028;&#30340;GPAIS&#65292;&#25551;&#36848;&#20854;&#33258;&#20027;&#31243;&#24230;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08964</link><description>&lt;p&gt;
&#26223;&#35266;&#26367;&#20195;&#21697;&#65306;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#23398;&#20064;&#25968;&#23398;&#20248;&#21270;&#30340;&#20915;&#31574;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#20064;&#38598;&#25104;&#20248;&#21270;&#24037;&#20316;&#22312;&#20248;&#21270;&#38382;&#39064;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#25110;&#36890;&#29992;&#20248;&#21270;&#22120;&#22312;&#26080;&#19987;&#23478;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#22120;$ \mathbf{g} $&#26469;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;&#20248;&#21270;&#22120;&#21487;&#20197;&#36890;&#36807;&#24050;&#30693;&#26368;&#20248;&#35299;&#30340;&#30417;&#30563;&#25110;&#36890;&#36807;&#20248;&#21270;&#22797;&#21512;&#20989;&#25968;$ f\circ \mathbf{g} $&#30340;&#38544;&#24335;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#38544;&#24335;&#26041;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#26368;&#20248;&#35299;&#20316;&#20026;&#26631;&#31614;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#39057;&#32321;&#35843;&#29992;&#20248;&#21270;&#22120;$ \mathbf{g} $&#65292;&#22240;&#27492;&#35757;&#32451;&#21644;&#37096;&#32626;&#32531;&#24930;&#12290;&#23545;&#20110;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#30001;&#20110;$ \mathbf{g} $&#30340;&#31232;&#30095;&#26799;&#24230;&#65292;&#35757;&#32451;&#36827;&#19968;&#27493;&#21463;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#28369;&#21487;&#23398;&#20064;&#30340;&#26223;&#35266;&#26367;&#20195;&#21697;$ M $&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02345</link><description>&lt;p&gt;
LLQL: &#36923;&#36753;&#20284;&#28982; Q-Learning &#29992;&#20110;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLQL: Logistic Likelihood Q-Learning for Reinforcement Learning. (arXiv:2307.02345v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#21644;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#21457;&#29616;&#65292;Bellman &#35823;&#24046;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#20998;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447;&#20004;&#31181;&#21464;&#20307;&#12290;&#20316;&#20026;&#22312;&#32447;&#21644;&#31163;&#32447; RL &#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#24403;&#21069;&#23545; Bellman &#26041;&#31243;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#25216;&#26415;&#21644;&#24615;&#33021;&#22686;&#24378;&#19978;&#65292;&#32780;&#19981;&#26159;&#25506;&#32034; Bellman &#35823;&#24046;&#30340;&#22266;&#26377;&#32467;&#26500;&#29305;&#24615;&#65292;&#22914;&#20854;&#20998;&#24067;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545; Bellman &#26041;&#31243;&#36827;&#34892;&#36845;&#20195;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#22312;&#32447; RL &#21644;&#31163;&#32447; RL &#20013; Bellman &#36817;&#20284;&#35823;&#24046;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26080;&#35770;&#26159;&#22312;&#32447; RL &#36824;&#26159;&#31163;&#32447; RL&#65292;Bellman &#35823;&#24046;&#37117;&#31526;&#21512;&#36923;&#36753;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992; Logistic &#26368;&#22823;&#20284;&#28982;&#20989;&#25968;&#65288;LLoss&#65289;&#20316;&#20026;&#24120;&#29992;&#30340; MSE Loss &#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20551;&#35774; Bellman &#35823;&#24046;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#22312;&#19981;&#21516;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern reinforcement learning (RL) can be categorized into online and offline variants. As a pivotal aspect of both online and offline RL, current research on the Bellman equation revolves primarily around optimization techniques and performance enhancement rather than exploring the inherent structural properties of the Bellman error, such as its distribution characteristics. This study investigates the distribution of the Bellman approximation error in both online and offline settings through iterative exploration of the Bellman equation. We observed that both in online RL and offline RL, the Bellman error conforms to a Logistic distribution. Building upon this discovery, this study employed the Logistics maximum likelihood function (LLoss) as an alternative to the commonly used MSE Loss, assuming that Bellman errors adhere to a normal distribution. We validated our hypotheses through extensive numerical experiments across diverse online and offline environments. In particular, we app
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15136</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#30495;&#27491;&#37325;&#35201;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Truly Matters in Trajectory Prediction for Autonomous Driving?. (arXiv:2306.15136v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15136
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#24456;&#22909;&#65292;&#20294;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#21364;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#21160;&#21147;&#23398;&#24046;&#36317;&#21644;&#35745;&#31639;&#25928;&#29575;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#36712;&#36857;&#39044;&#27979;&#22312;&#30830;&#20445;&#23433;&#20840;&#21644;&#20419;&#36827;&#24179;&#31283;&#23548;&#33322;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#22120;&#20934;&#30830;&#24615;&#19982;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#39550;&#39542;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#28304;&#20110;&#24403;&#21069;&#36712;&#36857;&#39044;&#27979;&#35780;&#20272;&#21327;&#35758;&#20013;&#24573;&#35270;&#20102;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#25968;&#25454;&#38598;&#19982;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20043;&#38388;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#65307;2&#65289;&#39044;&#27979;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#31639;&#27861;&#24433;&#21709;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#34892;&#20026;&#65292;&#36827;&#32780;&#25913;&#21464;&#36947;&#36335;&#19978;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#20114;&#21160;&#20135;&#29983;&#20102;&#38024;&#23545;&#39044;&#27979;&#22120;&#30340;&#29305;&#23450;&#21160;&#21147;&#23398;&#65292;&#30452;&#25509;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#21453;&#24212;&#22312;&#25968;&#25454;&#38598;&#19978;&#26159;&#39044;&#20808;&#30830;&#23450;&#30340;&#65292;&#22240;&#27492;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#21160;&#21147;&#23398;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#26080;&#27861;&#28385;&#36275;&#23545;&#39044;&#27979;&#22120;&#21160;&#24577;&#34892;&#20026;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the autonomous driving system, trajectory prediction plays a vital role in ensuring safety and facilitating smooth navigation. However, we observe a substantial discrepancy between the accuracy of predictors on fixed datasets and their driving performance when used in downstream tasks. This discrepancy arises from two overlooked factors in the current evaluation protocols of trajectory prediction: 1) the dynamics gap between the dataset and real driving scenario; and 2) the computational efficiency of predictors. In real-world scenarios, prediction algorithms influence the behavior of autonomous vehicles, which, in turn, alter the behaviors of other agents on the road. This interaction results in predictor-specific dynamics that directly impact prediction results. As other agents' responses are predetermined on datasets, a significant dynamics gap arises between evaluations conducted on fixed datasets and actual driving scenarios. Furthermore, focusing solely on accuracy fails to ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10763</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Models of Code with Global Context using Monitors. (arXiv:2306.10763v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#35270;&#22120;&#24341;&#23548;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#31561;&#20840;&#23616;&#19978;&#19979;&#25991;&#26102;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21608;&#22260;&#20195;&#30721;&#25552;&#20379;&#36275;&#22815;&#19978;&#19979;&#25991;&#26102;&#25928;&#26524;&#24456;&#22909;&#12290;&#20294;&#24403;&#38656;&#35201;&#22312;&#23384;&#20648;&#24211;&#25110;&#38142;&#25509;&#24211;&#20013;&#20351;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#31867;&#22411;&#12289;&#21151;&#33021;&#25110;API&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#23601;&#19981;&#20877;&#25104;&#31435;&#12290;LMs&#22312;&#23545;&#36825;&#31181;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#24847;&#35782;&#26377;&#38480;&#26102;&#20250;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#20102;&#35299;&#23384;&#20648;&#24211;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#20154;&#21592;&#20139;&#21463;&#21040;&#30340;&#36825;&#31181;&#24110;&#21161;&#25193;&#23637;&#21040;&#20102;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#35270;&#22120;&#24341;&#23548;&#35299;&#30721;&#65288;MGD&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#30417;&#35270;&#22120;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;Java&#26041;&#27861;&#34917;&#20840;&#30340;&#23384;&#20648;&#24211;&#32423;&#25968;&#25454;&#38598;PragmaticCode&#65292;&#24182;&#22312;&#20854;&#19978;&#35780;&#20272;&#20102;MGD&#12290;&#22312;&#19981;&#21516;&#21442;&#25968;&#35268;&#27169;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#30417;&#35270;&#31867;&#22411;&#19968;&#33268;&#30340;&#23545;&#35937;&#35299;&#24341;&#29992;&#65292;MGD&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#32534;&#35793;&#29575;&#24182;&#19982;&#30495;&#23454;&#32467;&#26524;&#36798;&#25104;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#26356;&#23569;&#21442;&#25968;&#30340;LMs&#65292;&#22312;&#19982;MGD&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#36229;&#36234;&#26356;&#22823;&#30340;LMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.  Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02231</link><description>&lt;p&gt;
&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#30340;Fine-Tuning&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Advantage-Induced Policy Alignment. (arXiv:2306.02231v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;APA&#65292;&#20854;&#37319;&#29992;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65288;PPO&#65289;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#30340;&#23849;&#28291;&#19982;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#22312;&#20247;&#22810;RLHF&#25216;&#26415;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;PPO&#24456;&#27969;&#34892;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#36973;&#21463;&#27169;&#24335;&#23849;&#28291;&#12289;&#19981;&#31283;&#23450;&#21644;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;--&#22522;&#20110;&#20272;&#35745;&#20248;&#21183;&#30340;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#35825;&#23548;&#31574;&#30053;&#23545;&#40784;&#65288;APA&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#26102;&#65292;APA&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#22987;&#32456;&#27604;PPO&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;PPO&#30456;&#27604;&#65292;APA&#21487;&#20197;&#26356;&#31283;&#23450;&#22320;&#25511;&#21046;&#27169;&#22411;&#19982;&#21021;&#22987;&#31574;&#30053;&#30340;&#20559;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#20250;&#23849;&#28291;&#20026;&#30830;&#23450;&#24615;&#36755;&#20986;&#12290;&#38500;&#20102;&#32463;&#39564;&#32467;&#26524;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;APA&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment (APA), which leverages a squared error loss function based on the estimated advantages. We demonstrate empirically that APA consistently outperforms PPO in language tasks by a large margin, when a separate reward model is employed as the evaluator. In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output. In addition to empirical results, we also prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19234</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#35821;&#27861;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20174;&#20165;&#26377;&#20960;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#20174;&#35821;&#20041;&#35299;&#26512;&#21040;&#22797;&#26434;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65289;&#29983;&#25104;&#23383;&#31526;&#20018;&#65292;LLM&#21482;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;$\textbf{&#35821;&#27861;&#25552;&#31034;}$&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32972;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#20013;&#34920;&#36798;&#30340;&#35821;&#27861;&#26469;&#21551;&#29992;LLM&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35821;&#27861;&#25552;&#31034;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#65292;&#35813;&#35821;&#27861;&#36275;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#36755;&#20986;&#31034;&#20363;&#65292;&#20854;&#20013;&#35813;&#19987;&#38376;&#30340;&#35821;&#27861;&#26159;&#20840;DSL&#35821;&#27861;&#30340;&#23376;&#38598;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;LLM&#39318;&#20808;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;BNF&#35821;&#27861;&#65292;&#28982;&#21518;&#26681;&#25454;&#35821;&#27861;&#35268;&#21017;&#29983;&#25104;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#27861;&#25552;&#31034;&#21487;&#20197;&#20351;LLM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13484</link><description>&lt;p&gt;
Flover&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13484
&lt;/p&gt;
&lt;p&gt;
Flover&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#24182;&#34892;&#25512;&#26029;&#30340;&#26102;&#38388;&#34701;&#21512;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24182;&#34892;&#24615;&#19981;&#36275;&#21644;&#28789;&#27963;&#24615;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#39640;&#25928;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#27169;&#22411;&#25512;&#26029;&#24615;&#33021;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#24182;&#34987;&#37096;&#32626;&#22312;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24773;&#20917;&#19979;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#22312;&#20247;&#22810;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22240;&#27492;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#37319;&#29992;&#20102;&#19968;&#31181;&#26102;&#38388;&#20381;&#36182;&#32467;&#26500;&#65292;&#20854;&#20013;&#24403;&#21069;token&#30340;&#27010;&#29575;&#20998;&#24067;&#21463;&#21040;&#21069;&#38754;token&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26412;&#36136;&#19978;&#30340;&#24207;&#21015;&#29305;&#24615;&#36981;&#24490;&#39532;&#23572;&#21487;&#22827;&#38142;&#20551;&#35774;&#65292;&#32570;&#20047;&#26102;&#38388;&#24182;&#34892;&#24615;&#65292;&#22240;&#27492;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#32972;&#26223;&#19979;&#65292;&#25512;&#26029;&#35831;&#27714;&#36981;&#24490;&#27850;&#26494;&#26102;&#38388;&#20998;&#24067;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#21709;&#24212;&#38271;&#24230;&#65292;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#32570;&#22833;&#26356;&#21152;&#26126;&#26174;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#21160;&#24577;&#25209;&#22788;&#29702;&#21644;&#24182;&#21457;&#27169;&#22411;&#23454;&#20363;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#31895;&#31890;&#24230;&#30340;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#30340;&#24320;&#38144;&#21644;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of deep learning, the performance of model inference has become a pivotal aspect as models become more complex and are deployed in diverse applications. Among these, autoregressive models stand out due to their state-of-the-art performance in numerous generative tasks. These models, by design, harness a temporal dependency structure, where the current token's probability distribution is conditioned on preceding tokens. This inherently sequential characteristic, however, adheres to the Markov Chain assumption and lacks temporal parallelism, which poses unique challenges. Particularly in industrial contexts where inference requests, following a Poisson time distribution, necessitate diverse response lengths, this absence of parallelism is more profound. Existing solutions, such as dynamic batching and concurrent model instances, nevertheless, come with severe overheads and a lack of flexibility, these coarse-grained methods fall short of achieving optimal la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#26368;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#20174;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#30340;&#32570;&#22833;&#25968;&#25454;&#20013;&#20998;&#26512;&#20102;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10050</link><description>&lt;p&gt;
&#22312;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Missing Data in a Multicentric Clinical Study. (arXiv:2305.10050v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#26368;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#20174;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#30340;&#32570;&#22833;&#25968;&#25454;&#20013;&#20998;&#26512;&#20102;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#21644;&#30456;&#20851;&#32852;&#30340;&#22240;&#26524;&#22270;&#36890;&#24120;&#19981;&#23384;&#22312;&#65292;&#22240;&#27492;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26816;&#39564;&#20020;&#24202;&#20551;&#35774;&#30340;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#35768;&#22810;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#32570;&#22833;&#30340;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#20020;&#24202;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#23376;&#23467;&#20869;&#33180;&#30284;&#30340;&#22810;&#20013;&#24515;&#30740;&#31350;&#25968;&#25454;&#65292;&#20998;&#26512;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#32780;&#19981;&#25439;&#22833;&#29702;&#35770;&#30340;&#20005;&#35880;&#24615;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference for testing clinical hypotheses from observational data presents many difficulties because the underlying data-generating model and the associated causal graph are not usually available. Furthermore, observational data may contain missing values, which impact the recovery of the causal graph by causal discovery algorithms: a crucial issue often ignored in clinical studies. In this work, we use data from a multi-centric study on endometrial cancer to analyze the impact of different missingness mechanisms on the recovered causal graph. This is achieved by extending state-of-the-art causal discovery algorithms to exploit expert knowledge without sacrificing theoretical soundness. We validate the recovered graph with expert physicians, showing that our approach finds clinically-relevant solutions. Finally, we discuss the goodness of fit of our graph and its consistency from a clinical decision-making perspective using graphical separation to validate causal pathways.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.12040</link><description>&lt;p&gt;
&#21327;&#21516;&#20154;&#24037;&#26234;&#33021;&#30340;&#26681;&#28304;&#21644;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Roots and Requirements for Collaborative AI. (arXiv:2303.12040v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12040
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AI&#21327;&#21516;&#21512;&#20316;&#30340;&#21382;&#21490;&#21644;&#35201;&#27714;&#65292;&#26159;&#21327;&#21516;AI&#30740;&#31350;&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21327;&#20316;&#32773;&#30340;&#24895;&#26223;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31185;&#24187;&#23567;&#35828;&#30340;&#32463;&#20856;&#32032;&#26448;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#29702;&#35299;&#21327;&#20316;&#21644;&#20154;&#31867;&#27807;&#36890;&#30340;&#24494;&#22937;&#24046;&#21035;&#12290;&#23427;&#20204;&#36890;&#36807;&#36129;&#29486;&#29305;&#27530;&#30340;&#25165;&#33021;&#32473;&#20182;&#20204;&#30340;&#20154;&#31867;&#21512;&#20316;&#32773;&#21644;&#22242;&#38431;&#24102;&#26469;&#20248;&#21183;&#12290;&#22810;&#24180;&#26469;&#65292;&#25919;&#24220;&#21672;&#35810;&#22242;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#39046;&#34966;&#19968;&#30452;&#20513;&#23548;AIs&#24212;&#35813;&#20855;&#26377;&#20154;&#31867;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#21327;&#20316;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20855;&#22791;&#20687;&#25165;&#21326;&#27178;&#28322;&#30340;&#20154;&#37027;&#26679;&#21327;&#20316;&#33021;&#21147;&#30340;&#24378;&#22823;&#30340;AI&#20173;&#28982;&#36965;&#19981;&#21487;&#21450;&#12290;&#36825;&#31687;&#35770;&#25991;&#20381;&#25454;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20195;&#29702;&#26377;&#25928;&#21644;&#24378;&#22823;&#21327;&#20316;&#25152;&#38656;&#35748;&#30693;&#30340;&#20998;&#26512;&#65292;&#27010;&#36848;&#20102;&#20844;&#20247;&#21644;AI&#24895;&#26223;&#20013;&#20851;&#20110;&#20154;&#24037;&#21327;&#20316;&#32773;&#30340;&#21382;&#21490;&#65292;&#24320;&#22987;&#20110;&#26089;&#26399;&#26234;&#33021;&#22686;&#24378;(IA)&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#24895;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#25104;&#20026;&#21327;&#21516;AI&#30340;&#31532;&#20108;&#20010;&#31435;&#22330;&#25991;&#20214;(Stefik &amp; Price, 2023)&#30340;&#21160;&#26426;&#21644;&#32972;&#26223;&#12290;&#31532;&#20108;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;&#22810;&#23398;&#31185;&#30340;&#29616;&#29366;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;AI&#21327;&#20316;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vision of AI collaborators has long been a staple of science fiction, where artificial agents understand nuances of collaboration and human communication. They bring advantages to their human collaborators and teams by contributing their special talents. Government advisory groups and leaders in AI have advocated for years that AIs should be human compatible and be capable of effective collaboration. Nonetheless, robust AIs that can collaborate like talented people remain out of reach. This position paper draws on a cognitive analysis of what effective and robust collaboration requires of human and artificial agents. It sketches a history of public and AI visions for artificial collaborators, starting with early visions of intelligence augmentation (IA) and artificial intelligence (AI). It is intended as motivation and context for a second position paper on collaborative AI (Stefik &amp; Price, 2023). The second paper reviews the multi-disciplinary state-of-the-art and proposes a roadm
&lt;/p&gt;</description></item><item><title>xASTNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;&#23427;&#30340;&#20248;&#28857;&#21253;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#19977;&#20010;&#35774;&#35745;&#26469;&#20445;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07104</link><description>&lt;p&gt;
xASTNN&#65306;&#29992;&#20110;&#24037;&#19994;&#23454;&#36341;&#30340;&#25913;&#36827;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
xASTNN: Improved Code Representations for Industrial Practice. (arXiv:2303.07104v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07104
&lt;/p&gt;
&lt;p&gt;
xASTNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;&#23427;&#30340;&#20248;&#28857;&#21253;&#25324;&#36866;&#29992;&#20110;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#19977;&#20010;&#35774;&#35745;&#26469;&#20445;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#24320;&#21457;&#39640;&#36136;&#37327;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#12290;&#26368;&#36817;&#20960;&#24180;&#30740;&#31350;&#30028;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37096;&#32626;&#22256;&#38590;&#21644;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;xASTNN&#65292;&#19968;&#31181;&#22522;&#20110;&#26497;&#31471;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#23558;&#36825;&#31181;&#25216;&#26415;&#25512;&#24191;&#21040;&#24037;&#19994;&#23454;&#36341;&#20013;&#12290;xASTNN&#30340;&#19977;&#20010;&#20248;&#28857;&#65306;&#39318;&#20808;&#65292;xASTNN&#23436;&#20840;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;AST&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#21644;&#23454;&#38469;&#22330;&#26223;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#23494;&#20999;&#30456;&#20851;&#30340;&#35774;&#35745;&#26469;&#20445;&#35777;xASTNN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#29992;&#20110;&#20195;&#30721;&#33258;&#28982;&#24615;&#30340;&#35821;&#21477;&#23376;&#26641;&#24207;&#21015;&#12289;&#29992;&#20110;&#21477;&#27861;&#24314;&#27169;&#30340;&#38376;&#25511;&#36882;&#24402;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of deep learning techniques in software engineering becomes increasingly popular. One key problem is developing high-quality and easy-to-use source code representations for code-related tasks. The research community has acquired impressive results in recent years. However, due to the deployment difficulties and performance bottlenecks, seldom these approaches are applied to the industry. In this paper, we present xASTNN, an eXtreme Abstract Syntax Tree (AST)-based Neural Network for source code representation, aiming to push this technique to industrial practice. The proposed xASTNN has three advantages. First, xASTNN is completely based on widely-used ASTs and does not require complicated data pre-processing, making it applicable to various programming languages and practical scenarios. Second, three closely-related designs are proposed to guarantee the effectiveness of xASTNN, including statement subtree sequence for code naturalness, gated recursive unit for syntacti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SEGA&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#30340;&#20114;&#21160;&#65292;SEGA&#21487;&#20197;&#28789;&#27963;&#22320;&#22312;&#35821;&#20041;&#26041;&#21521;&#19978;&#24341;&#23548;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#23454;&#39564;&#35777;&#26126;SEGA&#22312;&#22810;&#31181;&#20219;&#21153;&#21644;&#29983;&#25104;&#26550;&#26500;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2301.12247</link><description>&lt;p&gt;
SEGA&#65306;&#20351;&#29992;&#35821;&#20041;&#24341;&#23548;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;SEGA&#30340;&#35821;&#20041;&#24341;&#23548;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#30340;&#20114;&#21160;&#65292;SEGA&#21487;&#20197;&#28789;&#27963;&#22320;&#22312;&#35821;&#20041;&#26041;&#21521;&#19978;&#24341;&#23548;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#23454;&#29616;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#23454;&#39564;&#35777;&#26126;SEGA&#22312;&#22810;&#31181;&#20219;&#21153;&#21644;&#29983;&#25104;&#26550;&#26500;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#20196;&#20154;&#24778;&#35766;&#30340;&#33021;&#21147;&#21487;&#20197;&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#21333;&#27425;&#29983;&#25104;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#32780;&#36755;&#20837;&#25552;&#31034;&#30340;&#24494;&#23567;&#25913;&#21464;&#24448;&#24448;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#22270;&#20687;&#12290;&#36825;&#20351;&#24471;&#29992;&#25143;&#22312;&#35821;&#20041;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;&#20026;&#20102;&#20351;&#29992;&#25143;&#23454;&#29616;&#25511;&#21046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19982;&#25193;&#25955;&#36807;&#31243;&#20114;&#21160;&#26469;&#28789;&#27963;&#22320;&#24341;&#23548;&#35821;&#20041;&#26041;&#21521;&#12290;&#36825;&#31181;&#35821;&#20041;&#24341;&#23548;(SEGA)&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#29983;&#25104;&#26550;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20801;&#35768;&#36827;&#34892;&#32454;&#24494;&#21644;&#24191;&#27867;&#30340;&#32534;&#36753;&#65292;&#32452;&#25104;&#21644;&#39118;&#26684;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#20219;&#21153;&#23637;&#31034;&#20102;SEGA&#22312;&#28508;&#22312;&#21644;&#20687;&#32032;&#32423;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;Stable Diffusion&#65292;Paella&#21644;DeepFloyd-IF&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#28789;&#27963;&#24615;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free guidance. More importantly, it allows for subtle and extensive edits, changes in composition and style, as well as optimizing the overall artistic conception. We demonstrate SEGA's effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, thus providing strong evidence for its versatility, flexibility, and improvements ov
&lt;/p&gt;</description></item><item><title>Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.05062</link><description>&lt;p&gt;
Tracr: &#32534;&#35793;&#21464;&#21387;&#22120;&#27169;&#22411;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#23460;
&lt;/p&gt;
&lt;p&gt;
Tracr: Compiled Transformers as a Laboratory for Interpretability. (arXiv:2301.05062v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05062
&lt;/p&gt;
&lt;p&gt;
Tracr&#26159;&#19968;&#20010;&#32534;&#35793;&#22120;&#65292;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#21644;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21487;&#35835;&#24615;&#24378;&#30340;&#31243;&#24207;&#32534;&#35793;&#25104;&#26631;&#20934;&#30340;&#20165;&#35299;&#30721;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32534;&#35793;&#22120;Tracr&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#30740;&#31350;&#25191;&#34892;&#22810;&#27493;&#31639;&#27861;&#30340;&#21464;&#21387;&#22120;&#20013;&#30340;&#8220;&#21472;&#21152;&#8221;&#12290;&#27492;&#22806;&#65292;Tracr&#32534;&#35793;&#27169;&#22411;&#30340;&#24050;&#30693;&#32467;&#26500;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#30495;&#23454;&#22522;&#20934;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#21464;&#21387;&#22120;&#23398;&#20064;&#30340;&#8220;&#31243;&#24207;&#8221;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#19981;&#28165;&#26970;&#35299;&#37322;&#26159;&#21542;&#25104;&#21151;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21644;&#26816;&#26597;&#21253;&#25324;&#35745;&#31639;&#20196;&#29260;&#39057;&#29575;&#12289;&#25490;&#24207;&#21644;&#25324;&#21495;&#26816;&#26597;&#22312;&#20869;&#30340;&#31243;&#24207;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;https://github.com/deepmind/tracr&#25552;&#20379;&#20102;Tracr&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;(MORN)&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04873</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition. (arXiv:2212.04873v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;(MORN)&#29992;&#20110;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#21407;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#36981;&#24490;ProtoNet&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20102;&#21407;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#30456;&#23545;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#25928;&#26524;&#34987;&#24573;&#30053;&#65292;&#20363;&#22914;&#26631;&#31614;&#25991;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;&#32593;&#32476;&#65288;MORN&#65289;&#65292;&#23427;&#21033;&#29992;&#26631;&#31614;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#22810;&#27169;&#24577;&#20449;&#24687;&#26469;&#22686;&#24378;&#21407;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20923;&#32467;&#30340;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#22810;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#22312;&#35270;&#35273;&#27969;&#31243;&#20013;&#65292;&#36890;&#36807;&#19968;&#20010;&#26102;&#38388;&#20851;&#31995;&#20132;&#21449;&#21464;&#25442;&#22120;(TRX)&#27169;&#22359;&#35745;&#31639;&#35270;&#35273;&#21407;&#22411;&#12290;&#22312;&#25991;&#26412;&#27969;&#31243;&#20013;&#65292;&#20351;&#29992;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;(SE)&#27169;&#22359;&#21644;&#19968;&#20010;&#25193;&#24352;&#25805;&#20316;&#26469;&#33719;&#21462;&#25991;&#26412;&#21407;&#22411;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#21407;&#22411;&#22686;&#24378;(MPE)&#27169;&#22359;&#35745;&#31639;&#26368;&#32456;&#30340;&#22810;&#27169;&#24577;&#21407;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#21407;&#22411;&#30456;&#20284;&#24615;&#24046;&#24322;(PRIDE)&#26469;&#35780;&#20272;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for few-shot action recognition mainly fall into the metric learning framework following ProtoNet, which demonstrates the importance of prototypes. Although they achieve relatively good performance, the effect of multimodal information is ignored, e.g. label texts. In this work, we propose a novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic information of label texts as multimodal information to enhance prototypes. A CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain features with good multimodal initialization. Then in the visual flow, visual prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module for example. In the text flow, a semantic-enhanced (SE) module and an inflating operation are used to obtain text prototypes. The final multimodal prototypes are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we define a PRototype SImilarity DiffErence (PRIDE) to evaluate the qu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.09090</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#22522;&#22240;&#28418;&#21464;&#21040;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Understanding Genetic Drift to a Smart-Restart Mechanism for Estimation-of-Distribution Algorithms. (arXiv:2206.09090v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#30340;&#20998;&#24067;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#20572;&#27490;&#36816;&#34892;&#65292;&#24182;&#23547;&#25214;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20197;&#36816;&#34892;EDA&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20998;&#24067;&#31639;&#27861;&#65288;EDAs&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20174;&#25628;&#32034;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#20010;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#36731;&#26494;&#22320;&#37319;&#26679;&#20986;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;EDAs&#30340;&#20851;&#38190;&#21442;&#25968;&#26159;&#26679;&#26412;&#22823;&#23567;&#65288;&#31181;&#32676;&#22823;&#23567;&#65289;&#12290;&#22914;&#26524;&#31181;&#32676;&#22823;&#23567;&#22826;&#23567;&#65292;&#27010;&#29575;&#27169;&#22411;&#26356;&#26032;&#20165;&#22522;&#20110;&#23569;&#37327;&#26679;&#26412;&#65292;&#23548;&#33268;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#22522;&#22240;&#28418;&#21464;&#25928;&#24212;&#12290;&#31181;&#32676;&#22823;&#23567;&#36807;&#22823;&#20250;&#36991;&#20813;&#36951;&#20256;&#28418;&#21464;&#65292;&#20294;&#20250;&#20943;&#32531;&#36827;&#31243;&#12290;&#22522;&#20110;&#26368;&#36817;&#37327;&#21270;&#20998;&#26512;&#30340;&#31181;&#32676;&#22823;&#23567;&#22914;&#20309;&#23548;&#33268;&#22522;&#22240;&#28418;&#21464;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;EDAs&#30340;&#26234;&#33021;&#37325;&#21551;&#26426;&#21046;&#12290;&#24403;&#22522;&#22240;&#28418;&#21464;&#39118;&#38505;&#24456;&#39640;&#26102;&#20572;&#27490;&#36816;&#34892;&#65292;&#23427;&#20250;&#33258;&#21160;&#22312;&#33391;&#22909;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#36816;&#34892;EDA&#12290;&#36890;&#36807;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#20026;&#36825;&#31181;&#26234;&#33021;&#37325;&#21551;&#26041;&#26696;&#35777;&#26126;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#34920;&#26126;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#24050;&#30693;&#26368;&#20339;&#30340;&#65288;&#38382;&#39064;&#29305;&#23450;&#30340;&#65289;&#21442;&#25968;&#20540;&#65292;&#37325;&#21551;&#26041;&#26696;&#20250;&#33258;&#21160;&#21457;&#29616;&#36825;&#20123;&#20540;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation-of-distribution algorithms (EDAs) are optimization algorithms that learn a distribution on the search space from which good solutions can be sampled easily. A key parameter of most EDAs is the sample size (population size). If the population size is too small, the update of the probabilistic model builds on few samples, leading to the undesired effect of genetic drift. Too large population sizes avoid genetic drift, but slow down the process.  Building on a recent quantitative analysis of how the population size leads to genetic drift, we design a smart-restart mechanism for EDAs. By stopping runs when the risk for genetic drift is high, it automatically runs the EDA in good parameter regimes.  Via a mathematical runtime analysis, we prove a general performance guarantee for this smart-restart scheme. This in particular shows that in many situations where the optimal (problem-specific) parameter values are known, the restart scheme automatically finds these, leading to the a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#32593;&#32476;&#20013;&#36873;&#25321;&#26368;&#20339;&#26367;&#20195;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#19982;&#25928;&#26524;&#30340;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#23545;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2202.00282</link><description>&lt;p&gt;
&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31283;&#23450;LIF&#31070;&#32463;&#20803;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#32593;&#32476;&#20013;&#36873;&#25321;&#26368;&#20339;&#26367;&#20195;&#26799;&#24230;&#30340;&#31283;&#23450;&#24615;&#19982;&#25928;&#26524;&#30340;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#23545;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#21033;&#29992;&#20108;&#36827;&#21046;&#27963;&#21160;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20108;&#36827;&#21046;&#27963;&#21160;&#30340;&#38750;&#24179;&#28369;&#24615;&#35201;&#27714;&#20351;&#29992;&#36817;&#20284;&#26799;&#24230;&#65292;&#20063;&#31216;&#20026;&#26367;&#20195;&#26799;&#24230;&#65288;SG&#65289;&#65292;&#20197;&#24357;&#21512;&#19982;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#20960;&#31181;SG&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#30830;&#23450;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#32593;&#32476;&#30340;&#26368;&#20339;SG&#12290;&#22312;&#26114;&#36149;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#21518;&#65292;&#22823;&#22810;&#25968;SG&#24418;&#29366;&#37117;&#21487;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;&#21387;&#21147;&#27979;&#35797;&#20013;&#23454;&#39564;&#35777;&#26126;&#26368;&#20339;SG&#65292;&#24182;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#20943;&#23569;&#26410;&#26469;&#23545;&#32593;&#26684;&#25628;&#32034;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#29702;&#35299;&#35813;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#38656;&#35201;&#26356;&#24910;&#37325;&#22320;&#36873;&#25321;SG&#65292;&#21363;&#20351;&#25972;&#20307;&#19978;&#65292;&#24555;&#36895;Sigmoid&#20989;&#25968;&#30340;&#23548;&#25968;&#22312;&#21508;&#31181;&#23398;&#20064;&#29575;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;SG&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#24615;&#30340;&#29702;&#35770;&#26041;&#27861;&#26469;&#36873;&#25321;&#21021;&#22987;&#21270;&#21644;SG&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GND-Nets &#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#26469;&#35299;&#20915;&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.09698</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Diffusion Networks for Semi-supervised Learning. (arXiv:2201.09698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GND-Nets &#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21644;&#23616;&#37096;&#12289;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#26469;&#35299;&#20915;&#22270;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#21644;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476; (GCN) &#26159;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20808;&#39537;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GCN &#22312;&#26631;&#35760;&#31232;&#30095;&#30340;&#22270;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20854;&#20004;&#23618;&#29256;&#26412;&#19981;&#33021;&#26377;&#25928;&#22320;&#23558;&#26631;&#31614;&#20449;&#24687;&#20256;&#25773;&#21040;&#25972;&#20010;&#22270;&#32467;&#26500;&#65288;&#21363;&#27424;&#24179;&#28369;&#38382;&#39064;&#65289;&#65292;&#32780;&#20854;&#28145;&#23618;&#29256;&#26412;&#21017;&#36807;&#24230;&#24179;&#28369;&#19988;&#38590;&#20197;&#35757;&#32451;&#65288;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026; GND-Nets&#65288;&#22270;&#31070;&#32463;&#25193;&#25955;&#32593;&#32476;&#65289;&#65292;&#23427;&#22312;&#21333;&#23618;&#20013;&#21033;&#29992;&#20102;&#39030;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#12290;&#21033;&#29992;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#32780;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#21487;&#20197;&#32531;&#35299;&#27424;&#24179;&#28369;&#38382;&#39064;&#12290;&#39030;&#28857;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#37051;&#22495;&#20449;&#24687;&#30340;&#21033;&#29992;&#26159;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#25193;&#25955;&#30340;&#26032;&#22270;&#25193;&#25955;&#26041;&#27861;&#23454;&#29616;&#30340;&#65292;&#35813;&#26041;&#27861;&#23558;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#20256;&#32479;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#22270;&#25193;&#25955;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCN) is a pioneering model for graph-based semi-supervised learning. However, GCN does not perform well on sparsely-labeled graphs. Its two-layer version cannot effectively propagate the label information to the whole graph structure (i.e., the under-smoothing problem) while its deep version over-smoothens and is hard to train (i.e., the over-smoothing problem). To solve these two issues, we propose a new graph neural network called GND-Nets (for Graph Neural Diffusion Networks) that exploits the local and global neighborhood information of a vertex in a single layer. Exploiting the shallow network mitigates the over-smoothing problem while exploiting the local and global neighborhood information mitigates the under-smoothing problem. The utilization of the local and global neighborhood information of a vertex is achieved by a new graph diffusion method called neural diffusions, which integrate neural networks into the conventional linear and nonlinear gra
&lt;/p&gt;</description></item><item><title>"Feature-Attending Recurrent Modules" (FARM)&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.08369</link><description>&lt;p&gt;
&#29305;&#24449;&#27880;&#24847;&#30340;&#36882;&#24402;&#27169;&#22359;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08369
&lt;/p&gt;
&lt;p&gt;
"Feature-Attending Recurrent Modules" (FARM)&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#37325;&#35201;&#30340;&#20219;&#21153;&#37117;&#26159;&#20197;&#29289;&#20307;&#20026;&#22522;&#30784;&#23450;&#20041;&#30340;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#23454;&#29616;&#27867;&#21270;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#21033;&#29992;&#29289;&#20307;&#25152;&#24341;&#21457;&#30340;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#30828;&#32534;&#30721;&#23545;&#35937;&#20013;&#24515;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#20351;&#29992;&#22797;&#26434;&#30340;&#23545;&#35937;&#20013;&#24515;&#29983;&#25104;&#27169;&#22411;&#65292;&#35201;&#20040;&#20351;&#29992;&#23616;&#37096;&#31354;&#38388;&#29305;&#24449;&#26356;&#26032;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#29616;&#27867;&#21270;RL&#20195;&#29702;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;"&#29305;&#24449;&#27880;&#24847;&#30340;&#36882;&#24402;&#27169;&#22359;"&#65288;FARM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20381;&#36182;&#20110;&#31616;&#21333;&#12289;&#24191;&#27867;&#36866;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#35268;&#24459;&#24615;&#12290;FARM&#23398;&#20064;&#20102;&#19968;&#31181;&#20998;&#24067;&#20110;&#22810;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#20351;&#29992;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#20851;&#27880;&#26102;&#31354;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;RL&#20195;&#29702;&#22312;&#23545;&#35937;&#20013;&#24515;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;2D&#21644;3D&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#20219;&#21153;&#22871;&#20214;&#65292;&#24182;&#21457;&#29616;FARM&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce "Feature-Attending Recurrent Modules" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2111.08452</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#28388;&#27874;&#22120;&#30340;&#29702;&#35770;&#36830;&#25509;&#21450;&#20854;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On minimizers and convolutional filters: theoretical connections and applications to genome analysis. (arXiv:2111.08452v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#25968;&#23398;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#31181;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#21462;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#36317;&#31163;&#36739;&#36817;&#20294;&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#30456;&#36317;&#36739;&#36828;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#21270;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26159;&#20004;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#22343;&#34987;&#29992;&#20110;&#20998;&#26512;&#29983;&#29289;&#24207;&#21015;&#12290;&#20174;&#34920;&#38754;&#19978;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#20284;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#26368;&#23567;&#21270;&#22120;&#20351;&#29992;&#28378;&#21160;&#31383;&#21475;&#30340;&#26368;&#23567;&#21704;&#24076;&#26041;&#27861;&#25552;&#21462;&#27599;&#20010;&#31383;&#21475;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;k-mer&#29305;&#24449;&#12290;CNN&#21017;&#20197;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#27744;&#21270;&#25805;&#20316;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#22810;&#20010;&#31070;&#32463;&#23618;&#26469;&#23398;&#20064;&#28388;&#27874;&#22120;&#26412;&#36523;&#21450;&#20854;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#21704;&#24076;&#20989;&#25968;&#23646;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#26174;&#31034;&#23545;&#20110;&#20998;&#31867;&#23383;&#27597;&#34920;&#19978;&#30340;&#24207;&#21015;&#65292;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#21644;&#26368;&#22823;&#27744;&#21270;&#31561;&#20215;&#20110;&#36873;&#25321;&#19968;&#20010;&#26368;&#23567;&#21270;&#22120;&#25490;&#24207;&#65292;&#20351;&#24471;&#36873;&#25321;&#30340;k-mer&#19982;&#24207;&#21015;&#20013;&#30340;k-mer&#65288;&#25353;&#27721;&#26126;&#36317;&#31163;&#65289;&#30456;&#36317;&#36739;&#36828;&#65292;&#20294;&#19982;&#20854;&#20182;&#26368;&#23567;&#21270;&#22120;&#30456;&#36317;&#36739;&#36817;&#12290;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizers and convolutional neural networks (CNNs) are two quite distinct popular techniques that have both been employed to analyze categorical biological sequences. At face value, the methods seem entirely dissimilar. Minimizers use min-wise hashing on a rolling window to extract a single important k-mer feature per window. CNNs start with a wide array of randomly initialized convolutional filters, paired with a pooling operation, and then multiple additional neural layers to learn both the filters themselves and how they can be used to classify the sequence.  Here, our main result is a careful mathematical analysis of hash function properties showing that for sequences over a categorical alphabet, random Gaussian initialization of convolutional filters with max-pooling is equivalent to choosing a minimizer ordering such that selected k-mers are (in Hamming distance) far from the k-mers within the sequence but close to other minimizers. In empirical experiments, we find that this pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2106.12915</link><description>&lt;p&gt;
ReLU'(0)&#23545;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical influence of ReLU'(0) on backpropagation. (arXiv:2106.12915v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#21453;&#21521;&#20256;&#25773;&#30340;&#25968;&#20540;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;32&#20301;&#31934;&#24230;&#19979;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#22312;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#20013;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35770;&#19978;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013;ReLU'(0)&#22312;[0, 1]&#33539;&#22260;&#20869;&#30340;&#36873;&#25321;&#23545;&#21453;&#21521;&#20256;&#25773;&#21644;&#35757;&#32451;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;32&#20301;&#40664;&#35748;&#31934;&#24230;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#30340;&#35268;&#27169;&#65292;&#20351;&#20854;&#25104;&#20026;&#35757;&#32451;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU'(0)&#20540;&#23545;&#20960;&#31181;&#31934;&#24230;&#27700;&#24179;&#65288;16&#20301;&#65292;32&#20301;&#65292;64&#20301;&#65289;&#12289;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;VGG&#12289;ResNet&#65289;&#21644;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;32&#20301;&#31934;&#24230;&#19979;&#65292;&#21453;&#21521;&#20256;&#25773;&#36755;&#20986;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#36825;&#31181;&#24773;&#20917;&#22823;&#32422;&#20986;&#29616;&#20102;&#19968;&#21322;&#30340;&#26102;&#38388;&#12290;&#36825;&#31181;&#24433;&#21709;&#22312;&#21452;&#31934;&#24230;&#19979;&#28040;&#22833;&#65292;&#32780;&#22312;16&#20301;&#31934;&#24230;&#19979;&#26159;&#31995;&#32479;&#24615;&#30340;&#12290;&#23545;&#20110;&#26222;&#36890;&#30340;SGD&#35757;&#32451;&#32780;&#35328;&#65292;&#36873;&#25321;ReLU'(0) = 0&#20284;&#20046;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25209;&#24402;&#19968;&#21270;&#25110;ADAM&#31561;&#37325;&#26032;&#35843;&#25972;&#26041;&#27861; tend to buffer ReLU'(0)&#20540;&#30340;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24819;&#35201;&#20256;&#36798;&#30340;&#20449;&#24687;&#26159;&#65292;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#31639;&#27861;&#24494;&#20998;&#21487;&#33021;&#38544;&#34255;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In theory, the choice of ReLU'(0) in [0, 1] for a neural network has a negligible influence both on backpropagation and training. Yet, in the real world, 32 bits default precision combined with the size of deep learning problems makes it a hyperparameter of training methods. We investigate the importance of the value of ReLU'(0) for several precision levels (16, 32, 64 bits), on various networks (fully connected, VGG, ResNet) and datasets (MNIST, CIFAR10, SVHN). We observe considerable variations of backpropagation outputs which occur around half of the time in 32 bits precision. The effect disappears with double precision, while it is systematic at 16 bits. For vanilla SGD training, the choice ReLU'(0) = 0 seems to be the most efficient. We also evidence that reconditioning approaches as batch-norm or ADAM tend to buffer the influence of ReLU'(0)'s value. Overall, the message we want to convey is that algorithmic differentiation of nonsmooth problems potentially hides parameters that 
&lt;/p&gt;</description></item></channel></rss>