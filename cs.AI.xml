<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27773;&#36710;&#36319;&#38543;&#34892;&#20026;&#24314;&#27169;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36926;80K&#20010;&#27773;&#36710;&#36319;&#38543;&#20107;&#20214;&#65292;&#26088;&#22312;&#22635;&#34917;&#27492;&#39046;&#22495;&#30340;&#25968;&#25454;&#32570;&#21475;&#24182;&#20419;&#36827;&#24494;&#35266;&#20132;&#36890;&#27969;&#24314;&#27169;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.05381</link><description>&lt;p&gt;
FollowNet: &#19968;&#31181;&#20840;&#38754;&#30340;&#27773;&#36710;&#21518;&#32493;&#34892;&#20026;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FollowNet: A Comprehensive Benchmark for Car-Following Behavior Modeling. (arXiv:2306.05381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27773;&#36710;&#36319;&#38543;&#34892;&#20026;&#24314;&#27169;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36926;80K&#20010;&#27773;&#36710;&#36319;&#38543;&#20107;&#20214;&#65292;&#26088;&#22312;&#22635;&#34917;&#27492;&#39046;&#22495;&#30340;&#25968;&#25454;&#32570;&#21475;&#24182;&#20419;&#36827;&#24494;&#35266;&#20132;&#36890;&#27969;&#24314;&#27169;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#36710;&#36319;&#38543;&#26159;&#19968;&#31181;&#25511;&#21046;&#36807;&#31243;&#65292;&#22312;&#36825;&#31181;&#36807;&#31243;&#20013;&#65292;&#36319;&#38543;&#36710;&#36742;&#65288;FV&#65289;&#20250;&#35843;&#25972;&#20854;&#21152;&#36895;&#24230;&#65292;&#20197;&#20445;&#25345;&#19982;&#21069;&#36710;&#65288;LV&#65289;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20351;&#27773;&#36710;&#36319;&#38543;&#30340;&#24314;&#27169;&#26356;&#21152;&#20934;&#30830;&#12290;&#23613;&#31649;&#26377;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#26684;&#24335;&#24182;&#19981;&#22987;&#32456;&#19968;&#33268;&#65292;&#36825;&#20351;&#24471;&#30830;&#23450;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20197;&#21450;&#26032;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#20687;&#22270;&#20687;&#35782;&#21035;&#21644;&#29289;&#20307;&#26816;&#27979;&#36825;&#26679;&#30340;&#30740;&#31350;&#39046;&#22495;&#20855;&#26377;&#20687;ImageNet&#12289;Microsoft COCO&#21644;KITTI&#36825;&#26679;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20419;&#36827;&#24494;&#35266;&#20132;&#36890;&#27969;&#24314;&#27169;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27773;&#36710;&#36319;&#38543;&#34892;&#20026;&#24314;&#27169;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#30001;&#20174;&#20116;&#20010;&#20844;&#20849;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#36926;80K&#20010;&#27773;&#36710;&#36319;&#38543;&#20107;&#20214;&#32452;&#25104;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#26631;&#20934;&#12290;&#36825;&#20123;&#20107;&#20214;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#36947;&#36335;&#31867;&#22411;&#12289;&#36947;&#36335;&#23485;&#24230;&#21644;&#20132;&#36890;&#27969;&#23494;&#24230;&#30340;&#21464;&#21270;&#20197;&#21450;&#20004;&#36742;&#36710;&#20043;&#38388;&#30340;&#30456;&#23545;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car-following is a control process in which a following vehicle (FV) adjusts its acceleration to keep a safe distance from the lead vehicle (LV). Recently, there has been a booming of data-driven models that enable more accurate modeling of car-following through real-world driving datasets. Although there are several public datasets available, their formats are not always consistent, making it challenging to determine the state-of-the-art models and how well a new model performs compared to existing ones. In contrast, research fields such as image recognition and object detection have benchmark datasets like ImageNet, Microsoft COCO, and KITTI. To address this gap and promote the development of microscopic traffic flow modeling, we establish a public benchmark dataset for car-following behavior modeling. The benchmark consists of more than 80K car-following events extracted from five public driving datasets using the same criteria. These events cover diverse situations including differ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#31995;&#32479;&#26041;&#26696;&#65292;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#22522;&#20934;&#27169;&#22411;&#24182;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#29983;&#25104;AI&#25945;&#24072;&#22238;&#24212;&#12290;&#36890;&#36807;&#23569;&#37327;&#25552;&#20379;&#25552;&#31034;&#20449;&#24687;&#65292;&#21033;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#22312;&#31454;&#36187;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05360</link><description>&lt;p&gt;
BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;ADAIO&#31995;&#32479;&#65306;&#29983;&#25104;&#25945;&#32946;&#23545;&#35805;&#20013;AI&#25945;&#24072;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. (arXiv:2306.05360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;BEA-2023&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#31995;&#32479;&#26041;&#26696;&#65292;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#22522;&#20934;&#27169;&#22411;&#24182;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#29983;&#25104;AI&#25945;&#24072;&#22238;&#24212;&#12290;&#36890;&#36807;&#23569;&#37327;&#25552;&#20379;&#25552;&#31034;&#20449;&#24687;&#65292;&#21033;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#22312;&#31454;&#36187;&#20013;&#33719;&#24471;&#31532;&#20108;&#21517;&#65292;&#24182;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ADAIO&#22242;&#38431;&#22312;"Building Educational Applications (BEA) 2023"&#20849;&#20139;&#20219;&#21153;&#20013;&#65292;&#29983;&#25104;&#25945;&#32946;&#23545;&#35805;&#20013;AI&#25945;&#24072;&#22238;&#24212;&#30340;&#31995;&#32479;&#26041;&#26696;&#12290;&#26412;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;AI&#25945;&#24072;&#65292;&#22312;&#23398;&#29983;&#19982;&#25945;&#24072;&#30340;&#23545;&#35805;&#20013;&#20135;&#29983;&#21512;&#36866;&#22238;&#24212;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI GPT-3&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#28608;&#21457;OpenAI&#27169;&#22411;&#29983;&#25104;&#25945;&#24072;&#22238;&#24212;&#12290;&#22312;&#31454;&#36187;&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20351;&#29992;&#20102;&#22522;&#20110;&#23569;&#37327;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#23588;&#20854;&#26159;OpenAI&#30340;GPT-3&#65289;&#22312;AI&#25945;&#24072;&#35282;&#33394;&#20013;&#30340;&#23569;&#37327;&#25552;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI's GPT-3, in the role of AI teachers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#65292;&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#34701;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MFF&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35777;&#26126;MFF&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05358</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#20013;&#21548;&#19981;&#35265;&#30340;&#25351;&#20196;&#25915;&#20987;&#30340;&#21487;&#20449;&#20256;&#24863;&#22120;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Sensor Fusion against Inaudible Command Attacks in Advanced Driver-Assistance System. (arXiv:2306.05358v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#65292;&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#30340;&#34701;&#21512;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;MFF&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35777;&#26126;MFF&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#20855;&#26377;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36973;&#21463;&#24694;&#24847;&#25915;&#20987;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#21152;&#12290;&#20854;&#20013;&#65292;&#21548;&#19981;&#35265;&#30340;&#35821;&#38899;&#25351;&#20196;&#25915;&#20987;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#35821;&#38899;&#25351;&#20196;&#24050;&#32463;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#22914;&#20309;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26469;&#25269;&#24481;&#36825;&#20123;&#21548;&#19981;&#35265;&#30340;&#25915;&#20987;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25506;&#35752;&#36816;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#25935;&#24863;&#30340;&#20219;&#21153;&#65292;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#22312;&#24110;&#21161;&#25552;&#39640;&#27169;&#22411;&#31283;&#20581;&#24615;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#20219;&#21153;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65288;MFF&#65289;&#20316;&#20026;&#26234;&#33021;&#23433;&#20840;&#31995;&#32479;&#26469;&#38450;&#33539;&#21548;&#19981;&#35265;&#30340;&#35821;&#38899;&#25351;&#20196;&#25915;&#20987;&#12290;MFF&#21033;&#29992;VGG&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#34701;&#21512;&#24322;&#26500;&#38899;&#39057;-&#35270;&#35273;&#27169;&#24577;&#65292;&#24182;&#22312;&#27604;&#36739;&#34701;&#21512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#23454;&#29616;&#20102;92.25%&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#23545;MFF&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are increasing concerns about malicious attacks on autonomous vehicles. In particular, inaudible voice command attacks pose a significant threat as voice commands become available in autonomous driving systems. How to empirically defend against these inaudible attacks remains an open question. Previous research investigates utilizing deep learning-based multimodal fusion for defense, without considering the model uncertainty in trustworthiness. As deep learning has been applied to increasingly sensitive tasks, uncertainty measurement is crucial in helping improve model robustness, especially in mission-critical scenarios. In this paper, we propose the Multimodal Fusion Framework (MFF) as an intelligent security system to defend against inaudible voice command attacks. MFF fuses heterogeneous audio-vision modalities using VGG family neural networks and achieves the detection accuracy of 92.25% in the comparative fusion method empirical study. Additionally, extensive experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.05357</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#32452;&#21512;&#24335;&#27010;&#24565;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22270;&#20687;&#20013;&#33258;&#21160;&#22320;&#21457;&#29616;&#19981;&#21516;&#30340;&#29983;&#25104;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#20123;&#29983;&#25104;&#27010;&#24565;&#21487;&#20197;&#34987;&#29992;&#20110;&#37325;&#26032;&#32452;&#21512;&#21644;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#29992;&#20110;&#19979;&#28216;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21512;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#38656;&#35201;&#29992;&#25143;&#25351;&#23450;&#20182;&#20204;&#24819;&#35201;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;&#8212;&#8212;&#22312;&#32473;&#20986;&#30340;&#19981;&#21516;&#22270;&#20687;&#38598;&#21512;&#20013;&#65292;&#25105;&#20204;&#33021;&#21542;&#21457;&#29616;&#20195;&#34920;&#27599;&#20010;&#22270;&#20687;&#30340;&#29983;&#25104;&#27010;&#24565;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20174;&#19968;&#32452;&#22270;&#20687;&#20013;&#21457;&#29616;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#23558;&#32472;&#30011;&#20013;&#19981;&#21516;&#30340;&#33402;&#26415;&#39118;&#26684;&#65292;&#23545;&#35937;&#21644;&#29031;&#26126;&#20174;&#21416;&#25151;&#22330;&#26223;&#20013;&#20998;&#35299;&#20986;&#26469;&#65292;&#24182;&#36890;&#36807;&#32473;&#23450;&#30340;ImageNet&#22270;&#20687;&#21457;&#29616;&#22270;&#20687;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#29983;&#25104;&#27010;&#24565;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#22270;&#20687;&#30340;&#20869;&#23481;&#65292;&#33021;&#22815;&#37325;&#26032;&#32452;&#21512;&#21644;&#32452;&#25104;&#20197;&#29983;&#25104;&#26032;&#30340;&#33402;&#26415;&#21644;&#28151;&#21512;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#31181;&#34920;&#31034;&#26469;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem -- given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#23454;&#39564;&#21050;&#28608;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#24102;&#26377;&#28145;&#24230;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#34987;&#21160;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.05331</link><description>&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#36125;&#21494;&#26031;&#30697;&#38453;&#34701;&#21512;&#27169;&#22411;&#24102;&#28145;&#24230;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Actively learning a Bayesian matrix fusion model with deep side information. (arXiv:2306.05331v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#23454;&#39564;&#21050;&#28608;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#24102;&#26377;&#28145;&#24230;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#34987;&#21160;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#27010;&#24565;&#30340;&#39640;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21487;&#20197;&#19982;&#20154;&#31867;&#27880;&#37322;&#23545;&#40784;&#65292;&#20294;&#36825;&#38656;&#35201;&#33719;&#21462;&#26114;&#36149;&#30340;&#34892;&#20026;&#21709;&#24212;&#65292;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#65292;&#20165;&#31232;&#30095;&#37319;&#26679;&#28145;&#24230;&#29305;&#24449;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#23454;&#39564;&#21050;&#28608;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#24102;&#26377;&#28145;&#24230;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#12290;&#30456;&#27604;&#20110;&#34987;&#21160;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39034;&#24207;&#25209;&#27425;&#37319;&#26679;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#20256;&#32479;&#23454;&#39564;&#25910;&#38598;&#30340;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#31934;&#30830;&#23545;&#40784;&#39044;&#35757;&#32451;&#32593;&#32476;&#27966;&#29983;&#30340;&#39640;&#32500;&#28145;&#23618;&#29305;&#24449;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#20247;&#21253;&#25968;&#25454;&#25910;&#38598;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional deep neural network representations of images and concepts can be aligned to predict human annotations of diverse stimuli. However, such alignment requires the costly collection of behavioral responses, such that, in practice, the deep-feature spaces are only ever sparsely sampled. Here, we propose an active learning approach to adaptively sampling experimental stimuli to efficiently learn a Bayesian matrix factorization model with deep side information. We observe a significant efficiency gain over a passive baseline. Furthermore, with a sequential batched sampling strategy, the algorithm is applicable not only to small datasets collected from traditional laboratory experiments but also to settings where large-scale crowdsourced data collection is needed to accurately align the high-dimensional deep feature representations derived from pre-trained networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05323</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#26041;&#27861;&#35770;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#22810;&#20013;&#24515;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application. (arXiv:2306.05323v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20986;&#22810;&#20013;&#24515;&#35782;&#21035;&#27169;&#22411;&#65292;&#25972;&#20307; F1&#24471;&#20998;&#20026;84.77%&#12290;&#35813;&#27169;&#22411;&#23558;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#24341;&#20837;&#35745;&#31639;&#26426;&#21270;&#21307;&#30103;&#35760;&#24405;&#26377;&#21161;&#20110;&#20943;&#23569;&#25163;&#20889;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#32321;&#29712;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#21307;&#30103;&#35760;&#24405;&#20013;&#25552;&#21462;&#25968;&#25454;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#22240;&#27492;&#21307;&#30103;&#35760;&#24405;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#20173;&#28982;&#34987;&#20805;&#20998;&#21033;&#29992;&#31243;&#24230;&#20302;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#20174;&#19994;&#32773;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#27969;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#24847;&#22823;&#21033;&#31070;&#32463;&#31934;&#31070;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25968;&#25454;&#38598; PsyNIT&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#24320;&#21457;&#36825;&#19968;&#20219;&#21153;&#30340;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#22806;&#37096;&#29420;&#31435;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#20013;&#24515;&#27169;&#22411;&#65292;&#25972;&#20307; F1 &#24471;&#20998;&#20026; 84.77%&#65292;&#31934;&#30830;&#29575;&#20026; 83.16%&#65292;&#21484;&#22238;&#29575;&#20026; 86.44%&#12290;&#25105;&#20204;&#23398;&#21040;&#30340;&#32463;&#39564;&#26159;: (i) &#19968;&#33268;&#30340;&#27880;&#37322;&#36807;&#31243;&#30340;&#20851;&#38190;&#20316;&#29992;&#21644; (ii) &#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#21644;&#8220;&#23569;&#37327;&#35757;&#32451;&#8221;&#30340; fine-tuning &#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a "few-shot" a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.05304</link><description>&lt;p&gt;
&#22270;&#19978;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19981;&#26029;&#28044;&#29616;&#25512;&#21160;&#20102;&#22312;&#22270;&#33410;&#28857;&#38598;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#22270;&#25628;&#32034;&#31639;&#27861;&#21487;&#29992;&#20110;&#27492;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#20851;&#20110;&#20989;&#25968;&#20540;&#30340;&#20449;&#24687;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#40657;&#30418;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23427;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#26032;&#39062;&#35774;&#32622;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#21270;&#22312;&#36890;&#29992;&#65292;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#30340;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#30340;&#20248;&#28857;&#12290;&#23616;&#37096;&#24314;&#27169;&#26041;&#27861;&#36827;&#19968;&#27493;&#20445;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#34892;&#21160;&#20998;&#22359;&#26426;&#21046;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#32463;&#36807;&#32451;&#20064;&#30340;&#34892;&#21160;&#24207;&#21015;&#26469;&#21152;&#36895;&#21644;&#25552;&#39640;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#23884;&#20837;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#35268;&#21010;&#22120;&#20013;&#12290;&#22312;&#29289;&#29702;&#26500;&#36896;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.05298</link><description>&lt;p&gt;
&#24605;&#32500;&#20064;&#24815;&#65306;&#37325;&#22797;&#21033;&#29992;&#34892;&#21160;&#24207;&#21015;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Habits of Mind: Reusing Action Sequences for Efficient Planning. (arXiv:2306.05298v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#34892;&#21160;&#20998;&#22359;&#26426;&#21046;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#32463;&#36807;&#32451;&#20064;&#30340;&#34892;&#21160;&#24207;&#21015;&#26469;&#21152;&#36895;&#21644;&#25552;&#39640;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#23884;&#20837;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#35268;&#21010;&#22120;&#20013;&#12290;&#22312;&#29289;&#29702;&#26500;&#36896;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#26102;&#65292;&#23427;&#20204;&#30340;&#25191;&#34892;&#21464;&#24471;&#26356;&#21152;&#27969;&#30021;&#21644;&#31934;&#30830;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24050;&#32463;&#32451;&#20064;&#36807;&#30340;&#34892;&#21160;&#24207;&#21015;&#20063;&#21487;&#20197;&#36890;&#36807;&#32858;&#28966;&#20110;&#36807;&#21435;&#32463;&#24120;&#20351;&#29992;&#30340;&#36335;&#24452;&#26469;&#21152;&#36895;&#21644;&#25552;&#39640;&#35268;&#21010;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#26641;&#20013;&#30340;&#22810;&#27493;&#36339;&#36291;&#23558;&#28145;&#23618;&#35268;&#21010;&#38382;&#39064;&#32553;&#23567;&#20026;&#27973;&#23618;&#38382;&#39064;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#26679;&#30340;&#24207;&#21015;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#34892;&#21160;&#20998;&#22359;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#19981;&#21516;&#30340;&#23610;&#24230;&#19978;&#25214;&#21040;&#21644;&#21033;&#29992;&#20102;&#32479;&#35745;&#19978;&#21487;&#38752;&#30340;&#32467;&#26500;&#12290;&#36825;&#20135;&#29983;&#20102;&#21487;&#20197;&#23884;&#20837;&#21040;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#35268;&#21010;&#22120;&#20013;&#30340;&#26356;&#30701;&#25110;&#26356;&#38271;&#30340;&#24120;&#35268;&#31243;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#19971;&#24039;&#26495;&#30340;&#29289;&#29702;&#26500;&#36896;&#20219;&#21153;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#26696;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we exercise sequences of actions, their execution becomes more fluent and precise. Here, we consider the possibility that exercised action sequences can also be used to make planning faster and more accurate by focusing expansion of the search tree on paths that have been frequently used in the past, and by reducing deep planning problems to shallow ones via multi-step jumps in the tree. To capture such sequences, we use a flexible Bayesian action chunking mechanism which finds and exploits statistically reliable structure at different scales. This gives rise to shorter or longer routines that can be embedded into a Monte-Carlo tree search planner. We show the benefits of this scheme using a physical construction task patterned after tangrams.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#20174;&#21496;&#26426;&#30340;&#22836;&#37096;&#21160;&#20316;&#20013;&#25910;&#38598;&#20449;&#21495;&#65292;&#24182;&#22522;&#20110;&#19968;&#27425;&#23398;&#20064;&#25216;&#26415;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21496;&#26426;&#30340;&#19981;&#21516;&#21160;&#20316;&#31867;&#22411;&#65292;&#23454;&#39564;&#20934;&#30830;&#29575;&#36229;&#36807;95%&#12290;</title><link>http://arxiv.org/abs/2306.05291</link><description>&lt;p&gt;
&#22522;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#19968;&#27425;&#23398;&#20064;&#24335;&#21496;&#26426;&#22836;&#37096;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One shot learning based drivers head movement identification using a millimetre wave radar sensor. (arXiv:2306.05291v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05291
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20256;&#24863;&#22120;&#20174;&#21496;&#26426;&#30340;&#22836;&#37096;&#21160;&#20316;&#20013;&#25910;&#38598;&#20449;&#21495;&#65292;&#24182;&#22522;&#20110;&#19968;&#27425;&#23398;&#20064;&#25216;&#26415;&#35774;&#35745;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#21496;&#26426;&#30340;&#19981;&#21516;&#21160;&#20316;&#31867;&#22411;&#65292;&#23454;&#39564;&#20934;&#30830;&#29575;&#36229;&#36807;95%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#28966;&#20110;&#20132;&#36890;&#23433;&#20840;&#65292;&#30417;&#27979;&#21496;&#26426;&#26159;&#21542;&#19987;&#27880;&#20110;&#39550;&#39542;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#22522;&#20110;&#27627;&#31859;&#27874;&#38647;&#36798;&#24212;&#29992;&#65292;&#21033;&#29992;&#23433;&#35013;&#22312;&#36710;&#36742;&#26041;&#21521;&#30424;&#22788;&#30340;&#23567;&#22411;&#27627;&#31859;&#27874;&#38647;&#36798;&#20174;&#21496;&#26426;&#19981;&#21516;&#30340;&#22836;&#37096;&#21160;&#20316;&#25910;&#38598;&#20449;&#21495;&#65292;&#20877;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#20998;&#31867;&#22120;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#21160;&#20316;&#31867;&#22411;&#12290;&#37492;&#20110;&#25968;&#25454;&#38598;&#36739;&#23567;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#19968;&#27425;&#23398;&#20064;&#26041;&#27861;&#20998;&#31867;&#21496;&#26426;&#30340;&#22235;&#31181;&#22836;&#37096;&#21160;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;95%&#12290;
&lt;/p&gt;
&lt;p&gt;
Concentration of drivers on traffic is a vital safety issue; thus, monitoring a driver being on road becomes an essential requirement. The key purpose of supervision is to detect abnormal behaviours of the driver and promptly send warnings to him her for avoiding incidents related to traffic accidents. In this paper, to meet the requirement, based on radar sensors applications, the authors first use a small sized millimetre wave radar installed at the steering wheel of the vehicle to collect signals from different head movements of the driver. The received signals consist of the reflection patterns that change in response to the head movements of the driver. Then, in order to distinguish these different movements, a classifier based on the measured signal of the radar sensor is designed. However, since the collected data set is not large, in this paper, the authors propose One shot learning to classify four cases of driver's head movements. The experimental results indicate that the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.05281</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#30340;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Fault Identification of Rotating Machinery Based on Dynamic Feature Reconstruction Signal Graph. (arXiv:2306.05281v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#22312;&#26059;&#36716;&#26426;&#26800;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#20851;&#38190;&#36827;&#23637;&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#36873;&#25321;&#26368;&#20248;&#23376;&#24102;&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22312;&#26059;&#36716;&#26426;&#26800;&#24378;&#22122;&#22768;&#19979;&#35782;&#21035;&#25925;&#38556;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#37325;&#26500;&#20449;&#21495;&#22270;&#27861;&#65292;&#23427;&#22312;&#25152;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#25925;&#38556;&#35786;&#26029;&#27169;&#22411;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21033;&#29992;&#23567;&#27874;&#21253;&#20998;&#35299;&#65288;WPD&#65289;&#23558;&#21407;&#22987;&#26426;&#26800;&#20449;&#21495;&#20998;&#35299;&#20026;&#21253;&#25324;&#31995;&#25968;&#30697;&#38453;&#22312;&#20869;&#30340;&#22810;&#20010;&#23376;&#24102;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#26368;&#21021;&#23450;&#20041;&#30340;&#20004;&#20010;&#35201;&#32032;MDD&#21644;DDD&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#33021;&#37327;&#33539;&#25968;&#30340;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;DFSL&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#33021;&#37327;&#33539;&#25968;&#20998;&#24067;&#30340;&#24046;&#24322;&#21160;&#24577;&#22320;&#36873;&#25321;WPD&#30340;&#29305;&#24449;&#31995;&#25968;&#30697;&#38453;&#65292;&#20351;&#27599;&#20010;&#23376;&#20449;&#21495;&#33021;&#22815;&#36827;&#34892;&#36866;&#24212;&#24615;&#20449;&#21495;&#37325;&#26500;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#26368;&#20248;&#29305;&#24449;&#23376;&#24102;&#30340;&#31995;&#25968;&#30697;&#38453;&#36827;&#34892;&#37325;&#26500;&#21644;&#37325;&#26032;&#32452;&#21512;&#65292;&#24471;&#21040;&#29305;&#24449;&#20449;&#21495;&#22270;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;2D-&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;2D-CNN&#65289;&#20174;&#29305;&#24449;&#20449;&#21495;&#22270;&#20013;&#25552;&#21462;&#28145;&#23618;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the performance in identifying the faults under strong noise for rotating machinery, this paper presents a dynamic feature reconstruction signal graph method, which plays the key role of the proposed end-to-end fault diagnosis model. Specifically, the original mechanical signal is first decomposed by wavelet packet decomposition (WPD) to obtain multiple subbands including coefficient matrix. Then, with originally defined two feature extraction factors MDD and DDD, a dynamic feature selection method based on L2 energy norm (DFSL) is proposed, which can dynamically select the feature coefficient matrix of WPD based on the difference in the distribution of norm energy, enabling each sub-signal to take adaptive signal reconstruction. Next the coefficient matrices of the optimal feature sub-bands are reconstructed and reorganized to obtain the feature signal graphs. Finally, deep features are extracted from the feature signal graphs by 2D-Convolutional neural network (2D-CNN). Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#38750;&#27491;&#24335;&#31243;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05276</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21462;&#26041;&#27861;&#30340;&#24191;&#27867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction. (arXiv:2306.05276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#22312;&#20855;&#26377;&#19981;&#21516;&#38750;&#27491;&#24335;&#31243;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#33391;&#20107;&#20214;&#65288;ADE&#65289;&#25552;&#21462;&#26159;&#25968;&#23383;&#33647;&#29289;&#30417;&#31649;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#65292;&#29305;&#21035;&#26159;&#24403;&#24212;&#29992;&#20110;&#38750;&#27491;&#24335;&#25991;&#26412;&#26102;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20351;&#29992;&#20687;BERT&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#21738;&#19968;&#20010;&#34920;&#29616;&#26356;&#22909;&#20197;&#21450;&#20026;&#20160;&#20040;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;19&#31181;&#22522;&#20110;Transformer&#30340;ADE&#25552;&#21462;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#24182;&#22312;&#36880;&#28176;&#22686;&#21152;&#38750;&#27491;&#24335;&#27700;&#24179;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#25152;&#26377;&#32771;&#34385;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#35770;&#22363;&#24086;&#23376;&#21644;&#25512;&#25991;&#65289;&#12290;&#25105;&#20204;&#36824;&#23558;&#32431;Transformer&#27169;&#22411;&#19982;&#20004;&#20010;&#24120;&#29992;&#30340;&#39069;&#22806;&#22788;&#29702;&#23618;&#65288;CRF&#21644;LSTM&#65289;&#30456;&#32467;&#21512;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#25104;&#29087;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#25216;&#26415;&#65288;SHAP&#65289;&#23558;&#27169;&#22411;&#24615;&#33021;&#19982;&#19968;&#32452;&#29305;&#24449;&#30456;&#20851;&#32852;&#65292;&#20197;&#36827;&#19968;&#27493;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adverse Event (ADE) extraction is one of the core tasks in digital pharmacovigilance, especially when applied to informal texts. This task has been addressed by the Natural Language Processing community using large pre-trained language models, such as BERT. Despite the great number of Transformer-based architectures used in the literature, it is unclear which of them has better performances and why. Therefore, in this paper we perform an extensive evaluation and analysis of 19 Transformer-based models for ADE extraction on informal texts. We compare the performance of all the considered models on two datasets with increasing levels of informality (forums posts and tweets). We also combine the purely Transformer-based models with two commonly-used additional processing layers (CRF and LSTM), and analyze their effect on the models performance. Furthermore, we use a well-established feature importance technique (SHAP) to correlate the performance of the models with a set of features that 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#23427;&#20351;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#21518;&#39564;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20351;&#29992;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26367;&#25442;KL&#25955;&#24230;&#65292;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.05256</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unscented Autoencoder. (arXiv:2306.05256v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#23427;&#20351;&#29992;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#32479;&#35745;&#37327;&#26469;&#25552;&#39640;&#21518;&#39564;&#34920;&#31034;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20351;&#29992;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26367;&#25442;KL&#25955;&#24230;&#65292;&#23454;&#29616;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#20854;&#37325;&#24314;&#36807;&#31243;&#35299;&#37322;&#20026;&#26469;&#33258;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#24182;&#24212;&#29992;&#26080;&#25439;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;UKF&#65289;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#20998;&#24067;&#36817;&#20284;&#8212;&#8212;&#26080;&#25439;&#21464;&#25442;&#65288;UT&#65289;&#12290;&#30830;&#23450;&#24615;&#37319;&#26679;&#30340;&#26377;&#38480;&#19968;&#32452;&#31216;&#20026;sigma&#28857;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#27604;&#37325;&#21442;&#25968;&#25216;&#24039;&#30340;&#26222;&#36941;&#22122;&#22768;&#32553;&#25918;&#26356;&#20855;&#20449;&#24687;&#37327;&#19988;&#26041;&#24046;&#26356;&#23567;&#30340;&#21518;&#39564;&#34920;&#31034;&#65292;&#21516;&#26102;&#30830;&#20445;&#26356;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#26367;&#25442;&#20026;Wasserstein&#20998;&#24067;&#24230;&#37327;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#23574;&#38160;&#30340;&#21518;&#39564;&#12290;&#21463;&#21040;&#36825;&#20004;&#20010;&#32452;&#20214;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;VAE&#65292;&#21363;&#38750;&#32447;&#24615;&#26080;&#25439;&#33258;&#32534;&#30721;&#22120;&#65288;UAE&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#20165;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#26679;&#26412;&#21518;&#39564;&#30340;&#31867;&#20284;&#27491;&#35268;&#21270;&#30340;&#39033;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Variational Autoencoder (VAE) is a seminal approach in deep generative modeling with latent variables. Interpreting its reconstruction process as a nonlinear transformation of samples from the latent posterior distribution, we apply the Unscented Transform (UT) -- a well-known distribution approximation used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite set of statistics called sigma points, sampled deterministically, provides a more informative and lower-variance posterior representation than the ubiquitous noise-scaling of the reparameterization trick, while ensuring higher-quality reconstruction. We further boost the performance by replacing the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric that allows for a sharper posterior. Inspired by the two components, we derive a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder (UAE), trained purely with regularization-like terms on the per-sample posterior
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#22312;&#35821;&#35328;&#23398;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22810;&#27169;&#24577;&#31995;&#32479;&#36824;&#27809;&#26377;&#35299;&#20915;&#22909;&#36825;&#20010;&#38382;&#39064;</title><link>http://arxiv.org/abs/2306.05240</link><description>&lt;p&gt;
&#22788;&#29702;&#22810;&#27169;&#24577;NLP&#20013;&#30340;&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dealing with Semantic Underspecification in Multimodal NLP. (arXiv:2306.05240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05240
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#22312;&#35821;&#35328;&#23398;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22810;&#27169;&#24577;&#31995;&#32479;&#36824;&#27809;&#26377;&#35299;&#20915;&#22909;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20687;&#20154;&#31867;&#19968;&#26679;&#25484;&#25569;&#35821;&#35328;&#30340;&#26234;&#33021;&#31995;&#32479;&#24517;&#39035;&#22788;&#29702;&#20854;&#35821;&#20041;&#26410;&#30830;&#23450;&#24615;&#65292;&#21363;&#35821;&#35328;&#20449;&#21495;&#21487;&#33021;&#20165;&#20256;&#36798;&#36890;&#20449;&#25152;&#38656;&#20449;&#24687;&#30340;&#19968;&#37096;&#20998;&#12290;&#26631;&#20934;&#30340;NLP&#27169;&#22411;&#21407;&#21017;&#19978;&#27809;&#26377;&#25110;&#20165;&#26377;&#26377;&#38480;&#30340;&#35775;&#38382;&#39069;&#22806;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#32780;&#23558;&#35821;&#35328;&#22522;&#20110;&#20854;&#20182;&#27169;&#24577;&#65288;&#20363;&#22914;&#35270;&#35273;&#65289;&#36827;&#34892;&#35828;&#26126;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#33258;&#28982;&#37197;&#22791;&#20197;&#35299;&#20915;&#36825;&#31181;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#38590;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can quickly and effortlessly integrate semantically-underspecified linguistic signals with a wide range of non-linguistic information, e.g., the multimodal context, social or cultural conventions, and shared knowledge. Standard NLP models have, in principle, no or limited access to such extra information, while multimodal systems grounding language into other modalities, such as vision, are naturally equipped to account for this phenomenon. However, we show that they struggle with it, which could ne
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.05183</link><description>&lt;p&gt;
&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving Long Context Document-Level Machine Translation. (arXiv:2306.05183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21463;&#38480;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#38271;&#31687;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26469;&#35828;&#65292;&#25991;&#26412;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#23545;&#20110;&#25552;&#39640;&#32763;&#35793;&#30340;&#19968;&#33268;&#24615;&#12289;&#20957;&#32858;&#24615;&#12289;&#27169;&#26865;&#20004;&#21487;&#36755;&#20837;&#30340;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#35821;&#35328;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#20851;&#20110;&#25991;&#26723;&#32423;&#21035; NMT &#30340;&#30456;&#20851;&#35770;&#25991;&#20986;&#29256;&#65292;&#20294;&#22823;&#22810;&#25968;&#23558;&#31995;&#32479;&#38480;&#21046;&#22312;&#26412;&#22320;&#19978;&#19979;&#25991;&#65292;&#36890;&#24120;&#21482;&#21253;&#25324;&#21069;&#19968;&#20004;&#20010;&#21477;&#23376;&#20316;&#20026;&#26356;&#22810;&#20449;&#24687;&#12290;&#36825;&#21487;&#33021;&#36275;&#20197;&#35299;&#20915;&#19968;&#20123;&#26326;&#26151;&#24615;&#36755;&#20837;&#65292;&#20294;&#21487;&#33021;&#19981;&#36275;&#20197;&#25429;&#25417;&#25991;&#26723;&#32423;&#21035;&#20449;&#24687;&#65292;&#20363;&#22914;&#35805;&#39064;&#25110;&#23545;&#35805;&#39118;&#26684;&#12290;&#24403;&#23558;&#19978;&#19979;&#25991;&#22823;&#23567;&#22686;&#21152;&#21040;&#26412;&#22320;&#19978;&#19979;&#25991;&#20043;&#22806;&#26102;&#65292;&#20250;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;i&#65289;&#20869;&#23384;&#20351;&#29992;&#23558;&#21576;&#25351;&#25968;&#22686;&#38271;&#65288;ii&#65289;&#32763;&#35793;&#24615;&#33021;&#24320;&#22987;&#38477;&#20302;&#12290;&#25105;&#20204;&#35748;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#27880;&#24847;&#26426;&#21046;&#26159;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#30340;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#24207;&#21015;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#65292;&#21516;&#26102;&#25511;&#21046;&#23545;&#40784;&#26435;&#37325;&#30340;&#24635;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena. Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information. This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation. When increasing the context size beyond just the local context, there are two challenges: (i) the~memory usage increases exponentially (ii) the translation performance starts to degrade. We argue that the widely-used attention mechanism is responsible for both issues. Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#26377;&#21521;&#22270;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;LLM prompt template Think_Net_Prompt&#26469;&#34920;&#31034;&#32467;&#26500;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#28176;&#36827;&#20998;&#35299;&#20219;&#21153;&#24182;&#29983;&#25104;&#20219;&#21153;&#26641;&#20197;&#21450;&#35299;&#32806;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#65292;&#20351;&#20219;&#21153;&#35268;&#21010;&#36807;&#31243;&#26356;&#21152;&#28789;&#27963;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#25351;&#23450;&#30340;&#20195;&#30721;&#26684;&#24335;&#12289;&#29702;&#35299;&#20219;&#21153;&#21644;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#21442;&#25968;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05171</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26377;&#21521;&#22270;&#32467;&#26500;&#34920;&#31034;&#30693;&#35782;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures. (arXiv:2306.05171v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#21644;&#26377;&#21521;&#22270;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;LLM prompt template Think_Net_Prompt&#26469;&#34920;&#31034;&#32467;&#26500;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36890;&#36807;&#28176;&#36827;&#20998;&#35299;&#20219;&#21153;&#24182;&#29983;&#25104;&#20219;&#21153;&#26641;&#20197;&#21450;&#35299;&#32806;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#65292;&#20351;&#20219;&#21153;&#35268;&#21010;&#36807;&#31243;&#26356;&#21152;&#28789;&#27963;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#25351;&#23450;&#30340;&#20195;&#30721;&#26684;&#24335;&#12289;&#29702;&#35299;&#20219;&#21153;&#21644;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#21442;&#25968;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#21644;&#22797;&#26434;&#20219;&#21153;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#35268;&#21010;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#19982;LLM&#30456;&#32467;&#21512;&#65292;&#24182;&#35774;&#35745;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;LLM&#25552;&#31034;&#27169;&#26495;Think_Net_Prompt&#65292;&#20197;&#34920;&#31034;&#32467;&#26500;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20998;&#35299;&#20219;&#21153;&#24182;&#29983;&#25104;&#20219;&#21153;&#26641;&#26469;&#20943;&#23569;&#27599;&#20010;&#20219;&#21153;&#30340;&#35268;&#21010;&#37327;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#31574;&#30053;&#26469;&#35299;&#32806;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#35268;&#21010;&#23454;&#20307;&#21010;&#20998;&#21644;&#23558;&#20219;&#21153;&#19982;&#23454;&#38469;&#26426;&#22120;&#32465;&#23450;&#36807;&#31243;&#20998;&#24320;&#65292;&#20219;&#21153;&#35268;&#21010;&#36807;&#31243;&#21464;&#24471;&#26356;&#21152;&#28789;&#27963;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#25351;&#23450;&#30340;&#20195;&#30721;&#26684;&#24335;&#12289;&#29702;&#35299;&#20219;&#21153;&#21644;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#25552;&#21462;&#21442;&#25968;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#20063;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#20219;&#21153;&#36923;&#36753;&#22788;&#29702;&#30340;&#22797;&#26434;&#24230;&#26377;&#38480;&#65292;&#22312;&#25968;&#37327;&#27495;&#20041;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional robot task planning methods face challenges when dealing with highly unstructured environments and complex tasks. We propose a task planning method that combines human expertise with an LLM and have designed an LLM prompt template, Think_Net_Prompt, with stronger expressive power to represent structured professional knowledge. We further propose a method to progressively decompose tasks and generate a task tree to reduce the planning volume for each task, and we have designed a strategy to decouple robot task planning. By dividing different planning entities and separating the task from the actual machine binding process, the task planning process becomes more flexible. Research results show that our method performs well in handling specified code formats, understanding the relationship between tasks and subtasks, and extracting parameters from text descriptions. However, there are also problems such as limited complexity of task logic handling, ambiguity in the quantity of
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#20849;&#21516;&#24320;&#21457;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#65292;&#22686;&#24378;&#21452;&#26041;&#21512;&#20316;&#30340;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05153</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#26159;&#26356;&#22909;&#30340;&#32534;&#31243;&#20249;&#20276;&#65311;&#20154;&#20154;&#23545;&#32534;&#31243; vs &#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming. (arXiv:2306.05153v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05153
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#20849;&#21516;&#24320;&#21457;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#65292;&#22686;&#24378;&#21452;&#26041;&#21512;&#20316;&#30340;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25484;&#25569;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#20197;&#21450;&#21830;&#19994;&#20135;&#21697;&#65292;&#22914;GitHub&#30340;Copilot&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#21512;&#20316;(pAIr&#32534;&#31243;)&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#20154;&#20154;&#23545;&#32534;&#31243;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#19981;&#30830;&#23450;&#20854;&#30740;&#31350;&#32467;&#26524;&#26159;&#21542;&#33021;&#36866;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20154;&#20154;&#23545;&#32534;&#31243;&#21644;&#20154;&#24037;&#26234;&#33021;&#23545;&#32534;&#31243;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#20132;&#20114;&#12289;&#34913;&#37327;&#12289;&#20248;&#28857;&#21644;&#25361;&#25112;&#26041;&#38754;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#25991;&#29486;&#20013;&#21508;&#19981;&#30456;&#21516;&#65288;&#23613;&#31649;&#29992;&#20110;pAIr&#32534;&#31243;&#30340;&#24230;&#37327;&#19981;&#22826;&#20840;&#38754;&#65289;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#24433;&#21709;&#20154;&#20154;&#23545;&#32534;&#31243;&#25104;&#21151;&#30340;&#35843;&#33410;&#22240;&#32032;&#65292;&#20026;pAIr&#32534;&#31243;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#19987;&#19994;&#30693;&#35782;&#20250;&#20351;&#23545;&#32534;&#31243;&#32570;&#20047;&#29983;&#20135;&#21147;&#65292;&#22240;&#27492;&#35774;&#35745;&#33391;&#22909;&#30340;AI&#32534;&#31243;&#21161;&#25163;&#21487;&#20197;&#28508;&#22312;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-language models (LLMs) that excel at code generation and commercial products such as GitHub's Copilot has sparked interest in human-AI pair programming (referred to as "pAIr programming") where an AI system collaborates with a human programmer. While traditional pair programming between humans has been extensively studied, it remains uncertain whether its findings can be applied to human-AI pair programming. We compare human-human and human-AI pair programming, exploring their similarities and differences in interaction, measures, benefits, and challenges. We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive). We summarize moderating factors on the success of human-human pair programming, which provides opportunities for pAIr programming research. For example, mismatched expertise makes pair programming less productive, therefore well-designed AI programming assistants
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05150</link><description>&lt;p&gt;
&#26114;&#36149;&#23884;&#22871;&#28784;&#30418;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#23884;&#22871;&#40657;&#30333;&#31665;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#30456;&#27604;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20840;&#23616;&#26368;&#20248;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#28784;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#30001;&#40657;&#31665;&#21644;&#30333;&#31665;&#20989;&#25968;&#32452;&#25104;&#30340;&#23884;&#22871;&#20989;&#25968;&#12290;&#32473;&#20986;&#20102;&#36825;&#31181;&#28784;&#30418;&#38382;&#39064;&#30340;&#19968;&#33324;&#24418;&#24335;&#65292;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#28784;&#30418;&#20248;&#21270;&#20844;&#24335;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#20041;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#40657;&#31665;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30456;&#20284;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#20294;&#20056;&#20197;&#20381;&#36182;&#20110;&#25152;&#32771;&#34385;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#30340;&#24120;&#25968;&#20056;&#39033;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#24773;&#20917;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;&#24120;&#29992;&#30340;&#26680;&#20989;&#25968;&#65292;&#21518;&#24724;&#36793;&#30028;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#21040;&#26368;&#20248;&#35299;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#40657;&#31665;&#20248;&#21270;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#28784;&#30418;&#20248;&#21270;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26174;&#30528;&#25552;&#39640;&#20102;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of optimizing a grey-box objective function, i.e., nested function composed of both black-box and white-box functions. A general formulation for such grey-box problems is given, which covers the existing grey-box optimization formulations as special cases. We then design an optimism-driven algorithm to solve it. Under certain regularity assumptions, our algorithm achieves similar regret bound as that for the standard black-box Bayesian optimization algorithm, up to a constant multiplicative term depending on the Lipschitz constants of the functions considered. We further extend our method to the constrained case and discuss several special cases. For the commonly used kernel functions, the regret bounds allow us to derive a convergence rate to the optimal solution. Experimental results show that our grey-box optimization method empirically improves the speed of finding the global optimal solution significantly, as compared to the standard black-box optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Gradient-Informed Discrete Emitter (ME-GIDE)&#30340;Map-Elites&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#31163;&#25955;&#31354;&#38388;&#30340;&#25628;&#32034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#31163;&#25955;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05138</link><description>&lt;p&gt;
&#31163;&#25955;&#31354;&#38388;&#30340;&#26799;&#24230;&#20449;&#24687;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradient-Informed Quality Diversity for the Illumination of Discrete Spaces. (arXiv:2306.05138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Gradient-Informed Discrete Emitter (ME-GIDE)&#30340;Map-Elites&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#31163;&#25955;&#31354;&#38388;&#30340;&#25628;&#32034;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#22312;&#31163;&#25955;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#26088;&#22312;&#25628;&#32034;&#22823;&#37327;&#21508;&#24322;&#19988;&#24615;&#33021;&#20248;&#36234;&#30340;&#35299;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#32452;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#23613;&#31649;&#26089;&#26399;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#23558;&#30446;&#26631;&#21644;&#25551;&#36848;&#31526;&#20989;&#25968;&#35270;&#20026;&#40657;&#30418;&#20989;&#25968;&#65292;&#20294;&#24341;&#20837;&#20102;&#26032;&#24037;&#20855;&#20197;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#65292;&#21152;&#36895;&#25628;&#32034;&#24182;&#25552;&#39640;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#30340;&#24212;&#29992;&#28041;&#21450;&#31163;&#25955;&#31354;&#38388;&#65292;&#20363;&#22914;&#33647;&#29289;&#21457;&#29616;&#25110;&#22270;&#20687;&#29983;&#25104;&#12290;&#25506;&#32034;&#36825;&#20123;&#31354;&#38388;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32452;&#21512;&#35268;&#27169;&#24456;&#22823;&#65292;&#24182;&#19988;&#19981;&#33021;&#20687;&#36830;&#32493;&#31354;&#38388;&#37027;&#26679;&#20351;&#29992;&#26799;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#30340;&#31163;&#25955;&#21457;&#23556;&#22120;&#30340; Map-Elites&#65288;ME-GIDE&#65289;&#25193;&#23637;&#20102;&#23545;&#31163;&#25955;&#25628;&#32034;&#31354;&#38388;&#30340; QD &#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#30446;&#26631;&#21644;&#25551;&#36848;&#31526;&#20989;&#25968;&#30456;&#23545;&#20110;&#20854;&#31163;&#25955;&#36755;&#20837;&#30340;&#26799;&#24230;&#20449;&#24687;&#26469;&#25552;&#20986;&#26799;&#24230;&#20449;&#24687;&#36873;&#25321;&#30340;&#35299;&#38598;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#31163;&#25955;&#22522;&#20934;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102; ME-GIDE&#65292;&#35777;&#26126;&#23427;&#20248;&#20110;&#23558;&#25628;&#32034;&#31354;&#38388;&#35270;&#20026;&#40657;&#30418;&#30340;&#32463;&#20856;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity (QD) algorithms have been proposed to search for a large collection of both diverse and high-performing solutions instead of a single set of local optima. While early QD algorithms view the objective and descriptor functions as black-box functions, novel tools have been introduced to use gradient information to accelerate the search and improve overall performance of those algorithms over continuous input spaces. However a broad range of applications involve discrete spaces, such as drug discovery or image generation. Exploring those spaces is challenging as they are combinatorially large and gradients cannot be used in the same manner as in continuous spaces. We introduce map-elites with a Gradient-Informed Discrete Emitter (ME-GIDE), which extends QD optimisation with differentiable functions over discrete search spaces. ME-GIDE leverages the gradient information of the objective and descriptor functions with respect to its discrete inputs to propose gradient-inform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#34920;&#26126;&#20256;&#32479;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#36896;&#25104;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#20840;&#36523;&#36827;&#34892;&#21311;&#21517;&#21270;&#26102;&#12290;&#28982;&#32780;&#65292;&#36924;&#30495;&#21311;&#21517;&#21270;&#21487;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#65292;&#23588;&#20854;&#26159;&#38754;&#37096;&#21311;&#21517;&#21270;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05135</link><description>&lt;p&gt;
&#22270;&#20687;&#21311;&#21517;&#21270;&#26159;&#21542;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#35757;&#32451;&#20135;&#29983;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Image Anonymization Impact Computer Vision Training?. (arXiv:2306.05135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#34920;&#26126;&#20256;&#32479;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#36896;&#25104;&#23454;&#36136;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#23545;&#20840;&#36523;&#36827;&#34892;&#21311;&#21517;&#21270;&#26102;&#12290;&#28982;&#32780;&#65292;&#36924;&#30495;&#21311;&#21517;&#21270;&#21487;&#20197;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#65292;&#23588;&#20854;&#26159;&#38754;&#37096;&#21311;&#21517;&#21270;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22320;&#21306;&#65292;&#22270;&#20687;&#21311;&#21517;&#21270;&#26159;&#20026;&#20102;&#36981;&#23432;&#38544;&#31169;&#27861;&#35268;&#32780;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21311;&#21517;&#21270;&#36890;&#24120;&#20250;&#38477;&#20302;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#26426;&#35270;&#35273;&#24320;&#21457;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#35757;&#32451;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#20851;&#38190;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#26816;&#27979;&#12289;&#23454;&#20363;&#20998;&#21106;&#21644;&#23039;&#24577;&#20272;&#35745;&#65289;&#19978;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#24120;&#35265;&#30340;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20256;&#32479;&#21644;&#36924;&#30495;&#21311;&#21517;&#21270;&#23545;&#35782;&#21035;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#21253;&#25324;&#38754;&#37096;&#21644;&#20840;&#36523;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#32479;&#22270;&#20687;&#21311;&#21517;&#21270;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#24403;&#23545;&#20840;&#36523;&#36827;&#34892;&#21311;&#21517;&#21270;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36924;&#30495;&#21311;&#21517;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#65292;&#24403;&#38754;&#37096;&#36827;&#34892;&#21311;&#21517;&#21270;&#26102;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20165;&#26377;&#24494;&#23567;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36924;&#30495;&#21311;&#21517;&#21270;&#21487;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image anonymization is widely adapted in practice to comply with privacy regulations in many regions. However, anonymization often degrades the quality of the data, reducing its utility for computer vision development. In this paper, we investigate the impact of image anonymization for training computer vision models on key computer vision tasks (detection, instance segmentation, and pose estimation). Specifically, we benchmark the recognition drop on common detection datasets, where we evaluate both traditional and realistic anonymization for faces and full bodies. Our comprehensive experiments reflect that traditional image anonymization substantially impacts final model performance, particularly when anonymizing the full body. Furthermore, we find that realistic anonymization can mitigate this decrease in performance, where our experiments reflect a minimal performance drop for face anonymization. Our study demonstrates that realistic anonymization can enable privacy-preserving comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994;4.0&#26694;&#26550;&#19979;&#39044;&#27979;&#24615;&#32500;&#25252;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#21644;&#29616;&#26377;XAI&#26041;&#27861;&#19982;&#29305;&#23450;&#35299;&#37322;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.05120</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;&#24615;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
Explainable Predictive Maintenance. (arXiv:2306.05120v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#19994;4.0&#26694;&#26550;&#19979;&#39044;&#27979;&#24615;&#32500;&#25252;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#21644;&#29616;&#26377;XAI&#26041;&#27861;&#19982;&#29305;&#23450;&#35299;&#37322;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21457;&#25381;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#65292;&#20419;&#36827;&#22797;&#26434;&#30340;&#26234;&#33021;&#31995;&#32479;&#19982;&#25968;&#25454;&#31185;&#23398;&#23478;&#12289;&#39046;&#22495;&#19987;&#23478;&#12289;&#32456;&#31471;&#29992;&#25143;&#31561;&#21508;&#31181;&#20154;&#21592;&#20043;&#38388;&#30340;&#20132;&#27969;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#21644;&#24037;&#19994;4.0&#26694;&#26550;&#19979;&#29305;&#23450;&#35299;&#37322;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) fills the role of a critical interface fostering interactions between sophisticated intelligent systems and diverse individuals, including data scientists, domain experts, end-users, and more. It aids in deciphering the intricate internal mechanisms of ``black box'' Machine Learning (ML), rendering the reasons behind their decisions more understandable. However, current research in XAI primarily focuses on two aspects; ways to facilitate user trust, or to debug and refine the ML model. The majority of it falls short of recognising the diverse types of explanations needed in broader contexts, as different users and varied application areas necessitate solutions tailored to their specific needs.  One such domain is Predictive Maintenance (PdM), an exploding area of research under the Industry 4.0 \&amp; 5.0 umbrella. This position paper highlights the gap between existing XAI methodologies and the specific requirements for explanations within industr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05116</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25628;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Search Strategies for Document-Level Neural Machine Translation. (arXiv:2306.05116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#20316;&#32773;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#65292;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#33021;&#22815;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21477;&#23376;&#32423;&#31995;&#32479;&#30456;&#27604;&#65292;&#25991;&#26723;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#33021;&#22815;&#22312;&#19968;&#20221;&#25991;&#20214;&#20013;&#20135;&#29983;&#26356;&#19968;&#33268;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#36755;&#20837;&#20013;&#30340;&#27495;&#20041;&#12290;&#22312;&#25991;&#26723;&#32423;NMT&#19978;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#30528;&#37325;&#20110;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#31574;&#30053;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#24050;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#25191;&#34892;&#25628;&#32034;&#30340;&#38382;&#39064;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#26377;&#26102;&#29978;&#33267;&#26681;&#26412;&#19981;&#34987;&#25552;&#21450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#22914;&#20309;&#22312;&#35299;&#30721;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#26368;&#27969;&#34892;&#30340;&#25991;&#26723;&#32423;NMT&#26041;&#27861;&#24320;&#22987;&#65292;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#35299;&#30721;&#26041;&#26696;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#20197;&#21450;&#38024;&#23545;&#19977;&#20010;&#26631;&#20934;&#25991;&#26723;&#32423;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#30340;&#29305;&#23450;&#35821;&#35328;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#38024;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#26102;&#24182;&#19981;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05112</link><description>&lt;p&gt;
FheFL&#65306;&#25903;&#25345;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#19982;&#25308;&#21344;&#24237;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25216;&#26415;&#26368;&#21021;&#26159;&#20026;&#20102;&#32531;&#35299;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#12290;&#23613;&#31649;FL&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#22987;&#32456;&#20445;&#30041;&#22312;&#29992;&#25143;&#25163;&#20013;&#65292;&#20294;&#23616;&#37096;&#35757;&#32451;&#27169;&#22411;&#30340;&#26799;&#24230;&#24517;&#39035;&#19982;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#36890;&#20449;&#20197;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65292;&#20351;&#24471;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#25968;&#25454;&#30340;&#31169;&#23494;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#65292;&#19979;&#19968;&#20195;FL&#26550;&#26500;&#25552;&#20986;&#20102;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#20813;&#21463;&#26381;&#21153;&#22120;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24102;&#26469;&#20854;&#20182;&#25361;&#25112;&#65292;&#20363;&#22914;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#36890;&#36807;&#20849;&#20139;&#34394;&#20551;&#26799;&#24230;&#26469;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26799;&#24230;&#34987;&#21152;&#23494;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35782;&#21035;&#21644;&#25490;&#38500;&#19981;&#33391;&#29992;&#25143;&#20197;&#20445;&#25252;&#20840;&#23616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#31181;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
&lt;/p&gt;</description></item><item><title>PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.05087</link><description>&lt;p&gt;
PandaLM&#65306;LLM&#25351;&#20196;&#35843;&#20248;&#20248;&#21270;&#30340;&#33258;&#21160;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. (arXiv:2306.05087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05087
&lt;/p&gt;
&lt;p&gt;
PandaLM&#26159;&#19968;&#20010;&#35780;&#20272;LLM&#25351;&#20196;&#35843;&#20248;&#30340;&#33258;&#21160;&#22522;&#20934;&#65292;&#23427;&#33021;&#22815;&#21306;&#20998;&#26368;&#20248;&#27169;&#22411;&#65292;&#24182;&#20851;&#27880;&#20110;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#22797;&#26434;&#24615;&#21644;&#35780;&#20272;&#35843;&#25972;&#27169;&#22411;&#30340;&#22256;&#38590;&#24615;&#65292;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#25351;&#20196;&#35843;&#20248;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#30830;&#23450;&#26368;&#20339;&#36229;&#21442;&#25968;&#65292;&#38656;&#35201;&#19968;&#20010;&#33258;&#21160;&#30340;&#12289;&#24378;&#22823;&#19988;&#21487;&#38752;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#65292;&#24314;&#31435;&#36825;&#26679;&#19968;&#20010;&#22522;&#20934;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#27454;&#21517;&#20026;PandaLM&#30340;&#35780;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#21306;&#20998;&#20986;&#22810;&#20010;LLM&#20013;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;PandaLM&#30340;&#20851;&#27880;&#28857;&#19981;&#20165;&#38480;&#20110;&#20256;&#32479;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#23458;&#35266;&#27491;&#30830;&#24615;&#65292;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#30456;&#23545;&#31616;&#27905;&#24615;&#12289;&#28165;&#26224;&#24230;&#12289;&#36981;&#24490;&#35828;&#26126;&#12289;&#20840;&#38754;&#24615;&#21644;&#24418;&#24335;&#24615;&#31561;&#37325;&#35201;&#20027;&#35266;&#22240;&#32032;&#12290;&#20026;&#30830;&#20445;PandaLM&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#20154;&#24037;&#27880;&#37322;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#19978;&#19979;&#25991;&#37117;&#26159;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#38656;&#35201;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05082</link><description>&lt;p&gt;
&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#20013;&#26102;&#38388;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Time in Causal Algorithmic Recourse. (arXiv:2306.05082v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05082
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#38656;&#35201;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#34917;&#25937;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#20855;&#26377;&#23454;&#36341;&#24847;&#20041;&#65292;&#21487;&#20197;&#25552;&#20379;&#26377;&#21033;&#20110;&#25913;&#21464;&#19981;&#21033;&#20915;&#31574;&#30340;&#23454;&#29616;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#20102;&#25913;&#36827;&#65292;&#20294;&#26080;&#27861;&#32435;&#20837;&#26102;&#38388;&#32500;&#24230;&#20173;&#28982;&#26159;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30340;&#37325;&#22823;&#23616;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#32500;&#24230;&#32435;&#20837;&#22240;&#26524;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#20013;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#21512;&#29702;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Algorithmic Recourse in decision-making is a promising field that offers practical solutions to reverse unfavorable decisions. However, the inability of these methods to consider potential dependencies among variables poses a significant challenge due to the assumption of feature independence. Recent advancements have incorporated knowledge of causal dependencies, thereby enhancing the quality of the recommended recourse actions. Despite these improvements, the inability to incorporate the temporal dimension remains a significant limitation of these approaches. This is particularly problematic as identifying and addressing the root causes of undesired outcomes requires understanding time-dependent relationships between variables. In this work, we motivate the need to integrate the temporal dimension into causal algorithmic recourse methods to enhance recommendations' plausibility and reliability. The experimental evaluation highlights the significance of the role of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#21253;&#25324;&#34394;&#20551;&#24615;&#22270;&#34920;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30495;&#20551;&#30456;&#20851;&#20851;&#31995;&#19978;&#24110;&#21161;&#21306;&#20998;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.05071</link><description>&lt;p&gt;
&#20998;&#35299;&#20266;&#21464;&#24322;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework for Decomposing Spurious Variations. (arXiv:2306.05071v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#21253;&#25324;&#34394;&#20551;&#24615;&#22270;&#34920;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#30495;&#20551;&#30456;&#20851;&#20851;&#31995;&#19978;&#24110;&#21161;&#21306;&#20998;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20013;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#35299;&#37322;&#20026;&#20160;&#20040;&#20107;&#24773;&#20197;&#29305;&#23450;&#30340;&#26041;&#24335;&#21457;&#29983;&#65292;&#25110;&#36890;&#36807;&#21738;&#20123;&#26426;&#21046;&#26576;&#20010;&#21464;&#37327;$X$&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;$Y$&#26045;&#21152;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#39532;&#23572;&#21487;&#22827;&#21644;&#21322;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20998;&#35299;&#20266;&#21464;&#24322;&#65292;&#24341;&#20837;&#20102;&#34394;&#20551;&#24615;&#22270;&#34920;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#20266;&#25928;&#24212;&#20998;&#35299;&#20026;&#30452;&#25509;&#25928;&#24212;&#21644;&#38388;&#25509;&#25928;&#24212;&#65292;&#20197;&#35782;&#21035;&#29983;&#25104;&#36825;&#20123;&#34394;&#20551;&#20851;&#31995;&#30340;&#22522;&#30784;&#22240;&#26524;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable $X$ exerts influences over another variable $Y$. In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Mark
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36923;&#36753;&#31243;&#24207;&#30340;&#31283;&#23450;&#21644;&#25903;&#25345;&#27169;&#22411;&#65292;&#25429;&#33719;&#33021;&#22815;&#20135;&#29983;&#26494;&#24347;&#35745;&#21010;&#30340;&#34892;&#21160;&#23376;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26494;&#24347;&#35745;&#21010;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#35786;&#26029;&#32534;&#30721;&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05069</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#31243;&#24207;&#25429;&#33719;&#65288;&#26368;&#20248;&#30340;&#65289;&#26494;&#24347;&#35745;&#21010;&#21644;&#31283;&#23450;&#25903;&#25345;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Capturing (Optimal) Relaxed Plans with Stable and Supported Models of Logic Programs. (arXiv:2306.05069v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05069
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36923;&#36753;&#31243;&#24207;&#30340;&#31283;&#23450;&#21644;&#25903;&#25345;&#27169;&#22411;&#65292;&#25429;&#33719;&#33021;&#22815;&#20135;&#29983;&#26494;&#24347;&#35745;&#21010;&#30340;&#34892;&#21160;&#23376;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26494;&#24347;&#35745;&#21010;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#35786;&#26029;&#32534;&#30721;&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21024;&#38500;&#33258;&#30001;&#35268;&#21010;&#19982;&#36923;&#36753;&#32534;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#26159;AI Planning&#31038;&#21306;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#20063;&#34987;&#31216;&#20026;&#26494;&#24347;&#35268;&#21010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#19968;&#20010;&#35268;&#21010;&#38382;&#39064;&#65292;&#25152;&#26377;&#33021;&#22815;&#26377;&#24207;&#22320;&#20135;&#29983;&#35813;&#38382;&#39064;&#26494;&#24347;&#35745;&#21010;&#30340;&#34892;&#21160;&#23376;&#38598;&#37117;&#21487;&#20197;&#36890;&#36807;&#25551;&#36848;&#30456;&#24212;&#26494;&#24347;&#35268;&#21010;&#38382;&#39064;&#30340;&#36923;&#36753;&#31243;&#24207;&#30340;&#31283;&#23450;&#27169;&#22411;&#36827;&#34892;&#21452;&#23556;&#25429;&#33719;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#36923;&#36753;&#31243;&#24207;&#30340;&#25903;&#25345;&#27169;&#22411;&#35821;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#26494;&#24347;&#35268;&#21010;&#38382;&#39064;&#30340;&#19968;&#20010;&#22240;&#26524;&#32534;&#30721;&#21644;&#19968;&#20010;&#35786;&#26029;&#32534;&#30721;&#20316;&#20026;&#36923;&#36753;&#31243;&#24207;&#65292;&#37117;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#25903;&#25345;&#27169;&#22411;&#25429;&#33719;&#26494;&#24347;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26032;&#32534;&#30721;&#22312;&#35745;&#31639;&#26368;&#20248;&#26494;&#24347;&#35745;&#21010;&#26102;&#21487;&#20197;&#25552;&#20379;&#37325;&#22823;&#24615;&#33021;&#22686;&#30410;&#65292;&#25105;&#20204;&#30340;&#35786;&#26029;&#32534;&#30721;&#22312;&#19968;&#31995;&#21015;STRIPS&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#26494;&#24347;&#35268;&#21010;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#32473;&#23450;&#26102;&#38388;&#38480;&#21046;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a novel relation between delete-free planning, an important task for the AI Planning community also known as relaxed planning, and logic programming. We show that given a planning problem, all subsets of actions that could be ordered to produce relaxed plans for the problem can be bijectively captured with stable models of a logic program describing the corresponding relaxed planning problem. We also consider the supported model semantics of logic programs, and introduce one causal and one diagnostic encoding of the relaxed planning problem as logic programs, both capturing relaxed plans with their supported models. Our experimental results show that these new encodings can provide major performance gain when computing optimal relaxed plans, with our diagnostic encoding outperforming state-of-the-art approaches to relaxed planning regardless of the given time limit when measured on a wide collection of STRIPS planning benchmarks.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#23545;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#25277;&#26679;&#20559;&#24046;&#30340;&#21464;&#20307;&#65306;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#25581;&#31034;&#27495;&#35270;&#21487;&#20197;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#65292;&#24182;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05068</link><description>&lt;p&gt;
&#25581;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#21644;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Shedding light on underrepresentation and Sampling Bias in machine learning. (arXiv:2306.05068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05068
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#23545;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#25277;&#26679;&#20559;&#24046;&#30340;&#21464;&#20307;&#65306;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#25581;&#31034;&#27495;&#35270;&#21487;&#20197;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#65292;&#24182;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#34913;&#37327;&#27495;&#35270;&#23545;&#20110;&#24544;&#23454;&#22320;&#35780;&#20272;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#20844;&#27491;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23384;&#22312;&#20219;&#20309;&#27979;&#37327;&#27495;&#35270;&#30340;&#20559;&#35265;&#37117;&#20250;&#23548;&#33268;&#29616;&#26377;&#24046;&#36317;&#30340;&#25918;&#22823;&#25110;&#20302;&#20272;&#12290;&#23384;&#22312;&#20960;&#31181;&#20559;&#35265;&#26469;&#28304;&#65292;&#20551;&#35774;&#26426;&#22120;&#23398;&#20064;&#23548;&#33268;&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#65288;&#20363;&#22914;&#22899;&#24615;&#19982;&#30007;&#24615;&#12289;&#30333;&#20154;&#19982;&#40657;&#20154;&#31561;&#65289;&#20043;&#38388;&#24179;&#31561;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#32676;&#20307;&#20013;&#20998;&#21035;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#23545;&#29305;&#23450;&#20122;&#32676;&#20307;&#30340;&#27495;&#35270;&#12290;&#25277;&#26679;&#20559;&#24046;&#26159;&#25991;&#29486;&#20013;&#25551;&#36848;&#30001;&#25277;&#26679;&#31243;&#24207;&#24341;&#36215;&#30340;&#20559;&#24046;&#30340;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#24341;&#20837;&#26126;&#30830;&#23450;&#20041;&#30340;&#25277;&#26679;&#20559;&#24046;&#21464;&#20307;&#65292;&#21363;&#26679;&#26412;&#37327;&#20559;&#24046;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#20559;&#24046;&#65292;&#26469;&#28040;&#38500;&#36825;&#20010;&#26415;&#35821;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27495;&#35270;&#20998;&#35299;&#20026;&#26041;&#24046;&#12289;&#20559;&#24046;&#21644;&#22122;&#22768;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36890;&#24120;&#34987;&#25509;&#21463;&#30340;&#32531;&#35299;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#8220;&#31616;&#21333;&#22320;&#8221;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#25935;&#24863;&#23646;&#24615;&#65288;&#20363;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#65289;&#26469;&#35299;&#20915;&#27495;&#35270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately measuring discrimination is crucial to faithfully assessing fairness of trained machine learning (ML) models. Any bias in measuring discrimination leads to either amplification or underestimation of the existing disparity. Several sources of bias exist and it is assumed that bias resulting from machine learning is born equally by different groups (e.g. females vs males, whites vs blacks, etc.). If, however, bias is born differently by different groups, it may exacerbate discrimination against specific sub-populations. Sampling bias, is inconsistently used in the literature to describe bias due to the sampling procedure. In this paper, we attempt to disambiguate this term by introducing clearly defined variants of sampling bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We show also how discrimination can be decomposed into variance, bias, and noise. Finally, we challenge the commonly accepted mitigation approach that discrimination can be addressed b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#24182;&#30830;&#23450;&#20102;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#38750;&#31532;&#19968;&#20010;&#22270;&#22359;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#35760;&#21495;&#22359;&#20301;&#32622;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.05067</link><description>&lt;p&gt;
&#25552;&#39640;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#21487;&#35270;&#21270;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Visual Prompt Tuning for Self-supervised Vision Transformers. (arXiv:2306.05067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#24182;&#30830;&#23450;&#20102;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#38750;&#31532;&#19968;&#20010;&#22270;&#22359;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#20943;&#23569;&#30830;&#23450;&#26368;&#20339;&#25552;&#31034;&#35760;&#21495;&#22359;&#20301;&#32622;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#30340;&#26377;&#25928;&#35843;&#25972;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;&#35760;&#21495;&#65292;&#31216;&#20026;&#25552;&#31034;&#65292;&#26469;&#25351;&#23548;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;ViTs&#12290;&#34429;&#28982;VPT&#22312;&#21463;&#30417;&#30563;&#30340;&#35270;&#35273;Transformer&#20013;&#26174;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#20294;&#22312;&#33258;&#30417;&#30563;&#24773;&#20917;&#19979;&#24120;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#23454;&#35777;&#35266;&#23519;&#65292;&#25105;&#20204;&#25512;&#26029;VPT&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#35760;&#21495;&#19982;ViT&#22270;&#22359;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#25552;&#31034;&#35760;&#21495;&#25554;&#20837;&#21518;&#32493;&#22270;&#22359;&#32780;&#19981;&#26159;&#31532;&#19968;&#20010;&#22270;&#22359;&#26102;&#65292;VPT&#22312;MAE&#21644;MoCo v3&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35266;&#23519;&#34920;&#26126;&#65292;&#23384;&#22312;&#36866;&#29992;&#20110;&#25554;&#20837;&#25552;&#31034;&#35760;&#21495;&#30340;&#26368;&#20339;&#22359;&#30340;&#20301;&#32622;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30830;&#23450;&#27599;&#20010;&#33258;&#30417;&#30563;ViT&#20869;&#29992;&#20110;&#19981;&#21516;&#26410;&#26469;&#22330;&#26223;&#30340;&#25552;&#31034;&#30340;&#26368;&#20339;&#22359;&#26159;&#19968;&#20010;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Prompt Tuning (VPT) is an effective tuning method for adapting pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained ViTs. Although VPT has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of VPT hinges largely on the ViT blocks with which the prompt tokens interact. Specifically, VPT shows improved performance on image classification tasks for MAE and MoCo v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised ViT for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#21483;&#20570;&#32467;&#26524;&#25511;&#21046;&#65292;&#38024;&#23545;&#28041;&#21450;&#21040;&#21009;&#20107;&#21496;&#27861;&#12289;&#31119;&#21033;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#20844;&#20849;&#21355;&#29983;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#27010;&#24565;&#20197;&#21450;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05066</link><description>&lt;p&gt;
&#32467;&#26524;&#25511;&#21046;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness for Outcome Control. (arXiv:2306.05066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#21483;&#20570;&#32467;&#26524;&#25511;&#21046;&#65292;&#38024;&#23545;&#28041;&#21450;&#21040;&#21009;&#20107;&#21496;&#27861;&#12289;&#31119;&#21033;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#20844;&#20849;&#21355;&#29983;&#31561;&#22810;&#20010;&#26041;&#38754;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20844;&#24179;&#30340;&#27010;&#24565;&#20197;&#21450;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20250;&#21521;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#22522;&#30784;&#35774;&#26045;&#30340;&#36807;&#28193;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26366;&#30001;&#20154;&#31867;&#25511;&#21046;&#30340;&#20915;&#31574;&#29616;&#22312;&#34987;&#22996;&#25176;&#32473;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#21457;&#23637;&#20351;&#31038;&#20250;&#30340;&#21508;&#20010;&#26041;&#38754;&#26356;&#26377;&#25928;&#29575;&#65292;&#20294;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38656;&#35201;&#38750;&#24120;&#23567;&#24515;&#22320;&#20351;&#36825;&#31181;&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#21464;&#24471;&#20844;&#24179;&#21644;&#20844;&#27491;&#65292;&#21363;&#32771;&#34385;&#21040;&#35832;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#31561;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#20915;&#31574;&#20219;&#21153;&#65292;&#31216;&#20026;&#32467;&#26524;&#25511;&#21046;&#65292;&#20854;&#20013;&#33258;&#21160;&#21270;&#31995;&#32479;&#26088;&#22312;&#20248;&#21270;&#19968;&#20010;&#32467;&#26524;&#21464;&#37327;Y&#65292;&#21516;&#26102;&#20445;&#25345;&#20844;&#24179;&#21644;&#20844;&#27491;&#12290;&#23545;&#20110;&#36825;&#26679;&#19968;&#20010;&#35774;&#32622;&#30340;&#20852;&#36259;&#33539;&#22260;&#20174;&#19982;&#21009;&#20107;&#21496;&#27861;&#21644;&#31119;&#21033;&#26377;&#20851;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#19968;&#30452;&#21040;&#20020;&#24202;&#20915;&#31574;&#21644;&#20844;&#20849;&#21355;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#22240;&#26524;&#38236;&#29255;&#39318;&#20808;&#20998;&#26512;&#20102;&#21033;&#30410;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#25429;&#25417;&#20102;&#19968;&#20010;&#29305;&#23450;&#20010;&#20307;&#20174;&#31215;&#26497;&#20915;&#31574;&#20013;&#33719;&#24471;&#20102;&#22810;&#23569;&#22909;&#22788;&#65292;&#23545;&#29031;&#20107;&#23454;&#30340;&#20844;&#24179;&#24615;&#65292;&#25429;&#25417;&#20102;&#22914;&#26524;&#28041;&#21450;&#21040;&#19981;&#21516;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#38382;&#39064;&#25152;&#22312;&#65292;&#20197;&#21450;&#20805;&#20998;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#20102;&#38656;&#35201;&#22810;&#23569;&#24178;&#39044;&#26469;&#25913;&#21892;&#22240;&#26524;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#22240;&#26524;&#24037;&#20855;&#21644;&#25216;&#26415;&#26469;&#25512;&#26029;&#32467;&#26524;&#25511;&#21046;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#25511;&#21046;&#20102;&#36151;&#27454;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As society transitions towards an AI-based decision-making infrastructure, an ever-increasing number of decisions once under control of humans are now delegated to automated systems. Even though such developments make various parts of society more efficient, a large body of evidence suggests that a great deal of care needs to be taken to make such automated decision-making systems fair and equitable, namely, taking into account sensitive attributes such as gender, race, and religion. In this paper, we study a specific decision-making task called outcome control in which an automated system aims to optimize an outcome variable $Y$ while being fair and equitable. The interest in such a setting ranges from interventions related to criminal justice and welfare, all the way to clinical decision-making and public health. In this paper, we first analyze through causal lenses the notion of benefit, which captures how much a specific individual would benefit from a positive decision, counterfac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05064</link><description>&lt;p&gt;
&#23398;&#20064;&#22320;&#29699;&#31185;&#23398;&#30693;&#35782;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#36164;&#28304;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#20854;&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#21644;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM LLaMA-7B &#27169;&#22411;&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24120;&#35268;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#23558;LLM&#24341;&#20837;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#65292;&#26088;&#22312;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;LLM&#65292;&#21629;&#21517;&#20026;K2&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#36164;&#28304;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;LLM&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#20026;LLM&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#22320;&#29699;&#31185;&#23398;&#25945;&#23398;&#35843;&#38899;&#25968;&#25454;&#38598;GeoSignal&#65292;&#26088;&#22312;&#23558;LLM&#30456;&#24212;&#19982;&#22320;&#29699;&#31185;&#23398;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22320;&#36136;&#31185;&#23398;&#22522;&#20934;&#27979;&#35797;GeoBenchmark&#65292;&#20197;&#22312;&#22320;&#29699;&#31185;&#23398;&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23436;&#25972;&#30340;&#26041;&#27861;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#36890;&#29992;&#39046;&#22495;LLM&#36866;&#24212;&#21040;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36229;&#36807;100&#19975;&#31687;&#22320;&#29699;&#31185;&#23398;&#25991;&#29486;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;&#20102;LLaMA-7B&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;GeoSignal&#30340;&#30417;&#30563;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36801;&#31227;LLM&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs)have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience, with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pretrained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on over 1 million pieces of geoscience literature and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#22312;&#37096;&#32626;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.05058</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition. (arXiv:2306.05058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#22312;&#37096;&#32626;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#20256;&#24863;&#22120;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#37096;&#32626;&#24120;&#24120;&#21463;&#21040;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#21644;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#30693;&#35782;&#27880;&#20837;&#21040;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#38656;&#35201;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#35745;&#31639;&#22797;&#26434;&#30340;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#20351;&#23427;&#20204;&#19981;&#22826;&#36866;&#21512;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#65288;&#20363;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#19978;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;HAR&#30340;NeSy&#26041;&#27861;&#20174;&#26410;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#27880;&#20837;&#30693;&#35782;&#32422;&#26463;&#26469;&#36991;&#20813;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#36827;&#34892;&#31526;&#21495;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#25163;&#31295;&#21644;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models are a standard solution for sensor-based Human Activity Recognition (HAR), but their deployment is often limited by labeled data scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate these issues by infusing knowledge about context information into HAR deep learning classifiers. However, existing NeSy methods for context-aware HAR require computationally expensive symbolic reasoners during classification, making them less suitable for deployment on resource-constrained devices (e.g., mobile devices). Additionally, NeSy approaches for context-aware HAR have never been evaluated on in-the-wild datasets, and their generalization capabilities in real-world scenarios are questionable. In this work, we propose a novel approach based on a semantic loss function that infuses knowledge constraints in the HAR model during the training phase, avoiding symbolic reasoning during classification. Our results on scripted and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.05053</link><description>&lt;p&gt;
Hebbian&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#20027;&#21160;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference in Hebbian Learning Networks. (arXiv:2306.05053v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#65292;&#20197;&#25511;&#21046;&#21160;&#24577;&#20195;&#29702;&#12290;&#36890;&#36807;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#20010;&#21518;&#39564;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#28508;&#22312;&#29366;&#24577;&#65292;&#20197;&#21450;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19979;&#19968;&#20010;&#26399;&#26395;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#20351;&#29992;OpenAI gym&#22871;&#20214;&#20013;&#30340;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#21508;&#31181;Hebbian&#32593;&#32476;&#21442;&#25968;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#65292;&#22914;&#20856;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;Hebbian&#23398;&#20064;&#65292;&#20197;&#35774;&#35745;&#33021;&#22815;&#23398;&#20064;&#29615;&#22659;&#21160;&#24577;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35775;&#38382;&#36807;&#21435;&#32531;&#20914;&#21306;&#30340;AIF&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies how brain-inspired neural ensembles equipped with local Hebbian plasticity can perform active inference (AIF) in order to control dynamical agents. A generative model capturing the environment dynamics is learned by a network composed of two distinct Hebbian ensembles: a posterior network, which infers latent states given the observations, and a state transition network, which predicts the next expected latent state given current state-action pairs. Experimental studies are conducted using the Mountain Car environment from the OpenAI gym suite, to study the effect of the various Hebbian network parameters on the task performance. It is shown that the proposed Hebbian AIF approach outperforms the use of Q-learning, while not requiring any replay buffer, as in typical reinforcement learning systems. These results motivate further investigations of Hebbian learning for the design of AIF networks that can learn environment dynamics without the need for revisiting past buf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21355;&#26143;&#22270;&#20687;&#22788;&#29702;&#21644;&#22823;&#27668;&#20449;&#24687;&#30340;&#26862;&#26519;&#28779;&#28798;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#26862;&#26519;&#28779;&#28798;&#30340;&#32463;&#27982;&#21644;&#29983;&#24577;&#24433;&#21709;&#65292;&#24182;&#21327;&#21161;&#31649;&#29702;&#32773;&#23545;&#35199;&#29677;&#29273;&#30340;&#21361;&#38505;&#22320;&#21306;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#21644;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.05045</link><description>&lt;p&gt;
&#35199;&#29677;&#29273;&#20043;&#28779;&#65306;&#22522;&#20110;&#21355;&#26143;&#22270;&#20687;&#22788;&#29702;&#21644;&#22823;&#27668;&#20449;&#24687;&#30340;&#26032;&#22411;&#26862;&#26519;&#28779;&#28798;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spain on Fire: A novel wildfire risk assessment model based on image satellite processing and atmospheric information. (arXiv:2306.05045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21355;&#26143;&#22270;&#20687;&#22788;&#29702;&#21644;&#22823;&#27668;&#20449;&#24687;&#30340;&#26862;&#26519;&#28779;&#28798;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#26862;&#26519;&#28779;&#28798;&#30340;&#32463;&#27982;&#21644;&#29983;&#24577;&#24433;&#21709;&#65292;&#24182;&#21327;&#21161;&#31649;&#29702;&#32773;&#23545;&#35199;&#29677;&#29273;&#30340;&#21361;&#38505;&#22320;&#21306;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#21644;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#24180;&#65292;&#23665;&#28779;&#25703;&#27585;&#20102;&#35199;&#29677;&#29273;&#26356;&#22823;&#30340;&#38754;&#31215;&#65292;&#23041;&#32961;&#30528;&#20247;&#22810;&#29983;&#24577;&#31995;&#32479;&#12290;90%&#30001;&#20154;&#31867;&#24341;&#36215;&#65288;&#30095;&#24573;&#25110;&#25925;&#24847;&#65289;&#19988;&#20010;&#20307;&#34892;&#20026;&#19981;&#21487;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#27668;&#21644;&#29615;&#22659;&#21464;&#37327;&#24433;&#21709;&#26862;&#26519;&#28779;&#28798;&#30340;&#20256;&#25773;&#65292;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20998;&#26512;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#20107;&#20214;&#30340;&#25439;&#23475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#22411;&#26862;&#26519;&#28779;&#28798;&#35780;&#20272;&#27169;&#22411;&#65288;WAM&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#26862;&#26519;&#28779;&#28798;&#30340;&#32463;&#27982;&#21644;&#29983;&#24577;&#24433;&#21709;&#65292;&#21327;&#21161;&#31649;&#29702;&#32773;&#20026;&#35199;&#29677;&#29273;&#30340;&#21361;&#38505;&#22320;&#21306;&#65292;&#21345;&#26031;&#33922;&#21033;&#20122;-&#33713;&#26114;&#21644;&#23433;&#36798;&#21346;&#35199;&#20122;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#21644;&#20915;&#31574;&#12290;WAM&#20351;&#29992;&#27531;&#24046;&#24335;&#21367;&#31215;&#32593;&#32476;&#32467;&#26500;&#23545;&#22823;&#27668;&#21464;&#37327;&#21644;&#32511;&#24230;&#25351;&#25968;&#25191;&#34892;&#22238;&#24402;&#65292;&#35745;&#31639;&#24517;&#35201;&#30340;&#36164;&#28304;&#65292;&#25511;&#21046;&#21644;&#28781;&#28779;&#26102;&#38388;&#20197;&#21450;&#39044;&#26399;&#28903;&#20260;&#38754;&#31215;&#12290;&#23427;&#39318;&#20808;&#22312;100,000&#20010;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26631;&#35760;&#25968;&#25454;&#30340;&#30417;&#30563;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Each year, wildfires destroy larger areas of Spain, threatening numerous ecosystems. Humans cause 90% of them (negligence or provoked) and the behaviour of individuals is unpredictable. However, atmospheric and environmental variables affect the spread of wildfires, and they can be analysed by using deep learning. In order to mitigate the damage of these events we proposed the novel Wildfire Assessment Model (WAM). Our aim is to anticipate the economic and ecological impact of a wildfire, assisting managers resource allocation and decision making for dangerous regions in Spain, Castilla y Le\'on and Andaluc\'ia. The WAM uses a residual-style convolutional network architecture to perform regression over atmospheric variables and the greenness index, computing necessary resources, the control and extinction time, and the expected burnt surface area. It is first pre-trained with self-supervision over 100,000 examples of unlabelled data with a masked patch prediction objective and fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#26412;&#22320;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#20250;&#22686;&#21152;&#29992;&#25143;&#33021;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#24635;&#33021;&#32791;&#21487;&#20197;&#38477;&#20302;&#39640;&#36798;54%&#12290;</title><link>http://arxiv.org/abs/2306.05041</link><description>&lt;p&gt;
&#33021;&#28304;&#39640;&#25928;&#30340;&#19979;&#34892;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient Downlink Semantic Generative Communication with Text-to-Image Generators. (arXiv:2306.05041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21019;&#24314;&#26412;&#22320;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#20250;&#22686;&#21152;&#29992;&#25143;&#33021;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#24635;&#33021;&#32791;&#21487;&#20197;&#38477;&#20302;&#39640;&#36798;54%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#29983;&#25104;&#36890;&#20449; (SGC) &#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#29983;&#25104;&#22411;&#29992;&#25143;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687; (T2I) &#29983;&#25104;&#22120;&#20174;&#19979;&#36733;&#30340;&#25991;&#26412;&#25552;&#31034;&#26412;&#22320;&#21019;&#24314;&#22270;&#20687;&#65292;&#32780;&#38750;&#29983;&#25104;&#22411;&#29992;&#25143;&#30452;&#25509;&#20174;&#22522;&#31449; (BS) &#19979;&#36733;&#22270;&#20687;&#12290;&#34429;&#28982;&#29983;&#25104;&#22411;&#29992;&#25143;&#24110;&#21161;&#20943;&#23569;&#20102;&#22522;&#31449;&#19979;&#34892;&#20256;&#36755;&#33021;&#37327;&#65292;&#20294;&#23427;&#20204;&#28040;&#32791;&#20102;&#39069;&#22806;&#30340;&#33021;&#28304;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#21644;&#19978;&#20256;&#23427;&#20204;&#30340;&#29983;&#25104;&#22120;&#29366;&#24577;&#20449;&#24687; (GSI)&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#29992;&#25143;&#36873;&#25321;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26368;&#23567;&#21270;&#22522;&#31449;&#21644;&#29992;&#25143;&#24635;&#33021;&#37327;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#65292;&#19982;&#25152;&#26377;&#38750;&#29983;&#25104;&#22411;&#29992;&#25143;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23558;&#24635;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#20102;&#39640;&#36798; 54%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel semantic generative communication (SGC) framework, where generative users leverage text-to-image (T2I) generators to create images locally from downloaded text prompts, while non-generative users directly download images from a base station (BS). Although generative users help reduce downlink transmission energy at the BS, they consume additional energy for image generation and for uploading their generator state information (GSI). We formulate the problem of minimizing the total energy consumption of the BS and the users, and devise a generative user selection algorithm. Simulation results corroborate that our proposed algorithm reduces total energy by up to 54% compared to a baseline with all non-generative users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05036</link><description>&lt;p&gt;
HCI&#25361;&#25112;&#30340;&#26144;&#23556;&#65306;ChatGPT&#21644;GPT-4&#22312;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Cost-Efficient Question Answering. (arXiv:2306.05036v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102; ChatGPT &#21644; GPT-4 &#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36816;&#29992;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#36890;&#36807;&#20197;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#25361;&#25112;&#20026;&#20363;&#65292;&#32467;&#35770;&#26159; ChatGPT &#21644; GPT-4 &#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#19988;&#33410;&#30465;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#31181;LLM&#26159;&#38381;&#28304;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#24615;&#33021;&#30340;&#20102;&#35299;&#12290;&#22312;&#23398;&#26415;&#30028;&#20013;&#65292;LLM&#30340;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#37327;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#21487;&#33021;&#24050;&#27844;&#28431;&#21040;ChatGPT&#21644;GPT-4&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ChatGPT&#21644;GPT-4&#24212;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#38382;&#31572;&#30340;&#23454;&#38469;&#20219;&#21153;&#65292;&#20197;&#20174;2023&#24180;&#20154;&#26426;&#20132;&#20114;&#20250;&#35758;&#65288;CHI&#65289;&#30340;&#35770;&#25991;&#38598;&#20013;&#25552;&#21462;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#36825;&#20010;&#23454;&#38469;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;ChatGPT&#21644;GPT-4&#30340;&#32452;&#21512;&#26159;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#26497;&#20339;&#25104;&#26412;&#25928;&#30410;&#25163;&#27573;&#12290;&#25104;&#26412;&#25928;&#29575;&#23545;&#20110;&#21407;&#22411;&#30740;&#31350;&#24819;&#27861;&#21644;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#35821;&#26009;&#24211;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, the two LLMs are closed source, and little is known about the LLMs' performance in real-world use cases. In academia, LLM performance is often measured on benchmarks which may have leaked into ChatGPT's and GPT-4's training data. In this paper, we apply and evaluate ChatGPT and GPT-4 for the real-world task of cost-efficient extractive question answering over a text corpus that was published after the two LLMs completed training. More specifically, we extract research challenges for researchers in the field of HCI from the proceedings of the 2023 Conference on Human Factors in Computing Systems (CHI). We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for prototyping research ideas and analyzing text corpora from different persp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32463;&#39564;&#30340;&#28176;&#36827;&#35748;&#30693;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22478;&#24066;&#22810;&#20132;&#21449;&#21475;&#21160;&#24577;&#20132;&#36890;&#22330;&#26223;&#19979;&#35299;&#20915;&#20102;&#22810;&#36710;&#36742;&#36861;&#36880;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36861;&#36880;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05016</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#20808;&#32463;&#39564;&#30340;&#28176;&#36827;&#35748;&#30693;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#36710;&#36742;&#36861;&#36880;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Progression Cognition Reinforcement Learning with Prioritized Experience for Multi-Vehicle Pursuit. (arXiv:2306.05016v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32463;&#39564;&#30340;&#28176;&#36827;&#35748;&#30693;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22478;&#24066;&#22810;&#20132;&#21449;&#21475;&#21160;&#24577;&#20132;&#36890;&#22330;&#26223;&#19979;&#35299;&#20915;&#20102;&#22810;&#36710;&#36742;&#36861;&#36880;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#36861;&#36880;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36710;&#36742;&#36861;&#36880;&#65288;MVP&#65289;&#65292;&#22914;&#33258;&#20027;&#35686;&#36710;&#36861;&#36880;&#23244;&#30097;&#20154;&#65292;&#30001;&#20110;&#20854;&#20351;&#21629;&#21644;&#23433;&#20840;&#37325;&#35201;&#24615;&#32780;&#26174;&#24471;&#24456;&#37325;&#35201;&#65292;&#20294;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#31639;&#27861;&#20197;&#35299;&#20915;&#32467;&#26500;&#21270;&#32593;&#26684;&#27169;&#24335;&#36947;&#36335;&#19978;MVP&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#22312;&#38598;&#20013;&#24335;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#35757;&#32451;&#26679;&#26412;&#65292;&#23548;&#33268;&#34920;&#29616;&#20986;&#20302;&#21327;&#20316;&#24615;&#30340;&#21516;&#36136;&#21270;&#26234;&#33021;&#20307;&#12290;&#38024;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#36861;&#36880;&#22810;&#20010;&#36867;&#36991;&#36710;&#36742;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20250;&#36873;&#25321;&#22266;&#23450;&#30340;&#36867;&#36991;&#30446;&#26631;&#36710;&#36742;&#65292;&#32780;&#19981;&#32771;&#34385;&#21160;&#24577;&#20132;&#36890;&#24773;&#20917;&#65292;&#36825;&#20250;&#26174;&#33879;&#38477;&#20302;&#36861;&#36880;&#25104;&#21151;&#29575;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#22478;&#24066;&#22810;&#20132;&#21449;&#21475;&#21160;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#20808;&#32463;&#39564;&#30340;&#28176;&#36827;&#35748;&#30693;&#24378;&#21270;&#23398;&#20064;PEPCRL-MVP&#25216;&#26415;&#12290;PEPCRL-MVP&#20351;&#29992;&#20248;&#20808;&#32423;&#32593;&#32476;&#26469;&#35780;&#20272;&#20840;&#23616;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-vehicle pursuit (MVP) such as autonomous police vehicles pursuing suspects is important but very challenging due to its mission and safety critical nature. While multi-agent reinforcement learning (MARL) algorithms have been proposed for MVP problem in structured grid-pattern roads, the existing algorithms use randomly training samples in centralized learning, which leads to homogeneous agents showing low collaboration performance. For the more challenging problem of pursuing multiple evading vehicles, these algorithms typically select a fixed target evading vehicle for pursuing vehicles without considering dynamic traffic situation, which significantly reduces pursuing success rate. To address the above problems, this paper proposes a Progression Cognition Reinforcement Learning with Prioritized Experience for MVP (PEPCRL-MVP) in urban multi-intersection dynamic traffic scenes. PEPCRL-MVP uses a prioritization network to assess the transitions in the global experience replay buf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIFS&#30340;&#31471;&#21040;&#31471;&#21464;&#20998;&#25512;&#29702;&#29992;&#20110;Foley&#38899;&#25928;&#21512;&#25104;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#8220;&#31867;&#21035;&#21040;&#22768;&#38899;&#8221;&#30340;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#21270;&#22768;&#38899;&#65292;&#24182;&#24212;&#29992;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#25913;&#33391;&#65292;&#21253;&#25324;PhaseAug&#21644;Avocado&#12290;</title><link>http://arxiv.org/abs/2306.05004</link><description>&lt;p&gt;
VIFS&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#21464;&#20998;&#25512;&#29702;&#29992;&#20110;Foley&#38899;&#25928;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
VIFS: An End-to-End Variational Inference for Foley Sound Synthesis. (arXiv:2306.05004v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VIFS&#30340;&#31471;&#21040;&#31471;&#21464;&#20998;&#25512;&#29702;&#29992;&#20110;Foley&#38899;&#25928;&#21512;&#25104;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#8220;&#31867;&#21035;&#21040;&#22768;&#38899;&#8221;&#30340;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#21270;&#22768;&#38899;&#65292;&#24182;&#24212;&#29992;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#25913;&#33391;&#65292;&#21253;&#25324;PhaseAug&#21644;Avocado&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DCASE 2023&#25361;&#25112;&#20219;&#21153;7&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#8220;&#31867;&#21035;&#21040;&#22768;&#38899;&#8221;&#30340;&#26041;&#27861;&#20026;Foley&#22768;&#38899;&#21512;&#25104;&#65288;FSS&#65289;&#29983;&#25104;&#21508;&#31181;&#22768;&#38899;&#21098;&#36753;&#12290;&#25105;&#20204;&#37319;&#29992;&#20855;&#26377;&#21464;&#20998;&#25512;&#29702;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;VITS&#26469;&#29983;&#25104;&#32473;&#23450;&#31867;&#21035;&#30340;&#22810;&#26679;&#21270;&#22768;&#38899;&#65292;&#24182;&#24212;&#29992;&#20102;&#26469;&#33258;&#35821;&#38899;&#21512;&#25104;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;PhaseAug&#21644;Avocado&#12290;&#19982;TTS&#27169;&#22411;&#19981;&#21516;&#65292;&#31867;&#21035;&#21040;&#22768;&#38899;&#38382;&#39064;&#38656;&#35201;&#20165;&#20174;&#31867;&#21035;&#32034;&#24341;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22768;&#38899;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#24046;&#24322;&#65292;&#21516;&#26102;&#20445;&#25345;&#27599;&#20010;&#38899;&#39057;&#21098;&#36753;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#22823;&#24133;&#20462;&#25913;&#20102;&#20808;&#21069;&#30340;&#32534;&#30721;&#22120;&#20197;&#22686;&#24378;&#19982;&#21518;&#39564;&#28508;&#22312;&#21464;&#37327;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#39640;&#26031;&#20998;&#24067;&#65292;&#20197;&#20419;&#36827;&#31867;&#21035;&#20869;&#30340;&#26041;&#24046;&#12290;&#36890;&#36807;&#36825;&#20123;&#20462;&#25913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIFS&#65292;&#21363;&#21464;&#20998;&#25512;&#29702;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#29992;&#20110;Foley&#38899;&#25928;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of DCASE 2023 Challenge Task 7 is to generate various sound clips for Foley sound synthesis (FSS) by "category-to-sound" approach. "Category" is expressed by a single index while corresponding "sound" covers diverse and different sound examples. To generate diverse sounds for a given category, we adopt VITS, a text-to-speech (TTS) model with variational inference. In addition, we apply various techniques from speech synthesis including PhaseAug and Avocodo. Different from TTS models which generate short pronunciation from phonemes and speaker identity, the category-to-sound problem requires generating diverse sounds just from a category index. To compensate for the difference while maintaining consistency within each audio clip, we heavily modified the prior encoder to enhance consistency with posterior latent variables. This introduced additional Gaussian on the prior encoder which promotes variance within the category. With these modifications, we propose VIFS, variational i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24555;&#36895;&#35780;&#20272;&#20102;&#25968;&#20010;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#65292;&#25351;&#21335;&#21644;/&#25110;&#24037;&#20855;&#30340;&#36127;&#36131;&#20219;AI&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#26497;&#23569;&#25968;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#24037;&#20855;&#65292;&#19988;&#27809;&#26377;&#21487;&#21516;&#26102;&#25903;&#25345;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#26041;&#23454;&#29616;&#29616;&#23454;&#39033;&#30446;&#30340;&#8220;&#31548;&#32479;&#8221;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.05003</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;AI&#26694;&#26550;&#30340;&#24555;&#36895;&#35780;&#20272;&#65306;&#22914;&#20309;&#24341;&#23548;&#36947;&#24503;AI&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
A Rapid Review of Responsible AI frameworks: How to guide the development of ethical AI. (arXiv:2306.05003v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24555;&#36895;&#35780;&#20272;&#20102;&#25968;&#20010;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#65292;&#25351;&#21335;&#21644;/&#25110;&#24037;&#20855;&#30340;&#36127;&#36131;&#20219;AI&#26694;&#26550;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#26497;&#23569;&#25968;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#24037;&#20855;&#65292;&#19988;&#27809;&#26377;&#21487;&#21516;&#26102;&#25903;&#25345;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#26041;&#23454;&#29616;&#29616;&#23454;&#39033;&#30446;&#30340;&#8220;&#31548;&#32479;&#8221;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#23835;&#36215;&#21450;&#20854;&#22312;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#26377;&#20851;&#24212;&#35813;&#22312;&#31038;&#20250;&#20013;&#24341;&#39046;&#20854;&#23454;&#26045;&#21644;&#20351;&#29992;&#30340;&#36947;&#24503;&#21407;&#21017;&#30340;&#32321;&#33635;&#36777;&#35770;&#12290;&#30001;&#36825;&#20123;&#20851;&#20999;&#25152;&#39537;&#21160;&#65292;&#25105;&#20204;&#23545;&#20960;&#20010;&#26694;&#26550;&#36827;&#34892;&#20102;&#24555;&#36895;&#22238;&#39038;&#65292;&#36825;&#20123;&#26694;&#26550;&#25552;&#20379;&#20102;&#20026;&#24320;&#21457;&#21644;&#37096;&#32626;&#36127;&#36131;&#20219;AI&#65288;RAI&#65289;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#65292;&#25351;&#21335;&#21644;/&#25110;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#26694;&#26550;&#19982;&#19981;&#21516;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#65288;SDLC&#65289;&#38454;&#27573;&#36827;&#34892;&#26144;&#23556;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#26694;&#26550;&#20165;&#28085;&#30422;&#38656;&#27714;&#33719;&#21462;&#38454;&#27573;&#65292;&#21097;&#20313;&#38454;&#27573;&#21017;&#26080;&#28041;&#21450;&#12290;&#20854;&#20013;&#24456;&#23569;&#25968;&#30340;&#26694;&#26550;&#20026;&#20174;&#19994;&#32773;&#25552;&#20379;&#25903;&#25345;&#24037;&#20855;&#65292;&#20027;&#35201;&#26159;&#30001;&#31169;&#20154;&#20844;&#21496;&#25552;&#20379;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#24403;&#21069;&#19981;&#23384;&#22312;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#26041;&#23454;&#29616;&#29616;&#23454;&#39033;&#30446;&#30340;&#8220;&#31548;&#32479;&#8221;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#32570;&#20047;&#20840;&#38754;&#26694;&#26550;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last years, the raise of Artificial Intelligence (AI), and its pervasiveness in our lives, has sparked a flourishing debate about the ethical principles that should lead its implementation and use in society. Driven by these concerns, we conduct a rapid review of several frameworks providing principles, guidelines, and/or tools to help practitioners in the development and deployment of Responsible AI (RAI) applications. We map each framework w.r.t. the different Software Development Life Cycle (SDLC) phases discovering that most of these frameworks fall just in the Requirements Elicitation phase, leaving the other phases uncovered. Very few of these frameworks offer supporting tools for practitioners, and they are mainly provided by private companies. Our results reveal that there is not a "catching-all" framework supporting both technical and non-technical stakeholders in the implementation of real-world projects. Our findings highlight the lack of a comprehensive framework enc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#28082;&#24577;&#26102;&#38388;&#24120;&#25968;&#65288;LTC&#65289;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#25509;&#25910;&#20449;&#21495;&#21151;&#29575;&#20316;&#20026;&#31995;&#32479;&#36755;&#20837;&#26469;&#39044;&#27979;&#27627;&#31859;&#27874;&#38142;&#36335;&#20013;&#38556;&#30861;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LTC&#32593;&#32476;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#21487;&#38752;&#21644;&#20302;&#24310;&#36831;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2306.04997</link><description>&lt;p&gt;
&#21033;&#29992;&#28082;&#24577;&#26102;&#38388;&#24120;&#25968;&#32593;&#32476;&#22312;&#23450;&#21521;&#27627;&#31859;&#27874;&#38142;&#36335;&#20013;&#39044;&#27979;&#38556;&#30861;&#29289;
&lt;/p&gt;
&lt;p&gt;
Blockage Prediction in Directional mmWave Links Using Liquid Time Constant Network. (arXiv:2306.04997v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#28082;&#24577;&#26102;&#38388;&#24120;&#25968;&#65288;LTC&#65289;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#25509;&#25910;&#20449;&#21495;&#21151;&#29575;&#20316;&#20026;&#31995;&#32479;&#36755;&#20837;&#26469;&#39044;&#27979;&#27627;&#31859;&#27874;&#38142;&#36335;&#20013;&#38556;&#30861;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LTC&#32593;&#32476;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#21487;&#38752;&#21644;&#20302;&#24310;&#36831;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28082;&#24577;&#26102;&#38388;&#24120;&#25968;&#65288;LTC&#65289;&#32593;&#32476;&#65292;&#20165;&#20351;&#29992;&#25509;&#25910;&#20449;&#21495;&#21151;&#29575;&#20316;&#20026;&#31995;&#32479;&#36755;&#20837;&#65292;&#26469;&#39044;&#27979;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#38142;&#36335;&#30340;&#26410;&#26469;&#38556;&#30861;&#29366;&#24577;&#12290;LTC&#32593;&#32476;&#22522;&#20110;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#31995;&#32479;&#65292;&#24182;&#19987;&#38376;&#29992;&#20110;&#36817;&#26399;&#39044;&#27979;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#35266;&#23519;&#32467;&#26524;&#12290;&#21033;&#29992;60 GHz&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LTC&#30340;&#21487;&#38752;&#39044;&#27979;&#38556;&#30861;&#21457;&#29983;&#30340;&#24773;&#20917;&#21644;&#38556;&#30861;&#30340;&#38271;&#24230;&#65292;&#26080;&#38656;&#29305;&#23450;&#24773;&#22659;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;LTC&#21487;&#20197;&#22312;&#19981;&#20107;&#20808;&#20102;&#35299;&#23460;&#22806;&#24773;&#20917;&#25110;&#37325;&#26032;&#35757;&#32451;/&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#39640;&#36798;97.85&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;LTC&#32593;&#32476;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#20449;&#21495;&#30340;&#28508;&#22312;&#20248;&#21183;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#21487;&#38752;&#21644;&#20302;&#24310;&#36831;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to use a liquid time constant (LTC) network to predict the future blockage status of a millimeter wave (mmWave) link using only the received signal power as the input to the system. The LTC network is based on an ordinary differential equation (ODE) system inspired by biology and specialized for near-future prediction for time sequence observation as the input. Using an experimental dataset at 60 GHz, we show that our proposed use of LTC can reliably predict the occurrence of blockage and the length of the blockage without the need for scenario-specific data. The results show that the proposed LTC can predict with upwards of 97.85\% accuracy without prior knowledge of the outdoor scenario or retraining/tuning. These results highlight the promising gains of using LTC networks to predict time series-dependent signals, which can lead to more reliable and low-latency communication.
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.04974</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Prediction via Data-Driven Confidence Minimization. (arXiv:2306.04974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22788;&#29702;&#19981;&#24120;&#35265;&#26679;&#26412;&#26102;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#30340;&#20445;&#23432;&#27169;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36741;&#21161;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24863;&#20852;&#36259;&#30340;OOD&#65288;Out-of-Distribution&#65289;&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#23454;&#29616;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#65288;In-Distribution&#65289;&#21644;OOD&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#20195;&#20215;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#22312;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#36825;&#31181;&#38169;&#35823;&#21487;&#33021;&#20250;&#38459;&#27490;&#26426;&#22120;&#23398;&#20064;&#30340;&#37096;&#32626;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#20445;&#23432;&#24615;&#30340;&#27169;&#22411;&#8212;&#8212;&#24403;&#23427;&#20204;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#26102;&#21487;&#20197;&#25512;&#36831;&#21040;&#20154;&#31867;&#21028;&#26029;&#8212;&#8212;&#21487;&#33021;&#20250;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#24322;&#24120;&#25110;&#22797;&#26434;&#31034;&#20363;&#26126;&#26174;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26080;&#27861;&#39044;&#27979;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#36741;&#21161;&#20266;OOD&#25968;&#25454;&#38598;&#19978;&#26368;&#23567;&#21270;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#26159;&#20851;&#38190;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#36741;&#21161;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#24863;&#20852;&#36259;&#30340;OOD&#21306;&#22495;&#30340;&#26679;&#26412;&#65292;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#38752;&#22320;&#20998;&#31163;ID&#21644;OOD&#36755;&#20837;&#12290;&#21463;&#21040;&#36825;&#19968;&#32467;&#26524;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#32622;&#20449;&#24230;&#26368;&#23567;&#21270;&#65288;DCM&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Errors of machine learning models are costly, especially in safety-critical domains such as healthcare, where such mistakes can prevent the deployment of machine learning altogether. In these settings, conservative models -- models which can defer to human judgment when they are likely to make an error -- may offer a solution. However, detecting unusual or difficult examples is notably challenging, as it is impossible to anticipate all potential inputs at test time. To address this issue, prior work has proposed to minimize the model's confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the effect of confidence minimization and show that the choice of auxiliary dataset is critical. Specifically, if the auxiliary dataset includes samples from the OOD region of interest, confidence minimization provably separates ID and OOD inputs by predictive confidence. Taking inspiration from this result, we present data-driven confidence minimization (DCM), which minimizes confid
&lt;/p&gt;</description></item><item><title>arXiv4TGC&#25552;&#20379;&#20102;&#19968;&#32452;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#21487;&#38752;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.04962</link><description>&lt;p&gt;
arXiv4TGC&#65306;&#29992;&#20110;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv4TGC: Large-Scale Datasets for Temporal Graph Clustering. (arXiv:2306.04962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04962
&lt;/p&gt;
&lt;p&gt;
arXiv4TGC&#25552;&#20379;&#20102;&#19968;&#32452;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#21487;&#38752;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#22270;&#32858;&#31867;&#26159;&#26102;&#24577;&#22270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20854;&#37325;&#28857;&#26159;&#22312;&#26102;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#32858;&#31867;&#65292;&#24182;&#30001;&#20110;&#26102;&#24577;&#22270;&#26041;&#27861;&#30340;&#26426;&#21046;&#65292;&#20026;&#22823;&#35268;&#27169;&#22270;&#32467;&#26500;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#21457;&#23637;&#30446;&#21069;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#38382;&#39064;&#65306;&#32570;&#20047;&#36866;&#21512;&#21644;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#32858;&#31867;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;arXiv4TGC&#65292;&#36825;&#26159;&#19968;&#32452;&#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#24577;&#22270;&#32858;&#31867;&#30340;&#26032;&#39062;&#23398;&#26415;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#65292;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;arXivLarge&#21253;&#21547;130&#19975;&#20010;&#26377;&#26631;&#31614;&#21487;&#29992;&#33410;&#28857;&#21644;1000&#19975;&#20010;&#26102;&#24577;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph clustering (TGC) is a crucial task in temporal graph learning. Its focus is on node clustering on temporal graphs, and it offers greater flexibility for large-scale graph structures due to the mechanism of temporal graph methods. However, the development of TGC is currently constrained by a significant problem: the lack of suitable and reliable large-scale temporal graph datasets to evaluate clustering performance. In other words, most existing temporal graph datasets are in small sizes, and even large-scale datasets contain only a limited number of available node labels. It makes evaluating models for large-scale temporal graph clustering challenging. To address this challenge, we build arXiv4TGC, a set of novel academic datasets (including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for large-scale temporal graph clustering. In particular, the largest dataset, arXivLarge, contains 1.3 million labeled available nodes and 10 million temporal edges. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04959</link><description>&lt;p&gt;
FedMLSecurity&#65306;&#32852;&#37030;&#23398;&#20064;&#19982;LLMs&#20013;&#25915;&#20987;&#19982;&#38450;&#24481;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedMLSecurity&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#24212;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#24320;&#28304;&#24211;FedML&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;FedMLSecurity&#22686;&#24378;&#20102;FedML&#30340;&#23433;&#20840;&#35780;&#20272;&#33021;&#21147;&#12290;FedMLSecurity&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;FedMLAttacker&#27169;&#25311;&#22312;FL&#35757;&#32451;&#20013;&#27880;&#20837;&#30340;&#25915;&#20987;&#65292;&#32780;FedMLDefender&#21017;&#27169;&#25311;&#26088;&#22312;&#20943;&#36731;&#25915;&#20987;&#24433;&#21709;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;FedMLSecurity&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;ResNet&#65292;GAN&#31561;&#65289;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;FedAVG&#65292;FedOPT&#65292;FedNOVA&#31561;&#65289;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#35780;&#20272;&#36824;&#23637;&#31034;&#20102;&#23558;FedMLSecurity&#36731;&#26494;&#24212;&#29992;&#20110;LLMs&#30340;&#20415;&#21033;&#24615;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;&#20102;&#20854;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04955</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#32593;&#32476;&#23545;&#38477;&#35299;&#22810;&#36793;&#24418;&#30340;&#24863;&#30693;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65306;&#20174;&#23545;&#25239;&#25915;&#20987;&#21040;&#22270;&#20687;&#25439;&#22351;&#65292;&#28145;&#24230;&#23398;&#20064;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#28982;&#32780;&#20154;&#31867;&#21364;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24674;&#22797;&#21463;&#25439;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#20154;&#31867;&#35270;&#35273;&#30340;&#8220;&#35782;&#21035;&#32452;&#20214;&#8221;&#29702;&#35770;&#20013;&#39318;&#27425;&#24341;&#20837;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#21160;&#21270;&#24418;&#29366;&#21487;&#24674;&#22797;&#24615;&#27979;&#35797;&#65292;&#24555;&#36895;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#21382;&#21490;&#19978;&#25163;&#21160;&#21019;&#24314;&#22270;&#20687;&#21487;&#24674;&#22797;&#24615;&#23454;&#39564;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29616;&#20195;&#21270;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22810;&#36793;&#24418;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
&lt;/p&gt;</description></item><item><title>ShuttleSet&#26159;&#19968;&#20221;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;104&#22330;&#27604;&#36187;&#12289;3,685&#36718;&#27604;&#36187;&#12289;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;&#36825;&#20123;&#25293;&#32423;&#35760;&#24405;&#23558;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#20307;&#32946;&#20998;&#26512;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.04948</link><description>&lt;p&gt;
ShuttleSet: &#19968;&#20221;&#20154;&#24037;&#26631;&#27880;&#30340;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25112;&#26415;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis. (arXiv:2306.04948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04948
&lt;/p&gt;
&lt;p&gt;
ShuttleSet&#26159;&#19968;&#20221;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;104&#22330;&#27604;&#36187;&#12289;3,685&#36718;&#27604;&#36187;&#12289;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;&#36825;&#20123;&#25293;&#32423;&#35760;&#24405;&#23558;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#22312;&#20307;&#32946;&#20998;&#26512;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20307;&#32946;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#25366;&#25496;&#29699;&#21592;&#25112;&#26415;&#27934;&#23519;&#21147;&#20197;&#25552;&#39640;&#34920;&#29616;&#36136;&#37327;&#21644;&#29699;&#36855;&#21442;&#19982;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#24402;&#22240;&#20110;&#20844;&#20849;&#22522;&#30784;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#34429;&#28982;&#26377;&#19968;&#20123;&#29992;&#20110;&#34892;&#21160;&#26816;&#27979;&#30340;&#22238;&#21512;&#27604;&#36187;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#26469;&#28304;&#25968;&#25454;&#21644;&#25293;&#32423;&#35760;&#24405;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#35201;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#25104;&#26412;&#26631;&#35760;&#24037;&#20316;&#65292;&#24182;&#19988;&#24456;&#38590;&#20351;&#29992;&#33258;&#21160;&#25216;&#26415;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#24403;&#29616;&#26377;&#27169;&#22411;&#24212;&#29992;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32467;&#26500;&#21270;&#22238;&#21512;&#24207;&#21015;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#24320;&#21457;&#21463;&#21040;&#37325;&#22823;&#21046;&#32422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; ShuttleSet&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#32701;&#27611;&#29699;&#21333;&#25171;&#27604;&#36187;&#30340;&#25293;&#32423;&#35760;&#24405;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;2018&#24180;&#33267;2021&#24180;&#38388;&#30340;44&#22330;&#27604;&#36187;&#20013;&#30340;104&#30424;&#27604;&#36187;&#65292;3,685&#36718;&#27604;&#36187;&#21644;36,492&#20010;&#25293;&#20987;&#65292;&#24182;&#28085;&#30422;&#20102;27&#21517;&#25490;&#21517;&#21069;&#21015;&#30340;&#30007;&#23376;&#21644;&#22899;&#23376;&#21333;&#25171;&#36873;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and wome
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#25552;&#21462;&#38382;&#39064;&#30456;&#20851;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#35757;&#32451;&#30340;&#27169;&#22411;&#38169;&#35823;&#22320;&#22238;&#31572;&#19982;&#22270;&#20687;&#26080;&#20851;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04938</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#65292;&#36890;&#36807;&#30456;&#20851;&#38382;&#39064;&#21644;&#22270;&#20687;&#23646;&#24615;&#36827;&#34892;&#30693;&#35782;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge Detection by Relevant Question and Image Attributes in Visual Question Answering. (arXiv:2306.04938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#25552;&#21462;&#38382;&#39064;&#30456;&#20851;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#35757;&#32451;&#30340;&#27169;&#22411;&#38169;&#35823;&#22320;&#22238;&#31572;&#19982;&#22270;&#20687;&#26080;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#23454;&#36341;&#36861;&#27714;&#35299;&#20915;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#22270;&#20687;&#30340;&#20869;&#23481;&#33258;&#21160;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#26576;&#20123;&#27979;&#35797;&#38382;&#39064;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#25165;&#33021;&#24471;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#32034;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#29983;&#25104;&#22522;&#20110;&#30693;&#35782;&#30340;&#31572;&#26696;&#65292;&#20351;&#29992;&#38382;&#39064;&#30456;&#20851;&#25110;&#22270;&#20687;&#30456;&#20851;&#30340;&#30693;&#35782;&#26816;&#32034;&#26041;&#27861;&#12290;&#22914;&#26524;&#33719;&#21462;&#26377;&#20851;&#22270;&#20687;&#20013;&#25152;&#26377;&#23545;&#35937;&#30340;&#30693;&#35782;&#65292;&#21017;&#19981;&#26159;&#25152;&#26377;&#30693;&#35782;&#37117;&#19982;&#38382;&#39064;&#30456;&#20851;&#12290;&#22312;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#31572;&#26696;&#21644;&#36807;&#24230;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22238;&#31572;&#19982;&#22270;&#20687;&#26080;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#22270;&#20687;&#23646;&#24615;&#21644;&#38382;&#39064;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#36755;&#20837;&#21040;&#30693;&#35782;&#25512;&#23548;&#27169;&#22359;&#20013;&#65292;&#20165;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a Multidisciplinary research problem that pursued through practices of natural language processing and computer vision. Visual question answering automatically answers natural language questions according to the content of an image. Some testing questions require external knowledge to derive a solution. Such knowledge-based VQA uses various methods to retrieve features of image and text, and combine them to generate the answer. To generate knowledgebased answers either question dependent or image dependent knowledge retrieval methods are used. If knowledge about all the objects in the image is derived, then not all knowledge is relevant to the question. On other side only question related knowledge may lead to incorrect answers and over trained model that answers question that is irrelevant to image. Our proposed method takes image attributes and question features as input for knowledge derivation module and retrieves only question relevant knowledge 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;</title><link>http://arxiv.org/abs/2306.04926</link><description>&lt;p&gt;
covLLM&#65306;&#29992;&#20110;COVID-19&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
covLLM: Large Language Models for COVID-19 Biomedical Literature. (arXiv:2306.04926v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04926
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;covLLM&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;COVID-19&#25991;&#29486;&#12290;covLLM&#21487;&#20197;&#27719;&#24635;&#21644;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#22909;&#22320;&#24212;&#23545;COVID-19&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26032;&#20896;&#30149;&#27602;&#30340;&#30740;&#31350;&#22312;&#19981;&#26029;&#22686;&#21152;&#65292;&#20294;COVID-19&#22823;&#27969;&#34892;&#23548;&#33268;&#20102;&#32654;&#22269;110&#19975;&#20154;&#30340;&#27515;&#20129;&#12290;&#36825;&#20123;&#26032;&#21457;&#29616;&#22312;&#36716;&#21270;&#20026;&#20020;&#24202;&#24178;&#39044;&#26041;&#26696;&#26041;&#38754;&#32531;&#24930;&#65292;&#23548;&#33268;&#24739;&#32773;&#39044;&#21518;&#36739;&#24046;&#21644;&#19981;&#24517;&#35201;&#30340;&#27515;&#20129;&#12290;&#20854;&#20013;&#19968;&#31181;&#21407;&#22240;&#26159;&#20020;&#24202;&#21307;&#29983;&#22240;&#24739;&#32773;&#36807;&#22810;&#32780;&#38590;&#20197;&#36319;&#19978;&#26032;&#20896;&#30149;&#27602;&#25991;&#29486;&#30340;&#36895;&#24230;&#12290;&#21457;&#23637;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#24037;&#20855;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#33021;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;LLMs&#21487;&#29992;&#20110;&#27719;&#24635;&#21644;&#25552;&#21462;&#29992;&#25143;&#25351;&#23450;&#30340;&#20449;&#24687;&#12290;&#36739;&#22823;&#33539;&#22260;&#21644;&#20808;&#36827;&#30340;LLMs&#21644;&#39044;&#22788;&#29702;&#30340;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#36890;&#36807;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#29305;&#23450;LLM&#65288;covLLM&#65289;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#35780;&#20272;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#30340;&#26426;&#20250;&#65292;&#35813;&#24037;&#20855;&#30452;&#25509;&#36755;&#20837;&#30740;&#31350;&#25991;&#31456;&#21644;&#29992;&#25143;&#26597;&#35810;&#20197;&#36820;&#22238;&#31572;&#26696;&#12290;&#22312;&#20351;&#29992;COVID-19&#24320;&#25918;&#30740;&#31350;&#25968;&#25454;&#38598;&#65288;CORD-19&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;covLLM&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#24635;&#32467;&#21644;&#20174;&#20896;&#29366;&#30149;&#27602;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic led to 1.1 million deaths in the United States, despite the explosion of coronavirus research. These new findings are slow to translate to clinical interventions, leading to poorer patient outcomes and unnecessary deaths. One reason is that clinicians, overwhelmed by patients, struggle to keep pace with the rate of new coronavirus literature. A potential solution is developing a tool for evaluating coronavirus literature using large language models (LLMs) -- neural networks that are deployed for natural language processing. LLMs can be used to summarize and extract user-specified information. The greater availability and advancement of LLMs and pre-processed coronavirus literature databases provide the opportunity to assist clinicians in evaluating coronavirus literature through a coronavirus literature specific LLM (covLLM), a tool that directly takes an inputted research article and a user query to return an answer. Using the COVID-19 Open Research Dataset (CORD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#20013;&#20219;&#24847;&#39118;&#26684;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.04911</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65306;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20013;&#20219;&#24847;&#39118;&#26684;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Test-Time Style Shifting: Handling Arbitrary Styles in Domain Generalization. (arXiv:2306.04911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#20013;&#20219;&#24847;&#39118;&#26684;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#35201;&#27714;&#20026;&#26410;&#30693;&#30446;&#26631;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#22312;&#25512;&#29702;&#26102;&#38656;&#35201;&#25104;&#21151;&#24212;&#29992;&#20110;&#20219;&#24847;&#65288;&#29978;&#33267;&#26159;&#26410;&#35265;&#65289;&#30340;&#30446;&#26631;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#65288;test-time style shifting&#65289;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#23558;&#27979;&#35797;&#26679;&#26412;&#30340;&#39118;&#26684;&#65288;&#19982;&#28304;&#22495;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#30340;&#65289;&#36716;&#25442;&#20026;&#26368;&#25509;&#36817;&#27169;&#22411;&#24050;&#30693;&#30340;&#28304;&#22495;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#32479;&#35745;&#30340;&#20219;&#20309;&#30446;&#26631;&#22495;&#65292;&#26080;&#38656;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#39069;&#22806;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39118;&#26684;&#24179;&#34913;&#65288;style balancing&#65289;&#65292;&#23427;&#20026;&#26368;&#22823;&#21270;&#27979;&#35797;&#26102;&#38388;&#39118;&#26684;&#36716;&#25442;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#24179;&#21488;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;DG&#29305;&#23450;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#24605;&#36335;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#22320;&#23436;&#25104;&#20102;&#39046;&#22495;&#27867;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In domain generalization (DG), the target domain is unknown when the model is being trained, and the trained model should successfully work on an arbitrary (and possibly unseen) target domain during inference. This is a difficult problem, and despite active studies in recent years, it remains a great challenge. In this paper, we take a simple yet effective approach to tackle this issue. We propose test-time style shifting, which shifts the style of the test sample (that has a large style gap with the source domains) to the nearest source domain that the model is already familiar with, before making the prediction. This strategy enables the model to handle any target domains with arbitrary style statistics, without additional model update at test-time. Additionally, we propose style balancing, which provides a great platform for maximizing the advantage of test-time style shifting by handling the DG-specific imbalance issues. The proposed ideas are easy to implement and successfully wor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32423;&#32852;&#34507;&#30333;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#30340;&#24207;&#21015;&#21644;&#20960;&#20309;&#20998;&#26512;&#22120;&#65292;&#24341;&#23548;&#21464;&#24322;&#26041;&#21521;&#65292;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#24322;&#20307;&#30340;&#25928;&#24212;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04899</link><description>&lt;p&gt;
&#22810;&#32423;&#34507;&#30333;&#36136;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#30450;&#21464;&#24322;&#25928;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-level Protein Representation Learning for Blind Mutational Effect Prediction. (arXiv:2306.04899v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32423;&#32852;&#34507;&#30333;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#30340;&#24207;&#21015;&#21644;&#20960;&#20309;&#20998;&#26512;&#22120;&#65292;&#24341;&#23548;&#21464;&#24322;&#26041;&#21521;&#65292;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21464;&#24322;&#20307;&#30340;&#25928;&#24212;&#65292;&#24182;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21521;&#36827;&#21270;&#22312;&#34507;&#30333;&#24037;&#31243;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#20462;&#35746;&#29616;&#26377;&#30340;&#34507;&#30333;&#24207;&#21015;&#20197;&#33719;&#24471;&#26032;&#30340;&#25110;&#22686;&#24378;&#30340;&#21151;&#33021;&#12290;&#31934;&#30830;&#39044;&#27979;&#34507;&#30333;&#21464;&#24322;&#20307;&#30340;&#25928;&#24212;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#21151;&#33021;&#12290;&#34429;&#28982;&#22823;&#22411;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#20165;&#20351;&#29992;&#34507;&#30333;&#24207;&#21015;&#23601;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#22320;&#19981;&#35299;&#37322;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#34507;&#30333;&#36136;&#25240;&#21472;&#31283;&#23450;&#24615;&#21644;&#20998;&#23376;&#20869;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#23558;&#34507;&#30333;&#19968;&#32423;&#21644;&#19977;&#32423;&#32467;&#26500;&#30340;&#24207;&#21015;&#21644;&#20960;&#20309;&#20998;&#26512;&#22120;&#32423;&#32852;&#36215;&#26469;&#12290;&#36890;&#36807;&#27169;&#25311;&#37326;&#29983;&#22411;&#34507;&#30333;&#36136;&#19978;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#23558;&#21464;&#24322;&#26041;&#21521;&#24341;&#23548;&#21040;&#26399;&#26395;&#30340;&#29305;&#24615;&#19978;&#65292;&#24182;&#22522;&#20110;&#20854;&#25191;&#34892;&#21151;&#33021;&#30340;&#36866;&#24212;&#24615;&#35780;&#20272;&#21464;&#24322;&#20307;&#30340;&#25928;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;20&#31181;&#34507;&#30333;&#36136;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#30340;328,594&#20010;&#21464;&#24322;&#20307;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#23427;&#22312;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed evolution plays an indispensable role in protein engineering that revises existing protein sequences to attain new or enhanced functions. Accurately predicting the effects of protein variants necessitates an in-depth understanding of protein structure and function. Although large self-supervised language models have demonstrated remarkable performance in zero-shot inference using only protein sequences, these models inherently do not interpret the spatial characteristics of protein structures, which are crucial for comprehending protein folding stability and internal molecular interactions. This paper introduces a novel pre-training framework that cascades sequential and geometric analyzers for protein primary and tertiary structures. It guides mutational directions toward desired traits by simulating natural selection on wild-type proteins and evaluates the effects of variants based on their fitness to perform the function. We assess the proposed approach using a public datab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#29992;&#25143;&#21453;&#39304;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#27169;&#22411;&#26469;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#30340;&#20010;&#24615;&#21270;&#26381;&#21153;&#65292;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04887</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#20010;&#24615;&#21270;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Big-data-driven and AI-based framework to enable personalization in wireless networks. (arXiv:2306.04887v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#26102;&#29992;&#25143;&#21453;&#39304;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#27169;&#22411;&#26469;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#30340;&#20010;&#24615;&#21270;&#26381;&#21153;&#65292;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#36890;&#20449;&#32593;&#32476;&#20351;&#29992;&#30340;&#35774;&#35745;&#26041;&#27861;&#23548;&#33268;&#32593;&#32476;&#25928;&#29575;&#26080;&#27861;&#26368;&#22823;&#21270;&#12290;&#29616;&#26377;&#32593;&#32476;&#26222;&#36941;&#34987;&#35774;&#35745;&#25104;&#8220;&#36890;&#29992;&#22871;&#35013;&#8221;&#65292;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#23545;&#32593;&#32476;&#26381;&#21153;&#30340;&#19981;&#21516;&#38656;&#27714;&#12290;&#21516;&#26102;&#65292;&#24403;&#21069;&#30340;&#32593;&#32476;&#32570;&#20047;&#29992;&#25143;&#32423;&#21035;&#30340;&#25968;&#25454;&#35748;&#30693;&#26234;&#33021;&#65292;&#26080;&#27861;&#36890;&#36807;&#33258;&#21160;&#21270;&#23454;&#29616;&#24555;&#36895;&#30340;&#20010;&#24615;&#21270;&#32593;&#32476;&#20915;&#31574;&#21644;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#22823;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#26102;&#38750;&#20405;&#20837;&#24335;&#29992;&#25143;&#21453;&#39304;&#26469;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#30340;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#23454;&#38469;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#21644;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20248;&#21270;&#25552;&#20379;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#29992;&#25143;&#21453;&#39304;&#30340;&#36319;&#36394;&#21644;&#34913;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#28385;&#24847;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current communication networks use design methodologies that prevent the realization of maximum network efficiency. In the first place, while users' perception of satisfactory service diverges widely, current networks are designed to be a "universal fit," where they are generally over-engineered to deliver services appealing to all types of users. Also, current networks lack user-level data cognitive intelligence that would enable fast personalized network decisions and actions through automation. Thus, in this article, we propose the utilization of AI, big data analytics, and real-time non-intrusive user feedback in order to enable the personalization of wireless networks. Based on each user's actual QoS requirements and context, a multi-objective formulation enables the network to micro-manage and optimize the provided QoS and user satisfaction levels simultaneously. Moreover, in order to enable user feedback tracking and measurement, we propose a user satisfaction model based on the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04874</link><description>&lt;p&gt;
&#25193;&#22823;&#33539;&#22260;&#65306;&#23558;&#33521;&#25991;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expanding Scope: Adapting English Adversarial Attacks to Chinese. (arXiv:2306.04874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#33521;&#25991;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#25991;&#19978;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#23545;&#25239;&#23454;&#20363;&#12290;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#35821;&#35328;&#29305;&#28857;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#35774;&#35745;&#25915;&#20987;&#26041;&#24335;&#26469;&#35780;&#20272;&#33521;&#35821;&#35821;&#22659;&#19979;&#30340;NLP&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#23398;&#26415;&#30028;&#23545;&#20854;&#23427;&#35821;&#35328;&#30340;NLP&#35299;&#20915;&#26041;&#26696;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33258;&#28982;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#23427;&#35821;&#35328;&#20013;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#22312;&#33521;&#25991;&#29615;&#22659;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#31639;&#27861;&#36866;&#24212;&#21040;&#20013;&#25991;&#19978;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32467;&#21512;&#27491;&#30830;&#30340;&#25991;&#26412;&#20998;&#21106;&#21644;&#35821;&#35328;&#38480;&#21046;&#26102;&#65292;&#20808;&#21069;&#38024;&#23545;&#33521;&#25991;NLP&#30340;&#25915;&#20987;&#26041;&#27861;&#20063;&#33021;&#22815;&#22312;&#20013;&#25991;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#20851;&#27880;&#20013;&#25991;&#30340;&#24418;&#24577;&#21644;&#38899;&#31995;&#65292;&#29983;&#25104;&#30340;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#23454;&#29616;&#39640;&#27969;&#30021;&#24230;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#20013;&#25991;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed that NLP predictive models are vulnerable to adversarial attacks. Most existing studies focused on designing attacks to evaluate the robustness of NLP models in the English language alone. Literature has seen an increasing need for NLP solutions for other languages. We, therefore, ask one natural question: whether state-of-the-art (SOTA) attack methods generalize to other languages. This paper investigates how to adapt SOTA adversarial attack algorithms in English to the Chinese language. Our experiments show that attack methods previously applied to English NLP can generate high-quality adversarial examples in Chinese when combined with proper text segmentation and linguistic constraints. In addition, we demonstrate that the generated adversarial examples can achieve high fluency and semantic consistency by focusing on the Chinese language's morphology and phonology, which in turn can be used to improve the adversarial robustness of Chinese NLP models.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;</title><link>http://arxiv.org/abs/2306.04862</link><description>&lt;p&gt;
&#8220;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#8221;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review on Client Selection in Federated Learning. (arXiv:2306.04862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04862
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#30340;&#25285;&#24551;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;2017&#24180;&#34987;&#21457;&#26126;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#26356;&#26032;&#21457;&#36865;&#21040;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#12290;&#38543;&#26426;&#36873;&#25321;&#23458;&#25143;&#31471;&#36827;&#34892;FL&#21487;&#33021;&#20250;&#23545;&#23398;&#20064;&#24615;&#33021;&#36896;&#25104;&#20260;&#23475;&#65292;&#22240;&#20026;&#21407;&#22240;&#21508;&#24322;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#20915;FL&#23458;&#25143;&#31471;&#36873;&#25321;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20027;&#39064;&#30340;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#30340;SLR&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36873;&#25321;&#30340;&#29616;&#29366;&#65292;&#24182;&#22238;&#31572;&#20102;&#25361;&#25112;&#12289;&#35299;&#20915;&#26041;&#26696;&#21644;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#26102;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;47&#31687;&#20027;&#35201;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#38754;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#24322;&#36136;&#24615;&#12289;&#36164;&#28304;&#20998;&#37197;&#12289;&#36890;&#20449;&#25104;&#26412;&#21644;&#20844;&#24179;&#24615;&#12290;&#23458;&#25143;&#36873;&#25321;&#26041;&#26696;&#26088;&#22312;&#36890;&#36807;&#19987;&#27880;&#20110;&#19978;&#36848;&#19968;&#39033;&#25110;&#20960;&#39033;&#25361;&#25112;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;&#38543;&#26426;&#36873;&#25321;&#31639;&#27861;&#12290;&#26368;&#24120;&#29992;&#30340;&#25351;&#26631;&#26159;&#27979;&#35797;&#20934;&#30830;&#24615;&#19982;&#36890;&#20449;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the arising concerns of privacy within machine learning, federated learning (FL) was invented in 2017, in which the clients, such as mobile devices, train a model and send the update to the centralized server. Choosing clients randomly for FL can harm learning performance due to different reasons. Many studies have proposed approaches to address the challenges of client selection of FL. However, no systematic literature review (SLR) on this topic existed. This SLR investigates the state of the art of client selection in FL and answers the challenges, solutions, and metrics to evaluate the solutions. We systematically reviewed 47 primary studies. The main challenges found in client selection are heterogeneity, resource allocation, communication costs, and fairness. The client selection schemes aim to improve the original random selection algorithm by focusing on one or several of the aforementioned challenges. The most common metric used is testing accuracy versus communication rou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#32553;&#30701;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#39640;&#36798;59.4&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04846</link><description>&lt;p&gt;
&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Learned spatial data partitioning. (arXiv:2306.04846v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#32553;&#30701;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#39640;&#36798;59.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31354;&#38388;&#25968;&#25454;&#30340;&#22823;&#23567;&#26174;&#33879;&#22686;&#21152;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#24182;&#34892;&#22788;&#29702;&#31995;&#32479;&#26377;&#25928;&#22320;&#20998;&#26512;&#31354;&#38388;&#25968;&#25454;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#20301;&#32622;&#30340;&#20998;&#32452;&#23558;&#22823;&#35268;&#27169;&#31354;&#38388;&#25968;&#25454;&#20998;&#37197;&#21040;&#35745;&#31639;&#26426;&#19978;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#24418;&#24335;&#21270;&#20102;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#29305;&#24449;&#65292;&#24182;&#21098;&#26525;&#26080;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#20998;&#21306;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20351;&#29992;Apache Sedona&#21644;&#30495;&#23454;&#30340;&#31354;&#38388;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20998;&#21306;&#65292;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#24182;&#23558;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;59.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the significant increase in the size of spatial data, it is essential to use distributed parallel processing systems to efficiently analyze spatial data. In this paper, we first study learned spatial data partitioning, which effectively assigns groups of big spatial data to computers based on locations of data by using machine learning techniques. We formalize spatial data partitioning in the context of reinforcement learning and develop a novel deep reinforcement learning algorithm. Our learning algorithm leverages features of spatial data partitioning and prunes ineffective learning processes to find optimal partitions efficiently. Our experimental study, which uses Apache Sedona and real-world spatial data, demonstrates that our method efficiently finds partitions for accelerating distance join queries and reduces the workload run time by up to 59.4%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#31639;&#27861;INDUCE&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04835</link><description>&lt;p&gt;
&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#36890;&#36807;&#24402;&#32435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity. (arXiv:2306.04835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#31639;&#27861;INDUCE&#26469;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26377;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#20363;&#22914;&#33647;&#29289;&#21457;&#29616;&#12289;&#25512;&#33616;&#24341;&#25806;&#21644;&#33455;&#29255;&#35774;&#35745;&#12290;&#20294;&#26159;&#65292;GNN&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#26469;&#25903;&#25345;&#20854;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;GNN&#30340;&#36755;&#20837;&#22270;&#36827;&#34892;&#26368;&#23567;&#26356;&#25913;&#65292;&#20197;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#26469;&#35299;&#37322;GNN&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#36793;&#21024;&#38500;&#20316;&#20026;&#25200;&#21160;&#12290;&#20854;&#27425;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#27169;&#22411;&#26159;&#20256;&#23548;&#24615;&#30340;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19981;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;INDUCE&#30340;&#24402;&#32435;&#31639;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21253;&#25324;&#36793;&#28155;&#21152;&#22312;&#20869;&#30340;&#25913;&#36827;&#21487;&#33719;&#24471;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#24402;&#32435;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#20063;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#24418;&#12289;&#36716;&#25442;&#21644;&#22522;&#20110;&#26415;&#35821;&#30340;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#26816;&#32034;&#65292;&#20197;&#35299;&#20915;Etsy&#25628;&#32034;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#24212;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#24037;&#19994;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25913;&#21892;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2306.04833</link><description>&lt;p&gt;
Etsy&#25628;&#32034;&#20013;&#32479;&#19968;&#23884;&#20837;&#24335;&#20010;&#24615;&#21270;&#26816;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unified Embedding Based Personalized Retrieval in Etsy Search. (arXiv:2306.04833v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22270;&#24418;&#12289;&#36716;&#25442;&#21644;&#22522;&#20110;&#26415;&#35821;&#30340;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#30340;&#32479;&#19968;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#26816;&#32034;&#65292;&#20197;&#35299;&#20915;Etsy&#25628;&#32034;&#20013;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#24212;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26032;&#31574;&#30053;&#65292;&#20197;&#26500;&#24314;&#20855;&#26377;&#24037;&#19994;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25913;&#21892;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#26816;&#32034;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#23614;&#26597;&#35810;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#30340;&#26222;&#36941;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#28909;&#38376;&#26597;&#35810;&#36890;&#24120;&#32570;&#20047;&#19978;&#19979;&#25991;&#65292;&#26377;&#24191;&#27867;&#30340;&#24847;&#22270;&#65292;&#29992;&#25143;&#21382;&#21490;&#20114;&#21160;&#30340;&#38468;&#21152;&#19978;&#19979;&#25991;&#26377;&#21161;&#20110;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#35299;&#20915;&#35821;&#20041;&#24046;&#36317;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#35821;&#20041;&#26816;&#32034;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#24314;&#35758;&#23398;&#20064;&#19968;&#31181;&#32479;&#19968;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#22270;&#24418;&#12289;&#21464;&#21387;&#22120;&#21644;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20998;&#20139;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#23454;&#29616;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#29305;&#24449;&#24037;&#31243;&#12289;&#30828;&#36127;&#37319;&#26679;&#31574;&#30053;&#21644;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26041;&#38754;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#29992;&#20110;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#37096;&#32626;&#27492;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#20854;&#20182;&#25216;&#24039;&#12290;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#26816;&#32034;&#27169;&#22411;&#26174;&#30528;&#25552;&#39640;&#20102;&#25972;&#20307;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding-based neural retrieval is a prevalent approach to address the semantic gap problem which often arises in product search on tail queries. In contrast, popular queries typically lack context and have a broad intent where additional context from users historical interaction can be helpful. In this paper, we share our novel approach to address both: the semantic gap problem followed by an end to end trained model for personalized semantic retrieval. We propose learning a unified embedding model incorporating graph, transformer and term-based embeddings end to end and share our design choices for optimal tradeoff between performance and efficiency. We share our learnings in feature engineering, hard negative sampling strategy, and application of transformer model, including a novel pre-training strategy and other tricks for improving search relevance and deploying such a model at industry scale. Our personalized retrieval model significantly improves the overall search experience,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23436;&#25104;&#22522;&#20934;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#32452;&#36923;&#36753;&#35268;&#21017;&#65292;&#20351;&#24471;&#32570;&#22833;&#30340;&#20107;&#23454;&#26159;&#35268;&#21017;&#24212;&#29992;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#20182;&#20204;&#30340;&#22522;&#20934;FB15k-237&#65292;&#34920;&#26126;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#26377;&#21306;&#20998;&#24230;&#65292;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#25512;&#24191;&#21040;&#26032;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04814</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#38382;&#39064;&#30340;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Revisiting Inferential Benchmarks for Knowledge Graph Completion. (arXiv:2306.04814v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23436;&#25104;&#22522;&#20934;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#32452;&#36923;&#36753;&#35268;&#21017;&#65292;&#20351;&#24471;&#32570;&#22833;&#30340;&#20107;&#23454;&#26159;&#35268;&#21017;&#24212;&#29992;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#20182;&#20204;&#30340;&#22522;&#20934;FB15k-237&#65292;&#34920;&#26126;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#26377;&#21306;&#20998;&#24230;&#65292;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#25512;&#24191;&#21040;&#26032;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23436;&#25104;&#26159;&#23558;&#19981;&#23436;&#25972;&#30340;KG&#19982;&#32570;&#22833;&#30340;&#20107;&#23454;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;KG&#23436;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#33021;&#22815;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#65292;&#20197;&#20415;&#39044;&#27979;&#30340;&#20107;&#23454;&#26159;&#23558;&#36825;&#20123;&#27169;&#24335;&#24212;&#29992;&#20110;KG&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#23436;&#25104;&#22522;&#20934;&#19981;&#36866;&#21512;&#35780;&#20272;&#27169;&#22411;&#23398;&#20064;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#26159;&#32473;&#23450;KG&#30340;&#38543;&#26426;&#25286;&#20998;&#65292;&#22240;&#27492;&#19981;&#25429;&#25417;&#25512;&#29702;&#27169;&#24335;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23436;&#25104;&#22522;&#20934;&#35774;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#20197;&#19979;&#21407;&#21017;&#65306;&#26377;&#19968;&#32452;&#36923;&#36753;&#35268;&#21017;&#65292;&#20351;&#24471;&#32570;&#22833;&#30340;&#20107;&#23454;&#26159;&#35268;&#21017;&#24212;&#29992;&#30340;&#32467;&#26524;&#65307;&#35757;&#32451;&#38598;&#21253;&#25324;&#21305;&#37197;&#35268;&#21017;&#21069;&#25552;&#21644;&#30456;&#24212;&#32467;&#35770;&#30340;&#21069;&#25552;&#65307;&#27979;&#35797;&#38598;&#30001;&#23558;&#35268;&#21017;&#24212;&#29992;&#20110;&#35757;&#32451;&#38598;&#24471;&#20986;&#30340;&#32467;&#26524;&#32452;&#25104;&#65307;&#36127;&#38754;&#31034;&#20363;&#26088;&#22312;&#38450;&#27490;&#27169;&#22411;&#23398;&#20064;&#34394;&#20551;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22522;&#20934;&#23454;&#20363;&#21270;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;FB15k-237&#65292;&#24182;&#34920;&#26126;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20934;&#26356;&#20855;&#26377;&#21306;&#20998;&#24230;&#65292;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#25512;&#24191;&#21040;&#26032;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph (KG) completion is the problem of extending an incomplete KG with missing facts. A key feature of Machine Learning approaches for KG completion is their ability to learn inference patterns, so that the predicted facts are the results of applying these patterns to the KG. Standard completion benchmarks, however, are not well-suited for evaluating models' abilities to learn patterns, because the training and test sets of these benchmarks are a random split of a given KG and hence do not capture the causality of inference patterns. We propose a novel approach for designing KG completion benchmarks based on the following principles: there is a set of logical rules so that the missing facts are the results of the rules' application; the training set includes both premises matching rule antecedents and the corresponding conclusions; the test set consists of the results of applying the rules to the training set; the negative examples are designed to discourage the models from 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#29983;&#25104;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#20154;&#31867;&#30340;&#25351;&#23548;&#26469;&#36873;&#25321;&#21644;&#36807;&#28388;&#36825;&#20123;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04813</link><description>&lt;p&gt;
&#20154;&#20026;&#21442;&#19982;&#30340;&#21019;&#26032;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Human in the Loop Novelty Generation. (arXiv:2306.04813v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#29983;&#25104;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#20154;&#31867;&#30340;&#25351;&#23548;&#26469;&#36873;&#25321;&#21644;&#36807;&#28388;&#36825;&#20123;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#20197;&#24212;&#23545;&#26032;&#39062;&#12289;&#24847;&#22806;&#24773;&#20917;&#26159;&#19968;&#20010;&#22256;&#38590;&#32780;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#25512;&#21160;&#26032;&#39062;&#24615;&#23481;&#32435;&#30340;&#25216;&#26415;&#21457;&#23637;&#26041;&#38754;&#65292;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22312;&#26032;&#39062;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#22312;&#8220;&#31185;&#23398;&#40479;&#8221;&#21644;&#8220;&#22823;&#23500;&#32705;&#8221;&#31561;&#39046;&#22495;&#20013;&#20986;&#29616;&#30340;&#26032;&#39062;&#24615;&#29983;&#25104;&#26041;&#27861;&#21033;&#29992;&#20102;&#20154;&#31867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#21457;&#29616;&#26032;&#30340;&#26032;&#39062;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#39062;&#24615;&#29983;&#25104;&#20043;&#21069;&#24341;&#20837;&#20154;&#31867;&#25351;&#23548;&#65292;&#20135;&#29983;&#30340;&#21019;&#26032;&#21487;&#20197;&#30452;&#25509;&#21152;&#36733;&#21040;&#27169;&#25311;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21019;&#26032;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#30340;&#25277;&#35937;&#27169;&#22411;&#65288;&#21253;&#25324;&#27169;&#25311;&#39046;&#22495;&#65289;&#65292;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#20154;&#31867;&#25351;&#23548;&#26469;&#29983;&#25104;&#21019;&#26032;&#12290;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#26159;&#21487;&#20197;&#29983;&#25104;&#26356;&#22823;&#30340;&#12289;&#36890;&#24120;&#26159;&#26080;&#38480;&#30340;&#26032;&#39062;&#24615;&#31354;&#38388;&#65292;&#20294;&#38656;&#35201;&#22312;&#29983;&#25104;&#21518;&#28041;&#21450;&#20154;&#31867;&#25351;&#23548;&#20197;&#36873;&#25321;&#21644;&#36807;&#28388;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing artificial intelligence approaches to overcome novel, unexpected circumstances is a difficult, unsolved problem. One challenge to advancing the state of the art in novelty accommodation is the availability of testing frameworks for evaluating performance against novel situations. Recent novelty generation approaches in domains such as Science Birds and Monopoly leverage human domain expertise during the search to discover new novelties. Such approaches introduce human guidance before novelty generation occurs and yield novelties that can be directly loaded into a simulated environment. We introduce a new approach to novelty generation that uses abstract models of environments (including simulation domains) that do not require domain-dependent human guidance to generate novelties. A key result is a larger, often infinite space of novelties capable of being generated, with the trade-off being a requirement to involve human guidance to select and filter novelties post generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#21487;&#29992;&#20110;3D&#21307;&#23398;&#22270;&#20687;&#24182;&#19988;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2306.04811</link><description>&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation. (arXiv:2306.04811v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26694;&#26550;&#65292;&#19981;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#21487;&#29992;&#20110;3D&#21307;&#23398;&#22270;&#20687;&#24182;&#19988;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20174;&#22270;&#20687;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#30340;VLP&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#32780;&#21307;&#23398;&#39046;&#22495;&#20013;&#32570;&#20047;&#36825;&#31181;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;VLP&#20165;&#38480;&#20110;2D&#22270;&#20687;&#65292;&#32780;&#21307;&#23398;&#22270;&#20687;&#21253;&#25324;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#36890;&#24120;&#26159;3D&#22270;&#20687;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#25991;&#26412;&#24341;&#23548;&#30340;&#19977;&#32500;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;GTGM&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25193;&#23637;&#20102;VLP&#20197;&#36866;&#29992;&#20110;&#19981;&#20381;&#36182;&#37197;&#23545;&#25991;&#26412;&#25551;&#36848;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GTGM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#21307;&#23398;&#39118;&#26684;&#30340;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#21512;&#25104;&#25991;&#26412;&#34987;&#29992;&#26469;&#30417;&#30563;&#19977;&#32500;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#36127;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#31574;&#30053;&#65292;&#26469;&#22521;&#20859;&#19968;&#33268;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GTGM&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#38477;&#20302;&#21307;&#23398;&#27880;&#37322;&#38656;&#27714;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in learning visual representations from textual descriptions of images without annotations. Yet, effective VLP demands large-scale image-text pairs, a resource that suffers scarcity in the medical domain. Moreover, conventional VLP is limited to 2D images while medical images encompass diverse modalities, often in 3D, making the learning process more challenging. To address these challenges, we present Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation (GTGM), a framework that extends of VLP to 3D medical images without relying on paired textual descriptions. Specifically, GTGM utilizes large language models (LLM) to generate medical-style text from 3D medical images. This synthetic text is then used to supervise 3D visual representation learning. Furthermore, a negative-free contrastive learning objective strategy is introduced to cultivate consistent visual representat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#40657;&#30418;SDM&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#24314;&#31435;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25551;&#36848;&#20854;&#33021;&#21147;&#21644;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.04806</link><description>&lt;p&gt;
&#40657;&#30418;&#24207;&#36143;&#20915;&#31574;&#31995;&#32479;&#30340;&#33258;&#20027;&#33021;&#21147;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems. (arXiv:2306.04806v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#40657;&#30418;SDM&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#24314;&#31435;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25551;&#36848;&#20854;&#33021;&#21147;&#21644;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23545;&#20110;&#23433;&#20840;&#22320;&#20351;&#29992;&#23427;&#20204;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35753;&#29992;&#25143;&#35780;&#20272;&#20855;&#26377;&#19981;&#26029;&#21457;&#23637;&#30340;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#40657;&#30418;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#19982;&#40657;&#30418;SDM&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#23398;&#20064;&#25551;&#36848;&#20854;&#33021;&#21147;&#30340;&#21487;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#12290;&#23545;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#30830;&#23450;&#20102;&#23398;&#20064;&#36807;&#31243;&#25910;&#25947;&#21040;&#20195;&#29702;&#27491;&#30830;&#27169;&#22411;&#30340;&#26465;&#20214;&#65307;&#23545;&#19981;&#21516;&#20195;&#29702;&#21644;&#27169;&#25311;&#22330;&#26223;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25551;&#36848;&#20219;&#24847;&#40657;&#30418;SDM&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#36827;&#34892;&#23569;&#27425;&#36890;&#29992;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for users to understand what their AI systems can and can't do in order to use them safely. However, the problem of enabling users to assess AI systems with evolving sequential decision making (SDM) capabilities is relatively understudied. This paper presents a new approach for modeling the capabilities of black-box AI systems that can plan and act, along with the possible effects and requirements for executing those capabilities in stochastic settings. We present an active-learning approach that can effectively interact with a black-box SDM system and learn an interpretable probabilistic model describing its capabilities. Theoretical analysis of the approach identifies the conditions under which the learning process is guaranteed to converge to the correct model of the agent; empirical evaluations on different agents and simulated scenarios show that this approach is few-shot generalizable and can effectively describe the capabilities of arbitrary black-box SDM agents 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25351;&#20986;&#20165;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#24182;&#19981;&#33021;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.04792</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35266;&#27979;&#22240;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Use of Generative Models in Observational Causal Analysis. (arXiv:2306.04792v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25351;&#20986;&#20165;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#24182;&#19981;&#33021;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#20551;&#35774;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;&#29305;&#23450;&#27169;&#22411;&#30340;&#20551;&#35774;&#26159;&#23545;&#19968;&#32452;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#25215;&#35834;&#65292;&#20294;&#20165;&#20272;&#35745;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#24182;&#19981;&#36275;&#20197;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#20165;&#25551;&#36848;&#21333;&#20010;&#21487;&#35266;&#27979;&#20998;&#24067;&#65292;&#26080;&#27861;&#25551;&#36848;&#24178;&#39044;&#25928;&#24212;&#38142;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of a hypothetical generative model was been suggested for causal analysis of observational data. The very assumption of a particular model is a commitment to a certain set of variables and therefore to a certain set of possible causes. Estimating the joint probability distribution of can be useful for predicting values of variables in view of the observed values of others, but it is not sufficient for inferring causal relationships. The model describes a single observable distribution and cannot a chain of effects of intervention that deviate from the observed distribution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04766</link><description>&lt;p&gt;
&#29992;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#22312; $d \gg n$ &#24773;&#20917;&#19979;&#30340;&#34920;&#26684;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enabling tabular deep learning when $d \gg n$ with an auxiliary knowledge graph. (arXiv:2306.04766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20016;&#23500;&#26631;&#35760;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#38750;&#24120;&#39640;&#32500;&#29305;&#24449;&#20294;&#26679;&#26412;&#25968;&#26377;&#38480;&#65288;&#21363; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#65289;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#30340;&#20027;&#35201;&#27934;&#35265;&#22312;&#20110;&#36755;&#20837;&#29305;&#24449;&#36890;&#24120;&#20855;&#26377;&#20016;&#23500;&#30340;&#36741;&#21161;&#39046;&#22495;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#34987;&#32452;&#32455;&#25104;&#24322;&#26500;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102; PLATO &#26041;&#27861;&#65292;&#20351;&#29992;&#25551;&#36848;&#36755;&#20837;&#29305;&#24449;&#30340;&#36741;&#21161; KG &#26469;&#35268;&#33539;&#19968;&#20010; MLP&#65292;&#22312; $d \gg n$ &#30340;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models exhibit strong performance on datasets with abundant labeled samples. However, for tabular datasets with extremely high $d$-dimensional features but limited $n$ samples (i.e. $d \gg n$), machine learning models struggle to achieve strong performance due to the risk of overfitting. Here, our key insight is that there is often abundant, auxiliary domain information describing input features which can be structured as a heterogeneous knowledge graph (KG). We propose PLATO, a method that achieves strong performance on tabular data with $d \gg n$ by using an auxiliary KG describing input features to regularize a multilayer perceptron (MLP). In PLATO, each input feature corresponds to a node in the auxiliary KG. In the MLP's first layer, each input feature also corresponds to a weight vector. PLATO is based on the inductive bias that two input features corresponding to similar nodes in the auxiliary KG should have similar weight vectors in the MLP's first layer. PLATO
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20844;&#24320;&#37096;&#32626;&#65292;&#21457;&#29616;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#24433;&#21709;&#29992;&#25143;&#24863;&#30693;&#65292;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#24433;&#21709;&#21453;&#39304;&#29575;&#65292;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.04765</link><description>&lt;p&gt;
&#20844;&#24320;&#37096;&#32626;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#65306;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#12289;&#35774;&#35745;&#24314;&#35758;&#21644;&#24320;&#25918;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The HCI Aspects of Public Deployment of Research Chatbots: A User Study, Design Recommendations, and Open Challenges. (arXiv:2306.04765v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20844;&#24320;&#37096;&#32626;&#65292;&#21457;&#29616;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#24433;&#21709;&#29992;&#25143;&#24863;&#30693;&#65292;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#24433;&#21709;&#21453;&#39304;&#29575;&#65292;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#27492;&#30740;&#31350;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#37096;&#32626;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#20010;&#28041;&#21450;&#24517;&#35201;&#30340;&#39118;&#38505;&#19982;&#25910;&#30410;&#20998;&#26512;&#30340;&#24494;&#22937;&#35805;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#39057;&#32321;&#35752;&#35770;&#26159;&#21542;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;&#27492;&#31867;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#23454;&#29616;&#26356;&#26377;&#25928;&#30446;&#26631;&#30340;&#20132;&#20114;&#33539;&#24335;&#21644;&#35774;&#35745;&#26041;&#27861;&#21364;&#20851;&#27880;&#36739;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#25253;&#21578;&#23545;&#26368;&#36817;&#30740;&#31350;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#21147;&#22270;&#25552;&#20986;&#12289;&#22522;&#20110;&#24182;&#23581;&#35797;&#22238;&#31572;&#28041;&#21450;&#27492;&#33539;&#22260;&#30340;&#20154;&#26426;&#20132;&#20114;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#29702;&#20154;&#30340;&#25277;&#35937;&#25311;&#20154;&#21270;&#34920;&#29616;&#23545;&#29992;&#25143;&#30340;&#24863;&#30693;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#25552;&#20379;AI&#21487;&#35299;&#37322;&#24615;&#21487;&#33021;&#20250;&#23545;&#21453;&#39304;&#29575;&#20135;&#29983;&#24433;&#21709;&#65292;&#32780;&#32842;&#22825;&#20307;&#39564;&#30340;&#20004;&#31181;&#27700;&#24179;&#65288;&#25925;&#20107;&#20869;&#21644;&#25925;&#20107;&#22806;&#65289;&#24212;&#26377;&#24847;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#35774;&#35745;&#24314;&#35758;&#21644;&#30740;&#31350;&#31038;&#21306;&#36827;&#19968;&#27493;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly deploying research chatbots is a nuanced topic involving necessary risk-benefit analyses. While there have recently been frequent discussions on whether it is responsible to deploy such models, there has been far less focus on the interaction paradigms and design approaches that the resulting interfaces should adopt, in order to achieve their goals more effectively. We aim to pose, ground, and attempt to answer HCI questions involved in this scope, by reporting on a mixed-methods user study conducted on a recent research chatbot. We find that abstract anthropomorphic representation for the agent has a significant effect on user's perception, that offering AI explainability may have an impact on feedback rates, and that two (diegetic and extradiegetic) levels of the chat experience should be intentionally designed. We offer design recommendations and areas of further focus for the research community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#20041;&#30693;&#35782;&#22270;&#35889;&#30340;Academic Papers&#20449;&#24687;&#26816;&#32034;&#19982;&#20998;&#26512;&#26694;&#26550;(SKG)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#35821;&#26009;&#24211;&#65292;&#25903;&#25345;&#21508;&#31181;&#23398;&#26415;&#25991;&#29486;&#30340;&#35821;&#20041;&#26597;&#35810;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#27969;&#31995;&#32479;&#36827;&#34892;&#28789;&#27963;&#12289;&#20132;&#20114;&#30340;&#21508;&#31181;&#35821;&#20041;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2306.04758</link><description>&lt;p&gt;
SKG: &#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#22522;&#20110;&#35821;&#20041;&#30693;&#35782;&#22270;&#35889;&#30340;&#23398;&#26415;&#35770;&#25991;&#20449;&#24687;&#26816;&#32034;&#19982;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SKG: A Versatile Information Retrieval and Analysis Framework for Academic Papers with Semantic Knowledge Graphs. (arXiv:2306.04758v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#20041;&#30693;&#35782;&#22270;&#35889;&#30340;Academic Papers&#20449;&#24687;&#26816;&#32034;&#19982;&#20998;&#26512;&#26694;&#26550;(SKG)&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#35821;&#20041;&#27010;&#24565;&#34920;&#31034;&#35821;&#26009;&#24211;&#65292;&#25903;&#25345;&#21508;&#31181;&#23398;&#26415;&#25991;&#29486;&#30340;&#35821;&#20041;&#26597;&#35810;&#65292;&#24182;&#24320;&#21457;&#20102;&#25968;&#25454;&#27969;&#31995;&#32479;&#36827;&#34892;&#28789;&#27963;&#12289;&#20132;&#20114;&#30340;&#21508;&#31181;&#35821;&#20041;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29256;&#30340;&#30740;&#31350;&#35770;&#25991;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#22240;&#27492;&#24320;&#21457;&#26032;&#30340;&#39640;&#25928;&#12289;&#22810;&#21151;&#33021;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#21457;&#29616;&#26041;&#27861;&#21313;&#20998;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#30693;&#35782;&#22270;&#35889;&#65288;SKG&#65289;&#65292;&#35813;&#22270;&#35889;&#25972;&#21512;&#20102;&#26469;&#33258;&#25688;&#35201;&#21644;&#20854;&#20182;&#20803;&#20449;&#24687;&#30340;&#35821;&#20041;&#27010;&#24565;&#26469;&#34920;&#31034;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;SKG&#20013;&#23384;&#20648;&#20102;&#39640;&#24230;&#22810;&#26679;&#21270;&#21644;&#20016;&#23500;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#23398;&#26415;&#25991;&#29486;&#30340;&#35821;&#20041;&#26597;&#35810;&#12290;&#20026;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30693;&#35782;&#25552;&#21462;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#21322;&#30417;&#30563;&#31649;&#36947;&#29992;&#20110;&#23454;&#20307;&#25552;&#21462;&#21644;&#23454;&#20307;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#26412;&#20307;&#35770;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#20854;&#20182;&#20803;&#20449;&#24687;&#25972;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;SKG&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#27969;&#31995;&#32479;&#65292;&#28436;&#31034;&#22914;&#20309;&#22312;SKG&#19978;&#28789;&#27963;&#12289;&#20132;&#20114;&#22320;&#36827;&#34892;&#21508;&#31181;&#35821;&#20041;&#26597;&#35810;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#22823;&#35268;&#27169;&#30340;&#23398;&#26415;&#20986;&#29256;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#35828;&#26126;&#20102;SKG&#22914;&#20309;&#21327;&#21161;&#23398;&#26415;&#30740;&#31350;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#29486;&#32508;&#36848;&#12289;&#26597;&#35810;&#22238;&#31572;&#21644;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of published research papers has experienced exponential growth in recent years, which makes it crucial to develop new methods for efficient and versatile information extraction and knowledge discovery. To address this need, we propose a Semantic Knowledge Graph (SKG) that integrates semantic concepts from abstracts and other meta-information to represent the corpus. The SKG can support various semantic queries in academic literature thanks to the high diversity and rich information content stored within. To extract knowledge from unstructured text, we develop a Knowledge Extraction Module that includes a semi-supervised pipeline for entity extraction and entity normalization. We also create an ontology to integrate the concepts with other meta information, enabling us to build the SKG. Furthermore, we design and develop a dataflow system that demonstrates how to conduct various semantic queries flexibly and interactively over the SKG. To demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04750</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoML Systems For Medical Imaging. (arXiv:2306.04750v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#21307;&#29983;&#25552;&#20379;&#30340;&#21307;&#30103;&#20445;&#20581;&#26381;&#21153;&#30340;&#36136;&#37327;&#12290;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#31934;&#24230;&#12290;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#23450;&#21046;&#21270;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#12290;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#29992;&#20110;&#26080;&#21019;&#22320;&#21019;&#24314;&#20869;&#37096;&#22120;&#23448;&#21644;&#36523;&#20307;&#37096;&#20301;&#22270;&#20687;&#65292;&#20197;&#36827;&#34892;&#35786;&#26029;&#21644;&#25805;&#20316;&#30446;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#31361;&#20986;&#21307;&#23398;&#25104;&#20687;&#20013;AutoML&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning in medical image analysis can greatly enhance the quality of healthcare provided by physicians. The combination of human expertise and computerized systems can result in improved diagnostic accuracy. An automated machine learning approach simplifies the creation of custom image recognition models by utilizing neural architecture search and transfer learning techniques. Medical imaging techniques are used to non-invasively create images of internal organs and body parts for diagnostic and procedural purposes. This article aims to highlight the potential applications, strategies, and techniques of AutoML in medical imaging through theoretical and empirical evidence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#37326;&#22806;&#28857;&#20113;&#20013;&#23398;&#20064;3D&#20154;&#20307;&#20851;&#33410;&#23450;&#20301;&#65292;&#21033;&#29992;&#20960;&#20309;&#19968;&#33268;&#24615;&#24605;&#24819;&#26500;&#24314;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#22312;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.04745</link><description>&lt;p&gt;
&#37326;&#22806;&#28857;&#20113;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#20851;&#38190;&#28857;&#20272;&#35745;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels. (arXiv:2306.04745v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#37326;&#22806;&#28857;&#20113;&#20013;&#23398;&#20064;3D&#20154;&#20307;&#20851;&#33410;&#23450;&#20301;&#65292;&#21033;&#29992;&#20960;&#20309;&#19968;&#33268;&#24615;&#24605;&#24819;&#26500;&#24314;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#22312;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#37117;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#19979;&#65292;&#35757;&#32451;&#19968;&#20010;&#20174;&#28857;&#20113;&#20013;&#26816;&#27979;&#19977;&#32500;&#20154;&#20307;&#20851;&#38190;&#28857;&#30340;&#26816;&#27979;&#22120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#23613;&#31649;&#25429;&#25417;&#22823;&#37327;&#30340;&#20154;&#20307;&#28857;&#20113;&#30456;&#23545;&#23481;&#26131;&#65292;&#20294;&#26631;&#27880;&#19977;&#32500;&#20851;&#38190;&#28857;&#26159;&#26114;&#36149;&#12289;&#20027;&#35266;&#12289;&#23481;&#26131;&#20986;&#38169;&#65292;&#23588;&#20854;&#26159;&#22312;&#38271;&#23614;&#25968;&#25454;&#65288;&#20855;&#26377;&#32597;&#35265;&#23039;&#24577;&#30340;&#34892;&#20154;&#12289;&#39569;&#36718;&#28369;&#32773;&#31561;&#65289;&#26041;&#38754;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;GC-KPL (&#22522;&#20110;&#20960;&#20309;&#19968;&#33268;&#24615;&#30340;&#20851;&#38190;&#28857;&#23398;&#20064;)&#65292;&#29992;&#20110;&#20174;&#28857;&#20113;&#20013;&#23398;&#20064;3D&#20154;&#20307;&#20851;&#33410;&#23450;&#20301;&#65292;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20844;&#24335;&#65292;&#32771;&#34385;&#20102;&#20154;&#20307;&#32467;&#26500;&#21644;&#36816;&#21160;&#29366;&#24577;&#65292;&#36798;&#21040;&#20102;&#21512;&#29702;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Waymo Open Dataset&#30340;&#22823;&#22411;&#35757;&#32451;&#38598;&#36827;&#34892;&#26080;&#20154;&#24037;&#26631;&#27880;&#20851;&#38190;&#28857;&#30340;&#35757;&#32451;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#20063;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#20027;&#24178;&#32593;&#32476;&#20063;&#20174;&#26080;&#30417;&#30563;&#35757;&#32451;&#20013;&#21463;&#30410;&#65292;&#24182;&#22312;&#19979;&#28216;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20102;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a 3D human keypoint detector from point clouds in a supervised manner requires large volumes of high quality labels. While it is relatively easy to capture large amounts of human point clouds, annotating 3D keypoints is expensive, subjective, error prone and especially difficult for long-tail cases (pedestrians with rare poses, scooterists, etc.). In this work, we propose GC-KPL - Geometry Consistency inspired Key Point Leaning, an approach for learning 3D human joint locations from point clouds without human labels. We achieve this by our novel unsupervised loss formulations that account for the structure and movement of the human body. We show that by training on a large training set from Waymo Open Dataset without any human annotated keypoints, we are able to achieve reasonable performance as compared to the fully supervised approach. Further, the backbone benefits from the unsupervised training and is useful in downstream fewshot learning of keypoints, where fine-tuning on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#12298;MultiEarth 2023&#12299;&#24037;&#20316;&#22346;&#21644;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#30417;&#27979;&#22320;&#29699;&#29983;&#24577;&#31995;&#32479;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#21516;&#26102;&#20063;&#25552;&#20379;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#36965;&#24863;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#25361;&#25112;&#38598;&#20013;&#22312;&#30417;&#27979;&#20122;&#39532;&#36874;&#38632;&#26519;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2306.04738</link><description>&lt;p&gt;
&#12298;MultiEarth 2023&#65306;&#38754;&#21521;&#22320;&#29699;&#19982;&#29615;&#22659;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#24037;&#20316;&#22346;&#21644;&#25361;&#25112;&#12299;
&lt;/p&gt;
&lt;p&gt;
MultiEarth 2023 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2306.04738v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#12298;MultiEarth 2023&#12299;&#24037;&#20316;&#22346;&#21644;&#25361;&#25112;&#65292;&#36890;&#36807;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#30417;&#27979;&#22320;&#29699;&#29983;&#24577;&#31995;&#32479;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#21516;&#26102;&#20063;&#25552;&#20379;&#19968;&#20010;&#20844;&#20849;&#22522;&#20934;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#36965;&#24863;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#25361;&#25112;&#38598;&#20013;&#22312;&#30417;&#27979;&#20122;&#39532;&#36874;&#38632;&#26519;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#38754;&#21521;&#22320;&#29699;&#19982;&#29615;&#22659;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#24037;&#20316;&#22346;&#21644;&#25361;&#25112;(MultiEarth 2023)&#12299;&#26159;&#31532;&#20108;&#23626;CVPR&#24180;&#24230;&#24037;&#20316;&#22346;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25345;&#32493;&#25910;&#38598;&#30340;&#22823;&#37327;&#36965;&#24863;&#25968;&#25454;&#26469;&#30417;&#27979;&#21644;&#20998;&#26512;&#22320;&#29699;&#29983;&#24577;&#31995;&#32479;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#26412;&#24037;&#20316;&#22346;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23558;&#22320;&#29699;&#19982;&#29615;&#22659;&#31185;&#23398;&#31038;&#21306;&#20197;&#21450;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#31038;&#21306;&#27719;&#32858;&#22312;&#19968;&#36215;&#65292;&#25506;&#32034;&#21033;&#29992;&#25216;&#26415;&#36827;&#27493;&#25903;&#25345;&#29615;&#22659;&#30417;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;MultiEarth&#24037;&#20316;&#22346;&#36824;&#36890;&#36807;&#32452;&#32455;&#20844;&#20849;&#25361;&#25112;&#65292;&#20026;&#22788;&#29702;&#22810;&#27169;&#24577;&#36965;&#24863;&#20449;&#24687;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#22522;&#20934;&#65292;&#36825;&#20123;&#25361;&#25112;&#38598;&#20013;&#22312;&#30417;&#27979;&#20122;&#39532;&#36874;&#38632;&#26519;&#26041;&#38754;&#65292;&#24182;&#21253;&#25324;&#20272;&#35745;&#26862;&#26519;&#30733;&#20240;&#29575;&#12289;&#26816;&#27979;&#26862;&#26519;&#28779;&#28798;&#12289;&#23558;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#36716;&#21270;&#20026;&#21487;&#35270;&#39046;&#22495;&#21644;&#39044;&#27979;&#29615;&#22659;&#36235;&#21183;&#31561;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25361;&#25112;&#25351;&#21335;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multimodal Learning for Earth and Environment Workshop (MultiEarth 2023) is the second annual CVPR workshop aimed at the monitoring and analysis of the health of Earth ecosystems by leveraging the vast amount of remote sensing data that is continuously being collected. The primary objective of this workshop is to bring together the Earth and environmental science communities as well as the multimodal representation learning communities to explore new ways of harnessing technological advancements in support of environmental monitoring. The MultiEarth Workshop also seeks to provide a common benchmark for processing multimodal remote sensing information by organizing public challenges focused on monitoring the Amazon rainforest. These challenges include estimating deforestation, detecting forest fires, translating synthetic aperture radar (SAR) images to the visible domain, and projecting environmental trends. This paper presents the challenge guidelines, datasets, and evaluation metr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;AGIQA-3K&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;StairReward&#20197;&#25552;&#39640;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04717</link><description>&lt;p&gt;
AGIQA-3K&#65306;&#19968;&#20221;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment. (arXiv:2306.04717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;AGIQA-3K&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;StairReward&#20197;&#25552;&#39640;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#65288;AGIs&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#23089;&#20048;&#12289;&#25945;&#32946;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;AGIs&#20043;&#38388;&#30340;&#22823;&#37327;&#36136;&#37327;&#24046;&#24322;&#65292;&#36843;&#20999;&#38656;&#35201;&#19982;&#20154;&#31867;&#20027;&#35266;&#35780;&#20998;&#19968;&#33268;&#30340;&#36136;&#37327;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24191;&#27867;&#32771;&#34385;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;AGI&#27169;&#22411;&#12289;&#19981;&#21516;&#25552;&#31034;&#21644;&#27169;&#22411;&#21442;&#25968;&#29983;&#25104;&#30340;AGI&#65292;&#24182;&#25910;&#38598;&#20102;&#24863;&#30693;&#36136;&#37327;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#26041;&#38754;&#30340;&#20027;&#35266;&#35780;&#20998;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20840;&#38754;&#30340;AGI&#20027;&#35266;&#36136;&#37327;&#25968;&#25454;&#24211;AGIQA-3K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#27169;&#22411;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;StairReward&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;AGIQA-3K&#20013;&#30340;&#32454;&#31890;&#24230;&#20027;&#35266;&#35780;&#20998;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K wil
&lt;/p&gt;</description></item><item><title>BlenderBot 3x&#26159;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#65292;&#25216;&#26415;&#19978;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#30340;&#25945;&#24072;&#36991;&#20813;&#23398;&#20064;&#26377;&#27602;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2306.04707</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#26377;&#26426;&#20132;&#20114;&#65292;&#25913;&#36827;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Open Language Models by Learning from Organic Interactions. (arXiv:2306.04707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04707
&lt;/p&gt;
&lt;p&gt;
BlenderBot 3x&#26159;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#65292;&#25216;&#26415;&#19978;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#30340;&#25945;&#24072;&#36991;&#20813;&#23398;&#20064;&#26377;&#27602;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BlenderBot 3x&#65292;&#23427;&#26159;&#20250;&#35805;&#27169;&#22411;BlenderBot 3&#30340;&#19968;&#20010;&#26356;&#26032;&#29256;&#26412;&#65292;&#29616;&#22312;&#36890;&#36807;&#21442;&#19982;&#32773;&#30340;&#26377;&#26426;&#23545;&#35805;&#21644;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#25913;&#36827;&#20854;&#25216;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#21442;&#19982;&#32773;&#21311;&#21517;&#20132;&#20114;&#25968;&#25454;&#65292;&#20379;&#30740;&#31350;&#31038;&#21306;&#20351;&#29992;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#12290;&#20351;&#29992;&#26377;&#26426;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#19982;&#20154;&#20204;&#22312;&#8220;&#37326;&#22806;&#8221;&#30340;&#20114;&#21160;&#21253;&#25324;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#23545;&#25239;&#24615;&#21644;&#26377;&#27602;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#26377;&#30410;&#30340;&#25945;&#24072;&#23398;&#20064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20174;&#35797;&#22270;&#23558;&#27169;&#22411;&#35825;&#23548;&#20026;&#26080;&#29992;&#25110;&#26377;&#27602;&#21453;&#24212;&#30340;&#20154;&#20013;&#23398;&#20064;&#12290;BlenderBot 3x&#22312;&#23545;&#35805;&#20013;&#27604;BlenderBot 3&#26356;&#21463;&#27426;&#36814;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#26356;&#23433;&#20840;&#30340;&#21709;&#24212;&#12290;&#34429;&#28982;&#25105;&#20204;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#36828;&#38750;&#23436;&#32654;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36890;&#36807;&#32487;&#32493;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#20197;&#21450;&#25506;&#32034;&#20174;&#26377;&#26426;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BlenderBot 3x, an update on the conversational model BlenderBot 3, which is now trained using organic conversation and feedback data from participating users of the system in order to improve both its skills and safety. We are publicly releasing the participating de-identified interaction data for use by the research community, in order to spur further progress. Training models with organic data is challenging because interactions with people "in the wild" include both high quality conversations and feedback, as well as adversarial and toxic behavior. We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is both preferred in conversation to BlenderBot 3, and is shown to produce safer responses in challenging situations. While our current models are still far from perfect, we believe further improvement can be achieved by continued use of the techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.04660</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#65288;GLOSA&#65289;&#31995;&#32479;&#24314;&#35758;&#36710;&#36742;&#36895;&#24230;&#65292;&#20197;&#24110;&#21161;&#23427;&#20204;&#22312;&#32511;&#33394;&#26102;&#38388;&#36890;&#36807;&#36335;&#21475;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#36335;&#21475;&#20572;&#36710;&#21644;&#24608;&#36895;&#26102;&#38388;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#29123;&#26009;&#28040;&#32791;&#12290;&#20294;&#26159;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20248;&#21270;GLOSA&#31639;&#27861;&#65292;&#24573;&#30053;&#20102;GLOSA&#31995;&#32479;&#30340;&#36895;&#24230;&#24314;&#35758;&#39057;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#36895;&#24230;&#24314;&#35758;&#65292;&#23548;&#33268;&#20887;&#20313;&#24314;&#35758;&#65292;&#32780;&#20854;&#20182;&#20154;&#20165;&#20026;&#36710;&#36742;&#35745;&#31639;&#26368;&#20339;&#36895;&#24230;&#65292;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;PPO&#65288;H-PPO&#65289;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;GLOSA&#65288;AF-GLOSA&#65289;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#20102;&#19968;&#20010;actor-critic&#26550;&#26500;&#21644;&#19968;&#20010;&#28151;&#21512;actor&#32593;&#32476;&#12290;&#28151;&#21512;&#28436;&#21592;&#32593;&#32476;&#21253;&#25324;&#19968;&#20010;&#31163;&#25955;&#28436;&#21592;&#65292;&#36755;&#20986;&#21672;&#35810;&#39057;&#29575;&#21644;&#19968;&#20010;&#36830;&#32493;&#28436;&#21592;&#65292;&#36755;&#20986;&#21152;&#36895;&#24230;&#26354;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AF-GLOSA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20849;&#24773;&#24335;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#27169;&#22359;&#36873;&#25321;&#24120;&#35782;&#30693;&#35782;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#24773;&#20917;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.04657</link><description>&lt;p&gt;
&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#25552;&#21319;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge. (arXiv:2306.04657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27880;&#20837;&#24120;&#35782;&#30693;&#35782;&#30340;&#20849;&#24773;&#24335;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#27169;&#22359;&#36873;&#25321;&#24120;&#35782;&#30693;&#35782;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#23545;&#35805;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#24773;&#20917;&#30340;&#19968;&#33268;&#24615;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20849;&#24773;&#23545;&#35805;&#20013;&#65292;&#20010;&#20307;&#34920;&#36798;&#23545;&#20182;&#20154;&#30340;&#20849;&#24773;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#38752;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29983;&#25104;&#20849;&#24773;&#24335;&#22238;&#24212;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#30340;&#24120;&#35782;&#30693;&#35782;&#20063;&#21487;&#29992;&#20110;&#22686;&#24378;&#31995;&#32479;&#23545;&#35821;&#22659;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#24120;&#35782;&#30693;&#35782;&#24211;&#20013;&#21253;&#21547;&#21508;&#31181;&#20851;&#31995;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#35805;&#31995;&#32479;&#30340;&#28151;&#28102;&#12290;&#22240;&#27492;&#65292;&#24773;&#24863;&#12289;&#29983;&#25104;&#30340;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24773;&#22659;&#20449;&#24687;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20849;&#24773;&#24335;&#22238;&#24212;&#29983;&#25104;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#29992;&#20110;&#24120;&#35782;&#30693;&#35782;&#36873;&#25321;&#65292;&#20197;&#30830;&#20445;&#25152;&#29983;&#25104;&#30340;&#20849;&#24773;&#24335;&#22238;&#24212;&#21644;&#35828;&#35805;&#32773;&#30340;&#24773;&#20917;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36873;&#25321;&#30340;&#30693;&#35782;&#29992;&#20110;&#35843;&#25972;&#29983;&#25104;&#30340;&#21709;&#24212;&#30340;&#24120;&#35782;&#35748;&#30693;&#21644;&#20849;&#24773;&#34920;&#36798;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker's emotion. Besides, external commonsense knowledge has been applied to enhance the system's understandings of the speaker's situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion, generated response and speaker's contextual information. To this end, we propose a novel approach for empathetic response generation, which incorporates an adaptive module for commonsense knowledge selection to ensure consistency between the generated empathetic responses and the speaker's situation. This selected knowledge is used to refine the commonsense cognition and empathy expression for generated responses. Experimental results show that our approach significantly outperforms baseline mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#20351;&#24471;&#39044;&#27979;&#21306;&#38388;&#22312;&#20445;&#25345;&#36793;&#38469;&#26377;&#25928;&#30340;&#21516;&#26102;&#20855;&#26377;&#23545;&#35937;&#23646;&#24615;&#30456;&#20851;&#30340;&#22823;&#23567;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.04648</link><description>&lt;p&gt;
&#35770;&#35757;&#32451;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#25490;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
On training locally adaptive CP. (arXiv:2306.04648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#20351;&#24471;&#39044;&#27979;&#21306;&#38388;&#22312;&#20445;&#25345;&#36793;&#38469;&#26377;&#25928;&#30340;&#21516;&#26102;&#20855;&#26377;&#23545;&#35937;&#23646;&#24615;&#30456;&#20851;&#30340;&#22823;&#23567;&#12290;&#36890;&#36807;&#35757;&#32451;&#21487;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#38388;&#38548;&#26412;&#22320;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38598;&#20013;&#20110;&#36890;&#36807;&#21010;&#20998;&#25110;&#37325;&#26032;&#21152;&#26435;&#26657;&#20934;&#38598;&#26469;&#36817;&#20284;&#38388;&#38548;&#30340;&#23545;&#35937;&#26465;&#20214;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#26032;&#30340;&#19988;&#27010;&#24565;&#19978;&#19981;&#21516;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#21152;&#26435;&#26657;&#20934;&#25968;&#25454;&#65292;&#32780;&#26159;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#21464;&#37327;&#21464;&#25442;$A \to \phi_X(A)$&#37325;&#26032;&#23450;&#20041;&#31526;&#21512;&#24230;&#37327;&#65292;&#35813;&#21464;&#25442;&#26126;&#30830;&#22320;&#21462;&#20915;&#20110;&#23545;&#35937;&#23646;&#24615;$X$&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#22914;&#26524;$\phi_X$&#23545;&#20110;&#20219;&#20309;$X$&#22312;$A$&#20013;&#26159;&#21333;&#35843;&#30340;&#65292;&#21017;&#21464;&#25442;&#23558;&#29983;&#25104;&#20445;&#35777;&#26159;&#36793;&#38469;&#26377;&#25928;&#19988;&#20855;&#26377;$X$&#30456;&#20851;&#22823;&#23567;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#23545;$\phi_X$&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#38388;&#38548;&#25928;&#29575;&#12290;&#19982;&#20854;&#20182;CP-aware&#35757;&#32451;&#26041;&#27861;&#30456;&#21453;&#65292;&#30446;&#26631;&#20989;&#25968;&#26159;&#24179;&#28369;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#26368;&#23567;&#21270;&#32780;&#26080;&#38656;&#36827;&#34892;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of making Conformal Prediction (CP) intervals locally adaptive. Most existing methods focus on approximating the object-conditional validity of the intervals by partitioning or re-weighting the calibration set. Our strategy is new and conceptually different. Instead of re-weighting the calibration data, we redefine the conformity measure through a trainable change of variables, $A \to \phi_X(A)$, that depends explicitly on the object attributes, $X$. Under certain conditions and if $\phi_X$ is monotonic in $A$ for any $X$, the transformations produce prediction intervals that are guaranteed to be marginally valid and have $X$-dependent sizes. We describe how to parameterize and train $\phi_X$ to maximize the interval efficiency. Contrary to other CP-aware training methods, the objective function is smooth and can be minimized through standard gradient methods without approximations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decom-CAM&#30340;&#20004;&#38454;&#27573;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#29305;&#24449;&#32423;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#23450;&#20301;&#37325;&#35201;&#21306;&#22495;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#24182;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04644</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#31867;&#28608;&#27963;&#22270;&#30340;&#29305;&#24449;&#32423;&#35299;&#37322;&#26041;&#27861;&#65306;Decom-CAM
&lt;/p&gt;
&lt;p&gt;
Decom--CAM: Tell Me What You See, In Details! Feature-Level Interpretation via Decomposition Class Activation Map. (arXiv:2306.04644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Decom-CAM&#30340;&#20004;&#38454;&#27573;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#29305;&#24449;&#32423;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#23450;&#20301;&#37325;&#35201;&#21306;&#22495;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#24182;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#31867;&#28608;&#27963;&#22270;&#65288;CAM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36890;&#36807;&#31361;&#20986;&#23545;&#35937;&#20301;&#32622;&#26469;&#35299;&#37322;&#28145;&#24230;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#26410;&#33021;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#29992;&#20110;&#20570;&#20986;&#20915;&#31574;&#30340;&#26174;&#33879;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24573;&#30053;&#20102;&#21487;&#35299;&#37322;&#24615;&#34920;&#29616;&#19982;&#27169;&#22411;&#20915;&#31574;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22522;&#26412;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#35299;&#31867;&#28608;&#27963;&#22270;&#65288;Decom-CAM&#65289;&#20004;&#38454;&#27573;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#29305;&#24449;&#32423;&#35299;&#37322;&#12290;Decom-CAM&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#23558;&#20013;&#38388;&#28608;&#27963;&#22270;&#20998;&#35299;&#20026;&#27491;&#20132;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#12290;&#29305;&#24449;&#30340;&#27491;&#20132;&#24615;&#20351;CAM&#33021;&#22815;&#25429;&#25417;&#26412;&#22320;&#29305;&#24449;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#23450;&#20301;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#32452;&#20214;&#65292;&#22914;&#30524;&#30555;&#12289;&#40763;&#23376;&#21644;&#38754;&#37096;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23450;&#20301;&#37325;&#35201;&#21306;&#22495;&#21644;&#26174;&#33879;&#29305;&#24449;&#65292;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretation of deep learning remains a very challenging problem. Although the Class Activation Map (CAM) is widely used to interpret deep model predictions by highlighting object location, it fails to provide insight into the salient features used by the model to make decisions. Furthermore, existing evaluation protocols often overlook the correlation between interpretability performance and the model's decision quality, which presents a more fundamental issue. This paper proposes a new two-stage interpretability method called the Decomposition Class Activation Map (Decom-CAM), which offers a feature-level interpretation of the model's prediction. Decom-CAM decomposes intermediate activation maps into orthogonal features using singular value decomposition and generates saliency maps by integrating them. The orthogonality of features enables CAM to capture local features and can be used to pinpoint semantic components such as eyes, noses, and faces in the input image, making it more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#31639;&#27861;&#26816;&#27979;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#20132;&#26131;&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#30417;&#31649;&#23545;&#20943;&#23569;&#27450;&#35784;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04643</link><description>&lt;p&gt;
NFT&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#20132;&#26131;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Abnormal Trading Detection in the NFT Market. (arXiv:2306.04643v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32858;&#31867;&#31639;&#27861;&#26816;&#27979;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#20132;&#26131;&#24066;&#22330;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#30417;&#31649;&#23545;&#20943;&#23569;&#27450;&#35784;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#24066;&#22330;&#36817;&#24180;&#26469;&#21576;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#25454;DappRadar&#32479;&#35745;&#65292;&#26368;&#22823;&#30340;NFT&#24066;&#22330;OpenSea&#30340;&#24635;&#20132;&#26131;&#39069;&#22312;2023&#24180;2&#26376;&#36798;&#21040;&#20102;347&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;NFT&#24066;&#22330;&#22823;&#37096;&#20998;&#26159;&#26410;&#21463;&#30417;&#31649;&#30340;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#30340;&#27927;&#38065;&#12289;&#27450;&#35784;&#21644;&#34394;&#20551;&#20132;&#26131;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25581;&#31034;&#24120;&#35265;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#22914;&#34394;&#25311;&#20132;&#26131;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#20854;&#20182;&#20132;&#26131;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#24066;&#22330;&#25968;&#25454;&#20174;&#32593;&#32476;&#12289;&#36135;&#24065;&#21644;&#26102;&#38388;&#30340;&#35282;&#24230;&#35774;&#35745;&#37327;&#21270;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22522;&#20110;K&#22343;&#20540;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20197;&#23545;&#20132;&#26131;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#32858;&#31867;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#30417;&#31649;&#26469;&#20943;&#23569;&#19981;&#33391;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#33021;&#26377;&#21161;&#20110;&#37325;&#26032;&#24314;&#31435;&#20132;&#26131;&#32773;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Non-Fungible-Token (NFT) market has experienced explosive growth in recent years. According to DappRadar, the total transaction volume on OpenSea, the largest NFT marketplace, reached 34.7 billion dollars in February 2023. However, the NFT market is mostly unregulated and there are significant concerns about money laundering, fraud and wash trading. Amateur traders and retail investors comprise a significant fraction of the NFT market. Hence it is important that researchers highlight the relevant risks involved in NFT trading. In this paper, we attempt to uncover common fraudulent behaviors such as wash trading that could mislead other traders. Using market data, we design quantitative features from the network, monetary, and temporal perspectives that are fed into K-means clustering unsupervised learning algorithm to sort traders into groups. Lastly, we discuss the clustering results' significance and how regulations can reduce undesired behaviors. Our work can potentially help re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04641</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#20302;&#36164;&#28304;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Generalizable Low-Resource Activity Recognition with Diverse and Discriminative Representation Learning. (arXiv:2306.04641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04641
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#32771;&#34385;&#22810;&#26679;&#21270;&#21644;&#21306;&#20998;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#26159;&#19968;&#39033;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#20174;&#20154;&#31867;&#20256;&#24863;&#22120;&#35835;&#25968;&#20013;&#35782;&#21035;&#21160;&#20316;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#20805;&#36275;&#30340;&#25968;&#25454;&#26159;&#35757;&#32451;&#36890;&#29992;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#30340;&#20851;&#38190;&#38590;&#28857;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#32447;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#30340;&#23450;&#21046;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DDLearn&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#32771;&#34385;&#24046;&#24322;&#24615;&#21644;&#27495;&#35270;&#24615;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36890;&#29992;&#20302;&#36164;&#28304;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a time series classification task that focuses on identifying the motion patterns from human sensor readings. Adequate data is essential but a major bottleneck for training a generalizable HAR model, which assists customization and optimization of online web applications. However, it is costly in time and economy to collect large-scale labeled data in reality, i.e., the low-resource challenge. Meanwhile, data collected from different persons have distribution shifts due to different living habits, body shapes, age groups, etc. The low-resource and distribution shift challenges are detrimental to HAR when applying the trained model to new unseen subjects. In this paper, we propose a novel approach called Diverse and Discriminative representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn simultaneously considers diversity and discrimination learning. With the constructed self-supervised learning task, DDLearn enlarges the data dive
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.03604</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20419;&#36827;&#31639;&#27861;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21253;&#21547;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#25351;&#20196;&#26469;&#21327;&#21161;&#31639;&#27861;&#20195;&#29702;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#21487;&#33021;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21482;&#33021;&#37096;&#32626;&#22312;&#36828;&#31243;&#20113;&#26381;&#21153;&#22120;&#33410;&#28857;&#19978;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21830;&#19994;LLMs&#21487;&#33021;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20351;&#29992;&#39057;&#29575;&#25910;&#36153;&#12290;&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#26597;&#35810;LLMs&#20197;&#23436;&#25104;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#32423;&#25351;&#20196;&#12290;&#22312;&#28041;&#21450;&#35268;&#21010;&#23376;&#30446;&#26631;&#30340;4&#20010;MiniGrid&#29615;&#22659;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#25552;&#21319;&#20102;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.03414</link><description>&lt;p&gt;
DreamSparse: &#21033;&#29992; 2D &#25193;&#25955;&#27169;&#22411;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#21512;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views. (arXiv:2306.03414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#37327;&#35270;&#35282;&#20013;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#25110;&#22312;&#27492;&#31867;&#23569;&#35270;&#35282;&#35774;&#32622;&#20013;&#38656;&#35201;&#36880;&#20010;&#23545;&#35937;&#20248;&#21270;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#20449;&#24687;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#26469;&#21512;&#25104;&#26032;&#39062;&#30340;&#35270;&#35282;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;2D &#25193;&#25955;&#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#65292;&#23548;&#33268;&#22270;&#20687;&#21512;&#25104;&#22833;&#30495;&#65292;&#24433;&#21709;&#20102;&#22270;&#20687;&#30340;&#35782;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DreamSparse&#65292;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#20960;&#20309;&#21644;&#35782;&#21035;&#32852;&#21512;&#19968;&#33268;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamSparse &#21253;&#25324;&#19968;&#20010;&#20960;&#20309;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#35270;&#35282;&#33719;&#21462; 3D &#29305;&#24449;&#20316;&#20026; 3D &#20808;&#39564;&#65292;&#38543;&#21518;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#23558;&#36825;&#20123; 3D &#29305;&#24449;&#22270;&#36716;&#25442;&#20026;&#29983;&#25104;&#36807;&#31243;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#28982;&#21518;&#29992;&#20110;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#25351;&#23548;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DreamSparse &#22312;&#23569;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29289;&#20307;&#20960;&#20309;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;MSInterpreter&#65292;&#20854;&#20013;&#21253;&#25324;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#25104;&#30340;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#23454;&#29616;&#23545;GNN&#33258;&#25105;&#35299;&#37322;&#12290;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02081</link><description>&lt;p&gt;
&#20449;&#24687;&#20256;&#36882;&#36873;&#25321;&#65306;&#38754;&#21521;&#22270;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;GNN&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Message-passing selection: Towards interpretable GNNs for graph classification. (arXiv:2306.02081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;MSInterpreter&#65292;&#20854;&#20013;&#21253;&#25324;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#25104;&#30340;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#23454;&#29616;&#23545;GNN&#33258;&#25105;&#35299;&#37322;&#12290;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#25512;&#29702;&#33539;&#24335;&#65292;&#31216;&#20026;MSInterpreter&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#26696;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#22522;&#32447;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#21516;&#65292;MSInterpreter&#25552;&#20379;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#36873;&#25321;&#26041;&#26696;(MSScheme)&#65292;&#20197;&#36873;&#25321;GNN&#30340;&#20851;&#38190;&#36335;&#24452;&#36827;&#34892;&#28040;&#24687;&#32858;&#21512;&#65292;&#26088;&#22312;&#23454;&#29616;&#33258;&#25105;&#35299;&#37322;&#32780;&#19981;&#26159;&#20107;&#21518;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;MSScheme&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32771;&#34385;&#39321;&#33609;&#32467;&#26500;&#21644;&#33410;&#28857;&#23884;&#20837;&#32452;&#20214;&#26469;&#35745;&#31639;&#28040;&#24687;&#32858;&#21512;&#36335;&#24452;&#30340;&#26435;&#37325;&#22240;&#23376;&#65292;&#20854;&#20013;&#32467;&#26500;&#22522;&#26088;&#22312;&#26435;&#34913;&#33410;&#28857;&#24341;&#36215;&#30340;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#26435;&#37325;&#22240;&#23376;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#33410;&#28857;&#23884;&#20837;&#22522;&#30528;&#30524;&#20110;&#36890;&#36807;&#21333;&#23618;GNN&#33719;&#24471;&#30340;&#33410;&#28857;&#23884;&#20837;&#26469;&#35745;&#31639;&#26435;&#37325;&#22240;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22270;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we strive to develop an interpretable GNNs' inference paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme readily applicable to various GNNs' baselines. Unlike the most existing explanation methods, MSInterpreter provides a Message-passing Selection scheme(MSScheme) to select the critical paths for GNNs' message aggregations, which aims at reaching the self-explaination instead of post-hoc explanations. In detail, the elaborate MSScheme is designed to calculate weight factors of message aggregation paths by considering the vanilla structure and node embedding components, where the structure base aims at weight factors among node-induced substructures; on the other hand, the node embedding base focuses on weight factors via node embeddings obtained by one-layer GNN.Finally, we demonstrate the effectiveness of our approach on graph classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;X&#23556;&#32447;&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.02055</link><description>&lt;p&gt;
X-Ray&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies on X-Ray Imaging, MRI and Nuclear Imaging. (arXiv:2306.02055v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;X&#23556;&#32447;&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26159;&#21307;&#23398;&#31185;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#28041;&#21450;&#21508;&#31181;&#24418;&#24335;&#30340;&#36752;&#23556;&#26469;&#25429;&#33719;&#36523;&#20307;&#20869;&#37096;&#32452;&#32455;&#21644;&#22120;&#23448;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#22270;&#20687;&#20026;&#20020;&#24202;&#35786;&#26029;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;X&#23556;&#32447;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21457;&#29616;&#20005;&#37325;&#30142;&#30149;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35780;&#20272;&#21644;&#23384;&#20648;&#36825;&#20123;&#22270;&#20687;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29992;&#20110;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#22312;&#26412;&#32508;&#36848;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25506;&#35752;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22914;&#20309;&#36890;&#36807;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#24110;&#21161;&#30142;&#30149;&#26816;&#27979;&#12290;CNN&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#20687;&#29305;&#24449;&#24182;&#35782;&#21035;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#26816;&#27979;&#30340;&#27169;&#24335;&#32780;&#33719;&#24471;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#23398;&#25104;&#20687;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of medical imaging is an essential aspect of the medical sciences, involving various forms of radiation to capture images of the internal tissues and organs of the body. These images provide vital information for clinical diagnosis, and in this chapter, we will explore the use of X-ray, MRI, and nuclear imaging in detecting severe illnesses. However, manual evaluation and storage of these images can be a challenging and time-consuming process. To address this issue, artificial intelligence (AI)-based techniques, particularly deep learning (DL), have become increasingly popular for systematic feature extraction and classification from imaging modalities, thereby aiding doctors in making rapid and accurate diagnoses. In this review study, we will focus on how AI-based approaches, particularly the use of Convolutional Neural Networks (CNN), can assist in disease detection through medical imaging technology. CNN is a commonly used approach for image analysis due to its ability to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#22686;&#21152;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20294;&#38656;&#35201;&#27880;&#24847;&#29983;&#25104;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.02019</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Data Augmentation. (arXiv:2306.02019v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#22686;&#21152;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20294;&#38656;&#35201;&#27880;&#24847;&#29983;&#25104;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65288;GAN&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#26159;&#25193;&#23637;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;GAN&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;&#32593;&#32476;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#24182;&#30001;&#37492;&#21035;&#22120;&#32593;&#32476;&#35780;&#20272;&#20854;&#19982;&#30495;&#23454;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#26469;&#24037;&#20316;&#12290;&#37492;&#21035;&#22120;&#32593;&#32476;&#34987;&#25945;&#23548;&#21435;&#21306;&#20998;&#30495;&#23454;&#21644;&#21512;&#25104;&#26679;&#26412;&#65292;&#29983;&#25104;&#22120;&#31995;&#32479;&#21017;&#34987;&#35757;&#32451;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;&#25968;&#25454;&#12290;&#36825;&#20010;&#36807;&#31243;&#19968;&#30452;&#37325;&#22797;&#65292;&#30452;&#21040;&#29983;&#25104;&#22120;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#25968;&#25454;&#26080;&#27861;&#21306;&#20998;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;GAN&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#22686;&#24378;&#12289;&#22270;&#20687;&#21019;&#24314;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#29992;&#20110;&#22686;&#21152;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#22312;&#33719;&#21462;&#22823;&#37327;&#30495;&#23454;&#25968;&#25454;&#22256;&#38590;&#25110;&#19981;&#36947;&#24503;&#30340;&#24773;&#20917;&#19979;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GAN&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#38656;&#35201;&#35880;&#24910;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to expand the available dataset for training AI models in the medical field is through the use of Generative Adversarial Networks (GANs) for data augmentation. GANs work by employing a generator network to create new data samples that are then assessed by a discriminator network to determine their similarity to real samples. The discriminator network is taught to differentiate between actual and synthetic samples, while the generator system is trained to generate data that closely resemble real ones. The process is repeated until the generator network can produce synthetic data that is indistinguishable from genuine data. GANs have been utilized in medical image analysis for various tasks, including data augmentation, image creation, and domain adaptation. They can generate synthetic samples that can be used to increase the available dataset, especially in cases where obtaining large amounts of genuine data is difficult or unethical. However, it is essential to note that the us
&lt;/p&gt;</description></item><item><title>SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01981</link><description>&lt;p&gt;
SGEM&#65306;&#36890;&#36807;&#24207;&#21015;&#32423;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#23454;&#29616;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SGEM: Test-Time Adaptation for Automatic Speech Recognition via Sequential-Level Generalized Entropy Minimization. (arXiv:2306.01981v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01981
&lt;/p&gt;
&lt;p&gt;
SGEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#21644;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#35843;&#25972;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#20027;&#27969;ASR&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#36716;&#21464;&#26102;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#32463;&#24120;&#26292;&#38706;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#26377;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#36866;&#24212;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#23454;&#20363;&#12290;&#23613;&#31649;&#26377;&#20102;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#36138;&#24515;&#35299;&#30721;&#65292;&#24182;&#22312;&#24103;&#32423;&#21035;&#19978;&#36328;&#36234;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#22312;&#27169;&#22411;&#36755;&#20986;&#30340;&#24207;&#21015;&#24615;&#36136;&#19979;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20986;&#20110;&#36825;&#20010;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;TTA&#26694;&#26550;&#65292;&#31216;&#20026;SGEM&#65292;&#29992;&#20110;&#19968;&#33324;ASR&#27169;&#22411;&#12290;&#20026;&#20102;&#22788;&#29702;&#24207;&#21015;&#36755;&#20986;&#65292;SGEM&#39318;&#20808;&#21033;&#29992;&#27874;&#26463;&#25628;&#32034;&#26469;&#25506;&#32034;&#20505;&#36873;&#36755;&#20986;&#26631;&#24535;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#20449;&#30340;&#26631;&#24535;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#24191;&#20041;&#29109;&#26368;&#23567;&#21270;&#21644;&#36127;&#25277;&#26679;&#20316;&#20026;&#26080;&#30417;&#30563;&#30446;&#26631;&#26469;&#36866;&#24212;&#27169;&#22411;&#12290;&#22312;&#21508;&#31181;&#39046;&#22495;&#30340;&#36716;&#21464;&#19979;&#65292;SGEM&#23454;&#29616;&#20102;&#19977;&#31181;&#20027;&#27969;ASR&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) models are frequently exposed to data distribution shifts in many real-world scenarios, leading to erroneous predictions. To tackle this issue, an existing test-time adaptation (TTA) method has recently been proposed to adapt the pre-trained ASR model on unlabeled test instances without source data. Despite decent performance gain, this work relies solely on naive greedy decoding and performs adaptation across timesteps at a frame level, which may not be optimal given the sequential nature of the model output. Motivated by this, we propose a novel TTA framework, dubbed SGEM, for general ASR models. To treat the sequential output, SGEM first exploits beam search to explore candidate output logits and selects the most plausible one. Then, it utilizes generalized entropy minimization and negative sampling as unsupervised objectives to adapt the model. SGEM achieves state-of-the-art performance for three mainstream ASR models under various domain shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.01788</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Responsible Design Patterns for Machine Learning Pipelines. (arXiv:2306.01788v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#24615;&#21644;&#20844;&#27491;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#24182;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36947;&#24503;&#23454;&#36341;&#25972;&#21512;&#21040;&#20154;&#24037;&#26234;&#33021;(AI)&#24320;&#21457;&#36807;&#31243;&#20013;&#23545;&#20110;&#30830;&#20445;AI&#30340;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36127;&#36131;&#20219;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;AI&#20262;&#29702;&#28041;&#21450;&#23558;&#20262;&#29702;&#21407;&#21017;&#24212;&#29992;&#20110;AI&#31995;&#32479;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#23545;&#20110;&#20943;&#36731;&#19982;AI&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20260;&#23475;&#65288;&#22914;&#31639;&#27861;&#20559;&#35265;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#36127;&#36131;&#20219;&#35774;&#35745;&#27169;&#24335;&#65288;RDPs&#65289;&#23545;&#20110;&#30830;&#20445;&#20262;&#29702;&#21644;&#20844;&#24179;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#23558;RDPs&#32435;&#20837;ML&#27969;&#31243;&#20013;&#65292;&#20197;&#20943;&#36731;&#39118;&#38505;&#24182;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#20262;&#29702;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#26032;&#30340;&#36127;&#36131;&#20219;AI&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#36890;&#36807;&#23545;AI&#20262;&#29702;&#21644;&#25968;&#25454;&#31649;&#29702;&#19987;&#23478;&#30340;&#35843;&#26597;&#30830;&#23450;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#21453;&#39304;&#30340;&#23454;&#38469;&#24773;&#20917;&#36827;&#34892;&#39564;&#35777;&#12290;&#35813;&#26694;&#26550;&#25351;&#23548;AI&#24320;&#21457;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20915;&#31574;&#32773;&#22312;AI&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#23454;&#26045;&#20262;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating ethical practices into the AI development process for artificial intelligence (AI) is essential to ensure safe, fair, and responsible operation. AI ethics involves applying ethical principles to the entire life cycle of AI systems. This is essential to mitigate potential risks and harms associated with AI, such as algorithm biases. To achieve this goal, responsible design patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee ethical and fair outcomes. In this paper, we propose a comprehensive framework incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical development of AI systems. Our framework comprises new responsible AI design patterns for ML pipelines identified through a survey of AI ethics and data management experts and validated through real-world scenarios with expert feedback. The framework guides AI developers, data scientists, and policy-makers to implement ethical practices in AI development and deploy responsibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.19075</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#27169;&#20223;&#23398;&#20064;&#19982;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data. (arXiv:2305.19075v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#25805;&#20316;&#29289;&#20307;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#35821;&#35328;&#26465;&#20214;&#26041;&#27861;&#22312;&#29087;&#24713;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#35774;&#32622;&#26041;&#38754;&#36935;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#38646;-shot&#35774;&#32622;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CALVIN&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;-shot&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#23436;&#25104;&#20219;&#21153;&#30340;&#24179;&#22343;&#38271;&#24230;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#8212;&#8212;parity&#26657;&#20934;&#65292;&#20854;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#20013;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;parity&#26657;&#20934;&#65292;&#24182;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#26680;&#32858;&#21464;&#25511;&#21046;&#31561;&#39046;&#22495;&#20013;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18655</link><description>&lt;p&gt;
Parity&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Parity Calibration. (arXiv:2305.18655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#8212;&#8212;parity&#26657;&#20934;&#65292;&#20854;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#20013;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;parity&#26657;&#20934;&#65292;&#24182;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#26680;&#32858;&#21464;&#25511;&#21046;&#31561;&#39046;&#22495;&#20013;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#20915;&#31574;&#32773;&#21487;&#33021;&#26356;&#20851;&#27880;&#26410;&#26469;&#35266;&#27979;&#20540;&#26159;&#21542;&#27604;&#24403;&#21069;&#20540;&#22686;&#21152;&#25110;&#20943;&#23569;&#65292;&#32780;&#19981;&#26159;&#26410;&#26469;&#35266;&#27979;&#20540;&#30340;&#23454;&#38469;&#20540;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24179;&#31561;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#23427;&#25429;&#25417;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#20943;&#20107;&#20214;&#30340;&#26657;&#20934;&#39044;&#27979;&#30446;&#26631;&#12290;&#24179;&#31561;&#27010;&#29575;&#21487;&#20197;&#20174;&#36755;&#20986;&#30340;&#39044;&#27979;&#20998;&#24067;&#20013;&#25552;&#21462;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#36825;&#31181;&#31574;&#30053;&#23548;&#33268;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21644;&#24046;&#21170;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#21407;&#20219;&#21153;&#26159;&#22238;&#24402;&#65292;&#20294;&#24179;&#31561;&#26657;&#20934;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#20108;&#36827;&#21046;&#26657;&#20934;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#20108;&#36827;&#21046;&#26657;&#20934;&#26041;&#27861;&#23454;&#29616;&#20102;&#24179;&#31561;&#26657;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#27969;&#34892;&#30149;&#23398;&#12289;&#22825;&#27668;&#39044;&#25253;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26680;&#32858;&#21464;&#25511;&#21046;&#30340;&#23454;&#38469;&#26696;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a sequential regression setting, a decision-maker may be primarily concerned with whether the future observation will increase or decrease compared to the current one, rather than the actual value of the future observation. In this context, we introduce the notion of parity calibration, which captures the goal of calibrated forecasting for the increase-decrease (or "parity") event in a timeseries. Parity probabilities can be extracted from a forecasted distribution for the output, but we show that such a strategy leads to theoretical unpredictability and poor practical performance. We then observe that although the original task was regression, parity calibration can be expressed as binary calibration. Drawing on this connection, we use an online binary calibration method to achieve parity calibration. We demonstrate the effectiveness of our approach on real-world case studies in epidemiology, weather forecasting, and model-based control in nuclear fusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17155</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#20013;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#38271;&#26399;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stability of implicit neural networks for long-term forecasting in dynamical systems. (arXiv:2305.17155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#31283;&#23450;&#24615;&#29305;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21152;&#20837;&#20102;&#30828;&#24615;&#32422;&#26463;&#26469;&#20445;&#35777;&#20854;&#26435;&#37325;&#31283;&#23450;&#24615;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#38271;&#26399;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29289;&#29702;&#20449;&#21495;&#26159;&#20559;&#24494;&#20998;&#26041;&#31243;&#30740;&#31350;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#20026;&#20102;&#35268;&#36991;&#20256;&#32479;&#27714;&#35299;&#22120;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20204;&#37117;&#22522;&#20110;&#33258;&#22238;&#24402;&#26041;&#27861;&#24182;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#21463;&#38544;&#24335;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#29305;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31283;&#23450;&#30340;&#33258;&#22238;&#24402;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#20540;&#26041;&#26696;&#30340;&#31283;&#23450;&#24615;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#29702;&#35770;&#26469;&#20445;&#35777;&#32593;&#32476;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#12290;&#23427;&#23548;&#33268;&#25105;&#20204;&#23545;&#20854;&#26435;&#37325;&#28155;&#21152;&#20102;&#30828;&#24615;&#32422;&#26463;&#65292;&#24182;&#22312;&#28508;&#31354;&#38388;&#20013;&#20256;&#25773;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#36755;&#36816;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38271;&#26399;&#39044;&#27979;&#19978;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting physical signals in long time range is among the most challenging tasks in Partial Differential Equations (PDEs) research. To circumvent limitations of traditional solvers, many different Deep Learning methods have been proposed. They are all based on auto-regressive methods and exhibit stability issues. Drawing inspiration from the stability property of implicit numerical schemes, we introduce a stable auto-regressive implicit neural network. We develop a theory based on the stability definition of schemes to ensure the stability in forecasting of this network. It leads us to introduce hard constraints on its weights and propagate the dynamics in the latent space. Our experimental results validate our stability property, and show improved results at long-term forecasting for two transports PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.16317</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#24182;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Parallel Sampling of Diffusion Models. (arXiv:2305.16317v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;ParaDiGMS&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#22788;&#29702;&#22810;&#20010;&#27493;&#39588;&#26469;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#20351;&#35745;&#31639;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#37319;&#26679;&#36895;&#24230;&#32531;&#24930;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;1000&#27425;&#39034;&#24207;&#21435;&#22122;&#27493;&#39588;&#25165;&#33021;&#24471;&#21040;&#19968;&#20010;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20132;&#25442;&#35745;&#31639;&#26426;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29468;&#27979;&#26410;&#26469;&#30340;&#21435;&#22122;&#27493;&#39588;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#36880;&#27493;&#32454;&#21270;&#33267;&#25910;&#25947;&#30340;Picard&#36845;&#20195;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#21457;&#29616;&#65306;&#23613;&#31649;&#21435;&#22122;&#27493;&#39588;&#26377;&#39034;&#24207;&#24615;&#65292;&#20294;&#20173;&#28982;&#21487;&#20197;&#24182;&#34892;&#37319;&#26679;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ParaDiGMS&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#24182;&#34892;&#26041;&#24335;&#21435;&#22122;&#22810;&#20010;&#27493;&#39588;&#21152;&#36895;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;&#12290;ParaDiGMS&#26159;&#31532;&#19968;&#20010;&#22312;&#35745;&#31639;&#22788;&#29702;&#36895;&#24230;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#23454;&#29616;&#24179;&#34913;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#65292;&#29978;&#33267;&#36824;&#20860;&#23481;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models but suffer from slow sampling, often taking 1000 sequential denoising steps for one sample. As a result, considerable efforts have been directed toward reducing the number of denoising steps, but these methods hurt sample quality. Instead of reducing the number of denoising steps (trading quality for speed), in this paper we explore an orthogonal approach: can we run the denoising steps in parallel (trading compute for speed)? In spite of the sequential nature of the denoising steps, we show that surprisingly it is possible to parallelize sampling via Picard iterations, by guessing the solution of future denoising steps and iteratively refining until convergence. With this insight, we present ParaDiGMS, a novel method to accelerate the sampling of pretrained diffusion models by denoising multiple steps in parallel. ParaDiGMS is the first diffusion sampling method that enables trading compute for speed and is even compatible with existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#30340;&#35821;&#20041;&#24212;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19982;&#20154;&#31867;&#31526;&#21495;&#25512;&#29702;&#19981;&#21516;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#24182;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;</title><link>http://arxiv.org/abs/2305.14825</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19978;&#19979;&#25991;&#35821;&#20041;&#25512;&#29702;&#22120;&#32780;&#19981;&#26159;&#31526;&#21495;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners. (arXiv:2305.14825v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#30340;&#35821;&#20041;&#24212;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19982;&#20154;&#31867;&#31526;&#21495;&#25512;&#29702;&#19981;&#21516;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#24182;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20986;&#33394;&#30340;&#24212;&#29992;&#24615;&#33021;&#22791;&#21463;&#25512;&#23815;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#20551;&#35774;&#65306;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#21040;&#30340;"&#35821;&#20041;"&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19982;&#20154;&#31867;&#30340;&#31526;&#21495;&#25512;&#29702;&#36807;&#31243;&#19981;&#21516;&#65292;LLMs&#30340;&#35821;&#20041;&#34920;&#31034;&#21487;&#20197;&#22312;&#35821;&#35328;&#24207;&#21015;&#20013;&#24314;&#31435;&#24378;&#36830;&#25509;&#65292;&#22240;&#27492;&#32452;&#25104;&#19968;&#20010;&#34920;&#38754;&#36923;&#36753;&#38142;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#23558;&#35821;&#20041;&#20174;&#35821;&#35328;&#25512;&#29702;&#36807;&#31243;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#25512;&#29702;&#33021;&#21147;&#65306;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#35821;&#22659;&#20013;&#65292;&#35821;&#20041;&#23545;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24403;&#35821;&#20041;&#19982;&#24120;&#35782;&#19968;&#33268;&#26102;&#65292;LLMs&#30340;&#34920;&#29616;&#26356;&#20339;&#65292;&#20294;&#22312;&#35299;&#20915;&#31526;&#21495;&#25110;&#21453;&#24120;&#35782;&#25512;&#29702;&#38382;&#39064;&#19978;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent few-shot reasoning capabilities of Large Language Models (LLMs) have excited the natural language and machine learning community over recent years. Despite of numerous successful applications, the underlying mechanism of such in-context capabilities still remains unclear. In this work, we hypothesize that the learned \textit{semantics} of language tokens do the most heavy lifting during the reasoning process. Different from human's symbolic reasoning process, the semantic representations of LLMs could create strong connections among tokens, thus composing a superficial logical chain. To test our hypothesis, we decouple semantics from the language reasoning process and evaluate three kinds of reasoning abilities, i.e., deduction, induction and abduction. Our findings reveal that semantics play a vital role in LLMs' in-context reasoning -- LLMs perform significantly better when semantics are consistent with commonsense but struggle to solve symbolic or counter-commonsense re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.12837</link><description>&lt;p&gt;
&#25429;&#25417;&#20419;&#38144;&#26399;&#38388;&#30340;&#36716;&#21270;&#29575;&#27874;&#21160;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;CVR&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#35757;&#32451;&#33391;&#22909;&#30340;CVR&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20419;&#38144;&#26399;&#38388;&#20063;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#20256;&#32479;&#26041;&#27861;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#24320;&#21457;&#26367;&#20195;&#24314;&#27169;&#25216;&#26415;&#29992;&#20110;CVR&#39044;&#27979;&#12290;&#35266;&#23519;&#21040;&#19981;&#21516;&#20419;&#38144;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#30340;&#36141;&#20080;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#20197;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#65288;HDR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#26816;&#32034;&#21382;&#21490;&#19978;&#30456;&#20284;&#30340;&#20419;&#38144;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#33719;&#21462;&#30340;&#25968;&#25454;&#24494;&#35843;CVR&#39044;&#27979;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#12290;HDR&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#33258;&#21160;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11699</link><description>&lt;p&gt;
RGCVAE&#65306;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#34920;&#29616;&#20986;&#26576;&#20123;&#39044;&#23450;&#29305;&#24615;&#30340;&#20998;&#23376;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#12290;&#28145;&#24230;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#26368;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;RGCVAE&#65306;&#65288;i&#65289;&#21033;&#29992;&#20840;&#26032;&#30340;&#24378;&#22823;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#32534;&#30721;&#32593;&#32476;&#65307;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35299;&#30721;&#32452;&#20214;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#25968;&#31181;&#26368;&#20808;&#36827;&#30340;VAE&#26041;&#27861;&#30456;&#27604;&#65292;RGCVAE&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10782</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#20540;&#22823;&#23567;&#27604;&#36739;&#25928;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numeric Magnitude Comparison Effects in Large Language Models. (arXiv:2305.10782v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24182;&#27809;&#26377;&#21306;&#20998;&#20986;&#25991;&#23383;&#20013;&#30340;&#25968;&#23383;&#65292;&#32780;&#25968;&#23383;&#22312;&#25991;&#26412;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#23545;&#25968;&#23383;&#21644;&#21333;&#35789;&#26377;&#30528;&#19981;&#21516;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#34892;&#20026;&#35282;&#24230;&#25506;&#31350;&#27969;&#34892;&#30340;LLMs&#33021;&#22815;&#22810;&#22909;&#22320;&#25429;&#25417;&#25968;&#23383;&#30340;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;$4&lt;5$&#65289;&#12290;&#20197;&#24448;&#23545;LLMs&#34920;&#24449;&#33021;&#21147;&#30340;&#30740;&#31350;&#21697;&#35780;&#20182;&#20204;&#26159;&#21542;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#27604;&#22914;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#25972;&#20307;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#19982;&#35748;&#30693;&#31185;&#23398;&#30456;&#20851;&#30340;&#19981;&#21516;&#38382;&#39064;&#65306;LLMs&#25968;&#23383;&#34920;&#24449;&#19982;&#20154;&#31867;&#35821;&#35328;&#29992;&#25143;&#30340;&#34920;&#29616;&#26377;&#22810;&#25509;&#36817;&#65292;&#20182;&#20204;&#36890;&#24120;&#34920;&#29616;&#20986;&#36317;&#31163;&#12289;&#22823;&#23567;&#21644;&#27604;&#20363;&#25928;&#24212;? &#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#36830;&#25509;&#20551;&#35774;&#23558;&#25968;&#23383;&#21333;&#35789;&#21644;&#25968;&#23383;&#30340;&#27169;&#22411;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#20154;&#31867;&#21453;&#24212;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#31034;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#20855;&#26377;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4 &lt; 5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#24615;&#23545;&#25239;&#24615;&#32447;&#24615;MDP&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08841</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#22312;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Optimistic Proximal Policy Optimization in Linear Markov Decision Processes. (arXiv:2305.08841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#24615;&#23545;&#25239;&#24615;&#32447;&#24615;MDP&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#26159;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#23613;&#31649;PPO&#24456;&#25104;&#21151;&#65292;&#20294;&#26159;&#23545;&#20110;PPO&#21450;&#20854;&#20048;&#35266;&#21464;&#31181;&#26159;&#21542;&#33021;&#26377;&#25928;&#35299;&#20915;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDPs)&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#21608;&#26399;&#25932;&#23545;&#32447;&#24615;MDP&#65292;&#24182;&#20026;&#20854;&#24314;&#31435;&#20102;&#19968;&#20010;$\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$&#30340;&#21518;&#24724;&#20540;&#12290;&#20854;&#20013;$d$&#26159;&#32447;&#24615;MDPs&#30340;&#29615;&#22659;&#32500;&#25968;&#65292;$H$&#26159;&#27599;&#20010;&#21608;&#26399;&#30340;&#38271;&#24230;&#65292;$K$&#26159;&#21608;&#26399;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#38543;&#26426;&#32447;&#24615;MDP&#21644;&#24102;&#26377;&#20840;&#20449;&#24687;&#30340;&#25932;&#23545;&#32447;&#24615;MDP&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#20855;&#26377;&#26032;&#39062;&#30340;&#22810;&#25209;&#27425;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proximal policy optimization (PPO) algorithm stands as one of the most prosperous methods in the field of reinforcement learning (RL). Despite its success, the theoretical understanding of PPO remains deficient. Specifically, it is unclear whether PPO or its optimistic variants can effectively solve linear Markov decision processes (MDPs), which are arguably the simplest models in RL with function approximation. To bridge this gap, we propose an optimistic variant of PPO for episodic adversarial linear MDPs with full-information feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each episode, and $K$ is the number of episodes. Compared with existing policy-based algorithms, we achieve the state-of-the-art regret bound in both stochastic linear MDPs and adversarial linear MDPs with full information. Additionally, our algorithm design features a novel multi-batched updating mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25581;&#31034;&#20986;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#22312;&#36825;&#19968;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08714</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks. (arXiv:2305.08714v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#23545;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25581;&#31034;&#20986;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#22312;&#36825;&#19968;&#26041;&#38754;&#20063;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#36827;&#21457;&#23637;&#20027;&#23548;&#20102;&#25552;&#31034;&#24037;&#31243;&#30456;&#20851;&#30740;&#31350;&#30340;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#25935;&#24863;&#24615;&#21644;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#26085;&#35821;&#36825;&#26679;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35821;&#35328;&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#20960;&#20010;&#20195;&#34920;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PLM&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#20998;&#26512;&#24403;&#21069;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#24778;&#20154;&#30340;&#24046;&#24322;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#27169;&#26495;&#21477;&#23376;&#32467;&#26500;&#30340;&#20462;&#25913;&#23548;&#33268;GPT-4&#30340;&#20934;&#30830;&#29575;&#20174;49.21&#19979;&#38477;&#21040;&#20102;25.44&#12290;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#39640;&#24615;&#33021;&#30340;GPT-4&#27169;&#22411;&#20063;&#23384;&#22312;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering relevance research has seen a notable surge in recent years, primarily driven by advancements in pre-trained language models and large language models. However, a critical issue has been identified within this domain: the inadequate of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM). These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model 
&lt;/p&gt;</description></item><item><title>InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14293</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Natural Language Instructions. (arXiv:2304.14293v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14293
&lt;/p&gt;
&lt;p&gt;
InstructCTG&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#28436;&#31034;&#26469;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#24182;&#28385;&#36275;&#19981;&#21516;&#32422;&#26463;&#26465;&#20214;&#30340;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#25628;&#32034;&#25110;&#24471;&#20998;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#24182;&#33021;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23427;&#20204;&#30340;&#29983;&#25104;&#20197;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#21508;&#31181;&#32422;&#26463;&#26465;&#20214;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#24102;&#32422;&#26463;&#35843;&#33410;&#30340;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#8212;&#8212;InstructCTG&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#32422;&#26463;&#28436;&#31034;&#26469;&#32435;&#20837;&#19981;&#21516;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#31995;&#21015;&#29616;&#25104;&#30340;NLP&#24037;&#20855;&#21644;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#21462;&#33258;&#28982;&#25991;&#26412;&#30340;&#28508;&#22312;&#32422;&#26463;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#24418;&#25104;&#24369;&#30417;&#30563;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#28155;&#21152;&#33258;&#28982;&#35821;&#35328;&#32422;&#26463;&#25551;&#36848;&#21644;&#23569;&#37327;&#28436;&#31034;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#32435;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#25628;&#32034;&#25110;&#24471;&#20998;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;InstructCTG &#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04178</link><description>&lt;p&gt;
DynGFN: &#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#36125;&#21494;&#26031;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;GFlowNets&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGFN: Towards Bayesian Inference of Gene Regulatory Networks with GFlowNets. (arXiv:2302.04178v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04178
&lt;/p&gt;
&lt;p&gt;
DynGFN&#26159;&#19968;&#31181;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#36827;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#19978;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#25512;&#26029;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65288;GRN&#65289;&#65292;&#35813;&#32593;&#32476;&#25551;&#36848;&#20102;&#25511;&#21046;&#22522;&#22240;&#34920;&#36798;&#21644;&#32454;&#32990;&#21151;&#33021;&#30340;&#22522;&#22240;&#21450;&#20854;&#20135;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20511;&#21161;RNA&#36895;&#24230;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;DynGFN&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#29983;&#25104;&#27969;&#32593;&#32476;&#65292;&#20351;&#29992;RNA&#36895;&#24230;&#25968;&#25454;&#25191;&#34892;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#24182;&#25429;&#25417;&#32593;&#32476;&#32467;&#26500;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying cyclic structure from dynamics, or on challenge (2) learning complex Bayesian posteriors over DAGs, but not both. In this paper we leverage the fact that it is possible to estimate the "velocity" of gene expression with RNA velocity techniques to develop an approach that addresses both challenges. Because we have
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03629</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#20027;&#20041;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03629
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#25972;&#29702;&#23454;&#36341;&#32463;&#24120;&#24573;&#30053;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#25764;&#22238;&#21644;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38750;&#21516;&#24847;&#32593;&#32476;&#29228;&#21462;&#26500;&#24314;&#30340;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#20840;&#38754;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#35780;&#20272;&#25152;&#24517;&#38656;&#30340;&#20803;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#21518;&#26399;&#35299;&#20915;&#38382;&#39064;&#65292;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#37319;&#29992;&#29702;&#30001;&#25110;&#26410;&#33021;&#25552;&#20379;&#36866;&#24403;&#24212;&#29992;&#30340;&#21512;&#36866;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#20027;&#21160;&#39046;&#22495;&#29305;&#23450;&#24314;&#35758;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#24182;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#23454;&#36341;&#21644;&#25351;&#21335;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#35265;&#35299;&#21644;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#20854;&#21019;&#24847;&#36807;&#31243;&#24182;&#26174;&#30528;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.01416</link><description>&lt;p&gt;
&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#30340;&#31070;&#32463;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Neural Insights for Digital Marketing Content Design. (arXiv:2302.01416v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23383;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#30340;&#35265;&#35299;&#21644;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#20854;&#21019;&#24847;&#36807;&#31243;&#24182;&#26174;&#30528;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#33829;&#38144;&#20013;&#65292;&#23581;&#35797;&#26032;&#30340;&#32593;&#31449;&#20869;&#23481;&#26159;&#25552;&#39640;&#23458;&#25143;&#21442;&#19982;&#24230;&#30340;&#20851;&#38190;&#26464;&#26438;&#20043;&#19968;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#25104;&#21151;&#30340;&#33829;&#38144;&#20869;&#23481;&#26159;&#19968;&#39033;&#25163;&#21160;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#20026;&#33829;&#38144;&#20154;&#21592;&#25552;&#20379;&#22522;&#20110;AI&#39537;&#21160;&#30340;&#21487;&#34892;&#24615;&#35265;&#35299;&#65292;&#20197;&#25913;&#36827;&#20854;&#21019;&#24847;&#36807;&#31243;&#65292;&#20174;&#32780;&#23558;&#20869;&#23481;&#21019;&#24314;&#21644;&#22312;&#32447;&#23454;&#39564;&#20043;&#38388;&#30340;&#24490;&#29615;&#38381;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23545;&#33829;&#38144;&#20869;&#23481;&#35774;&#35745;&#36827;&#34892;&#35780;&#20998;&#21644;&#25552;&#21462;&#35265;&#35299;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33829;&#38144;&#20869;&#23481;&#30340;&#21560;&#24341;&#21147;&#65292;&#21518;&#22788;&#29702;&#24402;&#22240;&#26041;&#27861;&#20026;&#33829;&#38144;&#20154;&#21592;&#29983;&#25104;&#26377;&#38024;&#23545;&#24615;&#30340;&#35265;&#35299;&#65292;&#20197;&#25913;&#21892;&#29305;&#23450;&#33829;&#38144;&#20301;&#32622;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#19981;&#20165;&#25351;&#20986;&#20102;&#24403;&#21069;&#32473;&#23450;&#20869;&#23481;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#36824;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#25552;&#20379;&#20102;&#35774;&#35745;&#24314;&#35758;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35780;&#20998;&#27169;&#22411;&#21644;&#35265;&#35299;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#19978;&#22343;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#30495;&#23454;&#32593;&#31449;&#19978;&#30340;&#23458;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital marketing, experimenting with new website content is one of the key levers to improve customer engagement. However, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles. This paper seeks to close the loop between content creation and online experimentation by offering marketers AI-driven actionable insights based on historical data to improve their creative process. We present a neural-network-based system that scores and extracts insights from a marketing content design, namely, a multimodal neural network predicts the attractiveness of marketing contents, and a post-hoc attribution method generates actionable insights for marketers to improve their content in specific marketing locations. Our insights not only point out the advantages and drawbacks of a given current content, but also provide design recommendations based on historical data. We show that our scoring model and insights work well both quantitatively 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#21644;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12601</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#36951;&#25022;&#36793;&#30028;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Regret Bounds for Markov Decision Processes with Recursive Optimized Certainty Equivalents. (arXiv:2301.12601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36882;&#24402;&#20248;&#21270;&#31561;&#20215;&#24615;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#25512;&#23548;&#20102;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#21644;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#31561;&#20215;&#24615;&#65288;OCE&#65289;&#26159;&#19968;&#31867;&#39118;&#38505;&#27979;&#37327;&#65292;&#28085;&#30422;&#20102;&#37325;&#35201;&#30340;&#23454;&#20363;&#65292;&#22914;&#29109;&#39118;&#38505;&#65292;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#21644;&#22343;&#20540;&#26041;&#24046;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#36882;&#24402;OCE&#30340;&#24773;&#33410;&#21270;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20540;&#36845;&#20195;&#21644;&#19978;&#32622;&#20449;&#30028;&#30340;&#26377;&#25928;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#30340;&#36951;&#25022;&#29575;&#23545;&#20110;&#24773;&#27468;&#25968;&#21644;&#21160;&#20316;&#25968;&#20855;&#26377;&#26368;&#20248;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimized certainty equivalent (OCE) is a family of risk measures that cover important examples such as entropic risk, conditional value-at-risk and mean-variance models. In this paper, we propose a new episodic risk-sensitive reinforcement learning formulation based on tabular Markov decision processes with recursive OCEs. We design an efficient learning algorithm for this problem based on value iteration and upper confidence bound. We derive an upper bound on the regret of the proposed algorithm, and also establish a minimax lower bound. Our bounds show that the regret rate achieved by our proposed algorithm has optimal dependence on the number of episodes and the number of actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#25311;&#20013;&#30340;&#25509;&#35302;&#21033;&#29992;&#25805;&#20316;&#31574;&#30053;&#65292;&#29992;&#20110;&#25509;&#35302;&#20016;&#23500;&#30340;&#23478;&#24237;&#20219;&#21153;&#65292;&#38646;-shot&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#65292;&#24182;&#26368;&#23567;&#38480;&#24230;&#22320;&#20943;&#23567;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#65292;&#21487;&#20197;&#36890;&#29992;&#20110;&#19981;&#21516;&#23610;&#23544;&#30340;&#30424;&#23376;&#12290;</title><link>http://arxiv.org/abs/2301.12587</link><description>&lt;p&gt;
&#22522;&#20110;&#35302;&#35273;&#30340;&#29289;&#20307;&#25554;&#20837;&#31574;&#30053;&#30340;&#38646;-shot&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Transfer of Haptics-Based Object Insertion Policies. (arXiv:2301.12587v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#25311;&#20013;&#30340;&#25509;&#35302;&#21033;&#29992;&#25805;&#20316;&#31574;&#30053;&#65292;&#29992;&#20110;&#25509;&#35302;&#20016;&#23500;&#30340;&#23478;&#24237;&#20219;&#21153;&#65292;&#38646;-shot&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#65292;&#24182;&#26368;&#23567;&#38480;&#24230;&#22320;&#20943;&#23567;&#20102;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#65292;&#21487;&#20197;&#36890;&#29992;&#20110;&#19981;&#21516;&#23610;&#23544;&#30340;&#30424;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35013;&#22635;&#27927;&#30871;&#26426;&#25110;&#25670;&#25918;&#20070;&#26550;&#31561;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#20013;&#33258;&#28982;&#22320;&#21033;&#29992;&#35302;&#35273;&#21453;&#39304;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#19987;&#27880;&#20110;&#36991;&#20813;&#24847;&#22806;&#35302;&#30896;&#65292;&#36890;&#24120;&#20381;&#38752;&#31574;&#30053;&#24615;&#25918;&#32622;&#30340;&#29615;&#22659;&#20256;&#24863;&#22120;&#12290;&#26368;&#36817;&#65292;&#24050;&#22312;&#27169;&#25311;&#20013;&#35757;&#32451;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#37096;&#32626;&#20102;&#21033;&#29992;&#25509;&#35302;&#30340;&#25805;&#20316;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26576;&#31181;&#24418;&#24335;&#30340;&#29616;&#23454;&#19990;&#30028;&#36866;&#24212;&#26469;&#24357;&#21512;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#21487;&#33021;&#37117;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#20013;&#35757;&#32451;&#21033;&#29992;&#25509;&#35302;&#30340;&#25805;&#20316;&#31574;&#30053;&#65292;&#29992;&#20110;&#23558;&#30424;&#23376;&#35013;&#20837;&#25554;&#27133;&#24335;&#25903;&#26550;&#31561;&#25509;&#35302;&#20016;&#23500;&#30340;&#23478;&#24237;&#20219;&#21153;&#65292;&#35813;&#31574;&#30053;&#22312;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36716;&#31227;&#21040;&#30495;&#23454;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#29616;&#36825;&#31181;&#38646;-shot&#36716;&#31227;&#25152;&#38656;&#30340;&#21508;&#31181;&#35201;&#32032;&#65292;&#20363;&#22914;&#26102;&#38388;&#24310;&#36831;&#24314;&#27169;&#65292;&#35760;&#24518;&#34920;&#31034;&#21644;&#22495;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#20197;&#26368;&#23567;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#24046;&#36317;&#36716;&#31227;&#65292;&#24182;&#26174;&#30528;&#20248;&#20110;&#21551;&#21457;&#24335;&#21644;&#23398;&#20064;&#22522;&#32447;&#12290;&#23427;&#36824;&#25512;&#24191;&#21040;&#19981;&#21516;&#22823;&#23567;&#30340;&#30424;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans naturally exploit haptic feedback during contact-rich tasks like loading a dishwasher or stocking a bookshelf. Current robotic systems focus on avoiding unexpected contact, often relying on strategically placed environment sensors. Recently, contact-exploiting manipulation policies have been trained in simulation and deployed on real robots. However, they require some form of real-world adaptation to bridge the sim-to-real gap, which might not be feasible in all scenarios. In this paper we train a contact-exploiting manipulation policy in simulation for the contact-rich household task of loading plates into a slotted holder, which transfers without any fine-tuning to the real robot. We investigate various factors necessary for this zero-shot transfer, like time delay modeling, memory representation, and domain randomization. Our policy transfers with minimal sim-to-real gap and significantly outperforms heuristic and learnt baselines. It also generalizes to plates of different s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.10029</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10029
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#20204;&#24819;&#21040;&#20687;&#8220;&#40481;&#34507;&#8221;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#26102;&#65292;&#36890;&#24120;&#20250;&#26377;&#19968;&#20010;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#24515;&#29702;&#22270;&#20687;&#12290;&#36825;&#31181;&#24120;&#35782;&#24615;&#30693;&#35782;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#26085;&#24120;&#29992;&#21697;&#30340;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#22914;&#20309;&#19982;&#23427;&#20204;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31995;&#32479;&#23545;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#27809;&#26377;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#27604;&#22914;&#35748;&#20026;&#40481;&#34507;&#40644;&#21253;&#22260;&#30528;&#22771;&#65292;&#37027;&#20040;&#23427;&#21487;&#33021;&#19981;&#24471;&#19981;&#37319;&#21462;&#33618;&#35884;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#35797;&#22270;&#25226;&#40481;&#34507;&#40644;&#21038;&#19979;&#22771;&#25918;&#20837;&#24179;&#24213;&#38149;&#20013;&#29006;&#29038;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36825;&#31181;&#26085;&#24120;&#29992;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#31181;&#26085;&#24120;&#29992;&#21697;&#12289;&#23427;&#20204;&#30340;&#37096;&#20214;&#20197;&#21450;&#36825;&#20123;&#37096;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT-3&#21644;Macaw&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#26576;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.05793</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#20248;&#21270;&#21644;&#37327;&#23376;&#36866;&#29992;&#24615;&#30340;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#20854;&#29289;&#29702;&#29305;&#24615;&#65288;&#20363;&#22914;&#23616;&#37096;&#24577;&#23494;&#24230;&#25110;&#26465;&#20214;&#30005;&#23548;&#65289;&#22312;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#21518;&#20316;&#20026;&#36755;&#20986;&#12290;&#19982;&#21453;&#21521;&#20256;&#25773;&#31867;&#20284;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;FNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;FNN&#20063;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#31995;&#32479;&#65292;&#24182;&#22312;&#26080;&#39044;&#22788;&#29702;&#25110;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21407;&#20301;&#20998;&#26512;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21518;&#65292;FNN&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#12290;&#23427;&#20204;&#30340;&#37327;&#23376;&#29305;&#24615;&#20063;&#24102;&#26469;&#20102;&#21508;&#31181;&#20248;&#21183;&#65306;&#37327;&#23376;&#30456;&#20851;&#24615;&#20351;&#32593;&#32476;&#36830;&#25509;&#26356;&#21152;&#36890;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#28040;&#22833;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#37327;&#23376;&#32416;&#32544;&#21017;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
&lt;/p&gt;</description></item><item><title>FARE&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#23454;&#29616;&#23454;&#38469;&#20445;&#35777;&#24182;&#20445;&#25345;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2210.07213</link><description>&lt;p&gt;
FARE: &#20855;&#26377;&#23454;&#38469;&#35777;&#20070;&#30340;&#21487;&#35777;&#26126;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FARE: Provably Fair Representation Learning with Practical Certificates. (arXiv:2210.07213v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07213
&lt;/p&gt;
&lt;p&gt;
FARE&#26159;&#31532;&#19968;&#20010;&#25552;&#20379;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#23454;&#29616;&#23454;&#38469;&#20445;&#35777;&#24182;&#20445;&#25345;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#20135;&#29983;&#20844;&#24179;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#30417;&#31649;&#25351;&#20196;&#24378;&#35843;&#38656;&#35201;&#25552;&#20379;&#23454;&#38469;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#65292;&#21363;&#22312;&#20219;&#20309;&#39044;&#22788;&#29702;&#25968;&#25454;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#30340;&#19981;&#20844;&#24179;&#24615;&#19978;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#19978;&#38480;&#65292;&#20174;&#32780;&#30452;&#25509;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#25552;&#20379;&#20445;&#35777;&#12290;&#21019;&#24314;&#36825;&#26679;&#30340;FRL&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#23578;&#26410;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;FARE&#65288;&#20855;&#26377;&#21463;&#38480;&#32534;&#30721;&#22120;&#30340;&#20844;&#24179;&#24615;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#20844;&#24179;&#24615;&#35777;&#20070;&#30340;FRL&#26041;&#27861;&#12290;FARE&#22522;&#20110;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#35265;&#65292;&#21363;&#38480;&#21046;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21487;&#20197;&#25512;&#23548;&#20986;&#23454;&#38469;&#30340;&#20445;&#35777;&#65292;&#21516;&#26102;&#20173;&#28982;&#20801;&#35768;&#36866;&#24403;&#30340;&#20934;&#30830;&#24230;-&#20844;&#24179;&#24230;&#26435;&#34913;&#65292;&#27604;&#22914;&#25105;&#20204;&#22522;&#20110;&#20844;&#24179;&#26641;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20135;&#29983;&#23454;&#38469;&#30340;&#35777;&#20070;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#24212;&#29992;&#20102;&#19968;&#31181;&#32479;&#35745;&#36807;&#31243;&#65292;&#35745;&#31639;&#26377;&#38480;&#26679;&#26412;&#30340;h&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair representation learning (FRL) is a popular class of methods aiming to produce fair classifiers via data preprocessing. Recent regulatory directives stress the need for FRL methods that provide practical certificates, i.e., provable upper bounds on the unfairness of any downstream classifier trained on preprocessed data, which directly provides assurance in a practical scenario. Creating such FRL methods is an important challenge that remains unsolved. In this work, we address that challenge and introduce FARE (Fairness with Restricted Encoders), the first FRL method with practical fairness certificates. FARE is based on our key insight that restricting the representation space of the encoder enables the derivation of practical guarantees, while still permitting favorable accuracy-fairness tradeoffs for suitable instantiations, such as one we propose based on fair trees. To produce a practical certificate, we develop and apply a statistical procedure that computes a finite sample h
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05044</link><description>&lt;p&gt;
&#24314;&#27169;&#25277;&#35937;&#30446;&#26631;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Predicting the Next Action by Modeling the Abstract Goal. (arXiv:2209.05044v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#21160;&#20316;&#30340;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#26377;&#20851;&#20110;&#21160;&#20316;&#23454;&#29616;&#30446;&#26631;&#30340;&#24863;&#30693;&#65292;&#21487;&#20197;&#38477;&#20302;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#26469;&#20943;&#23569;&#26410;&#26469;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#35270;&#35273;&#34920;&#24449;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25277;&#35937;&#30446;&#26631;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#21462;&#20915;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#24207;&#21015;&#65292;&#29992;&#20110;&#34892;&#21160;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25277;&#35937;&#30446;&#26631;&#35774;&#35745;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#21442;&#25968;&#26159;&#20351;&#29992;&#21464;&#20998;&#36882;&#24402;&#32593;&#32476;&#20272;&#35745;&#30340;&#12290;&#25105;&#20204;&#23545;&#19979;&#19968;&#20010;&#21160;&#20316;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#24182;&#24341;&#20837;&#30446;&#26631;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#30830;&#23450;&#20174;&#25277;&#35937;&#30446;&#26631;&#24471;&#20986;&#30340;&#26368;&#20339;&#20505;&#36873;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2209.01535</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#20013;&#30340;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Simulation-Assisted Optimization for Large-Scale Evacuation Planning with Congestion-Dependent Delays. (arXiv:2209.01535v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#36741;&#21161;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#25317;&#22581;&#20381;&#36182;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#22823;&#35268;&#27169;&#30095;&#25955;&#35745;&#21010;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30095;&#25955;&#35745;&#21010;&#26159;&#28798;&#38590;&#31649;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23558;&#36335;&#32447;&#21644;&#35843;&#24230;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#24179;&#22343;&#30095;&#25955;&#26102;&#38388;&#25110;&#30095;&#25955;&#23436;&#25104;&#26102;&#38388;&#31561;&#30446;&#26631;&#65292;&#26159;&#19968;&#20010;&#35745;&#31639;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIP-LNS&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#21644;&#25968;&#23398;&#20248;&#21270;&#26469;&#20248;&#21270;&#21508;&#31181;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;MIP-LNS-SIM&#26041;&#27861;&#65292;&#32467;&#21512;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#20197;&#20272;&#35745;&#30001;&#20110;&#25317;&#22581;&#32780;&#24341;&#36215;&#30340;&#24310;&#36831;&#65292;&#24182;&#25214;&#21040;&#32771;&#34385;&#36825;&#20123;&#24310;&#36831;&#30340;&#20248;&#21270;&#35745;&#21010;&#12290;&#25105;&#20204;&#20197;&#24471;&#20811;&#33832;&#26031;&#24030;&#20241;&#26031;&#39039;&#24066;&#30340;&#21704;&#37324;&#26031;&#21439;&#20316;&#20026;&#30740;&#31350;&#21306;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#30340;&#26102;&#38388;&#38480;&#21046;&#19979;&#65292;MIP-LNS&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#25351;&#26631;&#26041;&#38754;&#25214;&#21040;&#26356;&#22909;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#32771;&#34385;&#21040;&#25317;&#22581;&#20381;&#36182;&#30340;&#24310;&#36831;&#26102;&#65292;MIP-LNS-SIM&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;MIP-LNS&#12290;&#27492;&#22806;&#65292;MIP-LNS-SIM&#20855;&#26377;&#26126;&#26174;&#36739;&#20302;&#30340;&#27714;&#35299;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evacuation planning is a crucial part of disaster management. However, joint optimization of its two essential components, routing and scheduling, with objectives such as minimizing average evacuation time or evacuation completion time, is a computationally hard problem. To approach it, we present MIP-LNS, a scalable optimization method that utilizes heuristic search with mathematical optimization and can optimize a variety of objective functions. We also present the method MIP-LNS-SIM, where we combine agent-based simulation with MIP-LNS to estimate delays due to congestion, as well as, find optimized plans considering such delays. We use Harris County in Houston, Texas, as our study area. We show that, within a given time limit, MIP-LNS finds better solutions than existing methods in terms of three different metrics. However, when congestion dependent delay is considered, MIP-LNS-SIM outperforms MIP-LNS in multiple performance metrics. In addition, MIP-LNS-SIM has a significantly low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#20351;&#24471;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#36793;&#30028;&#28857;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#21040;&#20869;&#37096;&#28857;&#12290;</title><link>http://arxiv.org/abs/2207.02338</link><description>&lt;p&gt;
&#21033;&#29992;Retain-Resample-Release (R3)&#37319;&#26679;&#26041;&#27861;&#20943;&#36731;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20256;&#25773;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mitigating Propagation Failures in Physics-informed Neural Networks using Retain-Resample-Release (R3) Sampling. (arXiv:2207.02338v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#20351;&#24471;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#33021;&#22815;&#25104;&#21151;&#22320;&#20174;&#36793;&#30028;&#28857;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#21040;&#20869;&#37096;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#28041;&#21450;&#22797;&#26434;PDE&#30340;&#38382;&#39064;&#20013;&#65292;PINN&#26377;&#26102;&#20250;&#26080;&#27861;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#35299;&#12290;&#36825;&#21453;&#26144;&#22312;&#36817;&#26399;&#20851;&#20110;&#8220;&#22833;&#36133;&#27169;&#24335;&#8221;&#29305;&#24449;&#30340;&#20960;&#39033;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;&#32570;&#20047;&#20851;&#20110;PINN&#22833;&#36133;&#27169;&#24335;&#21644;&#37319;&#26679;&#31574;&#30053;&#20043;&#38388;&#36830;&#25509;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PINN&#22833;&#36133;&#27169;&#24335;&#30340;&#35270;&#35282;&#65292;&#21363;&#20551;&#35774;&#35757;&#32451;PINN&#22522;&#20110;&#20174;&#21021;&#22987;&#21644;/&#25110;&#36793;&#30028;&#26465;&#20214;&#28857;&#21040;&#20869;&#37096;&#28857;&#30340;&#35299;&#8220;&#20256;&#25773;&#8221;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23384;&#22312;&#20256;&#25773;&#22833;&#36133;&#65292;&#21017;&#37319;&#29992;&#31967;&#31957;&#37319;&#26679;&#31574;&#30053;&#30340;PINN&#21487;&#33021;&#20250;&#21345;&#22312;&#24179;&#20961;&#35299;&#19978;&#65292;&#34920;&#29616;&#20026;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;PDE&#27531;&#24046;&#22330;&#12290;&#20026;&#20102;&#20943;&#36731;&#20256;&#25773;&#22833;&#36133;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Retain-Resample-Release&#37319;&#26679;&#65288;R3&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#22312;&#39640;PD&#21306;&#22495;&#36880;&#27493;&#32047;&#31215;&#37325;&#21512;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of physics-informed neural networks (PINNs) in approximating partial differential equations (PDEs), PINNs can sometimes fail to converge to the correct solution in problems involving complicated PDEs. This is reflected in several recent studies on characterizing the "failure modes" of PINNs, although a thorough understanding of the connection between PINN failure modes and sampling strategies is missing. In this paper, we provide a novel perspective of failure modes of PINNs by hypothesizing that training PINNs relies on successful "propagation" of solution from initial and/or boundary condition points to interior points. We show that PINNs with poor sampling strategies can get stuck at trivial solutions if there are propagation failures, characterized by highly imbalanced PDE residual fields. To mitigate propagation failures, we propose a novel Retain-Resample-Release sampling (R3) algorithm that can incrementally accumulate collocation points in regions of high PD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;QMIX&#31639;&#27861;&#30340;&#21464;&#20307;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#23454;&#29616;&#25216;&#24039;&#65292;&#21457;&#29616;&#26631;&#20934;&#21270;&#20248;&#21270;&#23545;SMAC&#29615;&#22659;&#30340;&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#65307;&#21333;&#35843;&#24615;&#32422;&#26463;&#33021;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2102.03479</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#29616;&#25216;&#24039;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v19 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;QMIX&#31639;&#27861;&#30340;&#21464;&#20307;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#30340;&#23454;&#29616;&#25216;&#24039;&#65292;&#21457;&#29616;&#26631;&#20934;&#21270;&#20248;&#21270;&#23545;SMAC&#29615;&#22659;&#30340;&#34920;&#29616;&#26377;&#26174;&#33879;&#24433;&#21709;&#65307;&#21333;&#35843;&#24615;&#32422;&#26463;&#33021;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#22914;&#26426;&#22120;&#20154;&#38598;&#32676;&#25511;&#21046;&#21644;&#33258;&#20027;&#36710;&#36742;&#21327;&#35843;&#65292;&#37117;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20219;&#21153;&#12290;QMIX&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;MARL&#31639;&#27861;&#65292;&#24050;&#34987;&#29992;&#20316;&#22522;&#20934;&#29615;&#22659;&#65292;&#20363;&#22914;&#26143;&#38469;&#20105;&#38712;&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#36187;&#65288;SMAC&#65289;&#21644;&#21319;&#32423;&#29256;&#30340;Predator-Prey&#65288;DEPP&#65289;&#12290;&#26368;&#36817;&#65292;QMIX&#30340;&#21464;&#20307;&#26088;&#22312;&#25918;&#26494;QMIX&#30340;&#21333;&#35843;&#24615;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;SMAC&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#21464;&#20307;&#30340;&#20195;&#30721;&#32423;&#20248;&#21270;&#21644;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;(1)&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#21464;&#20307;&#30340;&#25913;&#36827;&#21463;&#21040;&#21508;&#31181;&#20195;&#30721;&#32423;&#20248;&#21270;&#30340;&#26174;&#33879;&#24433;&#21709;&#65307;(2)&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#26631;&#20934;&#21270;&#20248;&#21270;&#30340;QMIX&#22312;SMAC&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65307;(3)&#38500;&#20102;&#36825;&#20123;&#31639;&#27861;&#20013;&#30340;&#24120;&#35782;&#65292;&#21333;&#35843;&#24615;&#32422;&#26463;&#36824;&#21487;&#20197;&#25552;&#39640;SMAC&#21644;DEPP&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#21333;&#35843;&#24615;&#32422;&#26463;&#22312;&#32431;&#21512;&#20316;MARL&#20013;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely coopera
&lt;/p&gt;</description></item></channel></rss>