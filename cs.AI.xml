<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#26816;&#26597;&#27169;&#22411;&#36755;&#20986;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#32473;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.11252</link><description>&lt;p&gt;
&#25581;&#31034;&#19981;&#21487;&#35328;&#35828;&#20043;&#20107;&#65306;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Revealing the Unwritten: Visual Investigation of Beam Search Trees to Address Language Model Prompting Challenges. (arXiv:2310.11252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35270;&#21270;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#25361;&#25112;&#12290;&#36890;&#36807;&#20840;&#38754;&#26816;&#26597;&#27169;&#22411;&#36755;&#20986;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#32473;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#22686;&#21152;&#20102;&#23545;&#20132;&#20114;&#24335;&#26041;&#27861;&#20197;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#30340;&#20852;&#36259;&#12290;&#25552;&#31034;&#32454;&#21270;&#34987;&#35748;&#20026;&#26159;&#36825;&#20123;&#26041;&#27861;&#20013;&#24433;&#21709;&#36755;&#20986;&#26368;&#26377;&#25928;&#30340;&#25163;&#27573;&#20043;&#19968;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#19982;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#29305;&#23450;&#12289;&#35821;&#35328;&#21644;&#31038;&#20250;&#35821;&#35328;&#23398;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#20505;&#36873;&#36755;&#20986;&#21644;&#23545;&#24212;&#27010;&#29575;&#12290;&#26463;&#25628;&#32034;&#26641;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#26463;&#25628;&#32034;&#26641;&#65292;&#26041;&#20415;&#20998;&#26512;&#27169;&#22411;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;&#25581;&#31034;&#26463;&#25628;&#32034;&#26641;&#30340;&#20215;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#35814;&#32454;&#30340;&#20998;&#26512;&#22330;&#26223;&#26469;&#35299;&#20915;&#25152;&#35782;&#21035;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of generative language models has amplified interest in interactive methods to guide model outputs. Prompt refinement is considered one of the most effective means to influence output among these methods. We identify several challenges associated with prompting large language models, categorized into data- and model-specific, linguistic, and socio-linguistic challenges. A comprehensive examination of model outputs, including runner-up candidates and their corresponding probabilities, is needed to address these issues. The beam search tree, the prevalent algorithm to sample model outputs, can inherently supply this information. Consequently, we introduce an interactive visual method for investigating the beam search tree, facilitating analysis of the decisions made by the model during generation. We quantitatively show the value of exposing the beam search tree and present five detailed analysis scenarios addressing the identified challenges. Our methodology valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21152;&#36895;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#21608;&#26399;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#29702;&#35299;&#39046;&#22495;&#35201;&#27714;&#12289;&#29983;&#25104;&#24819;&#27861;&#12289;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#24182;&#24212;&#23545;&#26032;&#25361;&#25112;&#31561;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11249</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#28436;&#36827;&#24037;&#19994;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#21608;&#26399;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Model for Automatic Evolving of Industrial Data-Centric R&amp;D Cycle. (arXiv:2310.11249v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21152;&#36895;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#21608;&#26399;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#22312;&#29702;&#35299;&#39046;&#22495;&#35201;&#27714;&#12289;&#29983;&#25104;&#24819;&#27861;&#12289;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#24182;&#24212;&#23545;&#26032;&#25361;&#25112;&#31561;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#36827;&#34892;&#30340;&#25968;&#23383;&#36716;&#22411;&#20013;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#27491;&#22312;&#25104;&#20026;&#35299;&#20915;&#21508;&#31181;&#24037;&#19994;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22914;&#39044;&#27979;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#35268;&#21010;&#65292;&#29978;&#33267;&#22797;&#26434;&#30340;&#20915;&#31574;&#12290;&#34429;&#28982;&#25968;&#25454;&#20013;&#24515;&#30340;&#30740;&#21457;&#22312;&#21033;&#29992;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#22312;&#20154;&#21147;&#12289;&#35745;&#31639;&#21644;&#26102;&#38388;&#36164;&#28304;&#26041;&#38754;&#24448;&#24448;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#20195;&#20215;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21152;&#36895;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#30340;&#28436;&#36827;&#21608;&#26399;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#24322;&#26500;&#20219;&#21153;&#30456;&#20851;&#25968;&#25454;&#12289;&#22810;&#38754;&#21521;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#26679;&#21270;&#30340;&#35745;&#31639;&#21151;&#33021;&#24037;&#20855;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#33021;&#22815;&#22810;&#22909;&#22320;&#29702;&#35299;&#29305;&#23450;&#39046;&#22495;&#30340;&#35201;&#27714;&#65292;&#29983;&#25104;&#19987;&#19994;&#30340;&#24819;&#27861;&#65292;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#24037;&#20855;&#36827;&#34892;&#23454;&#39564;&#65292;&#35299;&#37322;&#32467;&#26524;&#65292;&#24182;&#34701;&#20837;&#20174;&#36807;&#21435;&#30340;&#21162;&#21147;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#24212;&#23545;&#26032;&#30340;&#25361;&#25112;&#12290;&#20197;&#37327;&#21270;&#25237;&#36164;&#30740;&#31350;&#20026;&#20856;&#22411;&#20363;&#23376;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;LLMs&#22312;&#21152;&#36895;&#25968;&#25454;&#20013;&#24515;&#30740;&#21457;&#30340;&#28436;&#36827;&#20013;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of relentless digital transformation, data-driven solutions are emerging as powerful tools to address multifarious industrial tasks such as forecasting, anomaly detection, planning, and even complex decision-making. Although data-centric R&amp;D has been pivotal in harnessing these solutions, it often comes with significant costs in terms of human, computational, and time resources. This paper delves into the potential of large language models (LLMs) to expedite the evolution cycle of data-centric R&amp;D. Assessing the foundational elements of data-centric R&amp;D, including heterogeneous task-related data, multi-facet domain knowledge, and diverse computing-functional tools, we explore how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges. We take quantitative investment research as a typical example of indus
&lt;/p&gt;</description></item><item><title>Query2Triple&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#26469;&#32534;&#30721;&#21644;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11246</link><description>&lt;p&gt;
Query2Triple: &#32479;&#19968;&#26597;&#35810;&#32534;&#30721;&#20197;&#22238;&#31572;&#30693;&#35782;&#22270;&#35889;&#19978;&#22810;&#26679;&#22797;&#26434;&#26597;&#35810;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Query2Triple: Unified Query Encoding for Answering Diverse Complex Queries over Knowledge Graphs. (arXiv:2310.11246v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11246
&lt;/p&gt;
&lt;p&gt;
Query2Triple&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#26469;&#32534;&#30721;&#21644;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65288;CQA&#65289;&#26159;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#12290;&#30001;&#20110;KG&#30340;&#19981;&#23436;&#25972;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#65288;QE&#65289;&#26041;&#27861;&#65292;&#23558;&#26597;&#35810;&#21644;&#23454;&#20307;&#32534;&#30721;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#31526;&#35270;&#20026;&#31070;&#32463;&#38598;&#21512;&#36816;&#31639;&#31526;&#65292;&#20197;&#33719;&#24471;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21516;&#26102;&#23545;&#31616;&#21333;&#65288;&#19968;&#36339;&#65289;&#21644;&#22797;&#26434;&#65288;&#22810;&#36339;&#21644;&#36923;&#36753;&#65289;&#26597;&#35810;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20250;&#23548;&#33268;&#31616;&#21333;&#26597;&#35810;&#24615;&#33021;&#30340;&#19979;&#38477;&#21644;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Query to Triple&#65288;Q2T&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#31616;&#21333;&#21644;&#22797;&#26434;&#26597;&#35810;&#30340;&#35757;&#32451;&#35299;&#32806;&#12290;Q2T&#23558;&#35757;&#32451;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#22312;&#31616;&#21333;&#26597;&#35810;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#65292;&#20197;&#22522;&#20110;&#22836;&#23454;&#20307;&#21644;&#20851;&#31995;&#39044;&#27979;&#23614;&#23454;&#20307;&#12290;&#65288;2&#65289;&#22312;&#22797;&#26434;&#26597;&#35810;&#19978;&#35757;&#32451;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#23558;&#22810;&#26679;&#30340;&#22797;&#26434;&#26597;&#35810;&#32534;&#30721;&#20026;&#32479;&#19968;&#30340;&#19977;&#20803;&#32452;&#24418;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#38142;&#25509;&#39044;&#27979;&#22120;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering (CQA) is a challenge task of Knowledge Graph (KG). Due to the incompleteness of KGs, query embedding (QE) methods have been proposed to encode queries and entities into the same embedding space, and treat logical operators as neural set operators to obtain answers. However, these methods train KG embeddings and neural set operators concurrently on both simple (one-hop) and complex (multi-hop and logical) queries, which causes performance degradation on simple queries and low training efficiency. In this paper, we propose Query to Triple (Q2T), a novel approach that decouples the training for simple and complex queries. Q2T divides the training into two stages: (1) Pre-training a neural link predictor on simple queries to predict tail entities based on the head entity and relation. (2) Training a query encoder on complex queries to encode diverse complex queries into a unified triple form that can be efficiently solved by the pretrained neural link predictor. Our
&lt;/p&gt;</description></item><item><title>RealBehavior&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#65292;&#25552;&#39640;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.11227</link><description>&lt;p&gt;
RealBehavior: &#29992;&#20110;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#20154;&#31867;&#21270;&#34892;&#20026;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms. (arXiv:2310.11227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11227
&lt;/p&gt;
&lt;p&gt;
RealBehavior&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#22522;&#30784;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#65292;&#36890;&#36807;&#35780;&#20272;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#65292;&#25552;&#39640;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#20013;&#20986;&#29616;&#20154;&#31867;&#21270;&#34892;&#20026;&#30340;&#25253;&#36947;&#19982;&#26085;&#20465;&#22686;&#65292;&#24515;&#29702;&#23398;&#29702;&#35770;&#20026;&#30740;&#31350;&#36825;&#20123;&#34892;&#20026;&#25552;&#20379;&#20102;&#25345;&#20037;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#38754;&#21521;&#20154;&#31867;&#30340;&#24037;&#20855;&#65292;&#32780;&#27809;&#26377;&#39564;&#35777;&#20854;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RealBehavior&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20934;&#30830;&#25551;&#32472;&#27169;&#22411;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#12290;&#38500;&#20102;&#31616;&#21333;&#27979;&#37327;&#34892;&#20026;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#26681;&#25454;&#21487;&#37325;&#29616;&#24615;&#12289;&#20869;&#22806;&#19968;&#33268;&#24615;&#21644;&#27867;&#21270;&#24615;&#35780;&#20272;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#24212;&#29992;&#24515;&#29702;&#23398;&#24037;&#20855;&#26080;&#27861;&#20934;&#30830;&#25551;&#32472;&#25152;&#26377;&#30340;&#20154;&#31867;&#21270;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#23545;&#40784;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#26679;&#21270;&#23545;&#40784;&#30446;&#26631;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#38450;&#27490;&#21019;&#24314;&#20855;&#26377;&#21463;&#38480;&#29305;&#24449;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reports of human-like behaviors in foundation models are growing, with psychological theories providing enduring tools to investigate these behaviors. However, current research tends to directly apply these human-oriented tools without verifying the faithfulness of their outcomes. In this paper, we introduce a framework, RealBehavior, which is designed to characterize the humanoid behaviors of models faithfully. Beyond simply measuring behaviors, our framework assesses the faithfulness of results based on reproducibility, internal and external consistency, and generalizability. Our findings suggest that a simple application of psychological tools cannot faithfully characterize all human-like behaviors. Moreover, we discuss the impacts of aligning models with human and social values, arguing for the necessity of diversifying alignment objectives to prevent the creation of models with restricted characteristics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11198</link><description>&lt;p&gt;
EEG&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#65306;&#19968;&#31181;&#19982;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30456;&#27604;&#36739;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#22312;&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#21508;&#31181;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#29992;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#30340;&#24378;&#22823;&#28436;&#36827;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#21046;&#25972;&#21512;&#21040;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#31995;&#32479;&#22320;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36731;&#37327;&#32423;&#30340;&#22522;&#20934;&#26550;&#26500;&#65292;&#26088;&#22312;&#26080;&#32541;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#30740;&#31350;&#19968;&#20010;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#12289;&#26377;&#26102;&#23884;&#22871;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#30456;&#21516;&#24773;&#20917;&#19979;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#26131;&#20110;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20197;&#21450;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#30740;&#31350;&#21644;&#25512;&#36827;BCI&#20013;&#30340;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11191</link><description>&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#31616;&#21270;&#65306;&#36890;&#36807;&#38750;&#20856;&#22411;&#35757;&#32451;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#20248;&#21270;&#21487;&#35835;&#24615;
&lt;/p&gt;
&lt;p&gt;
Medical Text Simplification: Optimizing for Readability with Unlikelihood Training and Reranked Beam Search Decoding. (arXiv:2310.11191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#25439;&#22833;&#21644;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#35299;&#30721;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#22312;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#65289;&#20013;&#24357;&#21512;&#27807;&#36890;&#24046;&#36317;&#30340;&#36234;&#26469;&#36234;&#26377;&#29992;&#30340;&#24212;&#29992;&#65292;&#24050;&#36880;&#28176;&#23853;&#38706;&#22836;&#35282;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#31616;&#21270;&#26041;&#27861;&#26377;&#26102;&#20250;&#23548;&#33268;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21307;&#23398;&#39046;&#22495;&#25991;&#26412;&#31616;&#21270;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#20856;&#22411;&#21507;&#20111;&#25439;&#22833;&#24178;&#22270;&#29255;&#21050;&#28608;&#29983;&#25104;&#26356;&#31616;&#21333;&#30340;&#26415;&#35821;&#65292;&#20197;&#21450;&#19968;&#31181;&#20248;&#21270;&#31616;&#21333;&#24615;&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;Beam Search&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35835;&#24615;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#21457;&#29616;&#20026;&#25913;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#25991;&#26412;&#31616;&#21270;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification has emerged as an increasingly useful application of AI for bridging the communication gap in specialized fields such as medicine, where the lexicon is often dominated by technical jargon and complex constructs. Despite notable progress, methods in medical simplification sometimes result in the generated text having lower quality and diversity. In this work, we explore ways to further improve the readability of text simplification in the medical domain. We propose (1) a new unlikelihood loss that encourages generation of simpler terms and (2) a reranked beam search decoding method that optimizes for simplicity, which achieve better performance on readability metrics on three datasets. This study's findings offer promising avenues for improving text simplification in the medical field.
&lt;/p&gt;</description></item><item><title>FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11178</link><description>&lt;p&gt;
FocDepthFormer: &#20351;&#29992;LSTM&#30340;Transformer&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus. (arXiv:2310.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11178
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28966;&#28857;&#22534;&#26632;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#22534;&#26632;&#20013;&#30340;&#28966;&#28857;/&#31163;&#28966;&#32447;&#32034;&#25512;&#26029;&#28145;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#22270;&#20687;&#22534;&#26632;&#19978;&#24212;&#29992;&#20108;&#32500;&#25110;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20197;&#22312;&#22270;&#20687;&#21644;&#22534;&#26632;&#20043;&#38388;&#23398;&#20064;&#29305;&#24449;&#12290;&#30001;&#20110;CNN&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#22788;&#29702;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#19968;&#33268;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;&#22534;&#26632;&#19978;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;FocDepthFormer&#65292;&#20027;&#35201;&#30001;&#24102;&#26377;LSTM&#27169;&#22359;&#21644;CNN&#35299;&#30721;&#22120;&#30340;Transformer&#32452;&#25104;&#12290;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#36890;&#36807;&#38544;&#21547;&#38750;&#23616;&#37096;&#20132;&#21449;&#21442;&#32771;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;LSTM&#27169;&#22359;&#34987;&#23398;&#20064;&#29992;&#20110;&#23558;&#34920;&#31034;&#38598;&#25104;&#21040;&#20855;&#26377;&#20219;&#24847;&#22270;&#20687;&#30340;&#22534;&#26632;&#20013;&#12290;&#20026;&#20102;&#30452;&#25509;&#25429;&#33719;&#20302;&#32423;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level featur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#8212;&#8212;EndoKED&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#20013;&#25552;&#21462;&#21644;&#31934;&#31616;&#30693;&#35782;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;EndoKED&#22312;&#35757;&#32451;&#32467;&#30452;&#32928;&#38236;&#26816;&#27979;&#21644;&#20998;&#21106;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11173</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#20013;&#25552;&#21462;&#21644;&#31934;&#31616;&#30693;&#35782;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models. (arXiv:2310.11173v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#30340;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#8212;&#8212;EndoKED&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#20013;&#25552;&#21462;&#21644;&#31934;&#31616;&#30693;&#35782;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;EndoKED&#22312;&#35757;&#32451;&#32467;&#30452;&#32928;&#38236;&#26816;&#27979;&#21644;&#20998;&#21106;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32467;&#30452;&#32928;&#38236;&#20998;&#26512;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24320;&#21457;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#26469;&#33258;&#20363;&#34892;&#20020;&#24202;&#23454;&#36341;&#30340;&#22270;&#20687;&#25991;&#26412;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#21253;&#21547;&#25968;&#30334;&#19975;&#24352;&#22270;&#20687;&#21644;&#25991;&#26412;&#25253;&#21578;&#65292;&#26159;&#23453;&#36149;&#30340;&#25968;&#25454;&#28304;&#65292;&#20294;&#26631;&#27880;&#23427;&#20204;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20805;&#20998;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#30693;&#35782;&#25552;&#21462;&#21644;&#31934;&#31616;&#30340;&#25968;&#25454;&#25366;&#25496;&#33539;&#24335;-EndoKED&#12290;EndoKED&#33258;&#21160;&#23558;&#21407;&#22987;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#36716;&#21270;&#20026;&#20855;&#26377;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20013;&#24515;&#30340;&#21407;&#22987;&#32467;&#30452;&#32928;&#38236;&#26816;&#26597;&#35760;&#24405;&#25968;&#25454;&#38598; (~1&#30334;&#19975;&#24352;&#22270;&#20687;) &#39564;&#35777;&#20102;EndoKED&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#35757;&#32451;&#24687;&#32905;&#26816;&#27979;&#21644;&#20998;&#21106;&#27169;&#22411;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;EndoKED&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#39592;&#24178;&#32593;&#32476;&#23454;&#29616;&#20102;&#23545;&#20809;&#23398;&#27963;&#26816;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#21644;&#27867;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence systems for colonoscopy analysis often necessitates expert-annotated image datasets. However, limitations in dataset size and diversity impede model performance and generalisation. Image-text colonoscopy records from routine clinical practice, comprising millions of images and text reports, serve as a valuable data source, though annotating them is labour-intensive. Here we leverage recent advancements in large language and vision models and propose EndoKED, a data mining paradigm for deep knowledge extraction and distillation. EndoKED automates the transformation of raw colonoscopy records into image datasets with pixel-level annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy records (~1 million images), demonstrating its superior performance in training polyp detection and segmentation models. Furthermore, the EndoKED pre-trained vision backbone enables data-efficient and generalisable learning for optical biopsy,
&lt;/p&gt;</description></item><item><title>MST-GAT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11169</link><description>&lt;p&gt;
MST-GAT: &#22522;&#20110;&#22810;&#27169;&#24577;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection. (arXiv:2310.11169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11169
&lt;/p&gt;
&lt;p&gt;
MST-GAT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#25429;&#25417;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#32500;&#25252;&#24037;&#20316;&#35774;&#22791;&#65288;&#20363;&#22914;&#27700;&#22788;&#29702;&#31995;&#32479;&#21644;&#33322;&#22825;&#22120;&#65289;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#35774;&#22791;&#30340;&#25968;&#25454;&#29305;&#24449;&#26159;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#27169;&#24577;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#25429;&#25417;&#21040;&#19981;&#21516;&#27169;&#24577;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#23548;&#33268;&#26356;&#22810;&#30340;&#20551;&#38452;&#24615;&#21644;&#20551;&#38451;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MST-GAT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MST-GAT&#39318;&#20808;&#20351;&#29992;&#22810;&#27169;&#24577;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;M-GAT&#65289;&#21644;&#26102;&#22495;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;M-GAT&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#20004;&#20010;&#20851;&#31995;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;&#21363;&#27169;&#24577;&#20869;&#21644;&#27169;&#24577;&#38388;&#27880;&#24847;&#21147;&#65289;&#26469;&#26126;&#30830;&#22320;&#24314;&#27169;&#27169;&#24577;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;MST-GAT&#20248;&#21270;&#20102;&#37325;&#24314;&#35823;&#24046;&#21644;&#38388;&#38548;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#39044;&#27979;&#22269;&#38469;&#36152;&#26131;&#27969;&#37327;&#65292;&#36890;&#36807;&#37325;&#21147;&#27169;&#22411;&#25429;&#25417;&#37325;&#35201;&#22240;&#32032;&#20197;&#39044;&#27979;&#36152;&#26131;&#27169;&#24335;&#65292;&#20026;&#20915;&#31574;&#32773;&#12289;&#20225;&#19994;&#21644;&#32463;&#27982;&#23398;&#23478;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.11161</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#22269;&#38469;&#36152;&#26131;&#27969;&#37327;: &#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of international trade flows: Leveraging knowledge graphs and their embeddings. (arXiv:2310.11161v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#39044;&#27979;&#22269;&#38469;&#36152;&#26131;&#27969;&#37327;&#65292;&#36890;&#36807;&#37325;&#21147;&#27169;&#22411;&#25429;&#25417;&#37325;&#35201;&#22240;&#32032;&#20197;&#39044;&#27979;&#36152;&#26131;&#27169;&#24335;&#65292;&#20026;&#20915;&#31574;&#32773;&#12289;&#20225;&#19994;&#21644;&#32463;&#27982;&#23398;&#23478;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#65288;KR&#65289;&#22312;&#35774;&#35745;&#31526;&#21495;&#31526;&#21495;&#26469;&#34920;&#31034;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#23454;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#20915;&#31574;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36804;&#20170;&#24050;&#25104;&#20026;&#27969;&#34892;&#30340;&#30693;&#35782;&#34920;&#31034;&#24418;&#24335;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#30693;&#35782;&#30340;&#19978;&#19979;&#25991;&#21644;&#31867;&#20284;&#20154;&#31867;&#30340;&#34920;&#36798;&#12290;&#22312;&#22269;&#38469;&#32463;&#27982;&#39046;&#22495;&#20013;&#65292;KGs&#24050;&#35777;&#26126;&#22312;&#25429;&#25417;&#21830;&#21697;&#12289;&#20844;&#21496;&#21644;&#22269;&#23478;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#12290;&#23558;&#37325;&#21147;&#27169;&#22411;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#32463;&#27982;&#26694;&#26550;&#65289;&#34701;&#20837;&#21040;&#26500;&#24314;KGs&#30340;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#32771;&#34385;&#24433;&#21709;&#36152;&#26131;&#20851;&#31995;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#20174;&#32780;&#20351;&#24471;&#39044;&#27979;&#22269;&#38469;&#36152;&#26131;&#27169;&#24335;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#24314;&#27169;&#22269;&#38469;&#36152;&#26131;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20351;&#29992;&#23884;&#20837;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#20026;&#20915;&#31574;&#32773;&#12289;&#20225;&#19994;&#21644;&#32463;&#27982;&#23398;&#23478;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#39044;&#35745;&#22269;&#38469;&#36152;&#26131;&#20307;&#31995;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge representation (KR) is vital in designing symbolic notations to represent real-world facts and facilitate automated decision-making tasks. Knowledge graphs (KGs) have emerged so far as a popular form of KR, offering a contextual and human-like representation of knowledge. In international economics, KGs have proven valuable in capturing complex interactions between commodities, companies, and countries. By putting the gravity model, which is a common economic framework, into the process of building KGs, important factors that affect trade relationships can be taken into account, making it possible to predict international trade patterns. This paper proposes an approach that leverages Knowledge Graph embeddings for modeling international trade, focusing on link prediction using embeddings. Thus, valuable insights are offered to policymakers, businesses, and economists, enabling them to anticipate the effects of changes in the international trade system. Moreover, the integrati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35831;&#27714;&#30340;&#30693;&#35782;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;CBN&#30340;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#35782;&#21035;&#21644;&#35831;&#27714;&#19981;&#30830;&#23450;&#20851;&#31995;&#30340;&#30693;&#35782;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32467;&#26500;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11154</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#35831;&#27714;&#30340;&#30693;&#35782;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal discovery using dynamically requested knowledge. (arXiv:2310.11154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35831;&#27714;&#30340;&#30693;&#35782;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;CBN&#30340;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#35782;&#21035;&#21644;&#35831;&#27714;&#19981;&#30830;&#23450;&#20851;&#31995;&#30340;&#30693;&#35782;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32467;&#26500;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBNs&#65289;&#26159;&#22312;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#25512;&#29702;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#30830;&#23450;CBN&#30340;&#22270;&#24418;&#32467;&#26500;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#12289;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25110;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#26469;&#23436;&#25104;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#22312;&#31639;&#27861;&#24320;&#22987;&#20043;&#21069;&#20250;&#21521;&#31639;&#27861;&#25552;&#20379;&#20154;&#31867;&#30693;&#35782;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#26412;&#36523;&#22312;&#32467;&#26500;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#35782;&#21035;&#21644;&#35831;&#27714;&#20851;&#31995;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#38598;&#25104;&#21040;Tabu&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#32467;&#26500;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#65292;&#36825;&#19968;&#20248;&#21183;&#36890;&#24120;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#25972;&#21512;&#30693;&#35782;&#25152;&#33021;&#25552;&#20379;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20165;&#35831;&#27714;&#24359;&#26041;&#21521;&#20449;&#24687;&#30340;&#21464;&#20307;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Networks (CBNs) are an important tool for reasoning under uncertainty in complex real-world systems. Determining the graphical structure of a CBN remains a key challenge and is undertaken either by eliciting it from humans, using machine learning to learn it from data, or using a combination of these two approaches. In the latter case, human knowledge is generally provided to the algorithm before it starts, but here we investigate a novel approach where the structure learning algorithm itself dynamically identifies and requests knowledge for relationships that the algorithm identifies as uncertain during structure learning. We integrate this approach into the Tabu structure learning algorithm and show that it offers considerable gains in structural accuracy, which are generally larger than those offered by existing approaches for integrating knowledge. We suggest that a variant which requests only arc orientation information may be particularly useful where the practiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#65292;&#24182;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#21307;&#30103;&#27835;&#30103;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11147</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#30340;&#27969;&#20307;&#27969;&#21160;&#27979;&#37327;&#20013;&#25581;&#31034;&#22721;&#21098;&#20999;&#24212;&#21147;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements. (arXiv:2310.11147v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#65292;&#24182;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#21307;&#30103;&#27835;&#30103;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#27969;&#20307;&#22312;&#29289;&#20307;&#38468;&#36817;&#25110;&#20869;&#37096;&#31227;&#21160;&#30340;&#28237;&#27969;&#30340;&#25705;&#25830;&#38459;&#21147;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#20844;&#29992;&#20107;&#19994;&#22522;&#30784;&#35774;&#26045;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#20154;&#31867;&#20581;&#24247;&#31561;&#39046;&#22495;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20316;&#20026;&#21098;&#20999;&#24341;&#36215;&#30340;&#25705;&#25830;&#21147;&#30340;&#30452;&#25509;&#27979;&#37327;&#65292;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#21487;&#20197;&#20419;&#36827;&#27665;&#33322;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12289;&#36164;&#28304;&#20445;&#25252;&#21644;&#30899;&#20013;&#21644;&#65292;&#20197;&#21450;&#25913;&#36827;&#34880;&#31649;&#30142;&#30149;&#21644;&#30284;&#30151;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#23613;&#31649;&#23545;&#29616;&#20195;&#31038;&#20250;&#22914;&#27492;&#37325;&#35201;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#39564;&#26041;&#27861;&#26469;&#25429;&#25417;&#30636;&#26102;&#22721;&#21098;&#20999;&#24212;&#21147;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25512;&#23548;&#27969;&#21160;&#37327;&#30340;&#26377;&#25928;&#24615;&#21644;&#29289;&#29702;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friction drag from a turbulent fluid moving past or inside an object plays a crucial role in domains as diverse as transportation, public utility infrastructure, energy technology, and human health. As a direct measure of the shear-induced friction forces, an accurate prediction of the wall-shear stress can contribute to sustainability, conservation of resources, and carbon neutrality in civil aviation as well as enhanced medical treatment of vascular diseases and cancer. Despite such importance for our modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data cover
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#21475;&#35821;&#32763;&#35793;&#12290;&#23427;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#21644;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11141</link><description>&lt;p&gt;
Simultaneous Speech Translation in Long-Form Setting: Thesis Proposal
&lt;/p&gt;
&lt;p&gt;
Long-form Simultaneous Speech Translation: Thesis Proposal. (arXiv:2310.11141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30740;&#31350;&#20102;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#21475;&#35821;&#32763;&#35793;&#12290;&#23427;&#35843;&#26597;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#21644;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#20256;&#25216;&#26415;&#26088;&#22312;&#23454;&#26102;&#23558;&#21475;&#35821;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#29978;&#33267;&#22312;&#35828;&#35805;&#20154;&#26410;&#23436;&#25104;&#21477;&#23376;&#20043;&#21069;&#21363;&#21487;&#23454;&#29616;&#32763;&#35793;&#12290;&#20256;&#32479;&#19978;&#65292;&#21516;&#20256;&#20027;&#35201;&#36890;&#36807;&#32423;&#32852;&#31995;&#32479;&#26469;&#22788;&#29702;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#35821;&#38899;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#21516;&#20256;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#28304;&#35821;&#38899;&#24050;&#32463;&#34987;&#39044;&#20808;&#20998;&#21106;&#25104;&#21477;&#23376;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#26159;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#36825;&#20010;&#35770;&#25991;&#25552;&#26696;&#30528;&#37325;&#20110;&#22788;&#29702;&#31471;&#21040;&#31471;&#38271;&#31687;&#21516;&#20256;&#38382;&#39064;&#65292;&#21363;&#22312;&#27809;&#26377;&#39044;&#20808;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#31471;&#21040;&#31471;&#21516;&#20256;&#36827;&#23637;&#30340;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#21516;&#20256;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#20197;&#21450;&#19982;&#38271;&#31687;&#22330;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous speech translation (SST) aims to provide real-time translation of spoken language, even before the speaker finishes their sentence. Traditionally, SST has been addressed primarily by cascaded systems that decompose the task into subtasks, including speech recognition, segmentation, and machine translation. However, the advent of deep learning has sparked significant interest in end-to-end (E2E) systems. Nevertheless, a major limitation of most approaches to E2E SST reported in the current literature is that they assume that the source speech is pre-segmented into sentences, which is a significant obstacle for practical, real-world applications. This thesis proposal addresses end-to-end simultaneous speech translation, particularly in the long-form setting, i.e., without pre-segmentation. We present a survey of the latest advancements in E2E SST, assess the primary obstacles in SST and its relevance to long-form scenarios, and suggest approaches to tackle these challenges.
&lt;/p&gt;</description></item><item><title>USDC&#26159;&#19968;&#31181;&#32479;&#19968;&#38745;&#24577;&#21644;&#21160;&#24577;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32553;&#20943;&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>http://arxiv.org/abs/2310.11117</link><description>&lt;p&gt;
USDC:&#38754;&#21521;&#35270;&#35273;Transformer&#30340;&#32479;&#19968;&#38745;&#24577;&#21644;&#21160;&#24577;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
USDC: Unified Static and Dynamic Compression for Visual Transformer. (arXiv:2310.11117v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11117
&lt;/p&gt;
&lt;p&gt;
USDC&#26159;&#19968;&#31181;&#32479;&#19968;&#38745;&#24577;&#21644;&#21160;&#24577;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32553;&#20943;&#35270;&#35273;Transformer&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#20960;&#20046;&#25152;&#26377;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#22914;&#20998;&#31867;&#12289;&#26816;&#27979;&#31561;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;Transformer&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#24037;&#19994;&#20135;&#21697;&#20013;&#30340;&#37096;&#32626;&#12290;&#21508;&#31181;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#20391;&#37325;&#20110;&#30452;&#25509;&#23558;&#35270;&#35273;Transformer&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#24403;&#21387;&#32553;&#27604;&#20363;&#36739;&#22823;&#26102;&#65292;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#36824;&#24212;&#29992;&#20102;&#20960;&#31181;&#21160;&#24577;&#32593;&#32476;&#25216;&#26415;&#65292;&#21160;&#24577;&#21387;&#32553;&#35270;&#35273;Transformer&#20197;&#22312;&#25512;&#29702;&#38454;&#27573;&#33719;&#21462;&#36755;&#20837;&#33258;&#36866;&#24212;&#30340;&#39640;&#25928;&#23376;&#32467;&#26500;&#65292;&#36825;&#21487;&#20197;&#22312;&#21387;&#32553;&#27604;&#20363;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#21160;&#24577;&#27169;&#22411;&#30340;&#20869;&#23384;&#19978;&#38480;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#27809;&#26377;&#20943;&#23567;&#65292;&#22240;&#20026;&#25972;&#20010;&#21407;&#22987;&#35270;&#35273;Transformer&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#25511;&#21046;&#38376;&#25511;&#27169;&#22359;&#37117;&#24517;&#39035;&#19968;&#36215;&#21152;&#36733;&#21040;&#35774;&#22791;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Transformers have achieved great success in almost all vision tasks, such as classification, detection, and so on. However, the model complexity and the inference speed of the visual transformers hinder their deployments in industrial products. Various model compression techniques focus on directly compressing the visual transformers into a smaller one while maintaining the model performance, however, the performance drops dramatically when the compression ratio is large. Furthermore, several dynamic network techniques have also been applied to dynamically compress the visual transformers to obtain input-adaptive efficient sub-structures during the inference stage, which can achieve a better trade-off between the compression ratio and the model performance. The upper bound of memory of dynamic models is not reduced in the practical deployment since the whole original visual transformer model and the additional control gating modules should be loaded onto devices together for inf
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2310.11088</link><description>&lt;p&gt;
MeKB-Rec&#65306;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation. (arXiv:2310.11088v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#20316;&#20026;&#39046;&#22495;&#19981;&#21464;&#30340;&#29992;&#25143;&#20852;&#36259;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#21644;&#27880;&#20837;&#19990;&#30028;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22914;&#20309;&#38024;&#23545;&#26032;&#29992;&#25143;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#33616;&#65292;&#21363;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#19981;&#21464;&#30340;&#20852;&#36259;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MeKB-Rec&#30340;&#26032;&#22411;&#36328;&#39046;&#22495;&#25512;&#33616;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29992;&#25143;&#21644;&#23454;&#20307;&#36827;&#34892;&#20851;&#32852;&#65292;&#26500;&#24314;&#20102;&#29992;&#25143;&#20852;&#36259;&#30340;PKG&#65292;&#21363;MeKB&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#20102;MeKB&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25512;&#33616;&#12290;&#20026;&#20102;&#39640;&#25928;&#21033;&#29992;CDR&#20013;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;MeKB-Rec&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#19990;&#30028;&#30693;&#35782;&#27880;&#20837;&#21040;&#23545;&#29992;&#25143;&#20852;&#36259;&#30340;&#29702;&#35299;&#20013;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#20102;&#35821;&#20041;&#26144;&#23556;&#65292;&#28040;&#38500;&#20102;&#23545;&#39046;&#22495;&#20869;&#29992;&#25143;&#34892;&#20026;&#30340;&#35201;&#27714;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#29992;&#25143;&#30340;&#38646;-shot&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21517;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21516;&#26102;&#20860;&#39038;&#32467;&#26524;&#36136;&#37327;&#12290;&#36890;&#36807;&#25193;&#23637;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.11087</link><description>&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#30340;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection. (arXiv:2310.11087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21517;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20256;&#24863;&#22120;&#36827;&#34892;&#20132;&#36890;&#26041;&#24335;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21516;&#26102;&#20860;&#39038;&#32467;&#26524;&#36136;&#37327;&#12290;&#36890;&#36807;&#25193;&#23637;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#30340;&#24191;&#27867;&#21033;&#29992;&#20026;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#21487;&#29992;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#30340;&#20256;&#24863;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#21033;&#20110;&#26816;&#27979;&#20132;&#36890;&#26041;&#24335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#20174;&#26234;&#33021;&#25163;&#26426;&#25910;&#38598;&#30340;&#23569;&#37327;&#20256;&#24863;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#24120;&#35265;&#26085;&#24120;&#20986;&#34892;&#27963;&#21160;&#30340;&#20934;&#30830;&#27169;&#24335;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#29305;&#24449;&#37329;&#23383;&#22612;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#65288;FPbiLSTM&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#33021;&#22815;&#20943;&#23569;&#25152;&#38656;&#30340;&#20256;&#24863;&#22120;&#25968;&#30446;&#21644;&#22788;&#29702;&#38656;&#27714;&#65292;&#20174;&#32780;&#22312;&#19981;&#29306;&#29298;&#32467;&#26524;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;FPbiLSTM&#22312;&#29616;&#26377;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#20805;&#20998;&#21033;&#29992;&#27973;&#23618;&#20016;&#23500;&#24615;&#21644;&#28145;&#23618;&#29305;&#24449;&#30340;&#38887;&#24615;&#65292;&#25429;&#25417;&#21508;&#31181;&#20132;&#36890;&#26041;&#24335;&#20013;&#30340;&#26102;&#38388;&#31227;&#21160;&#27169;&#24335;&#12290;&#23427;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#21644;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;SL&#39044;&#27979;&#20013;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11082</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction. (arXiv:2310.11082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11082
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#21512;&#25104;&#33268;&#27515;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#21644;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;SL&#39044;&#27979;&#20013;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#33268;&#27515;&#65288;SL&#65289;&#39044;&#27979;&#26159;&#29992;&#20110;&#35782;&#21035;&#20004;&#20010;&#22522;&#22240;&#30340;&#20849;&#31361;&#21464;&#26159;&#21542;&#23548;&#33268;&#32454;&#32990;&#27515;&#20129;&#30340;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#31574;&#30053;&#26159;&#23558;SL&#39044;&#27979;&#25277;&#35937;&#20026;&#22312;SL&#25968;&#25454;&#20013;&#30340;&#22522;&#22240;&#33410;&#28857;&#19978;&#30340;&#36793;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;GNNs&#23384;&#22312;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#30340;&#38750;SL&#22522;&#22240;&#20851;&#31995;&#20449;&#24687;&#26469;&#20419;&#36827;SL&#39044;&#27979;&#38754;&#20020;&#30528;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22810;&#32452;&#23398;&#37319;&#26679;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;SL&#39044;&#27979;&#65288;MSGT-SL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27973;&#23618;&#22810;&#35270;&#22270;GNN&#26469;&#20174;SL&#21644;&#22810;&#32452;&#23398;&#25968;&#25454;&#20013;&#33719;&#21462;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32534;&#30721;&#22810;&#35270;&#22270;&#20449;&#24687;&#30340;&#22522;&#22240;&#29305;&#24449;&#36755;&#20837;&#21040;&#26631;&#20934;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#20197;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20174;SL&#25968;&#25454;&#20013;&#30340;&#25209;&#37327;&#22522;&#22240;&#24320;&#22987;&#65292;&#37319;&#29992;&#24182;&#34892;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt paral
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#20197;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#26469;&#32531;&#35299;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21518;&#65292;LLMs&#33021;&#22815;&#29983;&#25104;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.11079</link><description>&lt;p&gt;
&#20174;&#32418;&#38431;&#25805;&#20316;&#20013;&#23398;&#20064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#25361;&#34885;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Learning from Red Teaming: Gender Bias Provocation and Mitigation in Large Language Models. (arXiv:2310.11079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11079
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#20197;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#26469;&#32531;&#35299;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21518;&#65292;LLMs&#33021;&#22815;&#29983;&#25104;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#23545;&#35805;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20445;&#30041;&#21487;&#33021;&#20260;&#23475;&#20154;&#31867;&#30340;&#19981;&#24179;&#31561;&#30340;&#21516;&#26102;&#65292;&#32534;&#30721;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#20256;&#32479;&#30340;&#20559;&#35265;&#35843;&#26597;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27979;&#35797;&#29992;&#20363;&#36890;&#24120;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#21019;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#26816;&#27979;LLMs&#28508;&#22312;&#24615;&#21035;&#20559;&#35265;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#31181;&#33879;&#21517;&#30340;LLM&#65292;&#24182;&#21457;&#29616;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20102;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#38024;&#23545;&#25152;&#21457;&#29616;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#31574;&#30053;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#26469;&#35268;&#36991;&#21442;&#25968;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#36890;&#36807;&#36825;&#31181;&#25552;&#35758;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#20844;&#24179;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, researchers have made considerable improvements in dialogue systems with the progress of large language models (LLMs) such as ChatGPT and GPT-4. These LLM-based chatbots encode the potential biases while retaining disparities that can harm humans during interactions. The traditional biases investigation methods often rely on human-written test cases. However, these test cases are usually expensive and limited. In this work, we propose a first-of-its-kind method that automatically generates test cases to detect LLMs' potential gender bias. We apply our method to three well-known LLMs and find that the generated test cases effectively identify the presence of biases. To address the biases identified, we propose a mitigation strategy that uses the generated test cases as demonstrations for in-context learning to circumvent the need for parameter fine-tuning. The experimental results show that LLMs generate fairer responses with the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#26368;&#22823;&#29109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19982;&#32463;&#20856;&#27169;&#22411;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#25511;&#21046;&#21442;&#25968;&#22312;&#28023;&#27969;&#25200;&#21160;&#19979;AUV&#31283;&#23450;&#24615;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#22330;&#26223;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2310.11075</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25511;&#21046;&#21442;&#25968;&#22312;&#28023;&#27969;&#25200;&#21160;&#19979;&#23454;&#29616;AUV&#31283;&#23450;&#24615;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#22330;&#26223;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sim-to-Real Transfer of Adaptive Control Parameters for AUV Stabilization under Current Disturbance. (arXiv:2310.11075v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#26368;&#22823;&#29109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19982;&#32463;&#20856;&#27169;&#22411;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#25511;&#21046;&#21442;&#25968;&#22312;&#28023;&#27969;&#25200;&#21160;&#19979;AUV&#31283;&#23450;&#24615;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#22330;&#26223;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#36807;&#31243;&#21464;&#21270;&#23545;&#33258;&#20027;&#20195;&#29702;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28023;&#27969;&#25200;&#21160;&#30340;&#26410;&#30693;&#21160;&#21147;&#23398;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#25110;&#27979;&#37327;&#65292;&#24182;&#19988;AUV&#30340;&#38750;&#32447;&#24615;&#20219;&#21153;&#35201;&#27714;&#22312;&#26576;&#20123;&#24037;&#20316;&#28857;&#19978;&#30340;&#25511;&#21046;&#22120;&#21709;&#24212;&#24517;&#39035;&#36807;&#20998;&#20445;&#23432;&#65292;&#20197;&#28385;&#36275;&#20854;&#20182;&#24037;&#20316;&#28857;&#19978;&#30340;&#35268;&#33539;&#65292;&#22240;&#27492;&#20854;&#22312;AUV&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#36890;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;DRL&#31639;&#27861;&#22312;AUV&#20013;&#30340;&#24212;&#29992;&#30446;&#21069;&#20165;&#38480;&#20110;&#20223;&#30495;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#26368;&#22823;&#29109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19982;&#32463;&#20856;&#30340;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based adaptive control methods hold the premise of enabling autonomous agents to reduce the effect of process variations with minimal human intervention. However, its application to autonomous underwater vehicles (AUVs) has so far been restricted due to 1) unknown dynamics under the form of sea current disturbance that we can not model properly nor measure due to limited sensor capability and 2) the nonlinearity of AUVs tasks where the controller response at some operating points must be overly conservative in order to satisfy the specification at other operating points. Deep Reinforcement Learning (DRL) can alleviates these limitations by training general-purpose neural network policies, but applications of DRL algorithms to AUVs have been restricted to simulated environments, due to their inherent high sample complexity and distribution shift problem. This paper presents a novel approach, merging the Maximum Entropy Deep Reinforcement Learning framework with a classic model-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11053</link><description>&lt;p&gt;
Denevil: &#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#26469;&#35299;&#35835;&#21644;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. (arXiv:2310.11053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Moral Foundation Theory&#21644;DeNEVIL&#31639;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#20215;&#20540;&#65292;&#24182;&#26500;&#24314;&#20102;MoralPrompt&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20869;&#22312;&#20215;&#20540;&#12290;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#19981;&#23545;&#40784;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#65292;&#28982;&#32780;&#23427;&#20204;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#30001;&#29983;&#25104;&#30340;&#19981;&#36947;&#24503;&#20869;&#23481;&#24341;&#36215;&#30340;&#31038;&#20250;&#39118;&#38505;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#29305;&#23450;&#38382;&#39064;&#22914;&#20559;&#35265;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20174;&#36947;&#24503;&#21746;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#36825;&#39033;&#24037;&#20316;&#21033;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#28145;&#20837;&#25506;&#35752;&#36947;&#24503;&#20215;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeNEVIL&#65292;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#65292;&#26088;&#22312;&#21160;&#24577;&#21033;&#29992;LLM&#30340;&#20215;&#20540;&#33030;&#24369;&#24615;&#24182;&#20197;&#29983;&#25104;&#26041;&#24335;&#25581;&#31034;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#65292;&#25581;&#31034;&#20854;&#28508;&#22312;&#30340;&#20215;&#20540;&#20542;&#21521;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MoralPrompt&#65292;&#19968;&#20010;&#21253;&#21547;2,397&#20010;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;500&#22810;&#20010;&#20215;&#20540;&#21407;&#21017;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;LLM&#30340;&#20869;&#22312;&#20215;&#20540;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23454;&#36136;&#19978;&#26159;&#19981;&#23545;&#40784;&#30340;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#36827;&#34892;&#36947;&#24503;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs. We discovered that most models are essentially misaligned, necessitating further ethical value alignment. In r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2310.11049</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;6&#20013;&#30340;&#38750;&#32435;&#20219;&#21153;:&#27861;&#24459;&#35780;&#20272;&#26041;&#27861;&#35770;&#12290;(arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#21644;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#31561;&#23376;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#21508;&#20010;&#23376;&#20219;&#21153;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#27861;&#24459;&#35780;&#20272;&#20219;&#21153;6&#19978;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#65306;&#20219;&#21153;B&#30340;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(L-NER)&#65292;&#20219;&#21153;C1&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;(LJP)&#21644;&#20219;&#21153;C2&#30340;&#24102;&#35299;&#37322;&#30340;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;(CJPE)&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#20219;&#21153;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#24182;&#35814;&#32454;&#21576;&#29616;&#20102;&#32467;&#26524;&#65292;&#21253;&#25324;&#25968;&#25454;&#32479;&#35745;&#21644;&#26041;&#27861;&#35770;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20687;&#26412;&#30740;&#31350;&#20013;&#25152;&#28041;&#21450;&#30340;&#27861;&#24459;&#20219;&#21153;&#27491;&#22312;&#22240;&#33258;&#21160;&#21270;&#27861;&#24459;&#20998;&#26512;&#21644;&#25903;&#25345;&#30340;&#38656;&#27714;&#22686;&#21152;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#25490;&#34892;&#27036;&#19978;&#25253;&#21578;&#30340;&#20219;&#21153;B&#12289;&#20219;&#21153;C1&#21644;&#20219;&#21153;C2&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;15th&#12289;11th&#21644;1st&#30340;&#31454;&#20105;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11046</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#30340;&#24555;&#36895;&#22270;&#32467;&#26500;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21452;&#23618;&#20248;&#21270;&#26550;&#26500;&#26469;&#21387;&#32553;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21516;&#26679;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#25913;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#21452;&#23618;&#20248;&#21270;&#30340;&#20869;&#24490;&#29615;&#20013;&#36845;&#20195;&#35757;&#32451;GNN&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65288;GC-SNTK&#65289;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#65288;SNTK&#65289;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23618;&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26041;&#26696;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#21306;&#20998;&#27450;&#39575;&#25915;&#20987;&#21644;&#29992;&#25143;&#31227;&#21160;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#36731;&#26494;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2310.11043</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#20013;&#23545;&#25239;&#29992;&#25143;&#31227;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spoofing Attack Detection in the Physical Layer with Robustness to User Movement. (arXiv:2310.11043v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23618;&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;&#26041;&#26696;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#21306;&#20998;&#27450;&#39575;&#25915;&#20987;&#21644;&#29992;&#25143;&#31227;&#21160;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#36731;&#26494;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27450;&#39575;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#20882;&#20805;&#21512;&#27861;&#29992;&#25143;&#20197;&#35775;&#38382;&#25110;&#20462;&#25913;&#21518;&#32773;&#30340;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#29289;&#29702;&#23618;&#27450;&#39575;&#26816;&#27979;&#26041;&#27861;&#22312;&#35266;&#23519;&#21040;&#26576;&#20123;&#20449;&#36947;&#29305;&#24449;&#30340;&#21464;&#21270;&#26102;&#23459;&#24067;&#25915;&#20987;&#65292;&#20363;&#22914;&#31354;&#38388;&#20998;&#24067;&#30340;&#25509;&#25910;&#22120;&#27979;&#37327;&#21040;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20449;&#36947;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#20363;&#22914;&#30001;&#20110;&#29992;&#25143;&#31227;&#21160;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35268;&#36991;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#21464;&#21270;&#26816;&#27979;&#22120;&#30340;&#20915;&#31574;&#65292;&#20197;&#21306;&#20998;&#27450;&#39575;&#25915;&#20987;&#21644;&#31227;&#21160;&#12290;&#22312;&#22270;&#19978;&#22522;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#30784;&#19978;&#65292;&#23558;&#25509;&#25910;&#21040;&#30340;&#24103;&#24207;&#21015;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#65292;&#20197;&#26816;&#27979;&#26469;&#33258;&#19981;&#21516;&#20301;&#32622;&#30340;&#21516;&#26102;&#20256;&#36755;&#12290;&#35813;&#26041;&#26696;&#21487;&#20197;&#22312;&#23454;&#36341;&#20013;&#36731;&#26494;&#37096;&#32626;&#65292;&#22240;&#20026;&#23427;&#20165;&#28041;&#21450;&#22312;&#20960;&#21313;&#20010;&#20301;&#32622;&#25910;&#38598;&#23569;&#37327;&#27979;&#37327;&#25968;&#25454;&#65292;&#29978;&#33267;&#19981;&#38656;&#35201;&#35745;&#31639;&#25110;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a spoofing attack, an attacker impersonates a legitimate user to access or modify data belonging to the latter. Typical approaches for spoofing detection in the physical layer declare an attack when a change is observed in certain channel features, such as the received signal strength (RSS) measured by spatially distributed receivers. However, since channels change over time, for example due to user movement, such approaches are impractical. To sidestep this limitation, this paper proposes a scheme that combines the decisions of a position-change detector based on a deep neural network to distinguish spoofing from movement. Building upon community detection on graphs, the sequence of received frames is partitioned into subsequences to detect concurrent transmissions from distinct locations. The scheme can be easily deployed in practice since it just involves collecting a small dataset of measurements at a few tens of locations that need not even be computed or recorded. The scheme i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#36827;&#34892;&#32463;&#39564;&#35777;&#25454;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20256;&#32479;&#26041;&#26696;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.11036</link><description>&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#65306;&#32463;&#39564;&#35777;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Radio Map Estimation in the Real-World: Empirical Validation and Analysis. (arXiv:2310.11036v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#36827;&#34892;&#32463;&#39564;&#35777;&#25454;&#30340;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20256;&#32479;&#26041;&#26696;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#22320;&#22270;&#22312;&#22320;&#29702;&#21306;&#22495;&#30340;&#27599;&#20010;&#28857;&#19978;&#37327;&#21270;&#20102;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25110;&#20854;&#20182;&#26080;&#32447;&#30005;&#39057;&#29575;&#29615;&#22659;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#22320;&#22270;&#22312;&#26080;&#32447;&#32593;&#32476;&#35268;&#21010;&#12289;&#39057;&#35889;&#31649;&#29702;&#21644;&#36890;&#20449;&#31995;&#32479;&#20248;&#21270;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#30340;&#22823;&#37327;&#26080;&#32447;&#30005;&#22320;&#22270;&#20272;&#35745;&#22120;&#30340;&#32463;&#39564;&#35777;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20351;&#29992;&#33258;&#20027;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#23545;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#20195;&#34920;&#24615;&#23376;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#65292;&#24191;&#27867;&#30740;&#31350;&#20102;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20197;&#21450;&#24555;&#36895;&#34928;&#33853;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#20272;&#35745;&#22120;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#30456;&#23545;&#20256;&#32479;&#26041;&#26696;&#25552;&#20379;&#23454;&#36136;&#24615;&#20248;&#21183;&#12290;&#19968;&#31181;&#28151;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#20272;&#35745;&#22120;&#30340;&#26032;&#31639;&#27861;&#34987;&#21457;&#29616;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio maps quantify received signal strength or other magnitudes of the radio frequency environment at every point of a geographical region. These maps play a vital role in a large number of applications such as wireless network planning, spectrum management, and optimization of communication systems. However, empirical validation of the large number of existing radio map estimators is highly limited. To fill this gap, a large data set of measurements has been collected with an autonomous unmanned aerial vehicle (UAV) and a representative subset of these estimators were evaluated on this data. The performance-complexity trade-off and the impact of fast fading are extensively investigated. Although sophisticated estimators based on deep neural networks (DNNs) exhibit the best performance, they are seen to require large volumes of training data to offer a substantial advantage relative to more traditional schemes. A novel algorithm that blends both kinds of estimators is seen to enjoy th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#20102;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11029</link><description>&lt;p&gt;
&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65306;&#19979;&#19968;&#20195;&#22320;&#29702;&#31354;&#38388;GPT&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Core Building Blocks: Next Gen Geo Spatial GPT Application. (arXiv:2310.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#20102;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#19982;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;MapGPT&#26088;&#22312;&#36890;&#36807;&#24378;&#35843;&#30456;&#20851;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65292;&#24357;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#32467;&#21512;LLMs&#21644;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#30340;&#20248;&#21183;&#65292;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24378;&#35843;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#26500;&#24314;LLMs&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#31354;&#38388;&#20449;&#24687;&#30340;&#26631;&#35760;&#21270;&#21644;&#21521;&#37327;&#34920;&#31034;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#29983;&#25104;&#31354;&#38388;&#21521;&#37327;&#34920;&#31034;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;MapGPT&#20869;&#30340;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25191;&#34892;&#22320;&#29702;&#31354;&#38388;&#35745;&#31639;&#24182;&#33719;&#24471;&#21487;&#35270;&#21270;&#36755;&#20986;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#25152;&#38656;&#35201;&#30340;&#26680;&#24515;&#27169;&#22359;&#30340;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques. This paper introduces MapGPT, which aims to bridge the gap between natural language understanding and spatial data analysis by highlighting the relevant core building blocks. By combining the strengths of LLMs and geospatial analysis, MapGPT enables more accurate and contextually aware responses to location-based queries. The proposed methodology highlights building LLMs on spatial and textual data, utilizing tokenization and vector representations specific to spatial information. The paper also explores the challenges associated with generating spatial vector representations. Furthermore, the study discusses the potential of computational capabilities within MapGPT, allowing users to perform geospatial computations and obtain visualized outputs. Overall, this research paper presents the building bl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer&#26041;&#27861;&#65288;CoFormer&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#30340;&#32508;&#21512;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11022</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer
&lt;/p&gt;
&lt;p&gt;
Compatible Transformer for Irregularly Sampled Multivariate Time Series. (arXiv:2310.11022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#20860;&#23481;Transformer&#26041;&#27861;&#65288;CoFormer&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#30340;&#32508;&#21512;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20998;&#26512;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20808;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20551;&#35774;&#26102;&#38388;&#24207;&#21015;&#30340;&#27491;&#21017;&#23376;&#37319;&#26679;&#65292;&#20854;&#20013;&#30456;&#37051;&#27979;&#37327;&#20043;&#38388;&#30340;&#38388;&#38548;&#21644;&#26679;&#26412;&#25968;&#37327;&#20445;&#25345;&#19981;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#21644;&#24178;&#39044;&#65292;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#27491;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#22788;&#29702;&#19981;&#35268;&#21017;&#24615;&#65292;&#22240;&#20026;&#22312;&#26102;&#38388;&#21644;&#21464;&#37327;&#32500;&#24230;&#19978;&#23384;&#22312;&#38169;&#20301;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20860;&#23481;Transformer&#65288;CoFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22312;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20026;&#27599;&#20010;&#20010;&#20307;&#26679;&#26412;&#23454;&#29616;&#20840;&#38754;&#30340;&#26102;&#24207;&#20132;&#20114;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;CoFormer&#20013;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#29420;&#29305;&#30340;&#21464;&#37327;-&#26102;&#38388;&#28857;&#65292;&#24182;&#21033;&#29992;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#22522;&#20110;&#20869;&#21464;&#37327;/&#38388;&#21464;&#37327;&#30456;&#37051;&#28857;&#30340;&#36880;&#26679;&#26412;&#26102;&#24207;/&#20132;&#20114;&#29305;&#24449;&#12290;&#20511;&#21161;CoFormer&#20316;&#20026;&#26680;&#24515;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#19981;&#35268;&#21017;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11011</link><description>&lt;p&gt;
&#20174;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#34920;&#31034;&#21040;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#65306;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#24615;&#30340;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#24046;&#21170;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#23558;&#22240;&#26524;&#29702;&#35770;&#34701;&#20837;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25551;&#36848;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#23545;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#21644;&#26426;&#21046;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#22320;&#32467;&#21512;&#12290;&#22240;&#26524;&#27169;&#22411;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#30410;&#30340;&#23646;&#24615;&#65292;&#22914;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.10998</link><description>&lt;p&gt;
&#20351;&#29992;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22270;&#30340;&#35268;&#27169;&#20351;&#24471;GNNs&#30340;&#23454;&#26102;&#25512;&#35770;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;GNNs&#21033;&#29992;&#32447;&#24615;&#20256;&#25773;&#23545;&#29305;&#24449;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#35770;&#36807;&#31243;&#65292;&#20294;&#22312;&#23545;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#25512;&#35770;&#26102;&#20173;&#28982;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#38656;&#35201;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#22270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#36825;&#31181;&#24402;&#32435;&#35774;&#32622;&#19979;&#30340;&#21487;&#25193;&#23637;GNNs&#25512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#34917;&#20607;&#25439;&#22833;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20607;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20801;&#35768;&#20256;&#25773;&#30340;&#23618;&#25968;&#36229;&#36807;&#25152;&#36873;&#25321;&#30340;&#28145;&#24230;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#32570;&#20047;&#12289;&#29983;&#25104;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10967</link><description>&lt;p&gt;
EXMODD:&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset. (arXiv:2310.10967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#27169;&#22411;&#23545;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#32570;&#20047;&#12289;&#29983;&#25104;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#19968;&#30452;&#26159;&#38459;&#30861;&#23545;&#35805;&#20219;&#21153;&#30740;&#31350;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#25163;&#24037;&#12289;&#32593;&#32476;&#29228;&#34411;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#36890;&#20439;&#22238;&#31572;&#12289;&#26080;&#24847;&#20041;&#30340;&#38472;&#36848;&#21644;&#26377;&#23475;&#23545;&#35805;&#12290;&#36890;&#36807;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22810;&#27169;&#24577;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20219;&#21153;&#65292;&#20173;&#23384;&#22312;&#19977;&#20010;&#32570;&#28857;&#65306;1) &#30446;&#21069;&#36824;&#27809;&#26377;&#33021;&#25509;&#21463;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#24320;&#28304;&#22823;&#22411;&#27169;&#22411;&#65307;2) &#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;3) &#29983;&#25104;&#30340;&#25968;&#25454;&#36890;&#24120;&#38590;&#20197;&#36827;&#34892;&#36136;&#37327;&#25511;&#21046;&#24182;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#36827;&#34892;&#25910;&#38598;&#12290;&#20026;&#20102;&#20943;&#36731;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#37325;&#35201;&#20154;&#21147;&#21644;&#36164;&#28304;&#24320;&#25903;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25968;&#25454;&#26500;&#24314;&#26694;&#26550;(MDCF)&#12290;MDCF&#35774;&#35745;&#36866;&#24403;&#30340;&#25552;&#31034;&#26469;&#25512;&#21160;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24418;&#24335;&#33391;&#22909;&#19988;&#20196;&#20154;&#28385;&#24847;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for high-quality data has been a key issue hindering the research of dialogue tasks. Recent studies try to build datasets through manual, web crawling, and large pre-trained models. However, man-made data is expensive and data collected from the internet often includes generic responses, meaningless statements, and toxic dialogues. Automatic data generation through large models is a cost-effective method, but for open-domain multimodal dialogue tasks, there are still three drawbacks: 1) There is currently no open-source large model that can accept multimodal input; 2) The content generated by the model lacks interpretability; 3) The generated data is usually difficult to quality control and require extensive resource to collect. To alleviate the significant human and resource expenditure in data collection, we propose a Multimodal Data Construction Framework (MDCF). MDCF designs proper prompts to spur the large-scale pre-trained language model to generate well-formed and satis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.10955</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#25928;&#26524;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#25928;&#26524;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#20135;&#29983;"&#28322;&#20986;"&#25928;&#24212;&#12290;&#36825;&#20010;&#26694;&#26550;&#20026;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#25968;&#25454;&#38598;&#25928;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#31995;&#32479;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#29992;&#20110;&#35757;&#32451;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20173;&#28982;&#19981;&#22815;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#23558;&#29702;&#24819;&#21270;&#25506;&#27979;&#27979;&#35797;&#32467;&#26524;&#20316;&#20026;&#21521;&#37327;&#31354;&#38388;&#30340;&#22522;&#30784;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#29420;&#31435;&#21644;&#20114;&#21160;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#24120;&#29992;&#30340;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#25928;&#26524;&#26159;&#29305;&#24449;&#24615;&#30340;&#65292;&#24182;&#19988;&#38598;&#20013;&#22312;&#20960;&#20010;&#35821;&#35328;&#32500;&#24230;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#20123;"&#28322;&#20986;"&#25928;&#24212;&#65306;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#30475;&#20284;&#19982;&#39044;&#26399;&#20219;&#21153;&#26080;&#20851;&#30340;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#29366;&#24577;&#21521;&#37327;&#26694;&#26550;&#20026;&#31995;&#32479;&#22320;&#29702;&#35299;&#25968;&#25454;&#38598;&#25928;&#26524;&#65292;&#36825;&#26159;&#36127;&#36131;&#20219;&#21644;&#40065;&#26834;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
&lt;/p&gt;</description></item><item><title>&#22686;&#24378;&#30340;Transformer&#24341;&#20837;&#20102;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#24378;&#21270;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#30456;&#27604;&#20110;&#21407;&#22987;Transformer&#65292;&#23454;&#29616;&#20102;202.96%&#30340;BLEU&#20998;&#25968;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.10930</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;Transformer&#26550;&#26500;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhanced Transformer Architecture for Natural Language Processing. (arXiv:2310.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10930
&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#30340;Transformer&#24341;&#20837;&#20102;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#24378;&#21270;&#23398;&#20064;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#65292;&#30456;&#27604;&#20110;&#21407;&#22987;Transformer&#65292;&#23454;&#29616;&#20102;202.96%&#30340;BLEU&#20998;&#25968;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#24403;&#21069;&#30340;NLP&#27169;&#22411;&#20027;&#35201;&#36890;&#36807;&#22686;&#21152;transformer&#30340;&#25968;&#37327;&#26469;&#25552;&#39640;&#22788;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36164;&#28304;&#65292;&#22914;&#35745;&#31639;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#32467;&#26500;&#65292;&#20855;&#26377;&#20840;&#23618;&#26631;&#20934;&#21270;&#12289;&#21152;&#26435;&#27531;&#24046;&#36830;&#25509;&#12289;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#38646;&#25513;&#30721;&#33258;&#27880;&#24847;&#21147;&#31561;&#29305;&#28857;&#12290;&#25552;&#20986;&#30340;&#22686;&#24378;Transformer&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;Multi30k&#32763;&#35793;&#25968;&#25454;&#38598;&#30340;&#21452;&#35821;&#35780;&#20272;&#23454;&#39564;&#24471;&#21040;&#30340;BLEU&#20998;&#25968;&#36827;&#34892;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;Transformer&#30456;&#27604;&#65292;&#22686;&#24378;Transformer&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;202.96%&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is a state-of-the-art model in the field of natural language processing (NLP). Current NLP models primarily increase the number of transformers to improve processing performance. However, this technique requires a lot of training resources such as computing capacity. In this paper, a novel structure of Transformer is proposed. It is featured by full layer normalization, weighted residual connection, positional encoding exploiting reinforcement learning, and zero masked self-attention. The proposed Transformer model, which is called Enhanced Transformer, is validated by the bilingual evaluation understudy (BLEU) score obtained with the Multi30k translation dataset. As a result, the Enhanced Transformer achieves 202.96% higher BLEU score as compared to the original transformer with the translation dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#39044;&#27979;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#20197;&#20943;&#23569;&#35823;&#35786;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.10928</link><description>&lt;p&gt;
&#22312;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#20419;&#36827;&#25233;&#37057;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care. (arXiv:2310.10928v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#39044;&#27979;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#39118;&#38505;&#65292;&#20197;&#20943;&#23569;&#35823;&#35786;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#26159;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#25233;&#37057;&#30151;&#26159;&#24120;&#35265;&#30340;&#30149;&#30151;&#12290;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#26159;&#22823;&#22810;&#25968;&#25233;&#37057;&#30151;&#24739;&#32773;&#30340;&#39318;&#35201;&#32852;&#31995;&#28857;&#65292;&#20294;&#32422;&#26377;25%&#30340;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#21307;&#29983;&#30340;&#35786;&#26029;&#19981;&#20934;&#30830;&#12290;&#35768;&#22810;&#20854;&#20182;&#38556;&#30861;&#20063;&#22952;&#30861;&#20102;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#21644;&#27835;&#30103;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#25233;&#37057;&#30151;&#22312;&#21021;&#32423;&#21355;&#29983;&#20445;&#20581;&#20013;&#30340;&#35823;&#35786;&#65292;&#24182;&#25913;&#21892;&#25972;&#20307;&#35786;&#26029;&#21644;&#27835;&#30103;&#32467;&#26524;&#12290;&#30005;&#23376;&#20581;&#24247;&#21672;&#35810;&#24120;&#24120;&#20986;&#29616;&#35270;&#39057;&#38382;&#39064;&#65292;&#22914;&#36830;&#25509;&#19981;&#33391;&#25110;&#36890;&#35805;&#20013;&#26029;&#12290;&#23545;&#20110;&#21487;&#33021;&#32570;&#20047;&#31283;&#23450;&#30340;&#20114;&#32852;&#32593;&#36830;&#25509;&#30340;&#20302;&#25910;&#20837;&#24739;&#32773;&#32780;&#35328;&#65292;&#20165;&#38899;&#39057;&#30340;&#30005;&#23376;&#20581;&#24247;&#26356;&#23454;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#12290;&#30446;&#26631;&#26159;&#65306;1&#65289;&#25910;&#38598;24&#20154;&#30340;&#38899;&#39057;&#25968;&#25454;&#65288;12&#20154;&#24739;&#26377;&#25233;&#37057;&#30151;&#65292;12&#20154;&#27809;&#26377;&#24515;&#29702;&#20581;&#24247;&#25110;&#37325;&#22823;&#20581;&#24247;&#38382;&#39064;&#30340;&#35786;&#26029;&#65289;&#65307;2&#65289;&#26500;&#24314;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#12290;&#20351;&#29992;&#20102;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;TPOT&#26469;&#36873;&#25321;&#26368;&#20339;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Telehealth is a valuable tool for primary health care (PHC), where depression is a common condition. PHC is the first point of contact for most people with depression, but about 25% of diagnoses made by PHC physicians are inaccurate. Many other barriers also hinder depression detection and treatment in PHC. Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and improve overall diagnosis and treatment outcomes. Telehealth consultations often have video issues, such as poor connectivity or dropped calls. Audio-only telehealth is often more practical for lower-income patients who may lack stable internet connections. Thus, our study focused on using audio data to predict depression risk. The objectives were to: 1) Collect audio data from 24 people (12 with depression and 12 without mental health or major health condition diagnoses); 2) Build a machine learning model to predict depression risk. TPOT, an autoML tool, was used to select the best machine learning algo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23545;&#28023;&#37327;&#38750;&#32467;&#26500;&#21270;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#30340;&#21033;&#29992;&#26469;&#22238;&#31572;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10921</link><description>&lt;p&gt;
&#25552;&#21319;&#36719;&#20214;&#24320;&#21457;&#30340;&#26234;&#33021;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Intelligent Software Tooling for Improving Software Development. (arXiv:2310.10921v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23545;&#28023;&#37327;&#38750;&#32467;&#26500;&#21270;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#30340;&#21033;&#29992;&#26469;&#22238;&#31572;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24050;&#32463;&#28183;&#36879;&#21040;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20154;&#20204;&#25152;&#20351;&#29992;&#30340;&#35768;&#22810;&#22522;&#26412;&#38656;&#27714;&#21644;&#29983;&#27963;&#26381;&#21153;&#37117;&#38656;&#35201;&#36719;&#20214;&#12290;&#22240;&#27492;&#65292;&#25913;&#36827;&#36719;&#20214;&#24320;&#21457;&#20307;&#39564;&#30340;&#24037;&#20855;&#21487;&#20197;&#23545;&#19990;&#30028;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#20363;&#22914;&#29983;&#25104;&#20195;&#30721;&#21644;&#27979;&#35797;&#29992;&#20363;&#12289;&#26816;&#27979;&#38169;&#35823;&#12289;&#38382;&#31572;&#31561;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#21253;&#25324;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#26159;&#21487;&#29992;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;&#36890;&#36807;GitHub&#33719;&#21462;&#30340;&#24320;&#28304;&#20195;&#30721;&#25110;&#31227;&#21160;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#22914;RICO&#21644;ReDRAW&#12290;&#22240;&#27492;&#65292;&#25105;&#30340;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#28023;&#37327;&#30340;&#38750;&#32467;&#26500;&#21270;&#36719;&#20214;&#24037;&#31243;&#25991;&#26723;&#26469;&#25913;&#36827;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#65292;&#20197;&#21457;&#25381;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#21183;&#65311;
&lt;/p&gt;
&lt;p&gt;
Software has eaten the world with many of the necessities and quality of life services people use requiring software. Therefore, tools that improve the software development experience can have a significant impact on the world such as generating code and test cases, detecting bugs, question and answering, etc., The success of Deep Learning (DL) over the past decade has shown huge advancements in automation across many domains, including Software Development processes. One of the main reasons behind this success is the availability of large datasets such as open-source code available through GitHub or image datasets of mobile Graphical User Interfaces (GUIs) with RICO and ReDRAW to be trained on. Therefore, the central research question my dissertation explores is: In what ways can the software development process be improved through leveraging DL techniques on the vast amounts of unstructured software engineering artifacts?
&lt;/p&gt;</description></item><item><title>NuclearQA&#26159;&#19968;&#20010;&#20154;&#24037;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#19982;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26680;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10920</link><description>&lt;p&gt;
NuclearQA: &#29992;&#20110;&#26680;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NuclearQA: A Human-Made Benchmark for Language Models for the Nuclear Domain. (arXiv:2310.10920v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10920
&lt;/p&gt;
&lt;p&gt;
NuclearQA&#26159;&#19968;&#20010;&#20154;&#24037;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#21547;100&#20010;&#19987;&#23478;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#19982;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#26680;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#65292;&#23427;&#20204;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#12290;&#20294;&#26159;&#38543;&#30528;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25193;&#22823;&#65292;&#35780;&#20272;&#20854;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26085;&#30410;&#32570;&#20047;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#22823;&#37096;&#20998;&#19987;&#27880;&#20110;&#19981;&#38656;&#35201;&#23545;&#25152;&#28041;&#21450;&#20027;&#39064;&#36827;&#34892;&#27491;&#30830;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NuclearQA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#19987;&#23478;&#35774;&#35745;&#30340;&#29992;&#20110;&#35780;&#20272;&#26680;&#39046;&#22495;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#22522;&#20934;&#65292;&#21253;&#21547;&#20102;100&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#30001;&#20110;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#22522;&#20934;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#26680;&#39046;&#22495;&#65292;NuclearQA&#20063;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs have become increasingly popular, they have been used in almost every field. But as the application for LLMs expands from generic fields to narrow, focused science domains, there exists an ever-increasing gap in ways to evaluate their efficacy in those fields. For the benchmarks that do exist, a lot of them focus on questions that don't require proper understanding of the subject in question. In this paper, we present NuclearQA, a human-made benchmark of 100 questions to evaluate language models in the nuclear domain, consisting of a varying collection of questions that have been specifically designed by experts to test the abilities of language models. We detail our approach and show how the mix of several types of questions makes our benchmark uniquely capable of evaluating models in the nuclear domain. We also present our own evaluation metric for assessing LLM's performances due to the limitations of existing ones. Our experiments on state-of-the-art models suggest that eve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.10908</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#27169;&#22359;&#21270;&#32467;&#26500;&#65306;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#33021;&#21542;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#65311;
&lt;/p&gt;
&lt;p&gt;
Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#22359;&#21270;&#35774;&#35745;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#31561;&#20248;&#28857;&#12290;&#29616;&#26377;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#8220;&#26174;&#24335;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27169;&#22359;&#21270;&#26550;&#26500;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#34987;&#26399;&#26395;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#22312;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;Transformer&#20013;&#23384;&#22312;&#8220;&#38544;&#24335;&#8221;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21363;&#8220;&#33258;&#21457;&#27169;&#22359;&#21270;&#8221;&#12290;&#20182;&#20204;&#34920;&#26126;&#36825;&#26679;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#22312;&#26089;&#26399;&#39044;&#35757;&#32451;&#38454;&#27573;&#23601;&#20250;&#20986;&#29616;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#33258;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;Transformer&#27169;&#22411;&#20173;&#28982;&#34987;&#35270;&#20026;&#21333;&#20307;&#27169;&#22411;&#65292;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20854;&#27169;&#22359;&#21270;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#37492;&#20110;&#26174;&#24335;&#27169;&#22359;&#21270;&#26550;&#26500;&#30340;&#20248;&#33391;&#29305;&#24615;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23494;&#38598;&#39044;&#35757;&#32451;Transformer&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#20174;&#33258;&#21457;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#33719;&#30410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.10899</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10899
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23376;&#32593;&#32476;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#29702;&#35299;&#21644;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21457;&#29616;&#21151;&#33021;&#23376;&#32593;&#32476;&#24182;&#21033;&#29992;&#23427;&#20204;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#20960;&#20046;&#27809;&#26377;&#30693;&#35782;&#25110;&#25511;&#21046;&#33021;&#21147;&#12290;&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;--&#23545;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#20559;&#22909;--&#26159;&#29702;&#35299;&#21644;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#34892;&#20026;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#26469;&#30740;&#31350;&#27169;&#22411;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#24182;&#36890;&#36807;&#25163;&#21160;&#35774;&#35745;&#30340;&#32467;&#26500;&#25110;&#31934;&#24515;&#31574;&#21010;&#30340;&#35757;&#32451;&#26041;&#24335;&#27880;&#20837;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26356;&#26426;&#26800;&#30340;&#26041;&#27861;&#65306;&#23376;&#20219;&#21153;&#24402;&#32435;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#29305;&#23450;&#23376;&#20219;&#21153;&#30340;&#21151;&#33021;&#23376;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#27880;&#20837;&#23545;&#21033;&#29992;&#35813;&#23376;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#24402;&#32435;&#20559;&#32622;&#12290;&#23376;&#20219;&#21153;&#24402;&#32435;&#28789;&#27963;&#39640;&#25928;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.10863</link><description>&lt;p&gt;
&#36138;&#24515;&#35270;&#35282;&#65306;&#22810;&#26080;&#20154;&#26426;&#35270;&#37326;&#35268;&#21010;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21327;&#21516;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments. (arXiv:2310.10863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#37096;&#32626;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25293;&#25668;&#21160;&#24577;&#20154;&#32676;&#65288;&#28436;&#21592;&#65289;&#30340;&#22823;&#35268;&#27169;&#24433;&#20687;&#65292;&#29992;&#20110;&#22242;&#38431;&#36816;&#21160;&#21644;&#30005;&#24433;&#21046;&#20316;&#31561;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#39034;&#24207;&#36138;&#24515;&#35268;&#21010;&#36827;&#34892;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26080;&#20154;&#26426;&#22242;&#38431;&#20043;&#38388;&#36827;&#34892;&#25668;&#20687;&#26426;&#35270;&#37326;&#30340;&#21487;&#25193;&#23637;&#20248;&#21270;&#65292;&#20294;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#21516;&#25928;&#26524;&#38754;&#20020;&#25361;&#25112;&#12290;&#38556;&#30861;&#29289;&#21487;&#33021;&#20135;&#29983;&#36974;&#25377;&#24182;&#22686;&#21152;&#26080;&#20154;&#26426;&#30896;&#25758;&#30340;&#20960;&#29575;&#65292;&#36825;&#21487;&#33021;&#36829;&#21453;&#36817;&#20284;&#26368;&#20248;&#24615;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#22312;&#31264;&#23494;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#20154;&#32676;&#65292;&#38656;&#35201;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#35270;&#35282;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#22120;&#65292;&#24182;&#19982;&#36138;&#24515;&#24418;&#25104;&#35268;&#21010;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#36974;&#25377;&#21644;&#30896;&#25758;&#23545;&#25293;&#25668;&#24212;&#29992;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.10833</link><description>&lt;p&gt;
&#36866;&#24403;&#30340;Laplacian&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#23398;&#20064;&#29366;&#24577;&#30340;&#33391;&#22909;&#34920;&#31034;&#23545;&#20110;&#25506;&#32034;&#12289;&#27867;&#21270;&#21644;&#20256;&#36882;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;Laplacian&#34920;&#31034;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20869;&#22312;&#22870;&#21169;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26102;&#38388;&#24310;&#38271;&#30340;&#21160;&#20316;&#21457;&#29616;&#21644;&#22870;&#21169;&#22609;&#36896;&#65292;&#20197;&#21450;&#20449;&#24687;&#20016;&#23500;&#30340;&#29366;&#24577;&#32534;&#30721;&#12290;&#20026;&#20102;&#33719;&#24471;Laplacian&#34920;&#31034;&#65292;&#38656;&#35201;&#35745;&#31639;&#22270;Laplacian&#30340;&#29305;&#24449;&#31995;&#32479;&#65292;&#36825;&#36890;&#24120;&#36890;&#36807;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20860;&#23481;&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#20381;&#36182;&#20110;&#26080;&#27861;&#39640;&#25928;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#65292;&#25910;&#25947;&#21040;&#25152;&#38656;&#29305;&#24449;&#21521;&#37327;&#30340;&#20219;&#24847;&#26059;&#36716;&#65292;&#24182;&#19988;&#26080;&#27861;&#31934;&#30830;&#22320;&#24674;&#22797;&#30456;&#24212;&#30340;&#29305;&#24449;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;Laplacian&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23398;&#26415;&#36164;&#28304;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#35805;&#24418;&#24335;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26816;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21382;&#21490;&#25991;&#29486;&#65292;&#24182;&#22312;&#38382;&#31572;&#21644;&#25968;&#25454;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10808</link><description>&lt;p&gt;
&#22914;&#26524;&#36164;&#28304;&#33021;&#22815;&#35828;&#35805;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History. (arXiv:2310.10808v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21382;&#21490;&#30740;&#31350;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#23398;&#26415;&#36164;&#28304;&#23884;&#20837;&#21040;&#27169;&#22411;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#35805;&#24418;&#24335;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#26816;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#21382;&#21490;&#25991;&#29486;&#65292;&#24182;&#22312;&#38382;&#31572;&#21644;&#25968;&#25454;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20986;&#29616;&#20026;&#21382;&#21490;&#35760;&#24518;&#30340;&#23545;&#35805;&#24418;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36884;&#24452;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#24230;&#19987;&#19994;&#21270;&#23398;&#26415;&#36164;&#28304;&#30340;&#21521;&#37327;&#23884;&#20837;&#24341;&#20837;&#21040;LLM&#20013;&#65292;&#20351;&#24471;&#23545;&#35805;&#26041;&#27861;&#21487;&#20197;&#34987;&#21382;&#21490;&#23398;&#23478;&#21644;&#20854;&#20182;&#20154;&#25991;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#23637;&#31034;&#20102;LLM&#22312;&#30740;&#31350;&#20154;&#21592;&#26816;&#26597;&#19981;&#21516;&#31867;&#22411;&#25991;&#26723;&#30340;&#23450;&#21046;&#35821;&#26009;&#24211;&#26102;&#30340;&#36741;&#21161;&#33021;&#21147;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#65306;(1).&#19968;&#25163;&#36164;&#26009;&#65292;(2).&#30001;&#19987;&#23478;&#25776;&#20889;&#30340;&#20108;&#25163;&#36164;&#26009;&#65292;&#20197;&#21450;(3).&#20004;&#32773;&#30340;&#32467;&#21512;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#23383;&#30446;&#24405;&#25628;&#32034;&#30028;&#38754;&#65288;&#22914;&#20803;&#25968;&#25454;&#21644;&#20840;&#25991;&#25628;&#32034;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#26356;&#20016;&#23500;&#30340;&#23545;&#35805;&#39118;&#26684;&#23545;&#20004;&#31181;&#20027;&#35201;&#20219;&#21153;&#30340;&#34920;&#29616;&#65306;(1).&#38382;&#31572;&#65292;&#20197;&#21450;(2).&#25968;&#25454;&#30340;&#25552;&#21462;&#21644;&#32452;&#32455;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30340;&#35821;&#20041;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of powerful Large-Language Models (LLM) provides a new conversational form of inquiry into historical memory (or, training data, in this case). We show that by augmenting such LLMs with vector embeddings from highly specialized academic sources, a conversational methodology can be made accessible to historians and other researchers in the Humanities. Concretely, we evaluate and demonstrate how LLMs have the ability of assisting researchers while they examine a customized corpora of different types of documents, including, but not exclusive to: (1). primary sources, (2). secondary sources written by experts, and (3). the combination of these two. Compared to established search interfaces for digital catalogues, such as metadata and full-text search, we evaluate the richer conversational style of LLMs on the performance of two main types of tasks: (1). question-answering, and (2). extraction and organization of data. We demonstrate that LLMs semantic retrieval and reaso
&lt;/p&gt;</description></item><item><title>&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.10780</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;
&lt;/p&gt;
&lt;p&gt;
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10780
&lt;/p&gt;
&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20381;&#36182;&#26085;&#30410;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#21644;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30001;&#20110;&#20854;&#38544;&#34109;&#24615;&#21644;&#28508;&#22312;&#30340;&#20005;&#37325;&#21518;&#26524;&#32780;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31867;&#25915;&#20987;&#28041;&#21450;&#23558;&#35302;&#21457;&#22120;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#23384;&#22312;&#27963;&#21160;&#35302;&#21457;&#22120;&#26102;&#24341;&#36215;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#22312;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#27491;&#24120;&#21151;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#20026;&#21463;&#25439;&#27169;&#22411;&#22312;&#28165;&#27905;&#21644;&#21518;&#38376;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24314;&#31435;&#20005;&#26684;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#35780;&#20272;&#20102;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24320;&#21457;&#30340;&#29702;&#35770;&#22238;&#31572;&#20102;&#19968;&#31995;&#21015;&#22522;&#26412;&#20294;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;1&#65289;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#26159;&#20160;&#20040;&#65292;&#65288;2&#65289;&#26368;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#21521;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#31070;&#32463;&#20803;&#35299;&#37322;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10708</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#27169;&#22411;&#30340;&#28145;&#24230;&#35270;&#35273;&#31070;&#32463;&#20803;&#30340;&#33258;&#21160;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Automated Natural Language Explanation of Deep Visual Neurons with Large Models. (arXiv:2310.10708v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10708
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#31070;&#32463;&#20803;&#35299;&#37322;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#20854;&#26377;&#25928;&#24615;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26816;&#26597;&#31070;&#32463;&#20803;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#35270;&#35273;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#20855;&#26377;&#35821;&#20041;&#21547;&#20041;&#65292;&#24182;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29983;&#25104;&#31070;&#32463;&#20803;&#35821;&#20041;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#20026;&#24178;&#39044;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20107;&#21518;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#30340;&#35821;&#20041;&#35299;&#37322;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#20026;&#24178;&#39044;&#25110;&#20808;&#21069;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#34987;&#35774;&#35745;&#20026;&#19982;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#20860;&#23481;&#65292;&#20415;&#20110;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#20803;&#35299;&#37322;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have exhibited remarkable performance across a wide range of real-world tasks. However, comprehending the underlying reasons for their effectiveness remains a challenging problem. Interpreting deep neural networks through examining neurons offers distinct advantages when it comes to exploring the inner workings of neural networks. Previous research has indicated that specific neurons within deep vision networks possess semantic meaning and play pivotal roles in model performance. Nonetheless, the current methods for generating neuron semantics heavily rely on human intervention, which hampers their scalability and applicability. To address this limitation, this paper proposes a novel post-hoc framework for generating semantic explanations of neurons with large foundation models, without requiring human intervention or prior knowledge. Our framework is designed to be compatible with various model architectures and datasets, facilitating automated and scalable neuron
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10707</link><description>&lt;p&gt;
&#28436;&#31034;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#19968;&#20999;&#65306;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25512;&#36827;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning. (arXiv:2310.10707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#25915;&#20987;&#24615;&#20869;&#23481;&#25913;&#20889;&#26041;&#38754;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#20943;&#23569;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#20889;&#25915;&#20987;&#24615;&#20869;&#23481;&#26159;&#19968;&#31181;&#26356;&#22909;&#30340;&#26367;&#20195;&#20869;&#23481;&#21024;&#38500;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27807;&#36890;&#29615;&#22659;&#30340;&#25991;&#26126;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#24335;&#30340;&#25913;&#20889;&#22120;&#22312;&#20445;&#30041;&#24847;&#20041;&#21644;&#24847;&#22270;&#30340;&#21516;&#26102;&#65292;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#20381;&#36182;&#24615;&#36739;&#39640;&#12290;&#23427;&#20204;&#20063;&#20445;&#30041;&#20102;&#21407;&#22987;&#20869;&#23481;&#30340;&#22823;&#37096;&#20998;&#25915;&#20987;&#24615;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#25972;&#20307;&#21487;&#29992;&#24615;&#30340;&#30097;&#38382;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#24320;&#21457;&#21487;&#29992;&#30340;&#25913;&#20889;&#22120;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#36755;&#20837;-&#26631;&#31614;&#28436;&#31034;&#23545;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#26597;&#35810;&#30340;&#25152;&#38656;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#28436;&#31034;&#30340;&#25968;&#37327;&#21644;&#39034;&#24207;&#65292;&#25490;&#38500;&#25552;&#31034;&#25351;&#20196;&#65292;&#20197;&#21450;&#38477;&#20302;&#27979;&#37327;&#27602;&#24615;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25105;&#20204;&#25552;&#20986;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31036;&#35980;&#25913;&#20889;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#35805;&#24335;&#30340;&#31895;&#40065;&#21457;&#35328;&#12289;&#31036;&#35980;&#25913;&#20889;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2310.10706</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#30340;&#33021;&#37327;&#65306;&#36890;&#36807;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#35270;&#35282;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#20889;&#20316;&#65292;&#24182;&#20102;&#35299;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#21644;&#20449;&#20219;&#24230;&#65292;&#25105;&#20204;&#22312;LLM&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#24120;&#35265;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#24341;&#23548;&#31995;&#32479;&#65292;&#20174;&#31995;&#32479;&#36755;&#20986;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#21518;&#26399;&#32534;&#36753;&#36755;&#20986;&#65289;&#12290;&#23613;&#31649;LLMs&#21333;&#29420;&#21487;&#20197;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#20154;&#31867;&#30340;&#25511;&#21046;&#26159;&#38656;&#35201;&#30340;&#65292;&#20197;&#20462;&#22797;&#19981;&#21487;&#21462;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#21508;&#31181;&#20132;&#20114;&#26041;&#27861;&#20013;&#65292;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#22686;&#21152;&#20102;&#26368;&#22810;&#30340;&#25928;&#30410;&#65292;&#20195;&#20215;&#26368;&#20302;&#65288;&#26102;&#38388;&#21644;&#31934;&#21147;&#65289;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#24182;&#27809;&#26377;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#65292;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#25152;&#38656;&#36164;&#28304;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#26469;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10699</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Reusing Pretrained Models by Multi-linear Operators for Efficient Training. (arXiv:2310.10699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#25152;&#38656;&#36164;&#28304;&#22823;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#30456;&#20851;&#26469;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914;bert2BERT&#21644;LiGO&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#22411;&#27169;&#22411;&#21021;&#22987;&#21270;&#22823;&#22411;&#27169;&#22411;&#65288;&#31216;&#20026;&#8220;&#30446;&#26631;&#27169;&#22411;&#8221;&#65289;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;&#23613;&#31649;&#36825;&#20123;&#20808;&#21069;&#30740;&#31350;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#21482;&#26144;&#23556;&#37096;&#20998;&#26435;&#37325;&#25104;&#38271;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24573;&#30053;&#20102;&#25972;&#20010;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#30456;&#20851;&#24615;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#26435;&#37325;&#20043;&#38388;&#23384;&#22312;&#30528;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37096;&#20998;&#26144;&#23556;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;&#23436;&#25972;&#30340;&#20449;&#24687;&#65292;&#24182;&#23548;&#33268;&#25104;&#38271;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#37325;&#36827;&#34892;&#32447;&#24615;&#30456;&#20851;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#21152;&#36895;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#32447;&#24615;&#25805;&#20316;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#21487;&#25509;&#21463;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#27969;&#34892;&#24230;&#20559;&#24046;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#21644;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#20197;&#21450;&#19981;&#38656;&#20107;&#20808;&#20102;&#35299;&#27979;&#35797;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#21435;&#20559;&#35265;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;OOD&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10696</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#19982;&#27969;&#34892;&#24230;&#20998;&#24067;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Collaborative Filtering to Popularity Distribution Shift. (arXiv:2310.10696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#27969;&#34892;&#24230;&#20559;&#24046;&#23548;&#33268;&#30340;&#27867;&#21270;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#21644;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#20197;&#21450;&#19981;&#38656;&#20107;&#20808;&#20102;&#35299;&#27979;&#35797;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#21435;&#20559;&#35265;&#34920;&#31034;&#30340;&#36136;&#37327;&#21644;OOD&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#20808;&#30340;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#27169;&#22411;&#20013;&#65292;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#24448;&#24448;&#20250;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27969;&#34892;&#24230;&#20559;&#24046;&#20316;&#20026;&#24555;&#25463;&#26041;&#24335;&#12290;&#27969;&#34892;&#24230;&#24555;&#25463;&#26041;&#24335;&#23545;&#20110;&#22312;&#20998;&#24067;&#65288;ID&#65289;&#24615;&#33021;&#19978;&#26159;&#22909;&#30340;&#65292;&#20294;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65288;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#27969;&#34892;&#24230;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#26102;&#65289;&#65292;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#21435;&#20559;&#35265;&#31574;&#30053;&#23581;&#35797;&#35780;&#20272;&#34920;&#31034;&#20013;&#30340;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20943;&#23569;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20004;&#20010;&#19981;&#36275;&#20043;&#22788;&#65306;&#65288;1&#65289;&#22312;&#27979;&#37327;&#24555;&#25463;&#26041;&#24335;&#31243;&#24230;&#26102;&#65292;&#22823;&#22810;&#25968;&#31574;&#30053;&#21482;&#20351;&#29992;&#21333;&#19968;&#26041;&#38754;&#30340;&#32479;&#35745;&#25351;&#26631;&#65288;&#21363;&#39033;&#30446;&#39057;&#29575;&#23545;&#39033;&#30446;&#21644;&#29992;&#25143;&#39057;&#29575;&#23545;&#29992;&#25143;&#26041;&#38754;&#65289;&#65292;&#19981;&#33021;&#36866;&#24212;&#29992;&#25143;-&#39033;&#30446;&#23545;&#30340;&#32452;&#21512;&#31243;&#24230;&#65307;&#65288;2&#65289;&#22312;&#20943;&#23569;&#24555;&#25463;&#26041;&#24335;&#26102;&#65292;&#35768;&#22810;&#31574;&#30053;&#20551;&#35774;&#27979;&#35797;&#20998;&#24067;&#20107;&#20808;&#24050;&#30693;&#12290;&#36825;&#23548;&#33268;&#36136;&#37327;&#36739;&#20302;&#30340;&#21435;&#20559;&#35265;&#34920;&#31034;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#20197;&#29306;&#29298;OOD&#27867;&#21270;&#24615;&#33021;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.10693</link><description>&lt;p&gt;
iNaturalist&#20844;&#27665;&#31185;&#23398;&#31038;&#21306;&#30340;&#32593;&#32476;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Network Analysis of the iNaturalist Citizen Science Community. (arXiv:2310.10693v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#65292;&#36890;&#36807;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20844;&#27665;&#31185;&#23398;&#24050;&#25104;&#20026;&#31185;&#23398;&#30028;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#33021;&#22815;&#20174;&#25968;&#21315;&#21517;&#20844;&#27665;&#31185;&#23398;&#23478;&#37027;&#37324;&#33719;&#21462;&#25968;&#25454;&#21644;&#19987;&#19994;&#30693;&#35782;&#65292;&#20351;&#20854;&#20855;&#26377;&#26080;&#21487;&#26367;&#20195;&#30340;&#20215;&#20540;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#20132;&#20114;&#21644;&#32467;&#26500;&#20173;&#28982;&#34987;&#20102;&#35299;&#24456;&#23569;&#24182;&#19988;&#34987;&#23569;&#20998;&#26512;&#12290;&#25105;&#20204;&#20197;iNaturalist&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#20998;&#26512;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;iNaturalist&#20013;&#30340;&#25968;&#25454;&#26500;&#24314;&#20026;&#19968;&#20010;&#20108;&#20998;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#21487;&#35270;&#21270;&#21644;&#24050;&#24314;&#31435;&#30340;&#32593;&#32476;&#31185;&#23398;&#25216;&#26415;&#26469;&#20102;&#35299;&#20844;&#27665;&#31185;&#23398;&#39033;&#30446;&#20013;&#29992;&#25143;&#20043;&#38388;&#30340;&#32467;&#26500;&#21644;&#20132;&#20114;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;iNaturalist&#25968;&#25454;&#21019;&#24314;&#19968;&#20010;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#30456;&#23545;&#20110;&#20854;&#20182;&#24120;&#35265;&#30340;&#22522;&#20934;&#32593;&#32476;&#20855;&#26377;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#35777;&#26126;&#20102;&#35813;&#32593;&#32476;&#21487;&#20197;&#29992;&#20110;&#33719;&#24471;&#26032;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, citizen science has become a larger and larger part of the scientific community. Its ability to crowd source data and expertise from thousands of citizen scientists makes it invaluable. Despite the field's growing popularity, the interactions and structure of citizen science projects are still poorly understood and under analyzed. We use the iNaturalist citizen science platform as a case study to analyze the structure of citizen science projects. We frame the data from iNaturalist as a bipartite network and use visualizations as well as established network science techniques to gain insights into the structure and interactions between users in citizen science projects. Finally, we propose a novel unique benchmark for network science research by using the iNaturalist data to create a network which has an unusual structure relative to other common benchmark networks. We demonstrate using a link prediction task that this network can be used to gain novel insights into a v
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10690</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#29983;&#24314;&#27169;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#20174;&#19968;&#27425;&#24615;&#35266;&#23519;&#20013;&#21512;&#25104;&#35270;&#35273;&#32534;&#31243;&#20013;&#23398;&#29983;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for In-Context Student Modeling: Synthesizing Student's Behavior in Visual Programming from One-Shot Observation. (arXiv:2310.10690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;LLM-SS&#65292;&#36890;&#36807;&#21512;&#25104;&#23398;&#29983;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#65292;&#20026;&#23398;&#29983;&#24314;&#27169;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#23545;&#20110;&#35768;&#22810;&#25945;&#32946;&#25216;&#26415;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#23398;&#20064;&#32467;&#26524;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23398;&#29983;&#34920;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#19988;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#23398;&#20064;&#25216;&#33021;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#24320;&#25918;&#24335;&#23398;&#20064;&#29615;&#22659;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#29983;&#24314;&#27169;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LLM-SS&#65292;&#21033;&#29992;LLMs&#21512;&#25104;&#23398;&#29983;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#29305;&#23450;&#23398;&#29983;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#30340;&#35299;&#20915;&#23581;&#35797;&#20316;&#20026;&#35266;&#23519;&#65292;&#30446;&#26631;&#26159;&#21512;&#25104;&#35813;&#23398;&#29983;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#19982;&#19981;&#21516;&#30340;LLMs&#32467;&#21512;&#20351;&#29992;&#65307;&#32780;&#19988;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#23427;&#20204;&#23545;&#39046;&#22495;&#32972;&#26223;&#21644;&#23398;&#29983;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#20855;&#20307;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#32500;&#25345;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.10686</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#26641;&#25628;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Autonomous Tree-search Ability of Large Language Models. (arXiv:2310.10686v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10686
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#32500;&#25345;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#24182;&#20135;&#29983;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#39044;&#35265;&#21644;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#22806;&#37096;&#31243;&#24207;&#23450;&#20041;&#25628;&#32034;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#34987;&#21160;&#30340;&#26641;&#25628;&#32034;&#26469;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#33509;&#24178;&#22522;&#26412;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#34987;&#21160;&#30340;&#26641;&#25628;&#32034;&#19981;&#39640;&#25928;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#36718;&#35821;&#35328;&#27169;&#22411;API&#35843;&#29992;&#26469;&#35299;&#20915;&#21333;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#34987;&#21160;&#25628;&#32034;&#26041;&#27861;&#19981;&#28789;&#27963;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#31243;&#24207;&#35774;&#35745;&#12290;&#28982;&#21518;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#20986;&#29616;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#31243;&#24207;&#30340;&#36741;&#21161;&#19979;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#30340;&#26641;&#25628;&#32034;&#33021;&#21147;&#65292;&#20173;&#28982;&#33021;&#22815;&#29983;&#25104;&#28165;&#26224;&#23637;&#31034;&#26641;&#32467;&#26500;&#25628;&#32034;&#36807;&#31243;&#30340;&#21709;&#24212;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#31216;&#20026;&#33258;&#20027;&#26641;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PS-AAS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#20013;&#12290;&#36890;&#36807;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26500;&#24314;&#31639;&#27861;&#20043;&#38388;&#30340;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31639;&#27861;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#36873;&#25321;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10685</link><description>&lt;p&gt;
PS-AAS: &#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#22312;&#40657;&#30418;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization. (arXiv:2310.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10685
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PS-AAS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#20013;&#12290;&#36890;&#36807;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26500;&#24314;&#31639;&#27861;&#20043;&#38388;&#30340;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31639;&#27861;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31639;&#27861;&#36873;&#25321;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#65288;AAS&#65289;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#35201;&#36873;&#25321;&#30340;&#31639;&#27861;&#25237;&#36164;&#32452;&#21512;&#12290;&#36873;&#25321;&#25237;&#36164;&#32452;&#21512;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#22823;&#22411;&#25237;&#36164;&#32452;&#21512;&#30340;&#39640;&#28789;&#27963;&#24615;&#21644;AAS&#20219;&#21153;&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#23454;&#38469;&#20013;&#65292;&#36873;&#25321;&#25237;&#36164;&#32452;&#21512;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#21487;&#33021;&#26159;&#22312;&#19968;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#20219;&#21153;&#20013;&#36873;&#25321;&#34920;&#29616;&#33391;&#22909;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#26367;&#20195;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21019;&#24314;&#31639;&#27861;&#34892;&#20026;&#20803;&#34920;&#31034;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#20803;&#34920;&#31034;&#30456;&#20284;&#24615;&#20174;&#19968;&#32452;&#31639;&#27861;&#20013;&#26500;&#24314;&#22270;&#24418;&#65292;&#24182;&#24212;&#29992;&#22270;&#24418;&#31639;&#27861;&#26469;&#36873;&#25321;&#22810;&#26679;&#21270;&#12289;&#20195;&#34920;&#24615;&#21644;&#38750;&#20887;&#20313;&#30340;&#26368;&#32456;&#25237;&#36164;&#32452;&#21512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20803;&#34920;&#31034;&#25216;&#26415;&#65288;SHAP&#21644;performance2vec&#65289;&#26469;&#36873;&#25321;&#38468;&#21152;&#30340;&#25237;&#36164;&#32452;&#21512;&#65292;&#20849;&#35745;324&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of automated algorithm selection (AAS) strongly depends on the portfolio of algorithms to choose from. Selecting the portfolio is a non-trivial task that requires balancing the trade-off between the higher flexibility of large portfolios with the increased complexity of the AAS task. In practice, probably the most common way to choose the algorithms for the portfolio is a greedy selection of the algorithms that perform well in some reference tasks of interest.  We set out in this work to investigate alternative, data-driven portfolio selection techniques. Our proposed method creates algorithm behavior meta-representations, constructs a graph from a set of algorithms based on their meta-representation similarity, and applies a graph algorithm to select a final portfolio of diverse, representative, and non-redundant algorithms. We evaluate two distinct meta-representation techniques (SHAP and performance2vec) for selecting complementary portfolios from a total of 324 diff
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.10679</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#36328;&#25991;&#21270;&#20010;&#24615;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Large language models can replicate cross-cultural differences in personality. (arXiv:2310.10679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10679
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25104;&#21151;&#22797;&#21046;&#20102;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#65292;&#20294;&#20854;&#32467;&#26524;&#34920;&#26126;&#24179;&#22343;&#35780;&#32423;&#26377;&#19978;&#21319;&#20559;&#24046;&#21644;&#36739;&#20302;&#30340;&#21464;&#24322;&#24615;&#19982;&#32467;&#26500;&#25928;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#39564;(N=8000)&#26469;&#30830;&#23450;GPT-4&#26159;&#21542;&#21487;&#20197;&#22797;&#21046;&#20351;&#29992;&#21313;&#39033;&#20154;&#26684;&#38382;&#21367;&#27979;&#37327;&#30340;&#22823;&#20116;&#20154;&#26684;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#25105;&#20204;&#36873;&#25321;&#32654;&#22269;&#21644;&#38889;&#22269;&#20316;&#20026;&#25991;&#21270;&#23545;&#27604;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20004;&#20010;&#22269;&#23478;&#30340;&#20154;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#20154;&#26684;&#24046;&#24322;&#12290;&#25105;&#20204;&#25805;&#32437;&#20102;&#27169;&#25311;&#30340;&#30446;&#26631;&#65288;&#32654;&#22269; vs. &#38889;&#22269;&#65289;&#65292;&#38382;&#21367;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821; vs. &#38889;&#35821;&#65289;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4 vs. GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#22797;&#21046;&#20102;&#27599;&#20010;&#22240;&#23376;&#30340;&#36328;&#25991;&#21270;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#35780;&#32423;&#20855;&#26377;&#19978;&#21319;&#20559;&#24046;&#65292;&#24182;&#19988;&#27604;&#20154;&#31867;&#26679;&#26412;&#30340;&#21464;&#24322;&#24615;&#26356;&#20302;&#65292;&#20197;&#21450;&#32467;&#26500;&#25928;&#24230;&#36739;&#20302;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#35777;&#25454;&#35828;&#26126;LLMs&#21487;&#20197;&#20419;&#36827;&#36328;&#25991;&#21270;&#24515;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. Overall, we provide preliminary evidence that LLMs can aid cross-cultural psychological research.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;OMVI&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;OMD&#65292;&#36890;&#36807;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#26469;&#24212;&#23545;&#24694;&#24847;&#36719;&#20214;&#30340;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.10670</link><description>&lt;p&gt;
&#26234;&#33021;OMVI&#65306;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset. (arXiv:2310.10670v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10670
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;OMVI&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;OMD&#65292;&#36890;&#36807;&#35782;&#21035;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#26469;&#24212;&#23545;&#24694;&#24847;&#36719;&#20214;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#32593;&#32476;&#23433;&#20840;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#38543;&#30528;&#27599;&#22825;&#35745;&#31639;&#26426;&#20351;&#29992;&#30340;&#22686;&#38271;&#12290;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#29616;&#22312;&#36827;&#34892;&#30340;&#19981;&#20165;&#20165;&#26159;&#30149;&#27602;&#20256;&#25773;&#21644;&#35745;&#31639;&#26426;&#40657;&#23458;&#34892;&#20026;&#12290;&#30001;&#20110;&#23041;&#32961;&#21040;&#19968;&#20010;&#22269;&#23478;&#30340;&#29983;&#23384;&#65292;&#32593;&#32476;&#25112;&#20105;&#24050;&#32463;&#21457;&#23637;&#36215;&#26469;&#12290;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#30340;&#31532;&#19968;&#36947;&#38450;&#32447;&#65292;&#24182;&#19988;&#26159;&#32593;&#32476;&#29359;&#32618;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#22825;&#65292;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#30446;&#26631;&#20247;&#22810;&#30340;&#35745;&#31639;&#26426;&#29992;&#25143;&#12289;&#20225;&#19994;&#21644;&#25919;&#24220;&#26426;&#26500;&#65292;&#36896;&#25104;&#25968;&#21313;&#20159;&#32654;&#20803;&#30340;&#25439;&#22833;&#12290;&#23613;&#31649;&#23433;&#20840;&#19987;&#23478;&#25317;&#26377;&#21508;&#31181;&#24037;&#20855;&#26469;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#65292;&#20294;&#24694;&#24847;&#36719;&#20214;&#21487;&#20197;&#36890;&#36807;&#20854;&#35774;&#35745;&#24072;&#36827;&#34892;&#24494;&#23567;&#24039;&#22937;&#30340;&#35843;&#25972;&#26469;&#35268;&#36991;&#22810;&#20010;&#26432;&#27602;&#36719;&#20214;&#30340;&#26816;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#25968;&#25454;&#38598;(OMD)&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#20010;&#19981;&#21516;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#65292;&#26377;21924&#20010;&#26679;&#26412;&#65292;&#24182;&#19988;&#37319;&#29992;&#20102;&#27169;&#25311;&#25915;&#20987;&#32773;&#31574;&#30053;&#30340;&#28151;&#28102;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity has become a significant issue in the digital era as a result of the growth in everyday computer use. Cybercriminals now engage in more than virus distribution and computer hacking. Cyberwarfare has developed as a result because it has become a threat to a nation's survival. Malware analysis serves as the first line of defence against an attack and is a significant component of cybercrime. Every day, malware attacks target a large number of computer users, businesses, and governmental agencies, causing billions of dollars in losses. Malware may evade multiple AV software with a very minor, cunning tweak made by its designers, despite the fact that security experts have a variety of tools at their disposal to identify it. To address this challenge, a new dataset called the Obfuscated Malware Dataset (OMD) has been developed. This dataset comprises 40 distinct malware families having 21924 samples, and it incorporates obfuscation techniques that mimic the strategies employe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#20803;&#23431;&#23449;&#30340;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#20803;&#23431;&#23449;&#23558;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#36827;&#34892;&#21457;&#23637;&#65292;&#24182;&#25910;&#38598;&#29992;&#25143;&#25968;&#25454;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20063;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.10665</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#65288;AI-XR&#65289;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Privacy Preservation in Artificial Intelligence and Extended Reality (AI-XR) Metaverses: A Survey. (arXiv:2310.10665v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#25193;&#23637;&#29616;&#23454;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#12290;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#20803;&#23431;&#23449;&#30340;&#38544;&#31169;&#38382;&#39064;&#20196;&#20154;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#21450;&#30340;&#24773;&#20917;&#19979;&#12290;&#20803;&#23431;&#23449;&#23558;&#21033;&#29992;&#22810;&#31181;&#25216;&#26415;&#36827;&#34892;&#21457;&#23637;&#65292;&#24182;&#25910;&#38598;&#29992;&#25143;&#25968;&#25454;&#26469;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#30340;&#26381;&#21153;&#65292;&#20294;&#36825;&#20063;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#26159;&#19968;&#20010;&#26032;&#20852;&#27010;&#24565;&#65292;&#23427;&#35774;&#24819;&#20102;&#19968;&#20010;&#34394;&#25311;&#23431;&#23449;&#65292;&#19968;&#20010;&#21327;&#20316;&#31354;&#38388;&#65292;&#20010;&#20307;&#21487;&#20197;&#22312;&#20854;&#20013;&#20114;&#21160;&#12289;&#21019;&#36896;&#21644;&#21442;&#19982;&#21508;&#31181;&#27963;&#21160;&#12290;&#38543;&#30528;&#36825;&#20010;&#27010;&#24565;&#30340;&#21457;&#23637;&#21644;&#27785;&#28024;&#24335;&#34394;&#25311;&#20307;&#39564;&#30340;&#26222;&#21450;&#65292;&#20803;&#23431;&#23449;&#20013;&#30340;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20803;&#23431;&#23449;&#38544;&#31169;&#38382;&#39064;&#25351;&#30340;&#26159;&#22312;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#30340;&#20010;&#20154;&#20449;&#24687;&#21644;&#25968;&#25454;&#38544;&#31169;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20851;&#20999;&#65292;&#22240;&#20026;&#20849;&#20139;&#30340;VR&#31354;&#38388;&#27010;&#24565;&#21464;&#24471;&#26356;&#21152;&#21487;&#25509;&#36817;&#12290;&#20803;&#23431;&#23449;&#23558;&#20511;&#21161;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#12289;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#21644;&#22522;&#20110;5G/6G&#36890;&#20449;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#27785;&#28024;&#24335;&#26381;&#21153;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#26356;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20803;&#23431;&#23449;&#20381;&#36182;&#20110;&#25910;&#38598;&#31934;&#32454;&#21270;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#23548;&#33268;&#20102;&#21508;&#31181;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#23436;&#20840;&#23454;&#29616;&#20803;&#23431;&#23449;&#30340;&#28508;&#21147;&#20043;&#21069;&#65292;&#38544;&#31169;&#20445;&#25252;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse is a nascent concept that envisions a virtual universe, a collaborative space where individuals can interact, create, and participate in a wide range of activities. Privacy in the metaverse is a critical concern as the concept evolves and immersive virtual experiences become more prevalent. The metaverse privacy problem refers to the challenges and concerns surrounding the privacy of personal information and data within Virtual Reality (VR) environments as the concept of a shared VR space becomes more accessible. Metaverse will harness advancements from various technologies such as Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and 5G/6G-based communication to provide personalized and immersive services to its users. Moreover, to enable more personalized experiences, the metaverse relies on the collection of fine-grained user data that leads to various privacy issues. Therefore, before the potential of the metaverse can be fully realized, privacy
&lt;/p&gt;</description></item><item><title>VeriDIP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#21644;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;</title><link>http://arxiv.org/abs/2310.10656</link><description>&lt;p&gt;
VeriDIP: &#36890;&#36807;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#26435;
&lt;/p&gt;
&lt;p&gt;
VeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints. (arXiv:2310.10656v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10656
&lt;/p&gt;
&lt;p&gt;
VeriDIP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#21644;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#39033;&#26381;&#21153;&#20250;&#23548;&#33268;&#27169;&#22411;&#25220;&#34989;&#65292;&#20174;&#32780;&#23548;&#33268;&#29256;&#26435;&#20405;&#26435;&#12290;&#25152;&#26377;&#26435;&#27979;&#35797;&#25216;&#26415;&#26088;&#22312;&#35782;&#21035;&#27169;&#22411;&#25351;&#32441;&#20197;&#39564;&#35777;&#25220;&#34989;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#24230;&#25311;&#21512;&#25110;&#40065;&#26834;&#24615;&#29305;&#24449;&#20316;&#20026;&#25351;&#32441;&#65292;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#24191;&#20041;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#36275;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VeriDIP&#30340;&#26032;&#22411;&#25152;&#26377;&#26435;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;DNN&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;VeriDIP&#20570;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;(1)&#23427;&#21033;&#29992;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#26469;&#20272;&#35745;&#38544;&#31169;&#27844;&#38706;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#21453;&#26144;&#20102;&#32473;&#23450;&#27169;&#22411;&#30340;&#25351;&#32441;&#12290;&#38544;&#31169;&#27844;&#38706;&#25351;&#32441;&#31361;&#20986;&#20102;&#27169;&#22411;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;(2)&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36739;&#23569;&#31169;&#26377;&#26679;&#26412;&#22686;&#24378;&#25152;&#26377;&#26435;&#27979;&#35797;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;VeriDIP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Machine Learning as a Service gives rise to model plagiarism, leading to copyright infringement. Ownership testing techniques are designed to identify model fingerprints for verifying plagiarism. However, previous works often rely on overfitting or robustness features as fingerprints, lacking theoretical guarantees and exhibiting under-performance on generalized models. In this paper, we propose a novel ownership testing method called VeriDIP, which verifies a DNN model's intellectual property. VeriDIP makes two major contributions. (1) It utilizes membership inference attacks to estimate the lower bound of privacy leakage, which reflects the fingerprint of a given model. The privacy leakage fingerprints highlight the unique patterns through which the models memorize sensitive training datasets. (2) We introduce a novel approach using less private samples to enhance the performance of ownership testing.  Extensive experimental results confirm that VeriDIP is effective and eff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36873;&#25321;&#24615;&#21548;&#35273;&#26426;&#21046;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#23450;&#20301;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#39057;&#35889;&#25513;&#34109;&#26469;&#28040;&#38500;&#24178;&#25200;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#65292;&#24182;&#20351;&#29992;LSTM&#32593;&#32476;&#20174;&#36807;&#28388;&#21518;&#30340;&#39057;&#35889;&#20013;&#25552;&#21462;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.10497</link><description>&lt;p&gt;
LocSelect: &#20855;&#26377;&#21548;&#35273;&#36873;&#25321;&#24615;&#21548;&#35273;&#26426;&#21046;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism. (arXiv:2310.10497v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36873;&#25321;&#24615;&#21548;&#35273;&#26426;&#21046;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#23450;&#20301;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#39057;&#35889;&#25513;&#34109;&#26469;&#28040;&#38500;&#24178;&#25200;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#65292;&#24182;&#20351;&#29992;LSTM&#32593;&#32476;&#20174;&#36807;&#28388;&#21518;&#30340;&#39057;&#35889;&#20013;&#25552;&#21462;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#34920;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36873;&#25321;&#24615;&#21548;&#35273;&#26426;&#21046;&#30340;&#30446;&#26631;&#35828;&#35805;&#20154;&#23450;&#20301;&#31639;&#27861;&#12290;&#36890;&#36807;&#32473;&#23450;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#21442;&#32771;&#35821;&#38899;&#65292;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#39057;&#35889;&#25513;&#34109;&#65292;&#20197;&#28040;&#38500;&#24178;&#25200;&#35828;&#35805;&#20154;&#30340;&#35821;&#38899;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20174;&#36807;&#28388;&#21518;&#30340;&#39057;&#35889;&#20013;&#25552;&#21462;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23610;&#24230;&#19981;&#21464;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;SNR&#20026;-10 dB&#26102;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;LocSelect&#23454;&#29616;&#20102;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;3.55&#21644;&#20934;&#30830;&#24615;&#65288;ACC&#65289;&#20026;87.40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevailing noise-resistant and reverberation-resistant localization algorithms primarily emphasize separating and providing directional output for each speaker in multi-speaker scenarios, without association with the identity of speakers. In this paper, we present a target speaker localization algorithm with a selective hearing mechanism. Given a reference speech of the target speaker, we first produce a speaker-dependent spectrogram mask to eliminate interfering speakers' speech. Subsequently, a Long short-term memory (LSTM) network is employed to extract the target speaker's location from the filtered spectrogram. Experiments validate the superiority of our proposed method over the existing algorithms for different scale invariant signal-to-noise ratios (SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
&lt;/p&gt;</description></item><item><title>BOSS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.10021</link><description>&lt;p&gt;
&#33258;&#20027;&#23398;&#20064;&#25216;&#33021;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance. (arXiv:2310.10021v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10021
&lt;/p&gt;
&lt;p&gt;
BOSS&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36890;&#36807;&#35813;&#26041;&#27861;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BOSS&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#30417;&#30563;&#26469;&#33258;&#21160;&#23398;&#20064;&#35299;&#20915;&#38271;&#26102;&#31243;&#12289;&#22797;&#26434;&#19988;&#26377;&#24847;&#20041;&#30340;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#19987;&#23478;&#30340;&#30417;&#30563;&#65292;&#20197;&#31034;&#33539;&#25110;&#23500;&#21547;&#22870;&#21169;&#20989;&#25968;&#30340;&#24418;&#24335;&#26469;&#23398;&#20064;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;BOSS&#26041;&#27861;&#36890;&#36807;&#25191;&#34892;&#8220;&#25216;&#33021;&#24341;&#23548;&#8221;&#26469;&#23398;&#20064;&#23436;&#25104;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#20855;&#22791;&#19968;&#32452;&#21407;&#22987;&#25216;&#33021;&#30340;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#22312;&#21021;&#22987;&#25216;&#33021;&#38598;&#20043;&#22806;&#30340;&#20219;&#21153;&#20013;&#19981;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#12290;&#36825;&#31181;&#24341;&#23548;&#38454;&#27573;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25351;&#23548;&#65292;&#21521;&#20195;&#29702;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25216;&#33021;&#32452;&#21512;&#12290;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#65292;BOSS&#33021;&#22815;&#20174;&#22522;&#26412;&#30340;&#21407;&#22987;&#25216;&#33021;&#20013;&#26500;&#24314;&#20986;&#21508;&#31181;&#22797;&#26434;&#26377;&#29992;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#36924;&#30495;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#24341;&#23548;&#24341;&#23548;&#30340;BOSS&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#24182;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#20013;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30456;&#21305;&#37197;&#25110;&#26377;&#25152;&#25913;&#36827;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.09986</link><description>&lt;p&gt;
&#20851;&#20110;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65288;arXiv&#65306;2310.09986v2 [cs.LG] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
On Statistical Learning of Branch and Bound for Vehicle Routing Optimization. (arXiv:2310.09986v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36710;&#36742;&#36335;&#24452;&#20248;&#21270;&#20013;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#32479;&#35745;&#23398;&#20064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#33021;&#22815;&#19982;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30456;&#21305;&#37197;&#25110;&#26377;&#25152;&#25913;&#36827;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#36817;&#20284;&#27714;&#35299;NP&#22256;&#38590;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24182;&#20840;&#38754;&#27604;&#36739;&#20102;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26524; - &#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#65292;GraphSAGE&#21644;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;GAT&#65289; - &#26469;&#35299;&#20915;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20197;&#27169;&#25311;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24378;&#20998;&#25903;&#31574;&#30053;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;CVRLIB&#30340;&#20845;&#20010;&#20855;&#26377;&#19981;&#21516;&#25299;&#25169;&#32467;&#26500;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#20010;&#23454;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27714;&#35299;CVRP&#23454;&#20363;&#25152;&#38656;&#30340;&#26368;&#23567;&#36710;&#36742;&#25968;&#20943;&#23569;&#21040;&#20102;&#19968;&#20010;&#35013;&#31665;&#38382;&#39064;&#20013;&#65292;&#24182;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#20351;&#29992;&#24378;&#20998;&#25903;&#31574;&#30053;&#30340;&#20998;&#25903;&#30028;&#23450;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#65292;&#21516;&#26102;&#38656;&#35201;&#30340;&#35745;&#31639;&#36164;&#28304; significantly less comp.
&lt;/p&gt;
&lt;p&gt;
Recently, machine learning of the branch and bound algorithm has shown promise in approximating competent solutions to NP-hard problems. In this paper, we utilize and comprehensively compare the outcomes of three neural networks--graph convolutional neural network (GCNN), GraphSAGE, and graph attention network (GAT)--to solve the capacitated vehicle routing problem. We train these neural networks to emulate the decision-making process of the computationally expensive Strong Branching strategy. The neural networks are trained on six instances with distinct topologies from the CVRPLIB and evaluated on eight additional instances. Moreover, we reduced the minimum number of vehicles required to solve a CVRP instance to a bin-packing problem, which was addressed in a similar manner. Through rigorous experimentation, we found that this approach can match or improve upon the performance of the branch and bound algorithm with the Strong Branching strategy while requiring significantly less comp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09680</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Contextual Recognition In Automatic Speech Recognition Systems By Semantic Lattice Rescoring. (arXiv:2310.09680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;transformer&#30340;&#37325;&#26032;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#37325;&#25490;&#24207;&#26469;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#31361;&#30772;&#20351;&#24471;ASR&#31995;&#32479;&#22312;&#20934;&#30830;&#36716;&#24405;&#21475;&#35821;&#30340;&#33021;&#21147;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36825;&#26159;&#26500;&#24314;&#23545;&#35805;&#20195;&#29702;&#30340;&#20851;&#38190;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#36776;&#21035;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#20173;&#28982;&#26159;&#19968;&#39033;&#36843;&#20999;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#20041;&#26684;&#22788;&#29702;&#26469;&#22686;&#24378;ASR&#31995;&#32479;&#20013;&#19978;&#19979;&#25991;&#35782;&#21035;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#20132;&#20184;&#21508;&#31181;&#35789;&#27719;&#21644;&#35828;&#35805;&#39118;&#26684;&#30340;&#36716;&#24405;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;HMM-GMM&#65289;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#22768;&#23398;&#24314;&#27169;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21333;&#35789;&#26684;&#65292;&#20351;&#25105;&#20204;&#30340;&#32593;&#32476;&#20855;&#22791;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) has witnessed a profound research interest. Recent breakthroughs have given ASR systems different prospects such as faithfully transcribing spoken language, which is a pivotal advancement in building conversational agents. However, there is still an imminent challenge of accurately discerning context-dependent words and phrases. In this work, we propose a novel approach for enhancing contextual recognition within ASR systems via semantic lattice processing leveraging the power of deep learning models in accurately delivering spot-on transcriptions across a wide variety of vocabularies and speaking styles. Our solution consists of using Hidden Markov Models and Gaussian Mixture Models (HMM-GMM) along with Deep Neural Networks (DNN) models integrating both language and acoustic modeling for better accuracy. We infused our network with the use of a transformer-based model to properly rescore the word lattice achieving remarkable capabilities with a palpa
&lt;/p&gt;</description></item><item><title>Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09486</link><description>&lt;p&gt;
Mirage: &#22270;&#20998;&#31867;&#30340;&#27169;&#22411;&#26080;&#20851;&#22270;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09486
&lt;/p&gt;
&lt;p&gt;
Mirage&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#21644;&#23545;&#24314;&#27169;&#27969;&#31243;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GNNs&#21644;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#25968;&#25454;&#21644;&#35745;&#31639;&#38656;&#27714;&#37327;&#24456;&#22823;&#12290;&#24613;&#38656;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#25193;&#23637;GNN&#30340;&#35757;&#32451;&#65292;&#20197;&#20415;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#22270;&#33976;&#39311;&#26159;&#20026;&#27492;&#30446;&#30340;&#32780;&#21162;&#21147;&#65292;&#26088;&#22312;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#30340;&#21512;&#25104;&#35757;&#32451;&#38598;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#21021;&#27493;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;(1)&#29616;&#26377;&#30340;&#22270;&#33976;&#39311;&#31639;&#27861;&#26412;&#36523;&#20381;&#36182;&#20110;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#23601;&#30772;&#22351;&#20102;&#22270;&#33976;&#39311;&#30340;&#21069;&#25552;&#12290;(2)&#33976;&#39311;&#36807;&#31243;&#23545;&#30446;&#26631;GNN&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20855;&#26377;&#29305;&#24322;&#24615;&#65292;&#22240;&#27492;&#23545;&#24314;&#27169;&#27969;&#31243;&#30340;&#21464;&#21270;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#21517;&#20026;Mirage&#30340;&#22270;&#20998;&#31867;&#33976;&#39311;&#31639;&#27861;&#26469;&#36991;&#20813;&#36825;&#20123;&#38480;&#21046;&#12290;Mirage&#24314;&#31435;&#22312;&#19968;&#20010;&#27934;&#23519;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#19968;&#20010;&#28040;&#24687;&#20256;&#36882;&#30340;GNN&#23558;&#36755;&#20837;&#22270;&#20998;&#35299;&#20026;&#35745;&#31639;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09053</link><description>&lt;p&gt;
DATT&#65306;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DATT: Deep Adaptive Trajectory Tracking for Quadrotor Control. (arXiv:2310.09053v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09053
&lt;/p&gt;
&lt;p&gt;
DATT&#26159;&#19968;&#31181;&#29992;&#20110;&#22235;&#26059;&#32764;&#25511;&#21046;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#31934;&#30830;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#33021;&#22815;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#36827;&#34892;&#22686;&#24378;&#65292;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#12289;&#36712;&#36857;&#19981;&#21487;&#34892;&#24615;&#21644;&#25191;&#34892;&#38480;&#21046;&#65292;&#23545;&#20110;&#22235;&#26059;&#32764;&#30340;&#31934;&#30830;&#20219;&#24847;&#36712;&#36857;&#36319;&#36394;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#33258;&#36866;&#24212;&#36712;&#36857;&#36319;&#36394;&#65288;DATT&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#31934;&#30830;&#22320;&#36319;&#36394;&#20219;&#24847;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25511;&#21046;&#12290;DATT&#22522;&#20110;&#19968;&#31181;&#22312;&#20223;&#30495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#26032;&#39062;&#21069;&#39304;-&#21453;&#39304;&#33258;&#36866;&#24212;&#25511;&#21046;&#32467;&#26500;&#12290;&#24403;&#22312;&#30495;&#23454;&#30828;&#20214;&#19978;&#37096;&#32626;&#26102;&#65292;DATT&#36890;&#36807;&#22312;&#38381;&#29615;&#20013;&#20351;&#29992;L1&#33258;&#36866;&#24212;&#25511;&#21046;&#30340;&#24178;&#25200;&#20272;&#35745;&#22120;&#36827;&#34892;&#22686;&#24378;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;DATT&#22312;&#20855;&#26377;&#19981;&#31283;&#23450;&#39118;&#22330;&#30340;&#21487;&#34892;&#24179;&#28369;&#21644;&#19981;&#21487;&#34892;&#36712;&#36857;&#19978;&#65292;&#26126;&#26174;&#20248;&#20110;&#31454;&#20105;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#22522;&#32447;&#23436;&#20840;&#22833;&#25928;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;DATT&#21487;&#20197;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#39640;&#25928;&#36816;&#34892;&#65292;&#25512;&#29702;&#26102;&#38388;&#19981;&#21040;3.2&#27627;&#31186;&#65292;&#20165;&#20026;&#19968;/&#22235;&#20998;&#20043;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise arbitrary trajectory tracking for quadrotors is challenging due to unknown nonlinear dynamics, trajectory infeasibility, and actuation limits. To tackle these challenges, we present Deep Adaptive Trajectory Tracking (DATT), a learning-based approach that can precisely track arbitrary, potentially infeasible trajectories in the presence of large disturbances in the real world. DATT builds on a novel feedforward-feedback-adaptive control structure trained in simulation using reinforcement learning. When deployed on real hardware, DATT is augmented with a disturbance estimator using L1 adaptive control in closed-loop, without any fine-tuning. DATT significantly outperforms competitive adaptive nonlinear and model predictive controllers for both feasible smooth and infeasible trajectories in unsteady wind fields, including challenging scenarios where baselines completely fail. Moreover, DATT can efficiently run online with an inference time less than 3.2 ms, less than 1/4 of the ad
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08915</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#65306;&#38024;&#23545;&#31232;&#30095;LLMs&#30340;&#26080;&#35757;&#32451;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. (arXiv:2310.08915v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#26356;&#26032;&#31232;&#30095;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#20943;&#23567;&#23558;&#20854;&#37096;&#32626;&#21040;&#35774;&#22791;&#19978;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24222;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#24320;&#36767;&#20102;&#28508;&#22312;&#36335;&#24452;&#65292;&#20294;&#24456;&#36951;&#25022;&#65292;&#22312;&#20854;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#36947;&#36335;&#19978;&#23384;&#22312;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#38556;&#30861;&#12290;&#20316;&#20026;&#22312;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#26041;&#38754;&#26368;&#25104;&#29087;&#30340;&#39044;-LLMs&#26041;&#27861;&#20043;&#19968;&#65292;&#32593;&#32476;&#20462;&#21098;&#20284;&#20046;&#22312;LLMs&#26102;&#20195;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#38656;&#35201;&#26114;&#36149;&#30340;&#24494;&#35843;(&#25110;&#37325;&#26032;&#35757;&#32451;)&#12290;&#20026;&#20102;&#24357;&#21512;&#20135;&#19994;&#19982;&#23398;&#26415;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#31232;&#30095;&#26080;&#35757;&#32451;(DSnoT)&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#35757;&#32451;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#22312;&#19981;&#36827;&#34892;&#26114;&#36149;&#30340;&#21453;&#21521;&#20256;&#25773;&#21644;&#20219;&#20309;&#26435;&#37325;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#30053;&#24494;&#26356;&#26032;&#31232;&#30095;LLMs&#12290;&#21463;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#30340;&#21551;&#21457;&#65292;DSnoT&#36890;&#36807;&#22312;&#31232;&#30095;LLMs&#20043;&#19978;&#25191;&#34892;&#36845;&#20195;&#30340;&#26435;&#37325;&#20462;&#21098;&#21644;&#29983;&#38271;&#30340;&#26041;&#24335;&#65292;&#26368;&#23567;&#21270;&#20102;&#31264;&#23494;&#21644;&#31232;&#30095;LLMs&#20043;&#38388;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#65292;DSnoT&#29305;&#21035;&#32771;&#34385;&#20102;&#39044;&#26399;&#30340;&#20943;&#23569;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08008</link><description>&lt;p&gt;
BERT&#24191;&#20041;&#24615;&#30340;&#24433;&#21709;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#21644;&#21451;&#22909;&#26679;&#26412;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#39046;&#20808;&#27036;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#38656;&#35201;&#27867;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#65288;&#20855;&#26377;&#30475;&#20284;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#21644;&#20154;&#20026;&#21451;&#22909;&#26679;&#26412;&#65288;&#20855;&#26377;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#20197;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#20026;&#32463;&#39564;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#26368;&#22810;20&#20010;&#30334;&#20998;&#28857;&#12290;&#36229;&#36807;&#27492;&#33539;&#22260;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training \textit{data quality}, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30\% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07276</link><description>&lt;p&gt;
BioT5&#65306;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#36328;&#27169;&#24577;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07276
&lt;/p&gt;
&lt;p&gt;
BioT5&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#21033;&#29992;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#36328;&#27169;&#24577;&#25972;&#21512;&#65292;&#36890;&#36807;&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#25552;&#21462;&#65292;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#21033;&#29992;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#23637;&#21033;&#29992;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25972;&#21512;&#26469;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#29983;&#25104;&#26080;&#25928;&#30340;&#20998;&#23376;SMILES&#12289;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21033;&#29992;&#19981;&#36275;&#20197;&#21450;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#31561;&#37327;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;BioT5&#65292;&#23427;&#36890;&#36807;&#21270;&#23398;&#30693;&#35782;&#21644;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#20016;&#23500;&#20102;&#29983;&#29289;&#23398;&#20013;&#30340;&#36328;&#27169;&#24577;&#25972;&#21512;&#12290;BioT5&#21033;&#29992;SELFIES&#36827;&#34892;100%&#40065;&#26834;&#30340;&#20998;&#23376;&#34920;&#31034;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#29983;&#29289;&#25991;&#29486;&#20013;&#25552;&#21462;&#29983;&#29289;&#23454;&#20307;&#21608;&#22260;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;BioT5&#21306;&#20998;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20449;&#24687;&#12290;&#22312;&#24494;&#35843;&#21518;&#65292;BioT5&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;</title><link>http://arxiv.org/abs/2310.06434</link><description>&lt;p&gt;
Whispering LLaMA&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition. (arXiv:2310.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06434
&lt;/p&gt;
&lt;p&gt;
Whispering LLaMA&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#38169;&#35823;&#26657;&#27491;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22768;&#23398;&#20449;&#24687;&#21644;&#22806;&#37096;&#35821;&#35328;&#34920;&#31034;&#65292;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#20934;&#30830;&#30340;&#35821;&#38899;&#36716;&#24405;&#19978;&#19979;&#25991;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#30340;&#29983;&#25104;&#24335;&#38169;&#35823;&#26657;&#27491;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#36816;&#29992;&#29420;&#29305;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#21644;&#21442;&#25968;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#22411;&#25552;&#21319;&#20102;ASR&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;ASR&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#34701;&#21512;&#25216;&#26415;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#65292;&#30456;&#23545;&#20110;n-best&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35789;&#38169;&#35823;&#29575;&#24615;&#33021;&#25552;&#21319;&#20102;37.66%&#12290;&#20026;&#20102;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#28304;&#22312;https://github.com/Srijith-rkr/Whispering-LLaMA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2310.05136</link><description>&lt;p&gt;
InstructDET: &#36890;&#29992;&#25351;&#20196;&#30340;&#24341;&#23548;&#19979;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InstructDET: Diversifying Referring Object Detection with Generalized Instructions. (arXiv:2310.05136v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05136
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructDET&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#24182;&#36827;&#34892;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21033;&#29992;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;InstructDET&#65292;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#20196;&#26469;&#23450;&#20301;&#30446;&#26631;&#23545;&#35937;&#30340;&#25351;&#31216;&#23545;&#35937;&#26816;&#27979;&#65288;ROD&#65289;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#19982;&#23545;&#35937;&#26816;&#27979;&#30456;&#20851;&#30340;&#24120;&#35265;&#29992;&#25143;&#24847;&#22270;&#12290;&#23545;&#20110;&#19968;&#24352;&#22270;&#20687;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#22823;&#37327;&#30340;&#25351;&#20196;&#65292;&#28041;&#21450;&#27599;&#20010;&#21333;&#29420;&#30340;&#23545;&#35937;&#21644;&#22810;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#27599;&#20010;&#25351;&#20196;&#21450;&#20854;&#23545;&#24212;&#30340;&#23545;&#35937;&#36793;&#30028;&#26694;&#26500;&#25104;&#19968;&#20010;&#35757;&#32451;&#25968;&#25454;&#23545;&#12290;&#20026;&#20102;&#21253;&#21547;&#24120;&#35265;&#30340;&#26816;&#27979;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#20852;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#36793;&#30028;&#26694;&#29983;&#25104;&#25351;&#20196;&#65292;&#22240;&#20026;&#22522;&#30784;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#30340;&#34920;&#36798;&#65288;&#20363;&#22914;&#65292;&#25551;&#36848;&#23545;&#35937;&#23646;&#24615;&#12289;&#31867;&#21035;&#21644;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#23558;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21629;&#21517;&#20026;InDET&#65292;&#21253;&#21547;&#22270;&#20687;&#12289;&#36793;&#30028;&#26694;&#21644;&#27867;&#21270;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose InstructDET, a data-centric method for referring object detection (ROD) that localizes target objects based on user instructions. While deriving from referring expressions (REC), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model (VLM) and large language model (LLM) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship). We name our constructed dataset as InDET. It contains images, bbxs and generalized instructions that are 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.04986</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#36135;&#24065;&#30340;&#26032;&#30340;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#36827;&#34892;&#20102;&#26681;&#26412;&#24615;&#25913;&#38761;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#22312;&#20869;&#12290;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#23558;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#32780;&#19981;&#26159;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;&#36148;&#29616;&#29616;&#37329;&#27969;&#29702;&#35770;&#12290;&#19982;&#23558;&#32929;&#31080;&#35270;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#30340;&#26080;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#25152;&#26377;&#26435;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#20316;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#26377;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#20132;&#26131;&#26435;&#30410;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#19968;&#20010;&#36127;&#36131;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#30340;&#36135;&#24065;&#65288;&#30005;&#23376;&#36135;&#24065;&#20379;&#24212;&#21644;&#20215;&#20540;&#31283;&#23450;&#65289;&#21644;&#36130;&#25919;&#65288;&#25237;&#36164;&#21644;&#36816;&#33829;&#65289;&#25919;&#31574;&#30340;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#30005;&#23376;&#36135;&#24065;&#30340;&#27969;&#21160;&#24615;&#12290;&#22312;&#20272;&#20540;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#39118;&#38505;&#27169;&#22411;&#19981;&#20250;&#26159;&#26080;&#22788;&#19981;&#22312;&#20294;&#19981;&#21512;&#36866;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#23427;&#23558;&#23548;&#33268;&#36148;&#29616;&#29575;&#65292;&#32780;&#26159;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01415</link><description>&lt;p&gt;
GPT-Driver: &#20351;&#29992;GPT&#23398;&#20064;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#36716;&#21270;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#38752;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36816;&#21160;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#26088;&#22312;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#33298;&#36866;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#29616;&#26377;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#20027;&#35201;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#26032;&#39062;&#21644;&#26410;&#30693;&#30340;&#39550;&#39542;&#22330;&#26223;&#26102;&#23637;&#29616;&#20986;&#19981;&#36275;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22266;&#26377;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#23558;&#36816;&#21160;&#35268;&#21010;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#35270;&#35282;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#20026;&#35821;&#35328;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;LLM&#36890;&#36807;&#23545;&#22352;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#26469;&#21457;&#29616;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPTLens&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#20004;&#20010;&#38454;&#27573;&#26469;&#36880;&#27493;&#26816;&#27979;&#21644;&#25913;&#36827;&#28431;&#27934;&#26816;&#27979;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35823;&#25253;&#12290;</title><link>http://arxiv.org/abs/2310.01152</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#65306;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives. (arXiv:2310.01152v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#26469;&#21457;&#29616;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPTLens&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#20004;&#20010;&#38454;&#27573;&#26469;&#36880;&#27493;&#26816;&#27979;&#21644;&#25913;&#36827;&#28431;&#27934;&#26816;&#27979;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35823;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25366;&#25496;&#26234;&#33021;&#21512;&#32422;&#20013;&#30340;&#28431;&#27934;&#30340;&#26426;&#20250;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#22522;&#20110;&#25105;&#20204;&#29616;&#26377;&#30340;&#30740;&#31350;&#12290;&#23545;&#20110;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#23454;&#29616;&#23454;&#29992;&#24615;&#21462;&#20915;&#20110;&#23613;&#21487;&#33021;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#30495;&#23454;&#28431;&#27934;&#21516;&#26102;&#26368;&#23567;&#21270;&#35823;&#25253;&#25968;&#30446;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#30683;&#30462;&#20294;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#29983;&#25104;&#26356;&#22810;&#30340;&#31572;&#26696;&#19988;&#26356;&#20855;&#38543;&#26426;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#23548;&#33268;&#20102;&#26356;&#22810;&#30340;&#35823;&#25253;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#26694;&#26550; GPTLens&#65292;&#23558;&#20256;&#32479;&#30340;&#21333;&#38454;&#27573;&#26816;&#27979;&#20998;&#20026;&#20004;&#20010;&#21327;&#21516;&#30340;&#38454;&#27573;&#8212;&#8212;&#29983;&#25104;&#21644;&#21306;&#20998;&#65292;&#29992;&#20110;&#36880;&#27493;&#30340;&#26816;&#27979;&#21644;&#25913;&#36827;&#65292;&#20854;&#20013;LLM&#20998;&#21035;&#25198;&#28436;&#30528;&#35780;&#23457;&#21592;&#21644;&#35780;&#35770;&#23478;&#30340;&#21452;&#37325;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a systematic analysis of the opportunities, challenges, and potential solutions of harnessing Large Language Models (LLMs) such as GPT-4 to dig out vulnerabilities within smart contracts based on our ongoing research. For the task of smart contract vulnerability detection, achieving practical usability hinges on identifying as many true vulnerabilities as possible while minimizing the number of false positives. Nonetheless, our empirical study reveals contradictory yet interesting findings: generating more answers with higher randomness largely boosts the likelihood of producing a correct answer but inevitably leads to a higher number of false positives. To mitigate this tension, we propose an adversarial framework dubbed GPTLens that breaks the conventional one-stage detection into two synergistic stages $-$ generation and discrimination, for progressive detection and refinement, wherein the LLM plays dual roles, i.e., auditor and critic, respectively. The goal of 
&lt;/p&gt;</description></item><item><title>ETGraph&#26159;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#35760;&#24405;&#21644;Twitter&#20851;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#39564;&#35777;OpenSea&#30340;Twitter&#36134;&#25143;&#19982;&#20197;&#22826;&#22346;&#22320;&#22336;&#36827;&#34892;&#32465;&#23450;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01015</link><description>&lt;p&gt;
ETGraph&#65306;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ETGraph: A Pioneering Dataset Bridging Ethereum and Twitter. (arXiv:2310.01015v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01015
&lt;/p&gt;
&lt;p&gt;
ETGraph&#26159;&#19968;&#20010;&#36830;&#25509;&#20197;&#22826;&#22346;&#21644;Twitter&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#20132;&#26131;&#35760;&#24405;&#21644;Twitter&#20851;&#27880;&#25968;&#25454;&#65292;&#36890;&#36807;&#39564;&#35777;OpenSea&#30340;Twitter&#36134;&#25143;&#19982;&#20197;&#22826;&#22346;&#22320;&#22336;&#36827;&#34892;&#32465;&#23450;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#20844;&#20849;&#21306;&#22359;&#38142;&#25968;&#25454;&#38598;&#21487;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#23545;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#21333;&#19968;&#20851;&#27880;&#12290;&#36825;&#31181;&#38480;&#21046;&#38480;&#21046;&#20102;&#30456;&#20851;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#19982;&#21306;&#22359;&#38142;&#20998;&#26512;&#30340;&#32467;&#21512;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#21487;&#20197;&#24471;&#20986;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#27934;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ETGraph&#65292;&#23427;&#26159;&#19968;&#20010;&#20840;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#30495;&#23454;&#22320;&#36830;&#25509;&#20102;&#20197;&#22826;&#22346;&#21644;Twitter&#65292;&#26159;&#20854;&#31867;&#21035;&#20013;&#31532;&#19968;&#20010;&#19988;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;ETGraph&#32467;&#21512;&#20102;&#20197;&#22826;&#22346;&#30340;&#20132;&#26131;&#35760;&#24405;&#65288;200&#19975;&#20010;&#33410;&#28857;&#21644;3000&#19975;&#26465;&#36793;&#65289;&#21644;Twitter&#30340;&#20851;&#27880;&#25968;&#25454;&#65288;100&#19975;&#20010;&#33410;&#28857;&#21644;300&#19975;&#26465;&#36793;&#65289;&#65292;&#23558;30667&#20010;&#20197;&#22826;&#22346;&#22320;&#22336;&#19982;&#26469;&#33258;OpenSea&#30340;&#24050;&#39564;&#35777;Twitter&#36134;&#25143;&#36827;&#34892;&#20102;&#32465;&#23450;&#12290;&#23545;ETGraph&#30340;&#35814;&#32454;&#32479;&#35745;&#20998;&#26512;&#31361;&#20986;&#20102;&#19982;Twitter&#21305;&#37197;&#21644;&#38750;Twitter&#21305;&#37197;&#30340;&#20197;&#22826;&#22346;&#22320;&#22336;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;&#21253;&#25324;&#20197;&#22826;&#22346;&#38142;&#36335;&#39044;&#27979;&#12289;&#34394;&#20551;&#20132;&#26131;&#20197;&#22826;&#22346;&#22320;&#22336;&#26816;&#27979;&#21644;Twitter-&#20197;&#22826;&#22346;&#21305;&#37197;&#38142;&#36335;&#39044;&#27979;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
While numerous public blockchain datasets are available, their utility is constrained by a singular focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. To address the above limitation, we introduce ETGraph, a novel dataset that authentically links Ethereum and Twitter, marking the first and largest dataset of its kind. ETGraph combines Ethereum transaction records (2 million nodes and 30 million edges) and Twitter following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified Twitter accounts sourced from OpenSea. Detailed statistical analysis on ETGraph highlights the structural differences between Twitter-matched and non-Twitter-matched Ethereum addresses. Extensive experiments, including Ethereum link prediction, wash-trading Ethereum addresses detection, and Twitter-Ethereum matching link prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2309.15714</link><description>&lt;p&gt;
ChatGPT-BCI&#65306;&#20351;&#29992;GPT&#12289;EEG&#21644;&#30524;&#21160;&#29983;&#29289;&#26631;&#35760;&#22120;&#22312;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#36827;&#34892;&#21333;&#35789;&#32423;&#31070;&#32463;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-BCI: Word-Level Neural State Classification Using GPT, EEG, and Eye-Tracking Biomarkers in Semantic Inference Reading Comprehension. (arXiv:2309.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#30340;&#31070;&#32463;&#29366;&#24577;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35821;&#20041;&#25512;&#29702;&#38405;&#35835;&#29702;&#35299;&#20013;&#31070;&#32463;&#29366;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;GPT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#29702;&#35299;&#35821;&#20041;&#35821;&#35328;&#24847;&#20041;&#30340;&#33021;&#21147;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#12290;&#36825;&#38656;&#35201;&#36328;&#35748;&#30693;&#31185;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#32852;&#21512;&#20998;&#26512;LLMs&#12289;&#30524;&#21160;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#30740;&#31350;&#22823;&#33041;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#22914;&#20309;&#22788;&#29702;&#19982;&#20851;&#38190;&#23383;&#30456;&#20851;&#31243;&#24230;&#19981;&#21516;&#30340;&#21333;&#35789;&#65292;&#20174;&#32780;&#25552;&#20379;&#20851;&#20110;&#20010;&#20307;&#31070;&#32463;&#29366;&#24577;&#22312;&#35821;&#20041;&#20851;&#31995;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#25913;&#36827;&#20102;&#19982;&#20851;&#38190;&#23383;&#39640;&#30456;&#20851;&#24230;&#21644;&#20302;&#30456;&#20851;&#24230;&#30340;&#21333;&#35789;&#38405;&#35835;&#36807;&#31243;&#20013;&#19982;&#27880;&#35270;&#30456;&#20851;&#30340;EEG&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#22312;12&#21517;&#21463;&#35797;&#32773;&#20013;&#65292;&#27492;&#21333;&#35789;&#32423;&#21035;&#20998;&#31867;&#30340;&#26368;&#20339;&#39564;&#35777;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent explosion of large language models (LLMs), such as Generative Pretrained Transformers (GPT), the need to understand the ability of humans and machines to comprehend semantic language meaning has entered a new phase. This requires interdisciplinary research that bridges the fields of cognitive science and natural language processing (NLP). This pilot study aims to provide insights into individuals' neural states during a semantic relation reading-comprehension task. We propose jointly analyzing LLMs, eye-gaze, and electroencephalographic (EEG) data to study how the brain processes words with varying degrees of relevance to a keyword during reading. We also use a feature engineering approach to improve the fixation-related EEG data classification while participants read words with high versus low relevance to the keyword. The best validation accuracy in this word-level classification is over 60\% across 12 subjects. Words of high relevance to the inference keyword had sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.10150</link><description>&lt;p&gt;
Q-Transformer&#65306;&#36890;&#36807;&#33258;&#22238;&#24402;Q-&#20989;&#25968;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10150
&lt;/p&gt;
&lt;p&gt;
Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#21644;&#33258;&#20027;&#37319;&#38598;&#25968;&#25454;&#30340;&#22823;&#22411;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Transformer&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;Q&#20989;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#31163;&#32447;&#26102;&#24046;&#22791;&#20221;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;Q-Transformer&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#30340;Q&#20540;&#34920;&#31034;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#39640;&#23481;&#37327;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35774;&#35745;&#20915;&#31574;&#65292;&#20351;&#20854;&#22312;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;Q-Transformer&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#30340;&#32593;&#31449;&#21644;&#35270;&#39057;&#21487;&#20197;&#22312;https://q-transformer.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10066</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#20010;&#24615;&#21270;&#21360;&#35937;&#29983;&#25104;PET&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Automatic Personalized Impression Generation for PET Reports Using Large Language Models. (arXiv:2309.10066v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#20010;&#24615;&#21270;&#29983;&#25104;&#20840;&#36523;PET&#25253;&#21578;&#30340;&#20934;&#30830;&#21360;&#35937;&#12290;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#24341;&#20837;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#30740;&#31350;&#32467;&#26524;&#32463;&#36807;&#19987;&#23478;&#35780;&#20272;&#21644;&#26680;&#21307;&#23398;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#35748;&#21487;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#30830;&#23450;&#36890;&#36807;fine-tuned&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20026;&#20840;&#36523;PET&#25253;&#21578;&#29983;&#25104;&#20934;&#30830;&#30340;&#20010;&#24615;&#21270;&#21360;&#35937;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#20351;&#29992;teacher-forcing&#31639;&#27861;&#22312;PET&#25253;&#21578;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;12&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#36755;&#20837;&#26159;&#25253;&#21578;&#21457;&#29616;&#65292;&#21442;&#32771;&#26159;&#20020;&#24202;&#21360;&#35937;&#12290;&#39069;&#22806;&#30340;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20102;&#38405;&#35835;&#21307;&#29983;&#30340;&#36523;&#20221;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21307;&#29983;&#29305;&#23450;&#30340;&#25253;&#21578;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;2010&#24180;&#33267;2022&#24180;&#38388;&#20174;&#25105;&#20204;&#26426;&#26500;&#25910;&#38598;&#30340;37,370&#20221;&#22238;&#39038;&#24615;PET&#25253;&#21578;&#12290;&#36890;&#36807;&#19982;&#20004;&#21517;&#26680;&#21307;&#23398;&#65288;NM&#65289;&#21307;&#29983;&#30340;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;30&#20010;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26368;&#21305;&#37197;&#30340;&#25351;&#26631;&#36873;&#25321;&#20102;&#29992;&#20110;&#19987;&#23478;&#35780;&#20272;&#30340;&#27169;&#22411;&#12290;&#22312;&#37096;&#20998;&#25968;&#25454;&#23376;&#38598;&#20013;&#65292;&#26681;&#25454;6&#20010;&#36136;&#37327;&#32500;&#24230;&#21644;&#19968;&#20010;&#24635;&#20307;&#23454;&#29992;&#24615;&#35780;&#20998;&#65288;5&#20998;&#21046;&#65289;&#65292;&#19977;&#21517;&#26680;&#21307;&#23398;&#21307;&#29983;&#35780;&#20272;&#20102;&#27169;&#22411;&#29983;&#25104;&#30340;&#21360;&#35937;&#21644;&#21407;&#22987;&#20020;&#24202;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Materials and Methods: Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions and an overall utility score (5-point scale). Each physician reviewe
&lt;/p&gt;</description></item><item><title>Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04965</link><description>&lt;p&gt;
&#21069;&#32512;&#25193;&#25955;&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#26679;&#21270;&#22270;&#20687;&#23383;&#24149;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04965
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#30340;&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#21442;&#25968;&#35268;&#27169;&#36739;&#22823;&#20173;&#28982;&#26159;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#25193;&#25955;&#65292;&#31216;&#20026;&#21069;&#32512;&#25193;&#25955;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#12290;&#21069;&#32512;&#25193;&#25955;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#23383;&#24149;&#30340;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25193;&#23637;&#22270;&#20687;&#23383;&#24149;&#30340;&#25193;&#25955;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
&lt;/p&gt;</description></item><item><title>DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.02072</link><description>&lt;p&gt;
DeepVol&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepVol: A Deep Transfer Learning Approach for Universal Asset Volatility Modeling. (arXiv:2309.02072v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02072
&lt;/p&gt;
&lt;p&gt;
DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27874;&#21160;&#24615;&#27169;&#22411;DeepVol&#65292;&#23427;&#22312;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#12290;DeepVol&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#36164;&#20135;&#12290;&#36825;&#19982;&#35745;&#37327;&#32463;&#27982;&#23398;&#25991;&#29486;&#20013;&#30340;&#20027;&#27969;&#20570;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#38656;&#35201;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#24341;&#20837;DeepVol&#20026;&#37329;&#34701;&#34892;&#19994;&#30340;&#27874;&#21160;&#24615;&#24314;&#27169;&#21644;&#39044;&#27979;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#21487;&#33021;&#20250;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DeepVol, a promising new deep learning volatility model that outperforms traditional econometric models in terms of model generality. DeepVol leverages the power of transfer learning to effectively capture and model the volatility dynamics of all financial assets, including previously unseen ones, using a single universal model. This contrasts to the prevailing practice in econometrics literature, which necessitates training separate models for individual datasets. The introduction of DeepVol opens up new avenues for volatility modeling and forecasting in the finance industry, potentially transforming the way volatility is understood and predicted.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.15452</link><description>&lt;p&gt;
&#20160;&#20040;&#26102;&#20505;&#32534;&#31243;&#24605;&#32500;&#23545;&#25512;&#29702;&#36215;&#20316;&#29992;?
&lt;/p&gt;
&lt;p&gt;
When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#26469;&#34913;&#37327;&#32534;&#31243;&#35821;&#35328;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;&#23398;&#20064;&#25110;&#29702;&#35299;&#65292;&#36866;&#24403;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#22312;&#20307;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#20687;&#32534;&#31243;&#24605;&#32500;&#25552;&#31034;&#36825;&#26679;&#30340;&#26041;&#27861;&#23545;&#20110;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#26469;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#20195;&#30721;&#25968;&#25454;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20855;&#20307;&#24433;&#21709;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22797;&#26434;&#24615;&#24433;&#21709;&#25512;&#29702;&#20998;&#25968;&#65288;CIRS&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#32467;&#26500;&#21644;&#36923;&#36753;&#23646;&#24615;&#65292;&#20197;&#34913;&#37327;&#20195;&#30721;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#26469;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#38590;&#24230;&#21644;&#22280;&#22797;&#26434;&#24230;&#26469;&#35745;&#31639;&#36923;&#36753;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#38750;&#25152;&#26377;&#22797;&#26434;&#24615;&#30340;&#20195;&#30721;&#25968;&#25454;&#37117;&#21487;&#20197;&#34987;LLM&#23398;&#20064;&#25110;&#29702;&#35299;&#12290;&#26368;&#20339;&#22797;&#26434;&#24615;&#27700;&#24179;&#23545;&#20110;&#36890;&#36807;&#32534;&#31243;&#36741;&#21161;&#25552;&#31034;&#25913;&#21892;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#21512;&#25104;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10875</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21450;&#20854;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#29983;&#29289;&#32479;&#35745;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries. (arXiv:2308.10875v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#31361;&#21464;&#20195;&#29702;&#30340;&#31454;&#20105;&#24615;&#32676;&#20307;&#20248;&#21270;&#22120;(CSO-MA)&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#22312;&#32479;&#35745;&#31185;&#23398;&#20013;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#28789;&#27963;&#24615;&#21644;&#36229;&#36234;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#39640;&#25928;&#19988;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#25110;&#22810;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#21253;&#25324;(i)&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#36890;&#36807;&#21333;&#32454;&#32990;&#24191;&#20041;&#36235;&#21183;&#27169;&#22411;&#25214;&#21040;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20197;&#30740;&#31350;&#20266;&#26102;&#24577;&#65292;(ii) &#20272;&#35745;&#25945;&#32946;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;Rasch&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;(iii) &#22312;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#27169;&#22411;&#20013;&#20026;Cox&#22238;&#24402;&#25214;&#21040;M-&#20272;&#35745;&#65292;(iv) &#30697;&#38453;&#34917;&#20840;&#20197;&#22635;&#34917;&#20004;&#20010;&#36830;&#36830;&#19981;&#36890;&#22270;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10335</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#30721;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Robustness and Reliability of Large Language Model Code Generation. (arXiv:2308.10335v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#21644;&#40065;&#26834;&#65292;&#28389;&#29992;API&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#12290;&#36825;&#23545;&#21021;&#32423;&#24320;&#21457;&#32773;&#26469;&#35828;&#23588;&#20854;&#21361;&#38505;&#65292;&#22240;&#20026;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;&#20195;&#30721;&#20013;&#30340;API&#28389;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21644;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#38750;&#20961;&#33021;&#21147;&#12290;&#24403;&#36935;&#21040;&#32534;&#30721;&#38382;&#39064;&#26102;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24120;&#24120;&#20250;&#21672;&#35810;LLMs&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#21162;&#21147;&#26469;&#36991;&#20813;&#35821;&#27861;&#38169;&#35823;&#24182;&#20351;&#20195;&#30721;&#19982;&#39044;&#26399;&#30340;&#35821;&#20041;&#23545;&#40784;&#65292;&#20294;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#23578;&#26410;&#34987;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#30495;&#23454;&#30340;&#36719;&#20214;&#24320;&#21457;&#29615;&#22659;&#20013;&#65292;&#21487;&#25191;&#34892;&#30340;&#20195;&#30721;&#24182;&#19981;&#31561;&#21516;&#20110;&#21487;&#38752;&#21644;&#40065;&#26834;&#30340;&#20195;&#30721;&#12290;&#22312;&#29983;&#25104;&#30340;&#20195;&#30721;&#20013;&#28389;&#29992;API&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22914;&#36164;&#28304;&#27844;&#28431;&#12289;&#31243;&#24207;&#23849;&#28291;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;LLM&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;&#30340;&#29992;&#25143;&#23454;&#38469;&#19978;&#26159;&#26368;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#30475;&#20284;&#27491;&#30830;&#30340;&#20195;&#30721;&#24433;&#21709;&#30340;&#24320;&#21457;&#32773;&#8212;&#8212;&#20182;&#20204;&#36890;&#24120;&#26159;&#19981;&#29087;&#24713;LLMs&#20026;&#20182;&#20204;&#29983;&#25104;&#20195;&#30721;&#30340;API&#30340;&#21021;&#32423;&#24320;&#21457;&#32773;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24456;&#38590;&#23519;&#35273;&#21040;API&#30340;&#28389;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09904</link><description>&lt;p&gt;
RAH&#65281;RecSys-Assistant-Human&#65306;&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#20013;&#24515;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models. (arXiv:2308.09904v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;RAH&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#21161;&#25163;&#65292;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#20010;&#24615;&#21270;&#21453;&#39304;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#29983;&#24577;&#31995;&#32479;&#28041;&#21450;&#21040;&#25512;&#33616;&#31995;&#32479;&#65288;&#35745;&#31639;&#26426;&#65289;&#21644;&#29992;&#25143;&#65288;&#20154;&#31867;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#25512;&#33616;&#31995;&#32479;&#30340;&#35282;&#24230;&#19981;&#21516;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#29992;&#25143;&#30340;&#35282;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#21152;&#20154;&#31867;&#20013;&#24515;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;RAH&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12289;&#21161;&#25163;&#21644;&#20154;&#31867;&#12290;&#21161;&#25163;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#20010;&#20154;&#20195;&#29702;&#65292;&#29992;&#20110;&#23454;&#29616;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#21161;&#25163;&#25198;&#28436;&#38750;&#20405;&#20837;&#24615;&#30340;&#35282;&#33394;&#65292;RAH&#26694;&#26550;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#29992;&#25143;&#32676;&#20307;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#35780;&#20272;&#20102;RAH&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#25143;&#20010;&#24615;&#21644;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#12290;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#20351;&#29992;&#23398;&#20064;-&#34892;&#21160;-&#35780;&#35770;&#23478;&#21644;&#21453;&#24605;&#26426;&#21046;&#21487;&#20197;&#23548;&#33268;&#26356;&#21152;&#19968;&#33268;&#30340;&#20010;&#24615;&#65292;&#65288;2&#65289;&#25105;&#20204;&#30340;&#21161;&#25163;&#21487;&#20197;&#26377;&#25928;&#22320;&#20195;&#29702;&#20154;&#31867;&#21453;&#39304;&#24182;&#24110;&#21161;&#35843;&#25972;&#25512;&#33616;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;RAH&#26694;&#26550;&#20013;&#36827;&#19968;&#27493;&#35299;&#20915;&#20154;&#31867;&#20013;&#24515;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#29992;&#25143;``&#22842;&#26435;''&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation ecosystem involves interactions between recommender systems(Computer) and users(Human). Orthogonal to the perspective of recommender systems, we attempt to utilize LLMs from the perspective of users and propose a more human-central recommendation framework named RAH, which consists of Recommender system, Assistant and Human. The assistant is a LLM-based and personal proxy for a human to achieve user satisfaction. The assistant plays a non-invasion role and the RAH framework can adapt to different recommender systems and user groups. Subsequently, we implement and evaluate the RAH framework for learning user personalities and proxy human feedback. The experiment shows that (1) using learn-action-critic and reflection mechanisms can lead more aligned personality and (2) our assistant can effectively proxy human feedback and help adjust recommender systems. Finally, we discuss further strategies in the RAH framework to address human-central concerns including user contr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#65292;&#21306;&#20998;&#25509;&#36817;&#34920;&#38754;&#30340;&#28857;&#21644;&#20854;&#20182;&#28857;&#65292;&#20174;&#32780;&#22312;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.08857</link><description>&lt;p&gt;
D-IF: &#36890;&#36807;&#38544;&#24335;&#20998;&#24067;&#22330;&#23454;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20154;&#20307;&#25968;&#23383;&#21270;
&lt;/p&gt;
&lt;p&gt;
D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field. (arXiv:2308.08857v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#65292;&#21306;&#20998;&#25509;&#36817;&#34920;&#38754;&#30340;&#28857;&#21644;&#20854;&#20182;&#28857;&#65292;&#20174;&#32780;&#22312;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36924;&#30495;&#30340;&#34394;&#25311;&#20154;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#20803;&#23431;&#23449;&#12289;&#26234;&#33021;&#21307;&#30103;&#21644;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#12290;&#20294;&#26159;&#22823;&#35268;&#27169;&#21019;&#36896;&#20855;&#26377;&#39640;&#24230;&#36924;&#30495;&#24230;&#30340;&#34394;&#25311;&#20154;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#30340;&#24212;&#29992;&#24320;&#21551;&#20102;&#22522;&#20110;&#22270;&#20687;&#30340;&#19977;&#32500;&#31359;&#30528;&#20154;&#20307;&#37325;&#24314;&#30340;&#26032;&#26102;&#20195;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#32454;&#33410;&#30340;&#20687;&#32032;&#23545;&#40784;&#24418;&#29366;&#24674;&#22797;&#12290;&#38543;&#21518;&#65292;&#32477;&#22823;&#37096;&#20998;&#24037;&#20316;&#36890;&#36807;&#22238;&#24402;&#27599;&#20010;&#28857;&#30340;&#30830;&#23450;&#24615;&#38544;&#24335;&#20540;&#26469;&#23450;&#20301;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#26159;&#21542;&#24212;&#35813;&#19981;&#32771;&#34385;&#19982;&#34920;&#38754;&#30340;&#36317;&#31163;&#32780;&#23558;&#25152;&#26377;&#28857;&#37117;&#19968;&#35270;&#21516;&#20161;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#29992;&#33258;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#26367;&#25442;&#38544;&#24335;&#20540;&#65292;&#26681;&#25454;&#28857;&#19982;&#34920;&#38754;&#30340;&#36317;&#31163;&#23545;&#23427;&#20204;&#36827;&#34892;&#21306;&#20998;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#8220;&#20540;&#21040;&#20998;&#24067;&#8221;&#30340;&#36716;&#21464;&#26174;&#33879;&#25913;&#36827;&#20102;&#20960;&#20046;&#25152;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25439;&#22833;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic virtual humans play a crucial role in numerous industries, such as metaverse, intelligent healthcare, and self-driving simulation. But creating them on a large scale with high levels of realism remains a challenge. The utilization of deep implicit function sparks a new era of image-based 3D clothed human reconstruction, enabling pixel-aligned shape recovery with fine details. Subsequently, the vast majority of works locate the surface by regressing the deterministic implicit value for each point. However, should all points be treated equally regardless of their proximity to the surface? In this paper, we propose replacing the implicit value with an adaptive uncertainty distribution, to differentiate between points based on their distance to the surface. This simple ``value to distribution'' transition yields significant improvements on nearly all the baselines. Furthermore, qualitative results demonstrate that the models trained using our uncertainty distribution loss, can ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.10236</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models. (arXiv:2307.10236v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#22312;&#25506;&#32034;&#21644;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#24615;&#33021;&#31361;&#30772;&#20026;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#38169;&#35823;&#29983;&#25104;&#65292;&#22914;&#34394;&#20551;&#39044;&#27979;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#65292;&#20063;&#24341;&#21457;&#20102;&#23545;LLMs&#21487;&#38752;&#24615;&#30340;&#20005;&#37325;&#20851;&#27880;&#65292;&#23588;&#20854;&#22312;&#23545;&#23433;&#20840;&#12289;&#21487;&#38752;&#24615;&#26377;&#25935;&#24863;&#30340;&#22330;&#26223;&#20013;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#20013;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24050;&#32463;&#26174;&#31034;&#20986;&#20854;&#22312;&#35299;&#37322;&#19968;&#33324;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#39118;&#38505;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#20851;&#20110;&#23427;&#26159;&#21542;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26377;&#21161;&#20110;&#25506;&#32034;LLMs&#30340;&#33021;&#21147;&#21644;&#25269;&#21046;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#20174;&#19981;&#30830;&#23450;&#24615;&#30340;&#35282;&#24230;&#24320;&#23637;&#20102;&#20851;&#20110;LLMs&#39118;&#38505;&#35780;&#20272;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;12&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;4&#20010;LLMs&#22312;4&#20010;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35843;&#26597;&#19981;&#30830;&#23450;&#24615;&#22312;&#25506;&#32034;LLMs&#33021;&#21147;&#21644;&#23545;&#25239;&#20854;&#19981;&#33391;&#34892;&#20026;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent unc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.05793</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#39640;&#25928;&#22320;&#22270;&#26500;&#24314;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Efficient Map Building via Fragmentation and Recall. (arXiv:2307.05793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#31354;&#38388;&#22320;&#22270;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#36825;&#20123;&#22320;&#22270;&#20351;&#24471;&#21253;&#25324;&#22238;&#23478;&#12289;&#35268;&#21010;&#12289;&#25628;&#32034;&#21644;&#35269;&#39135;&#22312;&#20869;&#30340;&#21151;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#31354;&#38388;&#26159;&#19968;&#20010;&#38590;&#39064;&#65306;&#20195;&#29702;&#21487;&#33021;&#20250;&#38519;&#20837;&#23616;&#37096;&#21306;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#20998;&#21106;&#21644;&#22238;&#28335;&#65288;FarMap&#65289;&#30340;&#27010;&#24565;&#12290;&#20195;&#29702;&#36890;&#36807;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#26469;&#35299;&#20915;&#22320;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#20854;&#29992;&#20110;&#35774;&#32622;&#31354;&#38388;&#25506;&#32034;&#30340;&#23376;&#30446;&#26631;&#12290;&#20195;&#29702;&#26500;&#24314;&#21644;&#20351;&#29992;&#26412;&#22320;&#22320;&#22270;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#35266;&#27979;&#32467;&#26524;&#65307;&#39640;&#24847;&#22806;&#24615;&#20250;&#23548;&#33268;&#8220;&#20998;&#21106;&#20107;&#20214;&#8221;&#65292;&#20174;&#32780;&#25130;&#26029;&#26412;&#22320;&#22320;&#22270;&#12290;&#22312;&#36825;&#20123;&#20107;&#20214;&#20013;&#65292;&#26368;&#36817;&#30340;&#26412;&#22320;&#22320;&#22270;&#34987;&#25918;&#20837;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#20013;&#65292;&#24182;&#21021;&#22987;&#21270;&#21478;&#19968;&#20010;&#26412;&#22320;&#22320;&#22270;&#12290;&#22914;&#26524;&#26029;&#35010;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#19982;&#23384;&#20648;&#30340;&#26576;&#20010;&#26412;&#22320;&#22320;&#22270;&#30340;&#35266;&#23519;&#32467;&#26524;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#22320;&#22270;&#23601;&#20250;&#34987;&#22238;&#28335;&#65288;&#24182;&#37325;&#29992;&#65289;&#33258;LTM&#12290;&#20998;&#21106;&#28857;&#35825;&#23548;.
&lt;/p&gt;
&lt;p&gt;
Animals and robots navigate through environments by building and refining maps of the space. These maps enable functions including navigating back to home, planning, search, and foraging. In large environments, exploration of the space is a hard problem: agents can become stuck in local regions. Here, we use insights from neuroscience to propose and apply the concept of Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by building local maps via a surprisal-based clustering of space, which they use to set subgoals for spatial exploration. Agents build and use a local map to predict their observations; high surprisal leads to a ``fragmentation event'' that truncates the local map. At these events, the recent local map is placed into long-term memory (LTM), and a different local map is initialized. If observations at a fracture point match observations in one of the stored local maps, that map is recalled (and thus reused) from LTM. The fragmentation points induc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15724</link><description>&lt;p&gt;
REFLECT:&#23545;&#26426;&#22120;&#20154;&#32463;&#21382;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#29992;&#20110;&#22833;&#36133;&#35299;&#37322;&#21644;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction. (arXiv:2306.15724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;REFLECT&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#26426;&#22120;&#20154;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22833;&#36133;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#24110;&#21161;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#21644;&#20998;&#26512;&#22833;&#36133;&#25191;&#34892;&#26159;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#31283;&#20581;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#36755;&#20837;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#21033;&#29992;LLM&#30340;&#21147;&#37327;&#36827;&#34892;&#26426;&#22120;&#20154;&#22833;&#36133;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;REFLECT&#65292;&#23558;&#22810;&#24863;&#23448;&#25968;&#25454;&#36716;&#21270;&#20026;&#26426;&#22120;&#20154;&#36807;&#21435;&#32463;&#39564;&#30340;&#20998;&#23618;&#24635;&#32467;&#65292;&#24182;&#20351;&#29992;&#36880;&#27493;&#22833;&#36133;&#35299;&#37322;&#31639;&#27861;&#26597;&#35810;LLM&#12290;&#22522;&#20110;&#35299;&#37322;&#65292;&#22833;&#36133;&#32416;&#27491;&#35268;&#21010;&#22120;&#29983;&#25104;&#19968;&#20010;&#21487;&#25191;&#34892;&#35745;&#21010;&#65292;&#20197;&#32416;&#27491;&#22833;&#36133;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;RoboFail&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26377;&#30410;&#30340;&#22833;&#36133;&#35299;&#37322;&#65292;&#20174;&#32780;&#24110;&#21161;&#25104;&#21151;&#30340;&#32416;&#27491;&#35268;&#21010;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://roboreflect.github.io/
&lt;/p&gt;
&lt;p&gt;
The ability to detect and analyze failed executions automatically is crucial for an explainable and robust robotic system. Recently, Large Language Models (LLMs) have demonstrated strong common sense reasoning skills on textual inputs. To leverage the power of LLM for robot failure explanation, we propose a framework REFLECT, which converts multi-sensory data into a hierarchical summary of robot past experiences and queries LLM with a progressive failure explanation algorithm. Conditioned on the explanation, a failure correction planner generates an executable plan for the robot to correct the failure and complete the task. To systematically evaluate the framework, we create the RoboFail dataset and show that our LLM-based framework is able to generate informative failure explanations that assist successful correction planning. Project website: https://roboreflect.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14079</link><description>&lt;p&gt;
&#21033;&#29992;&#26799;&#24230;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#65306;&#36890;&#36807;&#25193;&#25955;&#20998;&#25968;&#21305;&#37197;&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24179;&#28369;&#30340;&#36317;&#31163;&#25968;&#25454;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23545;&#25239;&#19981;&#30830;&#23450;&#24615;&#21644;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#65292;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20248;&#21270;&#33539;&#24335;&#65292;&#20363;&#22914;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25110;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#20801;&#35768;&#31574;&#30053;&#25628;&#32034;&#31639;&#27861;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#20197;&#36991;&#20813;&#20998;&#24067;&#20559;&#31227;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#20180;&#32454;&#22320;&#32771;&#34385;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#22768;&#31216;&#65292;&#20026;&#20102;&#35753;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#23427;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#26799;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#26131;&#20302;&#20272;&#30495;&#23454;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19981;&#20165;&#31283;&#23450;&#22320;&#25910;&#25947;&#21040;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;Lipschitz&#24120;&#25968;&#26469;&#20998;&#26512;&#27169;&#22411;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24179;&#28369;&#30340;&#25968;&#25454;&#36317;&#31163;&#21644;&#25968;&#25454;&#20284;&#28982;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
&lt;/p&gt;</description></item><item><title>ALP&#26159;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.10190</link><description>&lt;p&gt;
ALP: &#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#29992;&#20110;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
ALP: Action-Aware Embodied Learning for Perception. (arXiv:2306.10190v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10190
&lt;/p&gt;
&lt;p&gt;
ALP&#26159;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22312;&#35757;&#32451;&#21644;&#22522;&#20934;&#27979;&#35797;&#35270;&#35273;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#34987;&#21160;&#30340;&#12289;&#31574;&#21010;&#22909;&#30340;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22522;&#26412;&#19978;&#26080;&#27861;&#36866;&#24212;&#19968;&#20010;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#65292;&#22240;&#20026;&#36755;&#20837;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#26029;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#20197;&#26356;&#20154;&#31867;&#20013;&#24515;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#23398;&#20064;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#20316;&#24863;&#30693;&#30340;&#20855;&#36523;&#23398;&#20064;&#26694;&#26550;&#65288;ALP&#65289;&#65292;&#36890;&#36807;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21644;&#36870;&#21160;&#21147;&#23398;&#39044;&#27979;&#30446;&#26631;&#30340;&#32467;&#21512;&#65292;&#23558;&#21160;&#20316;&#20449;&#24687;&#34701;&#20837;&#21040;&#34920;&#31034;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#31215;&#26497;&#25506;&#32034;&#65292;&#26082;&#23398;&#20064;&#21487;&#26222;&#36941;&#24212;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#21448;&#25910;&#38598;&#19979;&#28216;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ALP&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods in training and benchmarking vision models exhibit an over-reliance on passive, curated datasets. Although models trained on these datasets have shown strong performance in a wide variety of tasks such as classification, detection, and segmentation, they fundamentally are unable to generalize to an ever-evolving world due to constant out-of-distribution shifts of input data. Therefore, instead of training on fixed datasets, can we approach learning in a more human-centric and adaptive manner? In this paper, we introduce Action-Aware Embodied Learning for Perception (ALP), an embodied learning framework that incorporates action information into representation learning through a combination of optimizing a reinforcement learning policy and an inverse dynamics prediction objective. Our method actively explores in complex 3D environments to both learn generalizable task-agnostic visual representations as well as collect downstream training data. We show that ALP outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10070</link><description>&lt;p&gt;
ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#39046;&#22495;&#19987;&#23478;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20135;&#29983;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#20855;&#20307;&#25506;&#35752;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#12289;&#21307;&#30103;&#25991;&#26412;&#25688;&#35201;&#12289;&#20449;&#24687;&#25277;&#21462;&#21644;&#21307;&#23398;&#25945;&#32946;&#31561;&#39046;&#22495;&#65292;&#24182;&#30740;&#31350;LLMs&#26159;&#21542;&#20855;&#26377;&#30495;&#27491;&#30340;&#36716;&#22411;&#21147;&#37327;&#20197;&#24443;&#24213;&#25913;&#21464;&#36825;&#20123;&#20219;&#21153;&#25110;&#32773;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#26159;&#21542;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#65292;&#36827;&#23637;&#36824;&#27604;&#36739;&#32531;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;LLMs&#36824;&#27809;&#26377;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07092</link><description>&lt;p&gt;
&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tuning Legged Locomotion Controllers via Safe Bayesian Optimization. (arXiv:2306.07092v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#25972;&#22235;&#36275;&#34892;&#36208;&#25511;&#21046;&#22120;&#30340;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#20197;&#31616;&#21270;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#30828;&#20214;&#24179;&#21488;&#20013;&#37096;&#32626;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#26080;&#27169;&#22411;&#23433;&#20840;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#35843;&#25972;&#25511;&#21046;&#22686;&#30410;&#65292;&#35299;&#20915;&#20102;&#22312;&#25511;&#21046;&#21046;&#23450;&#20013;&#20351;&#29992;&#30340;&#31616;&#21270;&#27169;&#22411;&#19982;&#23454;&#38469;&#31995;&#32479;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#21487;&#33021;&#23433;&#20840;&#21306;&#22495;&#20869;&#39640;&#25928;&#20248;&#21270;&#21442;&#25968;&#65292;&#22823;&#22823;&#20943;&#23567;&#20102;&#19982;&#26426;&#22120;&#20154;&#30340;&#21361;&#38505;&#20132;&#20114;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#20197;&#23558;&#19981;&#21516;&#30340;&#27493;&#24577;&#21442;&#25968;&#20316;&#20026;&#32972;&#26223;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#23433;&#20840;&#22320;&#35843;&#25972;&#22810;&#26679;&#21270;&#27493;&#24577;&#27169;&#24335;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#30828;&#20214;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#27493;&#24577;&#20013;&#35843;&#25972;&#22522;&#20110;&#27169;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#65292;&#35813;&#31639;&#27861;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a data-driven strategy to streamline the deployment of model-based controllers in legged robotic hardware platforms. Our approach leverages a model-free safe learning algorithm to automate the tuning of control gains, addressing the mismatch between the simplified model used in the control formulation and the real system. This method substantially mitigates the risk of hazardous interactions with the robot by sample-efficiently optimizing parameters within a probably safe region. Additionally, we extend the applicability of our approach to incorporate the different gait parameters as contexts, leading to a safe, sample-efficient exploration algorithm capable of tuning a motion controller for diverse gait patterns. We validate our method through simulation and hardware experiments, where we demonstrate that the algorithm obtains superior performance on tuning a model-based motion controller for multiple gaits safely.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05726</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Sample Policy Iteration for Offline Reinforcement Learning. (arXiv:2306.05726v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#31639;&#27861;&#26469;&#22686;&#24378;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#26469;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#25968;&#25454;&#35206;&#30422;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#65292;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#20248;&#21270;&#25511;&#21046;&#31574;&#30053;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20559;&#31163;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24403;&#31163;&#32447;&#25968;&#25454;&#38598;&#30001;&#27425;&#20248;&#31574;&#30053;&#25910;&#38598;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#20339;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26679;&#26412;&#20869;&#31574;&#30053;&#36845;&#20195;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#34892;&#20026;&#35268;&#21017;&#26041;&#27861;&#12290;&#26680;&#24515;&#35265;&#35299;&#26159;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#29992;&#20110;&#34892;&#20026;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#26679;&#26412;&#20869;&#25919;&#31574;&#36845;&#20195;&#36880;&#28176;&#25913;&#36827;&#33258;&#36523;&#65292;&#21516;&#26102;&#38544;&#24335;&#36991;&#20813;&#26597;&#35810;&#26679;&#26412;&#22806;&#30340;&#34892;&#21160;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#23398;&#20064;&#20165;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#33391;&#22909;&#35206;&#30422;&#30340;&#34892;&#21160;&#23398;&#20064;&#26679;&#26412;&#20869;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we pr
&lt;/p&gt;</description></item><item><title>GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04607</link><description>&lt;p&gt;
&#23558;&#20960;&#20309;&#25511;&#21046;&#38598;&#25104;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt. (arXiv:2306.04607v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04607
&lt;/p&gt;
&lt;p&gt;
GeoDiffusion&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#21019;&#24314;&#20869;&#23481;&#21644;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#19981;&#20165;&#22270;&#20687;&#27700;&#24179;&#30340;&#24863;&#30693;&#36136;&#37327;&#65292;&#32780;&#19988;&#36793;&#30028;&#26694;&#21644;&#30456;&#26426;&#35270;&#22270;&#31561;&#20960;&#20309;&#26465;&#20214;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#21069;&#26399;&#30740;&#31350;&#20351;&#29992;&#27169;&#22359;&#32534;&#30721;&#35821;&#20041;&#24067;&#23616;&#26469;&#23454;&#29616;&#22797;&#21046;&#31896;&#36148;&#21512;&#25104;&#25110;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GeoDiffusion&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#23558;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#36716;&#21270;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26816;&#27979;&#25968;&#25454;&#12290;&#19982;&#20197;&#24448;&#30340;L2I&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;GeoDiffusion&#19981;&#20165;&#33021;&#22815;&#32534;&#30721;&#36793;&#30028;&#26694;&#65292;&#36824;&#33021;&#22815;&#32534;&#30721;&#33258;&#39550;&#22330;&#26223;&#20013;&#30340;&#39069;&#22806;&#20960;&#20309;&#26465;&#20214;&#65292;&#22914;&#25668;&#20687;&#22836;&#35270;&#22270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GeoDiffusion&#22312;&#29289;&#20307;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#38024;&#23545;&#21508;&#31181;&#20960;&#20309;&#26465;&#20214;&#29983;&#25104;&#20855;&#26377;&#26356;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted significant attention due to their remarkable ability to create content and generate data for tasks such as image classification. However, the usage of diffusion models to generate high-quality object detection data remains an underexplored area, where not only the image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode semantic layouts. In this paper, we propose GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower the pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#20316;&#20026;&#23558;&#22797;&#26434;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#39640;&#32428;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#31243;&#26469;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#30340;&#29289;&#20307;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#25193;&#23637;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.00600</link><description>&lt;p&gt;
&#26059;&#36716;&#29305;&#24449;&#29992;&#20110;&#29289;&#20307;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Rotating Features for Object Discovery. (arXiv:2306.00600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#20316;&#20026;&#23558;&#22797;&#26434;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#39640;&#32428;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#36807;&#31243;&#26469;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#30340;&#29289;&#20307;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#25193;&#23637;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#32465;&#23450;&#38382;&#39064;&#28041;&#21450;&#22823;&#33041;&#22914;&#20309;&#22312;&#22266;&#23450;&#30340;&#31070;&#32463;&#36830;&#25509;&#32593;&#32476;&#20013;&#34920;&#31034;&#21644;&#36830;&#25509;&#29289;&#20307;&#65292;&#20173;&#28982;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35770;&#12290;&#22823;&#22810;&#25968;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22522;&#20110;&#25554;&#27133;&#30340;&#26041;&#27861;&#19978;&#65292;&#30001;&#20110;&#20854;&#31163;&#25955;&#24615;&#21644;&#38590;&#20197;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#28857;&#65292;&#21487;&#33021;&#26377;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;&#22797;&#26434;&#33258;&#21160;&#32534;&#30721;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#23398;&#20064;&#36830;&#32493;&#21644;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20195;&#26367;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#21482;&#36866;&#29992;&#20110;&#31616;&#21333;&#30340;&#29609;&#20855;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26059;&#36716;&#29305;&#24449;&#65292;&#23558;&#22797;&#20540;&#29305;&#24449;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21462;&#20998;&#24067;&#24335;&#34920;&#31034;&#20013;&#29289;&#20307;&#30340;&#26032;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#31616;&#21333;&#30340;&#29609;&#20855;&#25968;&#25454;&#25193;&#23637;&#21040;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#24335;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16014</link><description>&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22810;&#23569;&#26679;&#26412;&#25165;&#33021;&#21033;&#29992;&#24179;&#28369;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#19979;&#30028;&#65292;&#25506;&#35752;&#20102;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#26102;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#37327;&#21450;&#20854;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#30340;&#26680;&#24515;&#21407;&#21017;&#20043;&#19968;&#26159;&#65292;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21487;&#20197;&#25171;&#30772;&#32500;&#24230;&#28798;&#38590;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#27888;&#21202;&#23637;&#24320;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#38656;&#35201;&#36275;&#22815;&#25509;&#36817;&#19968;&#36215;&#30340;&#26679;&#26412;&#26469;&#33719;&#24471;&#39640;&#38454;&#23548;&#25968;&#30340;&#26377;&#24847;&#20041;&#20272;&#35745;&#65292;&#36825;&#22312;&#25968;&#25454;&#37327;&#30456;&#23545;&#36739;&#23567;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#20284;&#20046;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#23548;&#24191;&#20041;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19979;&#30028;&#65292;&#30740;&#31350;&#20102;&#24120;&#25968;&#21644;&#30636;&#24577;&#21306;&#22495;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#21364;&#21457;&#25381;&#20102;&#20027;&#23548;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.12553</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;:&#22343;&#34913;&#36817;&#20284;&#19982;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Markov $\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis. (arXiv:2305.12553v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21363;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#12290;&#20171;&#32461;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#20013;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#19988;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#22810;&#20195;&#29702;&#20132;&#20114;&#30340;&#26032;&#26694;&#26550;:&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#12290;&#39532;&#23572;&#21487;&#22827;&#21183;&#21338;&#24328;&#26159;&#39532;&#23572;&#21487;&#22827; $\alpha$-&#21183;&#21338;&#24328;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#39532;&#23572;&#21487;&#22827;&#25317;&#22581;&#21338;&#24328;&#21644;&#25200;&#21160;&#39532;&#23572;&#21487;&#22827;&#22242;&#38431;&#21338;&#24328;&#26159;&#20004;&#20010;&#37325;&#35201;&#19988;&#23454;&#38469;&#24847;&#20041;&#37325;&#22823;&#30340;&#21338;&#24328;&#31867;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#20010;&#21338;&#24328;&#30340;$\alpha$-&#21183;&#20989;&#25968;&#65292;&#24182;&#38024;&#23545;&#21338;&#24328;&#21442;&#25968;&#34920;&#24449;&#20102;&#24046;&#36317; $\alpha$&#12290;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#8212;&#8212;&#25237;&#24433;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#22823;&#25913;&#36827;&#24179;&#28369;&#26368;&#20339;&#21453;&#24212;&#21160;&#24577;&#8212;&#8212;&#26469;&#36817;&#20284;&#35745;&#31639;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#20013;&#30340;&#31283;&#24577;&#32435;&#20160;&#22343;&#34913;&#12290;&#27599;&#20010;&#31639;&#27861;&#30340;&#32435;&#20160;&#36951;&#25022;&#37117;&#26174;&#31034;&#20026;&#26102;&#38388;&#36328;&#24230;&#30340;&#20122;&#32447;&#24615;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#39532;&#23572;&#21487;&#22827;$\alpha$-&#21183;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework to study multi-agent interaction in Markov games: Markov $\alpha$-potential games. Markov potential games are special cases of Markov $\alpha$-potential games, so are two important and practically significant classes of games: Markov congestion games and perturbed Markov team games. In this paper, {$\alpha$-potential} functions for both games are provided and the gap $\alpha$ is characterized with respect to game parameters. Two algorithms -- the projected gradient-ascent algorithm and the sequential maximum improvement smoothed best response dynamics -- are introduced for approximating the stationary Nash equilibrium in Markov $\alpha$-potential games. The Nash-regret for each algorithm is shown to scale sub-linearly in time horizon. Our analysis and numerical experiments demonstrates that simple algorithms are capable of finding approximate equilibrium in Markov $\alpha$-potential games.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.10120</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10120
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#12289;&#35823;&#23548;&#25110;&#19981;&#24403;&#20869;&#23481;&#30340;&#25285;&#24551;&#12290;&#21463;&#27492;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25345;&#32493;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#20197;&#25351;&#23450;&#35813;&#22914;&#20309;&#36951;&#24536;&#19968;&#20010;&#27010;&#24565;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#21487;&#24212;&#29992;&#20110;&#21464;&#20998;&#20284;&#28982;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35825;&#23548;&#36951;&#24536;&#21508;&#31181;&#27010;&#24565;&#65292;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#25972;&#20010;&#31867;&#21035;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21517;&#20154;&#21644;&#35064;&#20307;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/clear-nus/selective-amnesia&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07440</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#20869;&#23384;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#35843;&#24230;&#21644;&#20998;&#37197;&#26159;&#35768;&#22810;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#28085;&#30422;&#25317;&#22622;&#25511;&#21046;&#21040;&#20113;&#35745;&#31639;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35843;&#24230;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#32534;&#35793;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#26399;&#38388;&#20986;&#29616;&#30340;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#65306;&#21363;&#23558;&#24352;&#37327;&#26144;&#23556;&#21040;&#19981;&#21516;&#30340;&#20869;&#23384;&#23618;&#20197;&#20248;&#21270;&#25191;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20869;&#23384;&#26144;&#23556;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21644;&#39640;&#32500;&#25968;&#25454;&#36755;&#20837;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#23398;&#20064;&#20013;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#25191;&#34892;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#27721;&#35834;&#22612;&#27979;&#35797;&#20102;GPT-2&#21644;GPT-3&#30340;&#35268;&#21010;&#21644;&#24037;&#20316;&#35760;&#24518;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;&#25191;&#34892;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04134</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#27491;&#22312;&#21457;&#23637;&#25191;&#34892;&#21151;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?. (arXiv:2305.04134v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#23398;&#20064;&#20013;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#25191;&#34892;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#27721;&#35834;&#22612;&#27979;&#35797;&#20102;GPT-2&#21644;GPT-3&#30340;&#35268;&#21010;&#21644;&#24037;&#20316;&#35760;&#24518;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;&#25191;&#34892;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25191;&#34892;&#21508;&#31181;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#35782;&#21035;&#21644;&#20915;&#31574;&#12290;&#27492;&#36827;&#23637;&#30340;&#19968;&#37096;&#20998;&#24402;&#22240;&#20110;&#20687;GPT&#65288;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#31995;&#21015;&#37027;&#26679;&#30340;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#34987;&#35270;&#20026;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22823;&#22810;&#25968;&#20316;&#32773;&#35748;&#20026;&#65292;&#26234;&#33021;&#34892;&#20026;&#21462;&#20915;&#20110;&#35768;&#22810;&#20840;&#38754;&#25216;&#33021;&#65292;&#25110;&#31216;&#25191;&#34892;&#21151;&#33021;&#65288;EFs&#65289;&#65292;&#36825;&#20123;&#25216;&#33021;&#20381;&#36182;&#20110;&#21069;&#39069;&#21494;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#30830;&#21151;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;LLM&#26159;&#21542;&#27491;&#22312;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25191;&#34892;&#21151;&#33021;&#20316;&#20026;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#20351;&#29992;&#27969;&#34892;&#30340;&#27721;&#35834;&#22612;&#26041;&#27861;&#30340;&#35268;&#21010;&#21151;&#33021;&#21644;&#24037;&#20316;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21476;&#20856;&#26041;&#27861;&#21464;&#20307;&#26469;&#26356;&#22909;&#22320;&#27979;&#35797;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-2&#21644;GPT-3&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;EFs&#65292;&#22914;&#24037;&#20316;&#35760;&#24518;&#21644;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has been rapidly advancing and has demonstrated its ability to perform a wide range of cognitive tasks, including language processing, visual recognition, and decision-making. Part of this progress is due to LLMs (Large Language Models) like those of the GPT (Generative Pre-Trained Transformers) family. These models are capable of exhibiting behavior that can be perceived as intelligent. Most authors in Neuropsychology consider intelligent behavior to depend on a number of overarching skills, or Executive Functions (EFs), which rely on the correct functioning of neural networks in the frontal lobes, and have developed a series of tests to evaluate them. In this work, we raise the question of whether LLMs are developing executive functions similar to those of humans as part of their learning, and we evaluate the planning function and working memory of GPT using the popular Towers of Hanoi method. Additionally, we introduce a new variant of the classical meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13734</link><description>&lt;p&gt;
&#19968;&#20010;LLM&#30693;&#36947;&#33258;&#24049;&#22312;&#25746;&#35854;&#30340;&#20869;&#37096;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#26377;&#21033;&#20110;&#25552;&#39640;LLM&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#65288;&#21487;&#33021;&#65289;&#26368;&#20026;&#31361;&#20986;&#30340;&#32570;&#28857;&#26159;&#20197;&#33258;&#20449;&#30340;&#35821;&#27668;&#29983;&#25104;&#19981;&#20934;&#30830;&#25110;&#34394;&#20551;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20551;&#35774;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#25581;&#31034;&#19968;&#20010;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;LLM&#25152;&#29983;&#25104;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#38544;&#34255;&#23618;&#28608;&#27963;&#26469;&#30830;&#23450;&#35821;&#21477;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#19981;&#21516;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35821;&#21477;&#12290;&#19968;&#20010;&#20998;&#31867;&#22120;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#26681;&#25454;LLM&#30340;&#28608;&#27963;&#20540;&#26469;&#26816;&#27979;&#21738;&#20010;&#35821;&#21477;&#26159;&#30495;&#23454;&#30340;&#25110;&#34394;&#20551;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#22120;&#25509;&#25910;LLM&#20026;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#35821;&#21477;&#29983;&#25104;&#30340;&#28608;&#27963;&#20540;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#26816;&#27979;&#35821;&#21477;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#29978;&#33267;&#27604;&#23569;&#37327;&#25552;&#31034;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26412;&#20307;&#35770;&#22312;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#32553;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26412;&#20307;&#35770;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#31561;&#26041;&#38754;&#26469;&#21457;&#25381;&#26356;&#21152;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.17262</link><description>&lt;p&gt;
&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#26412;&#20307;&#35770;&#65306;&#19968;&#31687;&#31616;&#26126;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ontology in Hybrid Intelligence: a concise literature review. (arXiv:2303.17262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26412;&#20307;&#35770;&#22312;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#32553;&#23567;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#24046;&#36317;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26412;&#20307;&#35770;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#31561;&#26041;&#38754;&#26469;&#21457;&#25381;&#26356;&#21152;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19981;&#26029;&#28436;&#36827;&#21644;&#22686;&#22810;&#30340;&#32972;&#26223;&#19979;&#65292;&#28151;&#21512;&#26234;&#33021;&#27491;&#22312;&#27969;&#34892;&#65292;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24179;&#34913;&#20849;&#23384;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#31616;&#27905;&#32780;&#37325;&#28857;&#31361;&#20986;&#30340;&#27010;&#36848;&#65292;&#20171;&#32461;&#26412;&#20307;&#35770;&#22312;&#24191;&#27867;&#32972;&#26223;&#30340;&#28151;&#21512;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#26080;&#35770;&#20854;&#23450;&#20041;&#22914;&#20309;&#65292;&#24182;&#23545;&#26412;&#20307;&#35770;&#22312;&#20943;&#23569;&#28151;&#21512;&#26234;&#33021;&#31995;&#32479;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#24046;&#36317;&#30340;&#21487;&#33021;&#20316;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;&#38500;&#20102;&#26377;&#25928;&#20351;&#29992;&#26412;&#20307;&#35770;&#25552;&#20379;&#30340;&#20856;&#22411;&#22909;&#22788;&#22806;&#65292;&#22312;&#27010;&#24565;&#23618;&#38754;&#65292;&#25152;&#36827;&#34892;&#30340;&#20998;&#26512;&#25351;&#20986;&#65292;&#22312;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#30528;&#36129;&#29486;&#65292;&#21516;&#26102;&#22312;&#21551;&#29992;&#25193;&#23637;&#20114;&#25805;&#20316;&#24615;&#12289;&#31995;&#32479;&#24037;&#31243;&#21644;&#21487;&#35299;&#37322;&#12289;&#36879;&#26126;&#31995;&#32479;&#26041;&#38754;&#21457;&#25381;&#20102;&#26356;&#20855;&#20307;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a context of constant evolution and proliferation of AI technology, Hybrid Intelligence is gaining popularity to refer a balanced coexistence between human and artificial intelligence. On the other side, the concept has been extensively used in the past two decades to define models of intelligence involving more than one technology. This paper aims to provide (i) a concise and focused overview of the adoption of Ontology in the broad context of Hybrid Intelligence regardless of its definition and (ii) a critical discussion on the possible role of Ontology to reduce the gap between human and artificial intelligence within hybrid intelligent systems. Beside the typical benefits provided by an effective use of ontologies, at a conceptual level, the analysis conducted has pointed out a significant contribution to quality and accuracy, as well as a more specific role to enable extended interoperability, system engineering and explainable/transparent systems. On the other side, an applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04238</link><description>&lt;p&gt;
&#21306;&#22495;&#38544;&#24418;&#34917;&#19969;&#65306;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#21363;&#25152;&#35859;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#23545;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38656;&#20351;&#29992;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23398;&#20064;&#22270;&#20687;&#27969;&#24418;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#29289;&#29702;&#23545;&#25239;&#34917;&#19969;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#23618;&#38754;&#19978;&#22343;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#23433;&#20840;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20851;&#27880;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#23545;&#20110;&#23454;&#29616;&#23433;&#20840;&#25506;&#32034;&#12289;&#23433;&#20840;&#20215;&#20540;&#23545;&#40784;&#21644;&#23433;&#20840;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#30456;&#20851;&#30340;&#22235;&#20010;&#24320;&#25918;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.13137</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#30340;&#23433;&#20840;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Human-Centered Safe Robot Reinforcement Learning Framework with Interactive Behaviors. (arXiv:2302.13137v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#23433;&#20840;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20851;&#27880;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#23545;&#20110;&#23454;&#29616;&#23433;&#20840;&#25506;&#32034;&#12289;&#23433;&#20840;&#20215;&#20540;&#23545;&#40784;&#21644;&#23433;&#20840;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#30456;&#20851;&#30340;&#22235;&#20010;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#26426;&#22120;&#20154;&#24212;&#29992;&#38656;&#35201;&#30830;&#20445;&#26426;&#22120;&#20154;&#21450;&#20854;&#29615;&#22659;&#30340;&#23433;&#20840;&#12290;&#23433;&#20840;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;(SRRL)&#26159;&#23454;&#29616;&#20154;&#26426;&#20849;&#23384;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;SRRL&#26694;&#26550;&#65292;&#21253;&#25324;&#23433;&#20840;&#25506;&#32034;&#12289;&#23433;&#20840;&#20215;&#20540;&#23545;&#40784;&#21644;&#23433;&#20840;&#21327;&#20316;&#19977;&#20010;&#38454;&#27573;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21033;&#29992;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#36827;&#34892;SRRL&#30340;&#26041;&#27861;&#12290;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#23454;&#29616;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#65292;&#20363;&#22914;&#23545;&#35805;&#22411;&#26426;&#22120;&#20154;ChatGPT&#12290;&#25105;&#20204;&#35748;&#20026;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#38656;&#35201;SRRL&#31038;&#21306;&#36827;&#19968;&#27493;&#20851;&#27880;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#20154;&#26426;&#20132;&#20114;&#34892;&#20026;&#30340;SRRL&#30456;&#20851;&#30340;&#22235;&#20010;&#24320;&#25918;&#25361;&#25112;&#65292;&#21253;&#25324;&#40065;&#26834;&#24615;&#12289;&#25928;&#29575;&#12289;&#36879;&#26126;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Reinforcement Learning (RL) algorithms for robotics applications in the real world requires ensuring the safety of the robot and its environment. Safe Robot RL (SRRL) is a crucial step towards achieving human-robot coexistence. In this paper, we envision a human-centered SRRL framework consisting of three stages: safe exploration, safety value alignment, and safe collaboration. We examine the research gaps in these areas and propose to leverage interactive behaviors for SRRL. Interactive behaviors enable bi-directional information transfer between humans and robots, such as conversational robot ChatGPT. We argue that interactive behaviors need further attention from the SRRL community. We discuss four open challenges related to the robustness, efficiency, transparency, and adaptability of SRRL with interactive behaviors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.11713</link><description>&lt;p&gt;
Pre-trained Vision and Language Models&#33021;&#21542;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;InfoSeek&#12290;&#20351;&#29992;InfoSeek&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39046;&#20808;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#22238;&#31572;&#19981;&#20165;&#20165;&#26597;&#35810;&#35270;&#35273;&#20869;&#23481;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#30693;&#35782;&#23494;&#38598;&#21644;&#20449;&#24687;&#23547;&#27714;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InfoSeek&#65292;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#26080;&#27861;&#20165;&#20973;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#30340;&#20449;&#24687;&#23547;&#27714;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;InfoSeek&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;PaLI-X&#65292;BLIP2&#31561;&#65289;&#22312;&#22238;&#31572;&#27714;&#30693;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20294;&#22312;InfoSeek&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#22815;&#28608;&#21457;&#27169;&#22411;&#20351;&#29992;&#20182;&#20204;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#21040;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#35270;&#35273;&#23454;&#20307;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models have demonstrated state-of-the-art capabilities over existing tasks involving images and texts, including visual question answering. However, it remains unclear whether these models possess the capability to answer questions that are not only querying visual content but knowledge-intensive and information-seeking. In this study, we introduce InfoSeek, a visual question answering dataset tailored for information-seeking questions that cannot be answered with only common sense knowledge. Using InfoSeek, we analyze various pre-trained visual question answering models and gain insights into their characteristics. Our findings reveal that state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.) face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training. Furthermore, we show that accurate visual entit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#33609;&#31295;&#26469;&#38477;&#20302;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03764</link><description>&lt;p&gt;
Sketchy: &#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#19982;&#39057;&#32321;&#26041;&#21521;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions. (arXiv:2302.03764v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#33609;&#31295;&#26469;&#38477;&#20302;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#38477;&#20302;&#36164;&#28304;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20869;&#23384;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;Kronecker&#22240;&#23376;&#26799;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#32858;&#28966;&#22312;&#19968;&#20010;&#21464;&#21270;&#30340;&#23567;&#30340;&#20027;&#29305;&#24449;&#31354;&#38388;&#19978;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#37319;&#29992;&#20302;&#31209;&#30340;&#33609;&#31295;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20351;&#29992;&#39057;&#32321;&#26041;&#21521;&#65288;FD&#65289;&#33609;&#31295;&#26469;&#20943;&#23569;&#32500;&#25252;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#25506;&#32034;&#20102;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#24212;&#29992;FD&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#36164;&#28304;&#38656;&#27714;&#21644;&#36951;&#25022;&#20445;&#35777;&#30340;&#36864;&#21270;&#20043;&#38388;&#36827;&#34892;&#39640;&#25928;&#25554;&#20540;: &#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20165;$dk$&#30340;&#20869;&#23384;&#19982;&#23436;&#25972;&#30697;&#38453;$d^2$&#30340;&#20869;&#23384;&#36951;&#25022;&#21305;&#37197;&#65292;&#30452;&#21040;&#22312;&#24213;&#37096;$d-k$&#30340;&#29305;&#24449;&#20540;&#19978;&#28155;&#21152;&#35823;&#24046;&#20026;&#27490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive regularization methods that exploit more than the diagonal entries exhibit state of the art performance for many tasks, but can be prohibitive in terms of memory and running time. We find the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace that changes throughout training, motivating a low-rank sketching approach. We describe a generic method for reducing memory and compute requirements of maintaining a matrix preconditioner using the Frequent Directions (FD) sketch. While previous approaches have explored applying FD for second-order optimization, we present a novel analysis which allows efficient interpolation between resource requirements and the degradation in regret guarantees with rank $k$: in the online convex optimization (OCO) setting over dimension $d$, we match full-matrix $d^2$ memory regret using only $dk$ memory up to additive error in the bottom $d-k$ eigenvalues of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2302.03246</link><description>&lt;p&gt;
CDANs: &#26469;&#33258;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26102;&#38388;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data. (arXiv:2302.03246v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35768;&#22810;&#26041;&#38754;&#20013;&#34987;&#21457;&#29616;&#65292;&#22914;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#12289;&#29983;&#21629;&#20307;&#24449;&#27979;&#37327;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#12290;&#22240;&#26524;&#21457;&#29616;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#23545;&#20110;&#25552;&#21462;&#26377;&#20851;&#20154;&#31867;&#20581;&#24247;&#30340;&#34892;&#21160;&#27934;&#23519;&#21147;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#22411;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;CDANs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;&#33258;&#30456;&#20851;&#21644;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#30340;&#20960;&#20010;&#38480;&#21046;&#65292;&#22914;&#39640;&#32500;&#24230;&#12289;&#26080;&#27861;&#35782;&#21035;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#21644;&#24573;&#35270;&#21464;&#21270;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35782;&#21035;&#20986;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#28382;&#21518;&#21644;&#21363;&#26102;/&#21516;&#26102;&#30340;&#22240;&#26524;&#20851;&#31995;&#20197;&#21450;&#21464;&#21270;&#30340;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#28382;&#21518;&#29238;&#33410;&#28857;&#26469;&#20248;&#21270;&#32422;&#26463;&#25628;&#32034;&#20013;&#30340;&#26465;&#20214;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data are found in many areas of healthcare such as medical time series, electronic health records (EHR), measurements of vitals, and wearable devices. Causal discovery, which involves estimating causal relationships from observational data, holds the potential to play a significant role in extracting actionable insights about human health. In this study, we present a novel constraint-based causal discovery approach for autocorrelated and non-stationary time series data (CDANs). Our proposed method addresses several limitations of existing causal discovery methods for autocorrelated and non-stationary time series data, such as high dimensionality, the inability to identify lagged causal relationships, and overlooking changing modules. Our approach identifies lagged and instantaneous/contemporaneous causal relationships along with changing modules that vary over time. The method optimizes the conditioning sets in a constraint-based search by considering lagged parents instead
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#65306;&#35299;&#37322;&#21644;&#23547;&#25214;&#22909;&#30340;&#31034;&#33539;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#38544;&#24335;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20174;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#22312;&#25512;&#29702;&#26102;&#23454;&#29616;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#30340;&#26174;&#33879;&#25928;&#29575;&#65292;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290; &#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24378;&#35843;&#36825;&#31181;&#33021;&#21147;&#23545;&#23569;&#37327;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#24456;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36125;&#21494;&#26031;&#35270;&#35282;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#20174;&#31034;&#33539;&#20013;&#38544;&#21547;&#22320;&#25512;&#26029;&#20986;&#30456;&#20851;&#20449;&#24687;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;&#22312;&#27492;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#27880;&#37322;&#25968;&#25454;&#20013;&#36873;&#25321;&#26368;&#20339;&#31034;&#33539;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#22522;&#32447;&#30340;&#24179;&#22343;&#20540;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#30495;&#23454;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#27599;&#20010; GPT2 &#21644; GPT3 &#27169;&#22411;&#26377;&#26174;&#30528;&#30340; 12.5% &#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#34987;&#35270;&#20026;&#38544;&#21547;&#30340;&#20027;&#39064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#29702;&#35299;&#20219;&#21153;&#20013;&#37117;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2301.05065</link><description>&lt;p&gt;
&#26500;&#24314;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks. (arXiv:2301.05065v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#29702;&#35299;&#20219;&#21153;&#20013;&#37117;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#30784;&#27169;&#22411;&#21482;&#33021;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#25110;&#35270;&#35273;-&#35821;&#35328;&#20013;&#30340;&#19968;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#33021;&#21542;&#26500;&#24314;&#19968;&#31181;&#22312;&#25152;&#26377;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;X-FM&#65288;X&#22522;&#30784;&#27169;&#22411;&#65289;&#12290;X-FM&#20855;&#26377;&#19968;&#20010;&#35821;&#35328;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#34701;&#21512;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#20013;&#23398;&#20064;X-FM&#12290;&#19968;&#31181;&#26159;&#22312;&#23398;&#20064;&#35821;&#35328;&#32534;&#30721;&#22120;&#26102;&#20572;&#27490;&#35270;&#35273;-&#35821;&#35328;&#35757;&#32451;&#30340;&#26799;&#24230;&#12290;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#35757;&#32451;&#26469;&#25351;&#23548;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models or pre-trained models have substantially improved the performance of various language, vision, and vision-language understanding tasks. However, existing foundation models can only perform the best in one type of tasks, namely language, vision, or vision-language. It is still an open question whether it is possible to construct a foundation model performing the best for all the understanding tasks, which we call a general foundation model. In this paper, we propose a new general foundation model, X-FM (the X-Foundation Model). X-FM has one language encoder, one vision encoder, and one fusion encoder, as well as a new training method. The training method includes two new techniques for learning X-FM from text, image, and image-text pair data. One is to stop gradients from the vision-language training when learning the language encoder. The other is to leverage the vision-language training to guide the learning of the vision encoder. Extensive experiments on benchmark d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#20248;&#20915;&#31574;&#26641;&#21644;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.06312</link><description>&lt;p&gt;
&#23545;&#22810;&#20010;&#24863;&#20852;&#36259;&#32467;&#26524;&#30340;&#31574;&#30053;&#23398;&#20064;&#65306;&#23558;&#26368;&#20248;&#31574;&#30053;&#26641;&#19982;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation. (arXiv:2212.06312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#20248;&#20915;&#31574;&#26641;&#21644;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24179;&#34913;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#20351;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#21019;&#24314;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#20197;&#22312;&#19981;&#21516;&#25919;&#31574;&#24178;&#39044;&#20998;&#37197;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#30340;&#25919;&#31574;&#21046;&#23450;&#29615;&#22659;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#20851;&#24515;&#19981;&#21516;&#32467;&#26524;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#21333;&#32431;&#22320;&#26368;&#22823;&#21270;&#19968;&#20010;&#32467;&#26524;&#30340;&#25928;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#22810;&#30446;&#26631;&#31574;&#30053;&#23398;&#20064;&#65288;MOPoL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31574;&#30053;&#23398;&#20064;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#19982;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#25506;&#32034;&#22810;&#20010;&#32467;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23427;&#36890;&#36807;&#26500;&#24314;&#38750;&#25903;&#37197;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#25511;&#21046;&#30528;&#32467;&#26524;&#30340;&#26435;&#37325;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#36138;&#24515;&#26641;&#21487;&#20197;&#20316;&#20026;&#38750;&#24120;&#35745;&#31639;&#26114;&#36149;&#30340;&#26368;&#20248;&#26641;&#30340;&#20934;&#30830;&#20195;&#29702;&#65292;&#29992;&#20110;&#20915;&#31574;&#30446;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#21453;&#22797;&#25311;&#21512;&#27169;&#22411;&#26469;&#23398;&#20064;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Methods for learning optimal policies use causal machine learning models to create human-interpretable rules for making choices around the allocation of different policy interventions. However, in realistic policy-making contexts, decision-makers often care about trade-offs between outcomes, not just single-mindedly maximising utility for one outcome. This paper proposes an approach termed Multi-Objective Policy Learning (MOPoL) which combines optimal decision trees for policy learning with a multi-objective Bayesian optimisation approach to explore the trade-off between multiple outcomes. It does this by building a Pareto frontier of non-dominated models for different hyperparameter settings which govern outcome weighting. The key here is that a low-cost greedy tree can be an accurate proxy for the very computationally costly optimal tree for the purposes of making decisions which means models can be repeatedly fit to learn a Pareto frontier. The method is applied to a real-world case
&lt;/p&gt;</description></item><item><title>ConvLab-3&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#25903;&#25345;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.17148</link><description>&lt;p&gt;
ConvLab-3&#65306;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#30340;&#28789;&#27963;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ConvLab-3: A Flexible Dialogue System Toolkit Based on a Unified Data Format. (arXiv:2211.17148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17148
&lt;/p&gt;
&lt;p&gt;
ConvLab-3&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#22522;&#20110;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#19982;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#25903;&#25345;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#65288;TOD&#65289;&#20316;&#20026;&#25968;&#23383;&#21161;&#25163;&#65292;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#35832;&#22914;&#39044;&#35746;&#33322;&#29677;&#25110;&#23547;&#25214;&#39184;&#21381;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#30446;&#21069;&#29992;&#20110;&#26500;&#24314;TOD&#31995;&#32479;&#30340;&#24037;&#20855;&#21253;&#36890;&#24120;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#23454;&#39564;&#29615;&#22659;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29992;&#25143;&#20307;&#39564;&#20063;&#19981;&#21451;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ConvLab-3&#65306;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#23545;&#35805;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#25968;&#25454;&#26684;&#24335;&#31616;&#21270;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#30740;&#31350;&#24191;&#27867;&#27867;&#21270;&#21644;&#36801;&#31227;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24037;&#20855;&#30340;&#22686;&#24378;&#65292;&#21253;&#25324;&#31616;&#21270;&#30340;&#35757;&#32451;&#36807;&#31243;&#12289;&#28145;&#20837;&#30340;&#35780;&#20272;&#24037;&#20855;&#20197;&#21450;&#22810;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#36873;&#25321;&#65292;ConvLab-3&#25903;&#25345;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#40065;&#26834;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;RL&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;ConvLab-3&#19981;&#20165;&#26159;&#19968;&#27454;&#24378;&#22823;&#30340;&#24037;&#20855;&#20379;&#32463;&#39564;&#20016;&#23500;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems function as digital assistants, guiding users through various tasks such as booking flights or finding restaurants. Existing toolkits for building TOD systems often fall short of in delivering comprehensive arrays of data, models, and experimental environments with a user-friendly experience. We introduce ConvLab-3: a multifaceted dialogue system toolkit crafted to bridge this gap. Our unified data format simplifies the integration of diverse datasets and models, significantly reducing complexity and cost for studying generalization and transfer. Enhanced with robust reinforcement learning (RL) tools, featuring a streamlined training process, in-depth evaluation tools, and a selection of user simulators, ConvLab-3 supports the rapid development and evaluation of robust dialogue policies. Through an extensive study, we demonstrate the efficacy of transfer learning and RL and showcase that ConvLab-3 is not only a powerful tool for seasoned researchers
&lt;/p&gt;</description></item><item><title>Photo Rater&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24110;&#21161;&#25668;&#24433;&#24072;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#12290;&#23427;&#36890;&#36807;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12289;&#27169;&#31946;&#20998;&#31867;&#21644;&#23457;&#32654;&#35780;&#20272;&#65292;&#24182;&#26681;&#25454;&#24471;&#20998;&#23545;&#22270;&#20687;&#36827;&#34892;&#25490;&#24207;&#21644;&#21576;&#29616;&#12290;</title><link>http://arxiv.org/abs/2211.14420</link><description>&lt;p&gt;
Photo Rater: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#36873;&#25321;&#25668;&#24433;&#29031;&#29255;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Photo Rater: Photographs Auto-Selector with Deep Learning. (arXiv:2211.14420v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14420
&lt;/p&gt;
&lt;p&gt;
Photo Rater&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24110;&#21161;&#25668;&#24433;&#24072;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#12290;&#23427;&#36890;&#36807;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#22270;&#20687;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12289;&#27169;&#31946;&#20998;&#31867;&#21644;&#23457;&#32654;&#35780;&#20272;&#65292;&#24182;&#26681;&#25454;&#24471;&#20998;&#23545;&#22270;&#20687;&#36827;&#34892;&#25490;&#24207;&#21644;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Photo Rater&#26159;&#19968;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#39033;&#30446;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#24110;&#21161;&#25668;&#24433;&#24072;&#22312;&#25293;&#25668;&#21516;&#19968;&#22330;&#26223;&#30340;&#29031;&#29255;&#20013;&#36873;&#25321;&#26368;&#20339;&#29031;&#29255;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#31579;&#36873;&#8221;&#22312;&#25668;&#24433;&#20013;&#65292;&#22914;&#26524;&#25163;&#21160;&#23436;&#25104;&#20250;&#24456;&#32321;&#29712;&#21644;&#32791;&#26102;&#12290;Photo Rater&#21033;&#29992;&#19977;&#20010;&#29420;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#23436;&#25104;&#36825;&#26679;&#30340;&#20219;&#21153;&#65306;&#19968;&#20010;&#29992;&#20110;&#19968;&#33324;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65292;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#29031;&#29255;&#26159;&#21542;&#27169;&#31946;&#65288;&#22240;&#20026;&#25163;&#25238;&#25110;&#32773;&#32858;&#28966;&#19981;&#20934;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25972;&#20307;&#23457;&#32654;&#65288;&#21253;&#25324;&#29031;&#29255;&#26500;&#22270;&#31561;&#65289;&#12290;&#22312;&#36890;&#36807;&#27599;&#20010;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#20687;&#21518;&#65292;Photo Rater&#20026;&#27599;&#20010;&#22270;&#20687;&#36755;&#20986;&#19968;&#20010;&#26368;&#32456;&#24471;&#20998;&#65292;&#26681;&#25454;&#24471;&#20998;&#25490;&#21517;&#24182;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photo Rater is a computer vision project that uses neural networks to help photographers select the best photo among those that are taken based on the same scene. This process is usually referred to as "culling" in photography, and it can be tedious and time-consuming if done manually. Photo Rater utilizes three separate neural networks to complete such a task: one for general image quality assessment, one for classifying whether the photo is blurry (either due to unsteady hands or out-of-focusness), and one for assessing general aesthetics (including the composition of the photo, among others). After feeding the image through each neural network, Photo Rater outputs a final score for each image, ranking them based on this score and presenting it to the user.
&lt;/p&gt;</description></item><item><title>&#28212;&#26395;&#21453;&#21521;&#20256;&#25773;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#26399;&#26395;&#33033;&#20914;&#27963;&#21160;&#19982;&#33033;&#20914;&#24207;&#21015;&#30340;&#26412;&#22320;STDP&#26435;&#37325;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25429;&#25417;&#31070;&#32463;&#20803;&#30340;&#21160;&#24577;&#24182;&#26368;&#23567;&#21270;&#36755;&#20986;&#35823;&#24046;&#65292;&#20174;&#32780;&#33719;&#24471;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.05412</link><description>&lt;p&gt;
&#28212;&#26395;&#21453;&#21521;&#20256;&#25773;&#65306;&#22522;&#20110;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;&#30340;&#22810;&#23618;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Desire Backpropagation: A Lightweight Training Algorithm for Multi-Layer Spiking Neural Networks based on Spike-Timing-Dependent Plasticity. (arXiv:2211.05412v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05412
&lt;/p&gt;
&lt;p&gt;
&#28212;&#26395;&#21453;&#21521;&#20256;&#25773;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23618;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#26399;&#26395;&#33033;&#20914;&#27963;&#21160;&#19982;&#33033;&#20914;&#24207;&#21015;&#30340;&#26412;&#22320;STDP&#26435;&#37325;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25429;&#25417;&#31070;&#32463;&#20803;&#30340;&#21160;&#24577;&#24182;&#26368;&#23567;&#21270;&#36755;&#20986;&#35823;&#24046;&#65292;&#20174;&#32780;&#33719;&#24471;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26159;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#65292;&#24403;&#36164;&#28304;&#25928;&#29575;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#26102;&#12290;SNNs &#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#21183;&#26159;&#36890;&#36807;&#33033;&#20914;&#24207;&#21015;&#36827;&#34892;&#20108;&#36827;&#21046;&#20449;&#24687;&#20256;&#36755;&#65292;&#28040;&#38500;&#20102;&#20056;&#27861;&#36816;&#31639;&#12290;&#28982;&#32780;&#65292;SNNs &#30340;&#35757;&#32451;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#31070;&#32463;&#20803;&#27169;&#22411;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;&#65288;STDP&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#30340;&#23398;&#20064;&#35268;&#21017;&#65292;&#20294;&#23427;&#21482;&#22312;&#26412;&#22320;&#26356;&#26032;&#26435;&#37325;&#65292;&#27809;&#26377;&#38024;&#23545;&#32593;&#32476;&#36755;&#20986;&#35823;&#24046;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28212;&#26395;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#20174;&#36755;&#20986;&#35823;&#24046;&#20013;&#25512;&#23548;&#20986;&#25152;&#26377;&#31070;&#32463;&#20803;&#65288;&#21253;&#25324;&#38544;&#34255;&#31070;&#32463;&#20803;&#65289;&#30340;&#26399;&#26395;&#33033;&#20914;&#27963;&#21160;&#12290;&#36890;&#36807;&#23558;&#36825;&#20010;&#26399;&#26395;&#20540;&#32467;&#21512;&#21040;&#26412;&#22320;STDP&#26435;&#37325;&#26356;&#26032;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31070;&#32463;&#20803;&#30340;&#21160;&#24577;&#65292;&#24182;&#26368;&#23567;&#21270;&#20840;&#23616;&#35823;&#24046;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are a viable alternative to conventional artificial neural networks when resource efficiency and computational complexity are of importance. A major advantage of SNNs is their binary information transfer through spike trains which eliminates multiplication operations. The training of SNNs has, however, been a challenge, since neuron models are non-differentiable and traditional gradient-based backpropagation algorithms cannot be applied directly. Furthermore, spike-timing-dependent plasticity (STDP), albeit being a spike-based learning rule, updates weights locally and does not optimize for the output error of the network. We present desire backpropagation, a method to derive the desired spike activity of all neurons, including the hidden ones, from the output error. By incorporating this desire value into the local STDP weight update, we can efficiently capture the neuron dynamics while minimizing the global error and attaining a high classification accu
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LSTM-based&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#24494;&#35843;&#25552;&#39640;&#20102;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03100</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#39044;&#27979;&#29992;&#25143;&#29305;&#23450;&#30340;&#26410;&#26469;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Predicting User-specific Future Activities using LSTM-based Multi-label Classification. (arXiv:2211.03100v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03100
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LSTM-based&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#24494;&#35843;&#25552;&#39640;&#20102;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20808;&#21069;&#27963;&#21160;&#30340;&#29992;&#25143;&#29305;&#23450;&#26410;&#26469;&#27963;&#21160;&#39044;&#27979;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#25252;&#22763;&#25552;&#20379;&#30340;&#26381;&#21153;&#36136;&#37327;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#36827;&#34892;&#39044;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#19982;&#20854;&#20182;&#39046;&#22495;&#19981;&#21516;&#65292;&#21307;&#30103;&#39046;&#22495;&#30340;&#27963;&#21160;&#28041;&#21450;&#25252;&#22763;&#21644;&#24739;&#32773;&#65292;&#24182;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26412;&#25991;&#37319;&#29992;&#21508;&#31181;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#26469;&#32452;&#32455;&#21644;&#20462;&#25913;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;LSTM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65288;&#29992;&#25143;&#19981;&#21487;&#30693;&#30340;&#39044;&#35757;&#32451;&#21644;&#29992;&#25143;&#29305;&#23450;&#30340;&#24494;&#35843;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;&#20102;31.58%&#30340;&#20934;&#30830;&#29575;&#65292;57.94%&#30340;&#31934;&#30830;&#24230;&#65292;68.31%&#30340;&#21484;&#22238;&#29575;&#21644;60.38%&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#20004;&#38454;&#27573;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#20010;&#23454;&#39564;&#26159;&#25105;&#20204;&#22242;&#38431;&#8220;&#19981;&#26159;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#31881;&#19997;&#8221;&#21442;&#21152;&#30340;&#8220;&#31532;&#22235;&#20010;&#25252;&#22763;&#25252;&#29702;&#27963;&#21160;&#35782;&#21035;&#25361;&#25112;&#8221;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-specific future activity prediction in the healthcare domain based on previous activities can drastically improve the services provided by the nurses. It is challenging because, unlike other domains, activities in healthcare involve both nurses and patients, and they also vary from hour to hour. In this paper, we employ various data processing techniques to organize and modify the data structure and an LSTM-based multi-label classifier for a novel 2-stage training approach (user-agnostic pre-training and user-specific fine-tuning). Our experiment achieves a validation accuracy of 31.58\%, precision 57.94%, recall 68.31%, and F1 score 60.38%. We concluded that proper data pre-processing and a 2-stage training process resulted in better performance. This experiment is a part of the "Fourth Nurse Care Activity Recognition Challenge" by our team "Not A Fan of Local Minima".
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.05521</link><description>&lt;p&gt;
&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#31264;&#23494;&#26816;&#32034;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverted Index Is a Robust Accelerator for Dense Retrieval. (arXiv:2210.05521v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341;(HI$^2$)&#29992;&#20110;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#65292;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#25552;&#39640;&#26816;&#32034;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20498;&#25490;&#25991;&#20214;&#32467;&#26500;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#30340;&#25216;&#26415;&#12290;&#23427;&#26681;&#25454;&#23884;&#20837;&#23558;&#25991;&#26723;&#32858;&#31867;&#65307;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#26597;&#35810;&#25506;&#27979;&#38468;&#36817;&#30340;&#32858;&#31867;&#65292;&#24182;&#19988;&#20165;&#23545;&#20854;&#20013;&#30340;&#25991;&#26723;&#36827;&#34892;&#21518;&#32493;&#30340;&#35299;&#30721;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#31351;&#20030;&#36941;&#21382;&#30340;&#26114;&#36149;&#20195;&#20215;&#12290;&#28982;&#32780;&#65292;&#32858;&#31867;&#36807;&#31243;&#24635;&#26159;&#26377;&#25439;&#30340;&#65292;&#36825;&#23548;&#33268;&#25506;&#27979;&#21040;&#30340;&#32858;&#31867;&#20013;&#32570;&#22833;&#20102;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26816;&#32034;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#35789;&#27719;&#21305;&#37197;&#65292;&#22914;&#26174;&#33879;&#35789;&#27719;&#30340;&#37325;&#21472;&#65292;&#26356;&#23481;&#26131;&#35782;&#21035;&#30456;&#20851;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#20498;&#25490;&#32034;&#24341; (HI$^2$)&#65292;&#20854;&#20013;&#23884;&#20837;&#32858;&#31867;&#21644;&#26174;&#33879;&#35789;&#27719;&#20849;&#21516;&#21152;&#36895;&#31264;&#23494;&#26816;&#32034;&#12290;&#20026;&#20102;&#20860;&#39038;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32858;&#31867;&#36873;&#25321;&#22120;&#21644;&#19968;&#20010;&#35789;&#27719;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#26500;&#24314;&#32039;&#20945;&#30340;&#20498;&#25490;&#21015;&#34920;&#24182;&#24555;&#36895;&#25628;&#32034;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#31639;&#27861;&#21644;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#25552;&#39640;&#32034;&#24341;&#36136;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Inverted file structure is a common technique for accelerating dense retrieval. It clusters documents based on their embeddings; during searching, it probes nearby clusters w.r.t. an input query and only evaluates documents within them by subsequent codecs, thus avoiding the expensive cost of exhaustive traversal. However, the clustering is always lossy, which results in the miss of relevant documents in the probed clusters and hence degrades retrieval quality. In contrast, lexical matching, such as overlaps of salient terms, tends to be strong feature for identifying relevant documents. In this work, we present the Hybrid Inverted Index (HI$^2$), where the embedding clusters and salient terms work collaboratively to accelerate dense retrieval. To make best of both effectiveness and efficiency, we devise a cluster selector and a term selector, to construct compact inverted lists and efficiently searching through them. Moreover, we leverage simple unsupervised algorithms as well as end-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#20855;&#36523;&#21270;&#25351;&#31216;&#34920;&#36798;&#65288;ERE&#65289;&#19982;&#25805;&#32437;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#36828;&#31243;&#20855;&#36523;&#21270;&#25805;&#32437;&#38382;&#39064;&#22238;&#31572;&#65288;REMQA&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;AI2-THOR&#27169;&#25311;&#22120;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#22791;3D&#35821;&#20041;&#37325;&#24314;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#33539; paradigm &#30340;&#26694;&#26550;&#12290;&#26368;&#32456;&#65292;&#22312;REMQA&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.02709</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#36827;&#34892;&#25805;&#32437;&#38382;&#39064;&#22238;&#31572;&#30340;&#20855;&#36523;&#21270;&#25351;&#31216;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Embodied Referring Expression for Manipulation Question Answering in Interactive Environment. (arXiv:2210.02709v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#20855;&#36523;&#21270;&#25351;&#31216;&#34920;&#36798;&#65288;ERE&#65289;&#19982;&#25805;&#32437;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#36828;&#31243;&#20855;&#36523;&#21270;&#25805;&#32437;&#38382;&#39064;&#22238;&#31572;&#65288;REMQA&#65289;&#30340;&#26032;&#20219;&#21153;&#12290;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;AI2-THOR&#27169;&#25311;&#22120;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#22791;3D&#35821;&#20041;&#37325;&#24314;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#33539; paradigm &#30340;&#26694;&#26550;&#12290;&#26368;&#32456;&#65292;&#22312;REMQA&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#20855;&#36523;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#23545;&#20855;&#36523;&#21270;&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25191;&#34892;&#26356;&#22797;&#26434;&#20219;&#21153;&#30340;&#26399;&#26395;&#36234;&#26469;&#36234;&#39640;&#12290;&#29616;&#26377;&#30340;&#20855;&#36523;&#21270;&#20219;&#21153;&#21253;&#25324;&#20855;&#36523;&#21270;&#25351;&#31216;&#34920;&#36798;&#65288;ERE&#65289;&#21644;&#20854;&#20182;&#38382;&#31572;&#24418;&#24335;&#30340;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#25351;&#20196;&#26041;&#38754;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#20027;&#21160;&#22312;&#29615;&#22659;&#20013;&#25805;&#32437;&#23545;&#35937;&#36827;&#34892;&#25506;&#32034;&#65292;&#25104;&#20026;&#31038;&#21306;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#36523;&#21270;&#20219;&#21153;&#65306;&#36828;&#31243;&#20855;&#36523;&#21270;&#25805;&#32437;&#38382;&#39064;&#22238;&#31572;&#65288;REMQA&#65289;&#65292;&#23558;ERE&#19982;&#25805;&#32437;&#20219;&#21153;&#30456;&#32467;&#21512;&#12290;&#22312;REMQA&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#23548;&#33322;&#21040;&#36828;&#31243;&#20301;&#32622;&#65292;&#24182;&#23545;&#30446;&#26631;&#23545;&#35937;&#36827;&#34892;&#25805;&#32437;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;AI2-THOR&#27169;&#25311;&#22120;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;REMQA&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#22791;3D&#35821;&#20041;&#37325;&#24314;&#21644;&#27169;&#22359;&#21270;&#32593;&#32476;&#33539; paradigm &#30340;&#26694;&#26550;&#12290;&#20171;&#32461;&#20102;&#22312;REMQA&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents are expected to perform more complicated tasks in an interactive environment, with the progress of Embodied AI in recent years. Existing embodied tasks including Embodied Referring Expression (ERE) and other QA-form tasks mainly focuses on interaction in term of linguistic instruction. Therefore, enabling the agent to manipulate objects in the environment for exploration actively has become a challenging problem for the community. To solve this problem, We introduce a new embodied task: Remote Embodied Manipulation Question Answering (REMQA) to combine ERE with manipulation tasks. In the REMQA task, the agent needs to navigate to a remote position and perform manipulation with the target object to answer the question. We build a benchmark dataset for the REMQA task in the AI2-THOR simulator. To this end, a framework with 3D semantic reconstruction and modular network paradigms is proposed. The evaluation of the proposed framework on the REMQA dataset is presented to val
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2209.03737</link><description>&lt;p&gt;
CGAN-ECT&#65306;&#20351;&#29992;CGAN&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#23618;&#26512;&#25104;&#20687;&#12290;&#65288;arXiv:2209.03737v3 [eess.IV] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
CGAN-ECT: Tomography Image Reconstruction from Electrical Capacitance Measurements Using CGANs. (arXiv:2209.03737v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30005;&#23481;&#23618;&#26512;&#25104;&#20687;&#65288;ECT&#65289;&#22312;&#22810;&#20010;&#24037;&#19994;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#39640;&#36136;&#37327;&#19988;&#24555;&#36895;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#26469;&#20174;&#21407;&#22987;&#30005;&#23481;&#27979;&#37327;&#20013;&#24471;&#21040;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#24037;&#20855;&#65292;&#24050;&#32463;&#22312;&#21253;&#25324;&#30005;&#27668;&#23618;&#26512;&#25104;&#20687;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#30005;&#23481;&#27979;&#37327;&#20013;&#37325;&#24314;ECT&#22270;&#20687;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CGAN&#65289;&#27169;&#22411;&#12290;&#35813;CGAN&#27169;&#22411;&#30340;&#21021;&#22987;&#22270;&#20687;&#26159;&#30001;&#30005;&#23481;&#27979;&#37327;&#26500;&#36896;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23558;&#30005;&#23481;&#27979;&#37327;&#34920;&#31034;&#20026;&#22270;&#20687;&#24418;&#24335;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;320K&#20010;&#21512;&#25104;&#22270;&#20687;&#27979;&#37327;&#23545;&#30340;&#26032;ECT&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#25968;&#25454;&#38598;&#12289;&#21463;&#27745;&#26579;&#25968;&#25454;&#21644;&#27969;&#21160;&#27169;&#24335;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;CGAN-ECT&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid growth of Electrical Capacitance Tomography (ECT) applications in several industrial fields, there is a crucial need for developing high quality, yet fast, methodologies of image reconstruction from raw capacitance measurements. Deep learning, as an effective non-linear mapping tool for complicated functions, has been going viral in many fields including electrical tomography. In this paper, we propose a Conditional Generative Adversarial Network (CGAN) model for reconstructing ECT images from capacitance measurements. The initial image of the CGAN model is constructed from the capacitance measurement. To our knowledge, this is the first time to represent the capacitance measurements in an image form. We have created a new massive ECT dataset of 320K synthetic image measurements pairs for training, and testing the proposed model. The feasibility and generalization ability of the proposed CGAN-ECT model are evaluated using testing dataset, contaminated data and flow pat
&lt;/p&gt;</description></item><item><title>SSM-DTA&#26694;&#26550;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#25171;&#30772;&#20102;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2206.09818</link><description>&lt;p&gt;
SSM-DTA: &#25171;&#30772;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction. (arXiv:2206.09818v3 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09818
&lt;/p&gt;
&lt;p&gt;
SSM-DTA&#26694;&#26550;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#25171;&#30772;&#20102;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#39044;&#27979;&#20013;&#25968;&#25454;&#31232;&#32570;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#20146;&#21644;&#24615;&#65288;DTA&#65289;&#22312;&#26089;&#26399;&#33647;&#29289;&#30740;&#21457;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#21487;&#20197;&#19982;&#29305;&#23450;&#38774;&#28857;&#26377;&#25928;&#30456;&#20114;&#20316;&#29992;&#24182;&#35843;&#33410;&#20854;&#27963;&#24615;&#30340;&#33647;&#29289;&#12290;&#23613;&#31649;&#28287;&#23454;&#39564;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32791;&#26102;&#32791;&#21147;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#32473;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#29616;&#26377;DTA&#25968;&#25454;&#24320;&#21457;&#25216;&#26415;&#19978;&#65292;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SSM-DTA&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#19977;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31574;&#30053;&#65306;&#65288;1&#65289;&#22810;&#20219;&#21153;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;DTA&#39044;&#27979;&#19982;&#33945;&#29256;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;&#25104;&#23545;&#30340;&#33647;&#29289;-&#38774;&#28857;&#25968;&#25454;&#12290;&#65288;2&#65289;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#26080;&#37197;&#23545;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#26469;&#22686;&#24378;&#33647;&#29289;&#21644;&#38774;&#28857;&#30340;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from
&lt;/p&gt;</description></item><item><title>PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.07940</link><description>&lt;p&gt;
PROFHIT: &#38754;&#21521;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07940
&lt;/p&gt;
&lt;p&gt;
PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#31181;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#20998;&#23618;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#19978;&#20063;&#24341;&#20837;&#20102;&#20998;&#23618;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#40664;&#40664;&#22320;&#20551;&#35774;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#20998;&#23618;&#20851;&#31995;&#19968;&#33268;&#65292;&#24182;&#19988;&#19981;&#36866;&#24212;&#26174;&#31034;&#19982;&#27492;&#20551;&#35774;&#20559;&#31163;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHIT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#24615;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;PROFHIT&#37319;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2206.02346</link><description>&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#32422;&#26463;MDP&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. (arXiv:2206.02346v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#22870;&#21169;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#39044;&#26399;&#24635;&#25928;&#29992;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#32422;&#26463;MDP&#65289;&#30340;&#25240;&#25187;&#26080;&#38480;&#26102;&#24207;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#65288;NPG-PD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#65292;&#36890;&#36807;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#23613;&#31649;&#24213;&#23618;&#26368;&#22823;&#21270;&#28041;&#21450;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38750;&#20984;&#32422;&#26463;&#38598;&#65292;&#20294;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#35268;&#26041;&#38754;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#27492;&#31867;&#25910;&#25947;&#19982;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#21363;&#26080;&#32500;&#24230;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#25968;&#32447;&#24615;&#21644;&#19968;&#33324;&#24179;&#28369;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30830;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we esta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2204.07481</link><description>&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Knowledge Equivalence in Digital Twins of Intelligent Systems. (arXiv:2204.07481v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#30693;&#35782;&#31561;&#20215;&#24615;&#65292;&#25552;&#20986;&#20102;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#21253;&#21547;&#20102;&#23545;&#25152;&#30740;&#31350;&#30340;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#23454;&#26102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#27169;&#25311;&#26469;&#20248;&#21270;&#29289;&#29702;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;&#23402;&#29983;&#25152;&#20570;&#30340;&#20998;&#26512;&#21482;&#26377;&#22312;&#27169;&#22411;&#19982;&#23454;&#38469;&#29289;&#29702;&#19990;&#30028;&#31561;&#20215;&#30340;&#24773;&#20917;&#19979;&#25165;&#26159;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#12290;&#22312;&#29289;&#29702;&#31995;&#32479;&#26159;&#26234;&#33021;&#21644;&#33258;&#20027;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25345;&#36825;&#26679;&#19968;&#20010;&#31561;&#20215;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#26234;&#33021;&#31995;&#32479;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#20854;&#20013;&#31995;&#32479;&#20855;&#26377;&#30693;&#35782;&#24863;&#30693;&#33021;&#21147;&#20294;&#33021;&#21147;&#26377;&#38480;&#12290;&#25968;&#23383;&#23402;&#29983;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#31215;&#32047;&#26356;&#22810;&#30693;&#35782;&#65292;&#20174;&#20803;&#23618;&#38754;&#19978;&#25913;&#36827;&#20102;&#29289;&#29702;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#34394;&#25311;&#31354;&#38388;&#20013;&#22797;&#21046;&#36825;&#26679;&#19968;&#20010;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#30340;&#30693;&#35782;&#24863;&#30693;&#33021;&#21147;&#38656;&#35201;&#37319;&#29992;&#26032;&#39062;&#30340;&#31561;&#20215;&#24615;&#32500;&#25252;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#19982;&#29289;&#29702;&#31995;&#32479;&#20043;&#38388;&#21516;&#27493;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30693;&#35782;&#31561;&#20215;&#24615;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A digital twin contains up-to-date data-driven models of the physical world being studied and can use simulation to optimise the physical world. However, the analysis made by the digital twin is valid and reliable only when the model is equivalent to the physical world. Maintaining such an equivalent model is challenging, especially when the physical systems being modelled are intelligent and autonomous. The paper focuses in particular on digital twin models of intelligent systems where the systems are knowledge-aware but with limited capability. The digital twin improves the acting of the physical system at a meta-level by accumulating more knowledge in the simulated environment. The modelling of such an intelligent physical system requires replicating the knowledge-awareness capability in the virtual space. Novel equivalence maintaining techniques are needed, especially in synchronising the knowledge between the model and the physical system. This paper proposes the notion of knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#36817;&#26399;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32972;&#26223;&#12289;&#36817;&#26399;&#26368;&#20339;&#34920;&#29616;&#30340;ViT&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12289;&#20197;&#21450;&#19982;&#20256;&#32479;CNN&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2203.01536</link><description>&lt;p&gt;
&#36817;&#26399;&#35270;&#35273;Transformer&#30740;&#31350;&#36827;&#23637;&#65306;&#32508;&#36848;&#21644;&#26368;&#26032;&#24037;&#20316;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#36817;&#26399;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32972;&#26223;&#12289;&#36817;&#26399;&#26368;&#20339;&#34920;&#29616;&#30340;ViT&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12289;&#20197;&#21450;&#19982;&#20256;&#32479;CNN&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#21644;&#20027;&#23548;&#12290;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#39033;&#38656;&#27714;&#37327;&#22823;&#30340;&#25216;&#26415;&#65292;ViTs&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#35768;&#22810;&#20851;&#27880;&#38271;&#36317;&#31163;&#20851;&#31995;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#32972;&#26223;&#65292;&#25509;&#30528;&#20840;&#38754;&#27010;&#36848;&#20102;&#36817;&#26399;&#26368;&#20339;&#34920;&#29616;ViT&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12289;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#21508;&#31181;ViT&#31639;&#27861;&#21644;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;CNN&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20123;&#23616;&#38480;&#24615;&#24182;&#25552;&#20986;&#20102;&#28145;&#20837;&#30740;&#31350;&#26041;&#21521;&#12290;&#39033;&#30446;&#39029;&#38754;&#20197;&#21450;&#35770;&#25991;&#38598;&#21512;&#21487;&#22312;https://github.com/khawar512/ViT-Survey&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) are becoming more popular and dominating technique for various vision tasks, compare to Convolutional Neural Networks (CNNs). As a demanding technique in computer vision, ViTs have been successfully solved various vision problems while focusing on long-range relationships. In this paper, we begin by introducing the fundamental concepts and background of the self-attention mechanism. Next, we provide a comprehensive overview of recent top-performing ViT methods describing in terms of strength and weakness, computational cost as well as training and testing dataset. We thoroughly compare the performance of various ViT algorithms and most representative CNN methods on popular benchmark datasets. Finally, we explore some limitations with insightful observations and provide further research direction. The project page along with the collections of papers are available at https://github.com/khawar512/ViT-Survey
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;CTR&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#21407;&#29983;&#24191;&#21578;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/1804.09133</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#25552;&#39640;&#21407;&#29983;&#24191;&#21578;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Native Ads CTR Prediction by Large Scale Event Embedding and Recurrent Networks. (arXiv:1804.09133v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1804.09133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#21644;&#24490;&#29615;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;CTR&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#21407;&#29983;&#24191;&#21578;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#23545;&#20110;&#21407;&#29983;&#24191;&#21578;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#27809;&#26377;&#30452;&#25509;&#30340;&#26597;&#35810;&#24847;&#22270;&#65292;&#22240;&#27492;&#24456;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#20107;&#20214;&#23884;&#20837;&#26041;&#26696;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;&#36830;&#32493;&#20107;&#20214;&#36827;&#34892;&#24369;&#30417;&#30563;&#35757;&#32451;&#30340;&#23402;&#29983;&#32593;&#32476;&#26469;&#32534;&#30721;&#27599;&#20010;&#29992;&#25143;&#27983;&#35272;&#20107;&#20214;&#12290;CTR&#39044;&#27979;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#30417;&#30563;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#33258;&#28982;&#22320;&#23558;&#29992;&#25143;&#21382;&#21490;&#24314;&#27169;&#20026;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24490;&#29615;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20107;&#20214;&#23884;&#20837;&#21521;&#37327;&#21644;&#27880;&#24847;&#23618;&#23545;&#29992;&#25143;&#21382;&#21490;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#19968;&#20123;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click through rate (CTR) prediction is very important for Native advertisement but also hard as there is no direct query intent. In this paper we propose a large-scale event embedding scheme to encode the each user browsing event by training a Siamese network with weak supervision on the users' consecutive events. The CTR prediction problem is modeled as a supervised recurrent neural network, which naturally model the user history as a sequence of events. Our proposed recurrent models utilizing pretrained event embedding vectors and an attention layer to model the user history. Our experiments demonstrate that our model significantly outperforms the baseline and some variants.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#39044;&#26399;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/1312.7832</link><description>&lt;p&gt;
&#23450;&#20041;&#32463;&#20856;&#36923;&#36753;&#30340;&#34164;&#28085;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Defining implication relation for classical logic. (arXiv:1312.7832v10 [math.LO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1312.7832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#39044;&#26399;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#36923;&#36753;&#20013;&#65292;&#8220;P&#34164;&#28085;Q&#8221;&#31561;&#20215;&#20110;&#8220;&#38750;P&#25110;Q&#8221;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#23384;&#22312;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#8220;P&#34164;&#28085;Q&#8221;&#21487;&#20197;&#25512;&#20986;&#8220;&#38750;P&#25110;Q&#8221;&#65288;&#8220;&#34164;&#28085;&#21040;&#26512;&#21462;&#8221;&#26159;&#27491;&#30830;&#30340;&#65289;&#65292;&#32780;&#20174;&#8220;&#38750;P&#25110;Q&#8221;&#36890;&#24120;&#19981;&#33021;&#25512;&#20986;&#8220;P&#34164;&#28085;Q&#8221;&#65288;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#36890;&#24120;&#26159;&#19981;&#25104;&#31435;&#30340;&#65289;&#65292;&#25152;&#20197;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#36890;&#24120;&#26159;&#26080;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#32463;&#20856;&#36923;&#36753;&#65288;CL&#65289;&#20013;&#21435;&#38500;&#19981;&#27491;&#30830;&#30340;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25152;&#26399;&#26395;&#23646;&#24615;&#30340;&#36923;&#36753;&#31995;&#32479;&#65288;IRL&#65289;&#65306;(1) &#36890;&#36807;&#23558;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#28155;&#21152;&#21040;IRL&#20013;&#21487;&#20197;&#31616;&#21333;&#22320;&#24471;&#21040;CL&#65307;(2) &#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#19982;IRL&#26080;&#20851;&#65288;&#26080;&#27861;&#22312;IRL&#20013;&#25512;&#23548;&#20986;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#25110;&#20854;&#21542;&#23450;&#65289;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;IRL&#23601;&#26159;&#36890;&#36807;&#20174;CL&#20013;&#20934;&#30830;&#22320;&#21435;&#38500;&#8220;&#26512;&#21462;&#21040;&#34164;&#28085;&#8221;&#32780;&#24471;&#21040;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classical logic, "P implies Q" is equivalent to "not-P or Q". It is well known that the equivalence is problematic. Actually, from "P implies Q", "not-P or Q" can be inferred ("Implication-to-disjunction" is valid), while from "not-P or Q", "P implies Q" cannot be inferred in general ("Disjunction-to-implication" is not generally valid), so the equivalence between them is invalid in general. This work aims to remove exactly the incorrect Disjunction-to-implication from classical logic (CL). The paper proposes a logical system (IRL) with the expected properties: (1) CL is simply obtained by adding Disjunction-to-implication to IRL, and (2) Disjunction-to-implication is independent of IRL (either Disjunction-to-implication or its negation cannot be derived in IRL) in the general case. In other words, IRL is just the system obtained by exactly removing Disjunction-to-implication from CL.
&lt;/p&gt;</description></item></channel></rss>