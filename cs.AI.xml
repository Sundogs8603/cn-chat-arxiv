<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12902</link><description>&lt;p&gt;
&#23454;&#39564;&#21465;&#20107;&#65306;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling. (arXiv:2310.12902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#34892;&#20026;&#21644;&#35745;&#31639;&#23454;&#39564;&#65292;&#21033;&#29992;&#34394;&#26500;&#30340;&#25552;&#31034;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;&#30740;&#31350;&#20154;&#31867;&#21644;&#29983;&#25104;&#24335;AI&#21465;&#20107;&#20013;&#30340;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;2019&#24180;6&#26376;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#20316;&#30340;250&#20010;&#25925;&#20107;&#21644;2023&#24180;3&#26376;&#30001;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;80&#20010;&#25925;&#20107;&#65292;&#23558;&#21465;&#20107;&#23398;&#21644;&#25512;&#29702;&#32479;&#35745;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37117;&#22238;&#31572;&#20102;&#20851;&#20110;&#19982;&#20154;&#24037;&#26234;&#33021;&#20154;&#31867;&#30456;&#24651;&#30340;&#20027;&#39064;&#30340;&#30456;&#21516;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#23454;&#39564;&#33539;&#24335;&#20351;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#21465;&#20107;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#25552;&#21040;&#26222;&#32599;&#31859;&#20462;&#26031;&#20027;&#39064;&#30340;&#22238;&#24212;&#35777;&#23454;&#20102;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#20307;&#24819;&#35937;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25152;&#26377;&#25552;&#20379;&#30340;&#21465;&#20107;&#37117;&#34920;&#29616;&#20986;&#31185;&#23398;&#25110;&#25216;&#26415;&#30340;&#36861;&#27714;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-3.5&#21644;&#23588;&#20854;&#26159;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more more progre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12900</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Personalized human mobility prediction for HuMob challenge. (arXiv:2310.12900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21019;&#24314;&#25552;&#20132;&#32473;HuMob Challenge&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#25968;&#25454;&#20998;&#26512;&#31454;&#36187;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#22522;&#20110;&#20010;&#20307;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#20854;&#36816;&#21160;&#36712;&#36857;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#25972;&#20307;&#36816;&#21160;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#20154;&#31867;&#36816;&#21160;&#23545;&#20110;&#27599;&#20010;&#20154;&#32780;&#35328;&#26159;&#29420;&#29305;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#65292;&#22914;&#26085;&#26399;&#21644;&#26102;&#38388;&#65292;&#27963;&#21160;&#26102;&#38388;&#65292;&#21608;&#20960;&#65292;&#19968;&#22825;&#20013;&#30340;&#26102;&#38388;&#21644;POI&#65288;&#20852;&#36259;&#28857;&#65289;&#35775;&#38382;&#39057;&#29575;&#31561;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#26469;&#34701;&#21512;&#20855;&#26377;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#20854;&#20182;&#20010;&#20307;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#36827;&#34892;&#20934;&#30830;&#24615;&#26816;&#39564;&#65292;&#24182;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#12290;&#23613;&#31649;&#24635;&#20307;&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#21517;&#29992;&#25143;&#30340;&#36712;&#36857;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#20102;2&#19975;&#21517;&#30446;&#26631;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#24182;&#19981;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;8&#19975;&#21517;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#33021;&#28023;&#28207;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;&#20256;&#32479;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#26159;&#26377;&#38480;&#30340;&#65292;&#32780;&#34588;&#32592;&#21017;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#34892;&#20026;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#34394;&#25311;&#34588;&#32592;&#30340;&#36924;&#30495;&#24615;&#65292;&#21560;&#24341;&#26356;&#22810;&#30340;&#25915;&#20987;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.12880</link><description>&lt;p&gt;
TwinPot: &#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#30340;&#26234;&#33021;&#28023;&#28207;
&lt;/p&gt;
&lt;p&gt;
TwinPot: Digital Twin-assisted Honeypot for Cyber-Secure Smart Seaports. (arXiv:2310.12880v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#34588;&#32592;&#65292;&#29992;&#20110;&#20445;&#25252;&#26234;&#33021;&#28023;&#28207;&#30340;&#32593;&#32476;&#23433;&#20840;&#12290;&#20256;&#32479;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#26159;&#26377;&#38480;&#30340;&#65292;&#32780;&#34588;&#32592;&#21017;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#34892;&#20026;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#21487;&#20197;&#22686;&#24378;&#34394;&#25311;&#34588;&#32592;&#30340;&#36924;&#30495;&#24615;&#65292;&#21560;&#24341;&#26356;&#22810;&#30340;&#25915;&#20987;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21313;&#24180;&#65292;&#38543;&#30528;&#25928;&#29575;&#38656;&#27714;&#30340;&#19978;&#21319;&#21644;&#36135;&#29289;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#19979;&#19968;&#20195;&#28207;&#21475;&#30340;&#27010;&#24565;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#22312;&#36825;&#20010;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#21644;&#35774;&#26045;&#30340;&#26032;&#26102;&#20195;&#65292;&#24456;&#26126;&#26174;&#32593;&#32476;&#23433;&#20840;&#24050;&#32463;&#25104;&#20026;&#28023;&#28207;&#21644;&#28023;&#20107;&#24403;&#23616;&#36817;&#26399;&#26368;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#26159;&#22823;&#22810;&#25968;&#28207;&#21475;&#35758;&#31243;&#19978;&#30340;&#39318;&#35201;&#20851;&#27880;&#28857;&#12290;&#20256;&#32479;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20445;&#25252;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#32593;&#32476;&#31995;&#32479;&#20813;&#21463;&#26377;&#23475;&#23454;&#20307;&#30340;&#20405;&#23475;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26356;&#36879;&#26126;&#22320;&#36816;&#34892;&#65292;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#21482;&#33021;&#35266;&#23519;&#12289;&#26816;&#26597;&#21644;&#20102;&#35299;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34588;&#32592;&#26159;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#26377;&#20851;&#25915;&#20987;&#32773;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#34588;&#32592;&#21487;&#20197;&#26159;&#34394;&#25311;&#30340;&#25110;&#29289;&#29702;&#30340;&#12290;&#34394;&#25311;&#34588;&#32592;&#24517;&#39035;&#26356;&#21152;&#36924;&#30495;&#20197;&#21560;&#24341;&#25915;&#20987;&#32773;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#22909;&#30340;&#39640;&#24230;&#36924;&#30495;&#24615;&#12290;&#20026;&#27492;&#65292;&#21487;&#20197;&#37319;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;(DT)&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of next-generation ports has become more apparent in the last ten years in response to the challenge posed by the rising demand for efficiency and the ever-increasing volume of goods. In this new era of intelligent infrastructure and facilities, it is evident that cyber-security has recently received the most significant attention from the seaport and maritime authorities, and it is a primary concern on the agenda of most ports. Traditional security solutions can be applied to safeguard IoT and Cyber-Physical Systems (CPS) from harmful entities. Nevertheless, security researchers can only watch, examine, and learn about the behaviors of attackers if these solutions operate more transparently. Herein, honeypots are potential solutions since they offer valuable information about the attackers. It can be virtual or physical. Virtual honeypots must be more realistic to entice attackers, necessitating better high-fidelity. To this end, Digital Twin (DT) technology can be employed t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;Hierarchical Vision Transformers&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#26469;&#39044;&#27979;&#21365;&#24034;&#30284;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#20026;&#21365;&#24034;&#30284;&#24739;&#32773;&#36873;&#25321;&#27835;&#30103;&#26041;&#26696;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#33258;&#21160;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12866</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#35270;&#35273;Transformer&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#39044;&#27979;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#21365;&#24034;&#30284;&#30340;&#27835;&#30103;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning. (arXiv:2310.12866v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;Hierarchical Vision Transformers&#21644;&#22810;&#23454;&#20363;&#23398;&#20064;&#26469;&#39044;&#27979;&#21365;&#24034;&#30284;&#30340;&#27835;&#30103;&#21453;&#24212;&#65292;&#20026;&#21365;&#24034;&#30284;&#24739;&#32773;&#36873;&#25321;&#27835;&#30103;&#26041;&#26696;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#33258;&#21160;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#24739;&#32773;&#26469;&#35828;&#65292;&#30446;&#21069;&#30340;&#21365;&#24034;&#30284;&#27835;&#30103;&#25928;&#26524;&#26377;&#38480;&#12290;&#23545;&#20110;&#26576;&#20123;&#27835;&#30103;&#26041;&#27861;&#65292;&#26080;&#27861;&#39044;&#27979;&#24739;&#32773;&#30340;&#21453;&#24212;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#20182;&#20204;&#26292;&#38706;&#20110;&#27835;&#30103;&#30340;&#19981;&#33391;&#25928;&#26524;&#32780;&#27809;&#26377;&#20219;&#20309;&#27835;&#30103;&#25928;&#26524;&#12290;&#20316;&#20026;&#21365;&#24034;&#30284;&#20013;&#20351;&#29992;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#33258;&#21160;&#39044;&#27979;&#27835;&#30103;&#25928;&#26524;&#65288;ATEC23&#65289;&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#39044;&#27979;&#21253;&#25324;&#25239;&#34880;&#31649;&#29983;&#25104;&#33647;&#29289;&#36125;&#20240;&#21333;&#25239;&#22312;&#20869;&#30340;&#27835;&#30103;&#26041;&#26696;&#33021;&#21542;&#22312;&#33267;&#23569;6&#20010;&#26376;&#20869;&#20419;&#20351;&#32531;&#35299;&#25110;&#39044;&#38450;&#30142;&#30149;&#36827;&#23637;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Hierarchical Image Pyramid Transformer&#65288;HIPT&#65289;&#25552;&#21462;&#21306;&#22495;&#32423;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;ABMIL&#65289;&#27169;&#22411;&#26469;&#32858;&#21512;&#29305;&#24449;&#21644;&#20998;&#31867;&#25972;&#20010;&#24187;&#28783;&#29255;&#12290;&#26368;&#20248;&#30340;HIPT-ABMIL&#27169;&#22411;&#22312;&#20869;&#37096;&#24179;&#34913;&#20934;&#30830;&#24230;&#26041;&#38754;&#20026;60.2%+-2.9%&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
For many patients, current ovarian cancer treatments offer limited clinical benefit. For some therapies, it is not possible to predict patients' responses, potentially exposing them to the adverse effects of treatment without any therapeutic benefit. As part of the automated prediction of treatment effectiveness in ovarian cancer using histopathological images (ATEC23) challenge, we evaluated the effectiveness of deep learning to predict whether a course of treatment including the antiangiogenic drug bevacizumab could contribute to remission or prevent disease progression for at least 6 months in a set of 282 histopathology whole slide images (WSIs) from 78 ovarian cancer patients. Our approach used a pretrained Hierarchical Image Pyramid Transformer (HIPT) to extract region-level features and an attention-based multiple instance learning (ABMIL) model to aggregate features and classify whole slides. The optimal HIPT-ABMIL model had an internal balanced accuracy of 60.2% +- 2.9% and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;Radau&#25968;&#20540;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#30452;&#25509;&#27714;&#35299;&#39640;&#38454;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#39046;&#22495;&#20998;&#35299;&#31574;&#30053;&#25552;&#39640;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12846</link><description>&lt;p&gt;
&#22522;&#20110;Radau&#26041;&#27861;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#39640;&#38454;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Physical Information Neural Networks for Solving High-index Differential-algebraic Equation Systems Based on Radau Methods. (arXiv:2310.12846v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;Radau&#25968;&#20540;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#30452;&#25509;&#27714;&#35299;&#39640;&#38454;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#39046;&#22495;&#20998;&#35299;&#31574;&#30053;&#25552;&#39640;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;(DAEs)&#33021;&#22815;&#25551;&#36848;&#21160;&#24577;&#21464;&#21270;&#21644;&#22522;&#30784;&#32422;&#26463;&#65292;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#27969;&#20307;&#21147;&#23398;&#12289;&#22810;&#20307;&#21160;&#21147;&#23398;&#12289;&#26426;&#26800;&#31995;&#32479;&#21644;&#25511;&#21046;&#29702;&#35770;&#31561;&#24037;&#31243;&#39046;&#22495;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#23454;&#38469;&#29289;&#29702;&#24314;&#27169;&#20013;&#65292;&#31995;&#32479;&#36890;&#24120;&#20250;&#20135;&#29983;&#39640;&#38454;DAEs&#12290;&#20256;&#32479;&#30340;&#38544;&#24335;&#25968;&#20540;&#26041;&#27861;&#22312;&#27714;&#35299;&#39640;&#38454;&#31995;&#32479;&#26102;&#36890;&#24120;&#20250;&#23548;&#33268;&#25968;&#20540;&#31934;&#24230;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINN)&#22312;&#35299;&#20915;DAE&#31995;&#32479;&#26041;&#38754;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#22914;&#19981;&#33021;&#30452;&#25509;&#27714;&#35299;&#39640;&#38454;&#31995;&#32479;&#12289;&#39044;&#27979;&#31934;&#24230;&#36739;&#20302;&#21644;&#27867;&#21270;&#33021;&#21147;&#36739;&#24369;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;Radau IIA&#25968;&#20540;&#26041;&#27861;&#19982;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;PINN&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#30452;&#25509;&#27714;&#35299;&#39640;&#38454;DAEs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#39046;&#22495;&#20998;&#35299;&#31574;&#30053;&#20197;&#22686;&#24378;&#20854;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As is well known, differential algebraic equations (DAEs), which are able to describe dynamic changes and underlying constraints, have been widely applied in engineering fields such as fluid dynamics, multi-body dynamics, mechanical systems and control theory. In practical physical modeling within these domains, the systems often generate high-index DAEs. Classical implicit numerical methods typically result in varying order reduction of numerical accuracy when solving high-index systems.~Recently, the physics-informed neural network (PINN) has gained attention for solving DAE systems. However, it faces challenges like the inability to directly solve high-index systems, lower predictive accuracy, and weaker generalization capabilities. In this paper, we propose a PINN computational framework, combined Radau IIA numerical method with a neural network structure via the attention mechanisms, to directly solve high-index DAEs. Furthermore, we employ a domain decomposition strategy to enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12819</link><description>&lt;p&gt;
&#24102;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#28151;&#21512;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#32570;&#20047;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#23384;&#22312;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#26469;&#25191;&#34892;&#22810;&#23618;&#27425;&#65288;&#28151;&#21512;&#65289;&#25628;&#32034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#25972;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#12290;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#21644;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#30340;&#26368;&#20339;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#23436;&#25972;&#23376;&#30446;&#26631;&#25628;&#32034;&#19981;&#20165;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12818</link><description>&lt;p&gt;
&#25552;&#21319;&#25512;&#29702;&#25928;&#29575;&#65306;&#37322;&#25918;&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27169;&#22411;&#23384;&#20648;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#25968;&#20849;&#20139;&#19981;&#33021;&#20943;&#36731;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#20005;&#26684;&#26102;&#24310;&#35201;&#27714;&#25110;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#21442;&#25968;&#20849;&#20139;&#30340;PLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22823;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#33258;&#22238;&#24402;&#21644;&#33258;&#32534;&#30721;PLMs&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.12817</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;&#28857;&#20113;&#20998;&#21106;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#38169;Transformer&#27169;&#22411;&#65288;MIT&#65289;&#65292;&#29992;&#20110;&#32771;&#34385;2D&#21644;3D&#25968;&#25454;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#22312;&#28857;&#20113;&#20998;&#21106;&#20013;&#20114;&#34917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;2D&#27880;&#37322;&#26469;&#23454;&#29616;2D-3D&#20449;&#24687;&#34701;&#21512;&#12290;&#37492;&#20110;&#28857;&#20113;&#30340;&#39640;&#27880;&#37322;&#25104;&#26412;&#65292;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#38656;&#27714;&#38750;&#24120;&#36843;&#20999;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#22330;&#26223;&#32423;&#31867;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#35745;&#31639;3D&#28857;&#20113;&#21644;2D&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#12290;&#35299;&#30721;&#22120;&#23454;&#29616;&#20132;&#38169;&#30340;2D-3D&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#24182;&#36827;&#34892;&#38544;&#24335;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#23618;&#20013;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#26159;&#20114;&#34917;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12808</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#21512;&#24182;&#65292;&#20294;&#20026;&#20160;&#20040;&#20250;&#36215;&#20316;&#29992;&#65292;&#20160;&#20040;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#24179;&#22343;&#30340;&#19981;&#20934;&#30830;&#24615;&#19982;&#26799;&#24230;&#19981;&#21305;&#37197;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#32852;&#31995;&#36824;&#25581;&#31034;&#20102;&#20854;&#20182;&#26041;&#26696;&#65288;&#22914;&#24179;&#22343;&#20540;&#12289;&#20219;&#21153;&#31639;&#26415;&#21644;Fisher&#21152;&#26435;&#24179;&#22343;&#65289;&#20013;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#38754;&#37117;&#22312;&#24615;&#33021;&#21644;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.12802</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#20307;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32806;&#21512;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#38598;&#20307;&#23398;&#20064;&#30340;&#20986;&#29616;&#26159;&#23545;&#29289;&#29702;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#31038;&#20250;&#23398;&#30340;&#24191;&#27867;&#24433;&#21709;&#30340;&#19968;&#39033;&#21162;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#20010;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#21160;&#24577;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#20043;&#38388;&#30340;&#31454;&#20105;&#65292;&#23558;&#20960;&#20010;&#26368;&#36817;&#30340;&#20998;&#25955;&#31639;&#27861;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19982;&#20855;&#26377;&#28140;&#28781;&#38543;&#26426;&#24615;&#30340;Ginzburg-Landau&#27169;&#22411;&#31867;&#20284;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#26377;&#25928;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#31895;&#31890;&#21270;&#34892;&#20026;&#12290;&#36825;&#20010;&#26694;&#26550;&#39044;&#27979;&#20102;&#21442;&#25968;&#35299;&#30340;&#65288;&#28145;&#24230;&#20381;&#36182;&#30340;&#65289;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#38598;&#20307;&#23398;&#20064;&#30456;&#30340;&#24320;&#22987;&#65292;&#20197;&#21450;&#28145;&#24230;&#24341;&#36215;&#30340;&#20020;&#30028;&#28857;&#24310;&#36831;&#21644;&#24494;&#35266;&#23398;&#20064;&#36335;&#24452;&#30340;&#40065;&#26834;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#22270;&#32467;&#26500;&#24182;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#29305;&#24449;&#12289;&#20844;&#24179;&#24615;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12800</link><description>&lt;p&gt;
&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Graph Neural Networks for Indian Legal Judgment Prediction. (arXiv:2310.12800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#22270;&#32467;&#26500;&#24182;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#29305;&#24449;&#12289;&#20844;&#24179;&#24615;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#30340;&#27861;&#23448;&#19982;&#26696;&#20214;&#27604;&#20363;&#23545;&#21496;&#27861;&#31995;&#32479;&#20135;&#29983;&#20102;&#32321;&#37325;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20026;&#22823;&#37327;&#31215;&#21387;&#30340;&#26410;&#20915;&#26696;&#20214;&#20197;&#21450;&#25345;&#32493;&#28044;&#20837;&#30340;&#26032;&#26696;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#21152;&#24555;&#21496;&#27861;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#20107;&#23454;&#35777;&#25454;&#21644;&#36807;&#21435;&#26696;&#20214;&#30340;&#20808;&#20363;&#26469;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#24314;&#35758;&#21464;&#24471;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20391;&#37325;&#20110;&#24320;&#21457;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#38382;&#39064;&#65292;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#20869;&#22312;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23884;&#20837;&#20316;&#20026;&#27169;&#22411;&#29305;&#24449;&#65292;&#21516;&#26102;&#28155;&#21152;&#21644;&#20462;&#21098;&#20102;&#26102;&#38388;&#33410;&#28857;&#21644;&#21496;&#27861;&#34892;&#20026;&#33410;&#28857;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#32771;&#34385;&#20102;&#36825;&#20123;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#30340;&#20262;&#29702;&#32500;&#24230;&#65292;&#32771;&#34385;&#20102;&#24615;&#21035;&#21644;&#22995;&#21517;&#30340;&#20559;&#35265;&#12290;&#36824;&#36827;&#34892;&#20102;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#26469;&#26696;&#20214;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12774</link><description>&lt;p&gt;
&#23384;&#27963;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#65306;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26377;&#25928;&#33539;&#20363;&#65292;&#20351;&#24471;&#23569;&#26679;&#26412;&#29978;&#33267;&#38646;&#26679;&#26412;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#26368;&#36817;&#65292;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;&#22240;&#20854;&#26799;&#24230;-free&#20248;&#21270;&#30340;&#29420;&#29305;&#29305;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#34987;&#35777;&#26126;&#22312;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;&#20351;&#29992;&#20013;&#29305;&#21035;&#26377;&#29992;&#21644;&#24378;&#22823;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#26412;&#36136;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#22312;&#25628;&#32034;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#26041;&#38754;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#21482;&#26377;&#23569;&#37327;&#30340;&#20196;&#29260;&#23545;LLM&#39044;&#27979;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Clustering and Pruning for Efficient Black-box Prompt Search&#65288;ClaPS&#65289;&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#32858;&#31867;&#21644;&#20462;&#21098;&#65292;&#21482;&#20851;&#27880;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusivel
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;RLHF&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#24179;&#34913;&#26469;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12773</link><description>&lt;p&gt;
&#23433;&#20840;RLHF&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12773
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;RLHF&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#24179;&#34913;&#26469;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#24179;&#34913;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;RLHF&#65306;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#26032;&#39062;&#31639;&#27861;&#12290;&#23433;&#20840;RLHF&#26126;&#30830;&#35299;&#32806;&#20102;&#20851;&#20110;&#26377;&#30410;&#24615;&#21644;&#26080;&#23475;&#24615;&#30340;&#20154;&#31867;&#20559;&#22909;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#24352;&#21147;&#30340;&#22256;&#24785;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;LLM&#30340;&#23433;&#20840;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#65292;&#21363;&#22312;&#28385;&#36275;&#25351;&#23450;&#25104;&#26412;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;Lagrangian&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#32422;&#26463;&#38382;&#39064;&#65292;&#23433;&#20840;RLHF&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#19977;&#36718;&#20351;&#29992;&#23433;&#20840;RLHF&#36827;&#34892;&#31934;&#35843;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#20855;&#26377;&#20248;&#33391;&#24615;&#33021;&#30340;AI&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12768</link><description>&lt;p&gt;
SemantIC: &#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#22312;6G&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#24178;&#25200;&#25216;&#26415;&#65292;&#21363;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;SemantIC&#21482;&#38656;&#35201;&#25509;&#25910;&#22120;&#23558;&#20449;&#36947;&#35299;&#30721;&#22120;&#19982;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#26500;&#24314;&#20102;&#19968;&#20010;&#36845;&#20195;&#24490;&#29615;&#65292;&#20132;&#26367;&#28040;&#38500;&#20449;&#21495;&#22495;&#21644;&#35821;&#20041;&#22495;&#20013;&#30340;&#22122;&#22768;&#12290;&#20174;&#32593;&#32476;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#35757;&#32451;&#23384;&#20648;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#22312;&#36845;&#20195;&#35299;&#30721;&#20013;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#65292;&#20316;&#20026;Wyner-Ziv&#23450;&#29702;&#30340;&#19968;&#31181;&#23454;&#29616;&#12290;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;SemantIC&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12690</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#19978;&#30340;&#32452;&#21512;&#24335;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Cosmos&#65292;&#19968;&#20010;&#38024;&#23545;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#35774;&#35745;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22312;&#36890;&#36807;&#24050;&#30693;&#30340;&#35270;&#35273;&#8220;&#21407;&#23376;&#8221;&#32452;&#21512;&#33719;&#24471;&#30340;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;Cosmos&#30340;&#26680;&#24515;&#27934;&#23519;&#21147;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#24037;&#20855;&#65306;&#65288;i&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#22330;&#26223;&#32534;&#30721;&#65292;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#22120;&#35745;&#31639;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#30340;&#23454;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25551;&#36848;&#23454;&#20307;&#23646;&#24615;&#30340;&#21487;&#32452;&#21512;&#31526;&#21495;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#36825;&#20123;&#23454;&#20307;&#19982;&#23398;&#20064;&#21040;&#30340;&#20132;&#20114;&#35268;&#21017;&#32465;&#23450;&#36215;&#26469;&#12290;Cosmos&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65307;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#23558;&#34920;&#31034;&#26144;&#23556;&#20026;&#31526;&#21495;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#30340;&#31526;&#21495;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;blocks&#22330;&#26223;&#36827;&#34892;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;CG&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Cosmos&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12688</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21387;&#32553;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#25110;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#37096;&#32626;&#27169;&#22411;&#26102;&#65292;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#23545;&#27169;&#22411;&#30340;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#34429;&#28982;&#22312;&#35757;&#32451;&#20043;&#21069;&#21487;&#20197;&#35774;&#32622;&#31209;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26082;&#19981;&#28789;&#27963;&#20063;&#19981;&#26368;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#30697;&#38453;&#36873;&#25321;&#19981;&#21516;&#30340;&#31209;&#12290;&#32467;&#21512;&#35757;&#32451;&#36866;&#24212;&#24615;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#25110;&#32773;&#26377;&#24456;&#23569;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#22312;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#33267;&#26368;&#22810;14&#20493;&#65292;&#19988;&#30456;&#23545;&#24615;&#33021;&#38477;&#20302;&#26368;&#22810;&#20026;1.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PSYCHIC&#65292;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#27169;&#22411;&#65292;&#22312;&#22269;&#38469;&#35821;&#20041;&#32593;&#20250;&#35758;&#30340;&#23398;&#26415;&#38382;&#31572;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.12638</link><description>&lt;p&gt;
PSYCHIC: &#19968;&#20010;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#22522;&#30784;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PSYCHIC: A Neuro-Symbolic Framework for Knowledge Graph Question-Answering Grounding. (arXiv:2310.12638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;PSYCHIC&#65292;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#27169;&#22411;&#65292;&#22312;&#22269;&#38469;&#35821;&#20041;&#32593;&#20250;&#35758;&#30340;&#23398;&#26415;&#38382;&#31572;&#25361;&#25112;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#38382;&#31572;&#22312;&#38142;&#25509;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#65288;Scholarly QALD&#65289;&#26159;&#22269;&#38469;&#35821;&#20041;&#32593;&#20250;&#35758;&#65288;ISWC&#65289;2023&#25361;&#25112;&#20013;&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;PSYCHIC&#30340;&#31070;&#32463;&#31526;&#21495;&#65288;NS&#65289;&#26694;&#26550;&#65292;&#26469;&#22238;&#31572;&#19982;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30456;&#20851;&#30340;&#38382;&#31572;&#65288;KGQA over DBLP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#36798;&#21040;&#20102;00.18%&#30340;F1&#24471;&#20998;&#65292;&#24182;&#22312;&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26041;&#38754;&#20197;71.00%&#30340;&#24471;&#20998;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Scholarly Question Answering over Linked Data (Scholarly QALD) at The International Semantic Web Conference (ISWC) 2023 challenge presents two sub-tasks to tackle question answering (QA) over knowledge graphs (KGs). We answer the KGQA over DBLP (DBLP-QUAD) task by proposing a neuro-symbolic (NS) framework based on PSYCHIC, an extractive QA model capable of identifying the query and entities related to a KG question. Our system achieved a F1 score of 00.18% on question answering and came in third place for entity linking (EL) with a score of 71.00%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12632</link><description>&lt;p&gt;
&#38754;&#21521;&#28938;&#25509;&#36807;&#31243;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#36136;&#37327;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#23383;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36136;&#37327;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#30340;&#21046;&#36896;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#26159;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#65288;GMAW&#65289;&#12290;&#28938;&#25509;&#36807;&#31243;&#20197;&#26448;&#26009;&#24615;&#36136;&#12289;&#24037;&#33402;&#26465;&#20214;&#21644;&#28938;&#25509;&#36136;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20026;&#29305;&#24449;&#12290;&#22312;&#39057;&#32321;&#26356;&#25913;&#24037;&#33402;&#21442;&#25968;&#30340;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#30772;&#22351;&#24615;&#27979;&#35797;&#20934;&#30830;&#30830;&#23450;&#28938;&#32541;&#36136;&#37327;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#20174;&#24037;&#33402;&#35266;&#23519;&#20013;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#12290;&#26680;&#24515;&#27010;&#24565;&#21253;&#25324;&#30001;&#22235;&#20010;&#20027;&#35201;&#38454;&#27573;&#32452;&#25104;&#30340;&#31649;&#32447;&#65306;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#30005;&#27969;&#21644;&#30005;&#21387;&#65289;&#30340;&#25910;&#38598;&#21644;&#31649;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;ECG&#22270;&#20687;&#30340;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#12290;&#24515;&#33039;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21307;&#30103;&#26465;&#20214;&#65292;&#26089;&#26399;&#20934;&#30830;&#26816;&#27979;&#23545;&#20020;&#24202;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#20808;&#36827;&#25216;&#26415;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#23545;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#36827;&#34892;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12630</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;ECG&#22270;&#20687;&#30340;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heart Disease Detection using Vision-Based Transformer Models from ECG Images. (arXiv:2310.12630v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;ECG&#22270;&#20687;&#30340;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#12290;&#24515;&#33039;&#30142;&#30149;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21307;&#30103;&#26465;&#20214;&#65292;&#26089;&#26399;&#20934;&#30830;&#26816;&#27979;&#23545;&#20020;&#24202;&#23454;&#36341;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#20808;&#36827;&#25216;&#26415;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#23545;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#36827;&#34892;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30142;&#30149;&#65292;&#20063;&#31216;&#20026;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#32780;&#20005;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#65292;&#20854;&#29305;&#24449;&#26159;&#24515;&#33039;&#21644;&#34880;&#31649;&#30340;&#25439;&#20260;&#65292;&#23548;&#33268;&#21508;&#31181;&#24182;&#21457;&#30151;&#65292;&#22914;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65292;&#24515;&#21147;&#34928;&#31469;&#21644;&#24515;&#32908;&#26775;&#27515;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#24515;&#33039;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26089;&#26399;&#35782;&#21035;&#24739;&#30149;&#39118;&#38505;&#30340;&#20010;&#20307;&#33021;&#22815;&#37319;&#21462;&#31215;&#26497;&#30340;&#24178;&#39044;&#25514;&#26045;&#12289;&#39044;&#38450;&#25514;&#26045;&#21644;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#65292;&#20197;&#20943;&#36731;&#30142;&#30149;&#30340;&#36827;&#23637;&#24182;&#38477;&#20302;&#19981;&#33391;&#32467;&#26524;&#12290;&#36817;&#24180;&#26469;&#65292;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#39046;&#22495;&#22312;&#25216;&#26415;&#21644;&#35745;&#31639;&#26041;&#27861;&#30340;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#21644;&#39044;&#27979;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#37327;&#30340;&#20020;&#24202;&#21644;&#29983;&#29702;&#25968;&#25454;&#26469;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#39118;&#38505;&#20998;&#23618;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;ECG&#22270;&#20687;&#30340;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart disease, also known as cardiovascular disease, is a prevalent and critical medical condition characterized by the impairment of the heart and blood vessels, leading to various complications such as coronary artery disease, heart failure, and myocardial infarction. The timely and accurate detection of heart disease is of paramount importance in clinical practice. Early identification of individuals at risk enables proactive interventions, preventive measures, and personalized treatment strategies to mitigate the progression of the disease and reduce adverse outcomes. In recent years, the field of heart disease detection has witnessed notable advancements due to the integration of sophisticated technologies and computational approaches. These include machine learning algorithms, data mining techniques, and predictive modeling frameworks that leverage vast amounts of clinical and physiological data to improve diagnostic accuracy and risk stratification. In this work, we propose to d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#36328;&#20851;&#27880;&#30340;&#21464;&#25442;&#22120;&#65288;U-SpaTem&#65289;&#34701;&#21512;&#26102;&#31354;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#21382;&#21490;&#22320;&#22270;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12616</link><description>&lt;p&gt;
&#36328;&#20851;&#27880;&#26102;&#31354;&#19978;&#19979;&#25991;&#21464;&#25442;&#22120;&#29992;&#20110;&#21382;&#21490;&#22320;&#22270;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps. (arXiv:2310.12616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#30340;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#36328;&#20851;&#27880;&#30340;&#21464;&#25442;&#22120;&#65288;U-SpaTem&#65289;&#34701;&#21512;&#26102;&#31354;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#21382;&#21490;&#22320;&#22270;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22320;&#29699;&#35266;&#27979;&#25216;&#26415;&#35806;&#29983;&#20043;&#21069;&#65292;&#21382;&#21490;&#22320;&#22270;&#25552;&#20379;&#20102;&#26377;&#20851;&#22320;&#29699;&#34920;&#38754;&#30340;&#26377;&#29992;&#26102;&#31354;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#22320;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#36817;&#24180;&#26469;&#24191;&#21463;&#27426;&#36814;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#21462;&#20195;&#20102;&#25163;&#24037;&#21046;&#20316;&#30340;&#22320;&#22270;&#22788;&#29702;&#26041;&#27861;&#21644;&#32321;&#29712;&#30340;&#25163;&#24037;&#21171;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21407;&#22987;&#22320;&#22270;&#34920;&#38754;&#30340;&#32472;&#21046;/&#25195;&#25551;/&#35114;&#33394;&#32570;&#38519;&#20197;&#21450;&#23558;&#22320;&#22270;&#35009;&#21098;&#25104;&#23567;&#22359;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#30340;&#20869;&#23384;&#38480;&#21046;&#26102;&#25152;&#20135;&#29983;&#30340;&#19981;&#20805;&#20998;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25361;&#25112;&#27169;&#22411;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#30001;&#20110;&#21363;&#20351;&#25910;&#38598;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25968;&#25454;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20063;&#19981;&#33021;&#38477;&#20302;&#65292;&#22240;&#27492;&#25105;&#20204;&#35748;&#20026;&#20114;&#34917;&#30340;&#26102;&#31354;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;U-Net&#30340;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#36328;&#20851;&#27880;&#30340;&#21464;&#25442;&#22120;&#65288;U-SpaTem&#65289;&#34701;&#21512;&#26102;&#31354;&#29305;&#24449;&#65292;&#20197;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#33539;&#22260;&#21644;&#26102;&#38388;&#24207;&#21015;&#20013;&#27719;&#38598;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical maps provide useful spatio-temporal information on the Earth's surface before modern earth observation techniques came into being. To extract information from maps, neural networks, which gain wide popularity in recent years, have replaced hand-crafted map processing methods and tedious manual labor. However, aleatoric uncertainty, known as data-dependent uncertainty, inherent in the drawing/scanning/fading defects of the original map sheets and inadequate contexts when cropping maps into small tiles considering the memory limits of the training process, challenges the model to make correct predictions. As aleatoric uncertainty cannot be reduced even with more training data collected, we argue that complementary spatio-temporal contexts can be helpful. To achieve this, we propose a U-Net-based network that fuses spatio-temporal features with cross-attention transformers (U-SpaTem), aggregating information at a larger spatial range as well as through a temporal sequence of im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#26041;&#27861;&#35782;&#21035;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#25928;&#26524;&#24182;&#20943;&#23569;&#20102;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2310.12611</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#35843;&#25972;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model. (arXiv:2310.12611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19977;&#31181;&#26041;&#27861;&#35782;&#21035;&#33521;&#25991;&#35821;&#35328;&#27169;&#22411;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;Transformer&#32452;&#20214;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#24615;&#21035;&#20559;&#35265;&#32531;&#35299;&#25928;&#26524;&#24182;&#20943;&#23569;&#20102;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23637;&#29616;&#21644;&#25918;&#22823;&#20102;&#35768;&#22810;&#31181;&#19981;&#24076;&#26395;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32570;&#20047;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#25913;&#21464;&#36825;&#31181;&#34892;&#20026;&#32780;&#19981;&#25439;&#23475;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;LM&#32452;&#20214;&#19982;&#29305;&#23450;&#36755;&#20986;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65306;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#12289;&#33258;&#21160;&#30005;&#36335;&#21457;&#29616;&#21644;&#25105;&#20204;&#30340;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;DiffMask+&#65292;&#22522;&#20110;&#24046;&#24322;&#25513;&#27169;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;GPT-2 small&#21644;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21457;&#29616;&#30340;&#32452;&#20214;&#38598;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#20559;&#35265;&#32531;&#35299;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#35201;&#27714;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#20294;&#35782;&#21035;&#20986;&#30340;&#32452;&#20214;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#37325;&#21472;&#65292;&#24182;&#19988;&#25104;&#21151;&#20943;&#36731;&#20102;&#24615;&#21035;&#20559;&#35265;&#65292;&#30456;&#27604;&#20110;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#23545;&#19968;&#33324;&#35821;&#35328;&#24314;&#27169;&#30340;&#25439;&#23475;&#36739;&#23567;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20063;&#24378;&#35843;&#20102;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;</title><link>http://arxiv.org/abs/2310.12585</link><description>&lt;p&gt;
&#26102;&#24577;&#25935;&#24863;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#24577;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#38382;&#39064;&#22238;&#31572;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#28982;&#32780;&#35821;&#35328;&#27169;&#22411;&#24456;&#38590;&#29702;&#35299;&#26102;&#38388;&#38480;&#23450;&#35789;&#22914;&#8220;&#20043;&#21518;&#8221;&#21644;&#8220;&#20043;&#21069;&#8221;&#19982;&#25968;&#23383;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#26102;&#38388;&#34920;&#36798;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#38382;&#39064;&#21644;&#22235;&#20010;&#21477;&#23376;&#20505;&#36873;&#39033;&#65292;&#26681;&#25454;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#20998;&#31867;&#20026;&#27491;&#30830;&#25110;&#38169;&#35823;&#12290;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20174;&#22312;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#19978;&#37117;&#27491;&#30830;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#31572;&#26696;&#21306;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;TCQA&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;TimeQA&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as 'after' and 'before', and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.12580</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65288;&#22914;&#23398;&#26415;&#32593;&#32476;&#12289;&#31038;&#20132;&#24179;&#21488;&#65289;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#19981;&#20165;&#19982;&#25991;&#26412;&#30456;&#20851;&#65292;&#36824;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#25277;&#35937;&#20026;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#65288;Text-Attributed Heterogeneous Graphs&#65292;TAHGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12557</link><description>&lt;p&gt;
DepWiGNN&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial Reasoning in Text. (arXiv:2310.12557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12557
&lt;/p&gt;
&lt;p&gt;
DepWiGNN&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#33021;&#22815;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DepWiGNN&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#25512;&#29702;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#31354;&#38388;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#20174;&#32431;&#25991;&#26412;&#20013;&#25512;&#26029;&#31354;&#38388;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#33258;&#28982;&#35821;&#35328;&#19982;&#31526;&#21495;&#32467;&#26500;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24341;&#23548;&#21644;&#32858;&#21512;&#31526;&#21495;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNN&#22312;&#22788;&#29702;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#26102;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#21363;&#38543;&#30528;&#22270;&#23618;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Depth-Wise Graph Neural Network&#65288;DepWiGNN&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#28857;&#35760;&#24518;&#26041;&#26696;&#65292;&#24182;&#22312;&#22270;&#30340;&#28145;&#24230;&#32500;&#24230;&#19978;&#32858;&#21512;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#22312;&#24191;&#24230;&#32500;&#24230;&#19978;&#65292;&#36825;&#26679;&#21487;&#20197;&#25910;&#38598;&#38271;&#26102;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#22534;&#21472;&#22810;&#20010;&#23618;&#27425;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#65292;DepWiGNN&#21487;&#20197;&#20197;&#27604;&#20256;&#32479;GNN&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#22810;&#36339;&#31354;&#38388;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial reasoning in text plays a crucial role in various real-world applications. Existing approaches for spatial reasoning typically infer spatial relations from pure text, which overlook the gap between natural language and symbolic structures. Graph neural networks (GNNs) have showcased exceptional proficiency in inducing and aggregating symbolic structures. However, classical GNNs face challenges in handling multi-hop spatial reasoning due to the over-smoothing issue, \textit{i.e.}, the performance decreases substantially as the number of graph layers increases. To cope with these challenges, we propose a novel \textbf{Dep}th-\textbf{Wi}se \textbf{G}raph \textbf{N}eural \textbf{N}etwork (\textbf{DepWiGNN}). Specifically, we design a novel node memory scheme and aggregate the information over the depth dimension instead of the breadth dimension of the graph, which empowers the ability to collect long dependencies without stacking multiple layers. Experimental results on two challen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;</title><link>http://arxiv.org/abs/2310.12541</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22810;&#30446;&#26631;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Multi-objective Evolutionary Optimization. (arXiv:2310.12541v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25104;&#21151;&#23558;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#26041;&#24335;&#20316;&#20026;MOEA/D&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#65292;&#24182;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#26159;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65288;MOPs&#65289;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;MOEAs&#65292;&#20854;&#25805;&#20316;&#31526;&#38656;&#35201;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#23581;&#35797;&#23558;MOEAs&#20013;&#25163;&#21160;&#35774;&#35745;&#30340;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#25805;&#20316;&#31526;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#25805;&#20316;&#31526;&#21487;&#33021;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35299;&#20915;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35774;&#35745;MOEA&#25805;&#20316;&#31526;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35753;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#20316;&#20026;&#20998;&#35299;&#22411;MOEA&#65288;MOEA/D&#65289;&#30340;&#40657;&#30418;&#25628;&#32034;&#25805;&#20316;&#31526;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;LLM&#34892;&#20026;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24615;&#30340;&#30333;&#30418;&#25805;&#20316;&#31526;&#65292;&#24182;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Multiobjective evolutionary algorithms (MOEAs) are major methods for solving multiobjective optimization problems (MOPs). Many MOEAs have been proposed in the past decades, of which the operators need carefully handcrafted design with domain knowledge. Recently, some attempts have been made to replace the manually designed operators in MOEAs with learning-based operators (e.g., neural network models). However, much effort is still required for designing and training such models, and the learned operators might not generalize well to solve new problems. To tackle the above challenges, this work investigates a novel approach that leverages the powerful large language model (LLM) to design MOEA operators. With proper prompt engineering, we successfully let a general LLM serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a zero-shot manner. In addition, by learning from the LLM behavior, we further design an explicit white-box operator with randomness and propose
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#21644;&#23454;&#39564;&#35774;&#32622;&#19968;&#33268;&#24615;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.12527</link><description>&lt;p&gt;
&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#21644;&#23454;&#39564;&#35774;&#32622;&#19968;&#33268;&#24615;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#12290;&#31185;&#23398;&#23478;&#22312;&#36827;&#34892;&#22522;&#30784;&#30740;&#31350;&#25110;&#20248;&#21270;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#36890;&#24120;&#20250;&#26681;&#25454;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24230;&#21644;&#29305;&#24322;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#35780;&#20272;&#21644;&#25490;&#21517;&#20998;&#31867;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#25253;&#21578;&#30340;&#24615;&#33021;&#24471;&#20998;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#30740;&#31350;&#25490;&#21517;&#20381;&#25454;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#26410;&#20844;&#24320;&#25110;&#38750;&#24120;&#35268;&#30340;&#20132;&#21449;&#39564;&#35777;&#23454;&#36341;&#12289;&#25490;&#29256;&#38169;&#35823;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#22312;&#32473;&#23450;&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#29305;&#23450;&#25968;&#37327;&#30340;&#38451;&#24615;&#21644;&#38452;&#24615;&#27979;&#35797;&#39033;&#65292;&#22823;&#22810;&#25968;&#24615;&#33021;&#24471;&#20998;&#21487;&#20197;&#20551;&#35774;&#30340;&#29305;&#23450;&#12289;&#30456;&#20114;&#30456;&#20851;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#20540;&#25216;&#26415;&#26469;&#35780;&#20272;&#25253;&#21578;&#30340;&#24615;&#33021;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#21644;&#20551;&#35774;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#20540;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12481</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#22269;&#23478;&#37117;&#24198;&#31069;&#24863;&#24681;&#33410;&#65306;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models. (arXiv:2310.12481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#21457;&#29616;&#30001;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65292;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;&#27169;&#22411;&#24448;&#24448;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#23384;&#22312;&#30340;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#28304;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#24403;&#29992;&#25143;&#20351;&#29992;&#38750;&#33521;&#35821;&#35821;&#35328;&#25552;&#38382;&#26102;&#65292;LLMs&#24448;&#24448;&#20250;&#25552;&#20379;&#19982;&#39044;&#26399;&#25991;&#21270;&#19981;&#30456;&#20851;&#30340;&#19981;&#24688;&#24403;&#30340;&#33521;&#35821;&#25991;&#21270;&#30456;&#20851;&#31572;&#26696;&#12290;&#20026;&#20102;&#31995;&#32479;&#35780;&#20272;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20855;&#20307;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20551;&#26085;&#21644;&#27468;&#26354;&#65289;&#21644;&#25277;&#35937;&#25991;&#21270;&#23545;&#35937;&#65288;&#22914;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#34920;&#24615;&#30340;GPT&#27169;&#22411;&#23384;&#22312;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;GPT-4&#21463;&#21040;&#26368;&#20005;&#37325;&#24433;&#21709;&#65292;&#32780;text-davinci-003&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21463;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#23545;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#36827;&#34892;&#25209;&#21028;&#24615;&#23457;&#35270;&#21644;&#20262;&#29702;&#32771;&#34385;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#65306;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#37096;&#32626;&#20013;&#30340;&#25991;&#21270;&#24863;&#30693;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#25991;&#21270;&#20027;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g. ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark that consists of both concrete (e.g. holidays and songs) and abstract (e.g. values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need for critical examination of cultural dominance and ethical consideration in their development and deployment. We show two straightforward methods in model development (i.e. pretraining on more diverse data) and deployment (e.g. culture-aware prompting) can signifi
&lt;/p&gt;</description></item><item><title>GRAPE-S&#26159;&#19968;&#20010;&#22522;&#20110;hedonic&#28216;&#25103;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#65292;&#33021;&#22312;&#38656;&#35201;&#20998;&#32452;&#25104;&#30334;&#19978;&#21315;&#21488;&#26426;&#22120;&#20154;&#30340;&#22823;&#35268;&#27169;&#38598;&#32676;&#20013;&#20197;&#36817;&#23454;&#26102;&#30340;&#26041;&#24335;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.12480</link><description>&lt;p&gt;
GRAPE-S: &#22810;&#20010;&#26381;&#21153;&#38598;&#32676;&#30340;&#36817;&#23454;&#26102;&#32852;&#30431;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
GRAPE-S: Near Real-Time Coalition Formation for Multiple Service Collectives. (arXiv:2310.12480v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12480
&lt;/p&gt;
&lt;p&gt;
GRAPE-S&#26159;&#19968;&#20010;&#22522;&#20110;hedonic&#28216;&#25103;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#65292;&#33021;&#22312;&#38656;&#35201;&#20998;&#32452;&#25104;&#30334;&#19978;&#21315;&#21488;&#26426;&#22120;&#20154;&#30340;&#22823;&#35268;&#27169;&#38598;&#32676;&#20013;&#20197;&#36817;&#23454;&#26102;&#30340;&#26041;&#24335;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#21644;&#28798;&#38590;&#21709;&#24212;&#24212;&#29992;&#30340;&#26426;&#22120;&#20154;&#38598;&#32676;&#38656;&#35201;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#23558;&#26426;&#22120;&#20154;&#20998;&#32452;&#20026;&#36866;&#24403;&#30340;&#20219;&#21153;&#22242;&#38431;&#12290;&#38598;&#32676;&#30340;&#20219;&#21153;&#36890;&#24120;&#28085;&#30422;&#38656;&#35201;&#22810;&#20010;&#39640;&#32423;&#26426;&#22120;&#20154;&#34892;&#20026;&#25110;&#26381;&#21153;&#30340;&#20219;&#21153;&#65292;&#32852;&#30431;&#24418;&#25104;&#24517;&#39035;&#20104;&#20197;&#32771;&#34385;&#12290;&#39640;&#24230;&#21160;&#24577;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#24212;&#29992;&#39046;&#22495;&#36824;&#35201;&#27714;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#33021;&#22815;&#22312;&#20960;&#30334;&#21488;&#26426;&#22120;&#20154;&#30340;&#22823;&#35268;&#27169;&#38598;&#32676;&#20013;&#20197;&#36817;&#20046;&#26368;&#20248;&#35299;&#65288;&#21363;&gt; 95&#65285;&#25928;&#29992;&#65289;&#30340;&#26041;&#24335;&#20197;&#36817;&#23454;&#26102;&#65288;&#21363;&lt;5&#20998;&#38047;&#65289;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;&#27809;&#26377;&#20197;&#21069;&#30340;&#32852;&#30431;&#24418;&#25104;&#31639;&#27861;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#21021;&#22987;&#35780;&#20272;&#21457;&#29616;&#20256;&#32479;&#30340;&#22522;&#20110;&#25293;&#21334;&#30340;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#22826;&#38271;&#65292;&#23613;&#31649;&#38598;&#20013;&#24335;&#27169;&#25311;&#22120;&#20013;&#37319;&#29992;&#30340;&#26159;&#19981;&#22826;&#21487;&#33021;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21457;&#29983;&#30340;&#29702;&#24819;&#26465;&#20214;&#65288;&#21363;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#21516;&#27493;&#21644;&#23436;&#32654;&#30340;&#21363;&#26102;&#36890;&#20449;&#65289;&#12290; hedonic&#28216;&#25103;&#31639;&#27861;GRAPE&#21487;&#20197;&#22312;&#36817;&#23454;&#26102;&#20013;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic collectives for military and disaster response applications require coalition formation algorithms to partition robots into appropriate task teams. Collectives' missions will often incorporate tasks that require multiple high-level robot behaviors or services, which coalition formation must accommodate. The highly dynamic and unstructured application domains also necessitate that coalition formation algorithms produce near optimal solutions (i.e., &gt;95% utility) in near real-time (i.e., &lt;5 minutes) with very large collectives (i.e., hundreds of robots). No previous coalition formation algorithm satisfies these requirements. An initial evaluation found that traditional auction-based algorithms' runtimes are too long, even though the centralized simulator incorporated ideal conditions unlikely to occur in real-world deployments (i.e., synchronization across robots and perfect, instantaneous communication). The hedonic game-based GRAPE algorithm can produce solutions in near real-t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12477</link><description>&lt;p&gt;
&#23545;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of In-Context Learning for Speech Language Model. (arXiv:2310.12477v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;&#35821;&#38899;LM&#21487;&#20197;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#20102;&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#26469;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;LM&#35805;&#35821;-&#26631;&#31614;&#31034;&#33539;&#65292;LM&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#26799;&#24230;&#19979;&#38477;&#25110;&#35201;&#27714;&#26174;&#24335;&#20462;&#25913;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36825;&#20351;&#24471;LM&#33021;&#20197;&#40657;&#30418;&#30340;&#26041;&#24335;&#23398;&#20064;&#21644;&#35843;&#25972;&#12290;&#23613;&#31649;ICL&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#65292;&#24456;&#23569;&#26377;&#20154;&#30740;&#31350;ICL&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;&#27809;&#26377;&#25991;&#26412;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#23545;&#35821;&#38899;LM&#30340;ICL&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#21069;&#30340;&#35821;&#38899;LM&#27809;&#26377;ICL&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#28909;&#36523;&#35757;&#32451;&#65292;&#35821;&#38899;LM&#22240;&#27492;&#21487;&#20197;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#25191;&#34892;ICL&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#23545;&#35821;&#38899;LM&#22312;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;ICL&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an important role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to learn and adapt in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study proposes the first exploration of ICL with a speech LM without text supervision. We first show that the current speech LM does not have the ICL capability. With the proposed warmup training, the speech LM can, therefore, perform ICL on unseen tasks. In this work, we verify the feasibility of ICL for speech LM on speech classification tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;745&#21517;&#21463;&#35775;&#32773;&#65292;&#30740;&#31350;&#20102;&#24773;&#24863;&#23545;&#35805;&#20195;&#29702;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;&#22312;&#20154;&#19982;&#20154;&#30340;&#20114;&#21160;&#12289;&#24773;&#24863;&#25903;&#25345;&#21644;&#21019;&#36896;&#24615;&#20219;&#21153;&#30340;&#22330;&#26223;&#20013;&#65292;&#24773;&#24863;&#33021;&#21147;&#24471;&#21040;&#20102;&#37325;&#35270;&#65292;&#24182;&#21463;&#21040;&#24773;&#32490;&#37325;&#26032;&#35780;&#20272;&#21644;&#20010;&#24615;&#29305;&#36136;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12459</link><description>&lt;p&gt;
&#24773;&#24863;&#23545;&#35805;&#20195;&#29702;: &#29702;&#35299;&#26399;&#26395;&#21644;&#20010;&#20154;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Affective Conversational Agents: Understanding Expectations and Personal Influences. (arXiv:2310.12459v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;745&#21517;&#21463;&#35775;&#32773;&#65292;&#30740;&#31350;&#20102;&#24773;&#24863;&#23545;&#35805;&#20195;&#29702;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;&#22312;&#20154;&#19982;&#20154;&#30340;&#20114;&#21160;&#12289;&#24773;&#24863;&#25903;&#25345;&#21644;&#21019;&#36896;&#24615;&#20219;&#21153;&#30340;&#22330;&#26223;&#20013;&#65292;&#24773;&#24863;&#33021;&#21147;&#24471;&#21040;&#20102;&#37325;&#35270;&#65292;&#24182;&#21463;&#21040;&#24773;&#32490;&#37325;&#26032;&#35780;&#20272;&#21644;&#20010;&#24615;&#29305;&#36136;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#23545;&#35805;&#20195;&#29702;&#30340;&#20852;&#36215;&#25193;&#23637;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#36825;&#20123;&#20195;&#29702;&#30340;&#26222;&#21450;&#65292;&#30740;&#31350;&#19981;&#21516;&#24773;&#24863;&#33021;&#21147;&#23545;&#20854;&#24615;&#33021;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;745&#21517;&#21463;&#35775;&#32773;&#65292;&#20197;&#20102;&#35299;&#21508;&#31181;&#24212;&#29992;&#20013;&#23545;&#24773;&#24863;&#25216;&#33021;&#30340;&#26399;&#26395;&#21644;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;AI&#20195;&#29702;&#22312;32&#20010;&#19981;&#21516;&#22330;&#26223;&#20013;&#24863;&#30693;&#12289;&#22238;&#24212;&#21644;&#27169;&#25311;&#24773;&#32490;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#19982;&#20154;&#30340;&#20114;&#21160;&#12289;&#24773;&#24863;&#25903;&#25345;&#21644;&#21019;&#36896;&#24615;&#20219;&#21153;&#30340;&#22330;&#26223;&#26356;&#21463;&#27426;&#36814;&#65292;&#21463;&#21040;&#24773;&#32490;&#37325;&#26032;&#35780;&#20272;&#21644;&#20010;&#24615;&#29305;&#36136;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;AI&#20195;&#29702;&#25152;&#38656;&#30340;&#24773;&#24863;&#25216;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24212;&#29992;&#30340;&#19978;&#19979;&#25991;&#21644;&#24615;&#36136;&#65292;&#24378;&#35843;&#20102;&#22312;&#24773;&#24863;AI&#23545;&#35805;&#20195;&#29702;&#30340;&#35774;&#35745;&#20013;&#38656;&#35201;&#36866;&#24212;&#24615;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of AI conversational agents has broadened opportunities to enhance human capabilities across various domains. As these agents become more prevalent, it is crucial to investigate the impact of different affective abilities on their performance and user experience. In this study, we surveyed 745 respondents to understand the expectations and preferences regarding affective skills in various applications. Specifically, we assessed preferences concerning AI agents that can perceive, respond to, and simulate emotions across 32 distinct scenarios. Our results indicate a preference for scenarios that involve human interaction, emotional support, and creative tasks, with influences from factors such as emotional reappraisal and personality traits. Overall, the desired affective skills in AI agents depend largely on the application's context and nature, emphasizing the need for adaptability and context-awareness in the design of affective AI conversational agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.12454</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Construction of Effective Metrics for Understanding the Mechanisms of Pretrained Language Models. (arXiv:2310.12454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26500;&#24314;&#29992;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#26377;&#25928;&#24230;&#37327;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#31995;&#21015;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#34987;&#26399;&#26395;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#36755;&#20837;&#25991;&#26412;&#26144;&#23556;&#21040;&#19968;&#32452;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26412;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#19968;&#20010;&#30333;&#30418;&#27169;&#22411;&#26469;&#35745;&#31639;&#21453;&#26144;&#36825;&#20123;&#21521;&#37327;&#20869;&#37096;&#20851;&#31995;&#23384;&#22312;&#30340;&#24230;&#37327;&#25104;&#20026;&#20998;&#26512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#21487;&#35299;&#37322;&#24615;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#28304;&#27169;&#22411;&#32570;&#20047;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#26102;&#65292;&#22312;&#30333;&#30418;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24182;&#20445;&#35777;&#24230;&#37327;&#35745;&#31639;&#30340;&#20005;&#35880;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#36825;&#31181;&#26435;&#34913;&#20013;&#23547;&#25214;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26426;&#21046;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#27839;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#26641;&#25299;&#25169;&#25506;&#38024;&#27169;&#22411;&#23545;BERT-large&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are expected to effectively map input text to a set of vectors while preserving the inherent relationships within the text. Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models. However, achieving interpretability in white-box models and ensuring the rigor of metric computation becomes challenging when the source model lacks inherent interpretability. Therefore, in this paper, we discuss striking a balance in this trade-off and propose a novel line to constructing metrics for understanding the mechanisms of pretrained language models. We have specifically designed a family of metrics along this line of investigation, and the model used to compute these metrics is referred to as the tree topological probe. We conducted measurements on BERT-large by using these metrics. Based on the e
&lt;/p&gt;</description></item><item><title>MTS-LOF&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#36974;&#25377;&#31574;&#30053;&#23454;&#29616;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.12451</link><description>&lt;p&gt;
MTS-LOF: &#21033;&#29992;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#36827;&#34892;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features. (arXiv:2310.12451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12451
&lt;/p&gt;
&lt;p&gt;
MTS-LOF&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#36974;&#25377;&#31574;&#30053;&#23454;&#29616;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#20026;&#30142;&#30149;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#24739;&#32773;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27934;&#23519;&#21147;&#12290;&#20808;&#36827;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#24102;&#26469;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20351;&#25968;&#25454;&#26631;&#27880;&#38754;&#20020;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#24191;&#27867;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;MTS-LOF&#12290;MTS-LOF&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;MTS-LOF&#36890;&#36807;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#21307;&#30103;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;MTS-LOF&#37319;&#29992;&#22810;&#36974;&#25377;&#31574;&#30053;&#65292;&#20419;&#36827;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical time series data are indispensable in healthcare, providing critical insights for disease diagnosis, treatment planning, and patient management. The exponential growth in data complexity, driven by advanced sensor technologies, has presented challenges related to data labeling. Self-supervised learning (SSL) has emerged as a transformative approach to address these challenges, eliminating the need for extensive human annotation. In this study, we introduce a novel framework for Medical Time Series Representation Learning, known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and Masked Autoencoder (MAE) methods, offering a unique approach to representation learning for medical time series data. By combining these techniques, MTS-LOF enhances the potential of healthcare applications by providing more sophisticated, context-rich representations. Additionally, MTS-LOF employs a multi-masking strategy to facilitate occlusion-invariant feature learning. This appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12443</link><description>&lt;p&gt;
&#20102;&#35299;&#20309;&#22788;&#21069;&#24448;&#65306;&#20351;LLM&#25104;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Know Where to Go: Make LLM a Relevant, Responsible, and Trustworthy Searcher. (arXiv:2310.12443v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#20998;&#21035;&#29992;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#32463;&#26174;&#31034;&#20986;&#23427;&#22312;&#25552;&#39640;&#25628;&#32034;&#30456;&#20851;&#24615;&#21644;&#25552;&#20379;&#30452;&#25509;&#31572;&#26696;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#32479;&#20449;&#24687;&#26816;&#32034;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;LLM&#30340;&#38169;&#35273;&#38382;&#39064;&#65292;&#39564;&#35777;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#21644;&#36129;&#29486;&#26469;&#28304;&#30340;&#21487;&#20449;&#24230;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#21019;&#24314;LLM&#26102;&#20195;&#30340;&#8220;PageRank&#8221;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#23558;LLM&#36716;&#21464;&#20026;&#19968;&#20010;&#30456;&#20851;&#12289;&#36127;&#36131;&#20219;&#19988;&#21487;&#20449;&#36182;&#30340;&#25628;&#32034;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29983;&#25104;&#26816;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#30693;&#35782;&#24314;&#31435;&#26597;&#35810;&#21644;&#22312;&#32447;&#26469;&#28304;&#20043;&#38388;&#30340;&#30452;&#25509;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#65306;&#29983;&#25104;&#22120;&#12289;&#39564;&#35777;&#22120;&#21644;&#20248;&#21270;&#22120;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#29983;&#25104;&#21487;&#20449;&#36182;&#30340;&#22312;&#32447;&#26469;&#28304;&#12289;&#39564;&#35777;&#26469;&#28304;&#30340;&#21487;&#38752;&#24615;&#21644;&#20248;&#21270;&#19981;&#21487;&#20449;&#36182;&#30340;&#26469;&#28304;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#30456;&#20851;&#24615;&#12289;&#36127;&#36131;&#20219;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30456;&#23545;&#20110;&#21508;&#31181;SOTA&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Language Models (LLMs) has shown the potential to improve relevance and provide direct answers in web searches. However, challenges arise in validating the reliability of generated results and the credibility of contributing sources, due to the limitations of traditional information retrieval algorithms and the LLM hallucination problem. Aiming to create a "PageRank" for the LLM era, we strive to transform LLM into a relevant, responsible, and trustworthy searcher. We propose a novel generative retrieval framework leveraging the knowledge of LLMs to foster a direct link between queries and online sources. This framework consists of three core modules: Generator, Validator, and Optimizer, each focusing on generating trustworthy online sources, verifying source reliability, and refining unreliable sources, respectively. Extensive experiments and evaluations highlight our method's superior relevance, responsibility, and trustfulness against various SOTA methods.
&lt;/p&gt;</description></item><item><title>PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12439</link><description>&lt;p&gt;
PoisonPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models. (arXiv:2310.12439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12439
&lt;/p&gt;
&lt;p&gt;
PoisonPrompt&#26159;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#32463;&#36807;&#20102;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23433;&#20840;&#23041;&#32961;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#31034;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#21508;&#31181;LLM&#24212;&#29992;&#22330;&#26223;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#32780;&#35328;&#65292;&#21518;&#38376;&#28431;&#27934;&#8212;&#8212;&#19968;&#31181;&#21487;&#20197;&#24694;&#24847;&#26356;&#25913;&#21463;&#23475;&#27169;&#22411;&#27491;&#24120;&#39044;&#27979;&#30340;&#20005;&#37325;&#23433;&#20840;&#23041;&#32961;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;POISONPROMPT&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#30772;&#22351;&#30828;&#20214;&#21644;&#36719;&#20214;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#31181;&#27969;&#34892;&#30340;&#25552;&#31034;&#26041;&#27861;&#12289;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#26469;&#35780;&#20272;POISONPROMPT&#30340;&#26377;&#25928;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21453;&#39304;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;</title><link>http://arxiv.org/abs/2310.12426</link><description>&lt;p&gt;
MAF: &#22810;&#26041;&#38754;&#21453;&#39304;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models. (arXiv:2310.12426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21453;&#39304;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#35832;&#22914;&#24187;&#35273;&#65292;&#29983;&#25104;&#38169;&#35823;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#21644;&#25968;&#23398;&#38169;&#35823;&#31561;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#21453;&#39304;&#33258;&#25105;&#25913;&#36827;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#36890;&#29992;&#21453;&#39304;&#26469;&#28304;&#65292;&#26080;&#27861;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#20013;&#30340;&#22810;&#26679;&#38169;&#35823;&#31867;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22810;&#20010;&#21453;&#39304;&#27169;&#22359;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#22806;&#37096;&#24037;&#20855;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#20013;&#30340;&#20960;&#20010;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20010;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30475;&#21040;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30456;&#23545;&#25552;&#21319;&#20102;&#22810;&#36798;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.12425</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#33258;&#21160;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12425
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#35821;&#35328;&#30340;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#22256;&#38590;&#24615;&#65292;&#20984;&#26174;&#20102;&#23545;&#36866;&#29992;&#20110;&#27492;&#31867;&#35821;&#35328;&#30340;&#26377;&#25928;&#33258;&#21160;&#21270;&#20462;&#22797;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#65292;&#22914;&#22522;&#20110;&#27169;&#26495;&#30340;&#20462;&#22797;&#12289;&#21453;&#39304;&#39537;&#21160;&#30340;&#36845;&#20195;&#20462;&#22797;&#21644;&#26377;&#30028;&#31351;&#20030;&#26041;&#27861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#35268;&#33539;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;OpenAI&#30340;ChatGPT&#20462;&#22797;&#29992;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#32534;&#20889;&#30340;&#36719;&#20214;&#35268;&#33539;&#30340;&#25928;&#26524;&#12290;&#19982;&#21629;&#20196;&#24335;&#35821;&#35328;&#19981;&#21516;&#65292;Alloy&#20013;&#30340;&#35268;&#33539;&#19981;&#20250;&#34987;&#25191;&#34892;&#65292;&#32780;&#26159;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#21518;&#31471;&#32422;&#26463;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;&#35268;&#33539;&#23454;&#20363;&#21644;&#26029;&#35328;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;ChatGPT&#22312;&#25913;&#36827;&#22768;&#26126;&#24335;&#35268;&#33539;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#32479;&#19968;&#20998;&#26512;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#26377;&#36259;&#30340;&#23398;&#20064;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.12408</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#32479;&#19968;&#20998;&#26512;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#26377;&#36259;&#30340;&#23398;&#20064;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#20294;&#30446;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#19981;&#36275;&#20197;&#29702;&#35299;&#20854;&#25104;&#21151;&#65292;&#20363;&#22914;&#31070;&#32463;&#20999;&#32447;&#26680;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#20854;&#20851;&#38190;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26368;&#36817;&#23545;&#29305;&#24449;&#23398;&#20064;&#30340;&#20998;&#26512;&#36890;&#24120;&#26159;&#38382;&#39064;&#29305;&#23450;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#38024;&#23545;&#30001;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#21452;&#23618;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20197;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#21407;&#29702;&#20026;&#26680;&#24515;&#65292;&#24182;&#36890;&#36807;&#22312;&#20960;&#20010;&#20856;&#22411;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#39640;&#26031;&#28151;&#21512;&#21644;&#22855;&#20598;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#36824;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#32593;&#32476;&#23398;&#20064;&#29616;&#35937;&#65292;&#22914;&#36229;&#36234;&#26680;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#20449;&#24687;&#26469;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12407</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#23454;&#29616;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing. (arXiv:2310.12407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#20449;&#24687;&#26469;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#27979;&#37327;&#23545;&#26410;&#30693;&#25968;&#37327;&#30446;&#26631;&#30340;&#36319;&#36394;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#36317;&#31163;-&#22810;&#26222;&#21202;&#35889;&#20449;&#24687;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#27979;&#37327;&#31867;&#21035;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#22686;&#24378;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#32479;&#19968;&#30340;&#28040;&#24687;&#20256;&#36882;&#33719;&#24471;&#30340;&#20449;&#24565;&#34987;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#12290;&#28982;&#21518;&#21033;&#29992;&#36755;&#20986;&#30340;&#20449;&#24565;&#26469;&#20248;&#21270;&#21407;&#22987;&#20449;&#24565;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#25216;&#26415;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#28040;&#24687;&#20256;&#36882;&#27169;&#22359;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21644;Dempster-Shafer&#27169;&#22359;&#12290;&#28040;&#24687;&#20256;&#36882;&#27169;&#22359;&#29992;&#20110;&#36890;&#36807;&#22240;&#23376;&#22270;&#34920;&#31034;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#25512;&#26029;&#30446;&#26631;&#36816;&#21160;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of tracking an unknown number of targets in strong clutter environments using measurements from a radar sensor. Leveraging the range-Doppler spectra information, we identify the measurement classes, which serve as additional information to enhance clutter rejection and data association, thus bolstering the robustness of target tracking. We first introduce a novel neural enhanced message passing approach, where the beliefs obtained by the unified message passing are fed into the neural network as additional information. The output beliefs are then utilized to refine the original beliefs. Then, we propose a classification-aided robust multiple target tracking algorithm, employing the neural enhanced message passing technique. This algorithm is comprised of three modules: a message-passing module, a neural network module, and a Dempster-Shafer module. The message-passing module is used to represent the statistical model by the factor graph and infers target kinema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#29702;&#38382;&#39064;&#20013;&#20351;&#29992;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#30340;&#36845;&#20195;&#33258;&#25105;&#25209;&#35780;&#21644;&#22806;&#37096;&#27491;&#30830;&#25512;&#29702;&#22120;&#39564;&#35777;&#23545;&#26368;&#32456;&#32467;&#26524;&#26377;&#23454;&#38469;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12397</link><description>&lt;p&gt;
GPT-4&#24182;&#19981;&#30693;&#36947;&#23427;&#38169;&#20102;&#65306;&#23545;&#36845;&#20195;&#25552;&#31034;&#22312;&#25512;&#29702;&#38382;&#39064;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems. (arXiv:2310.12397v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#29702;&#38382;&#39064;&#20013;&#20351;&#29992;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#22270;&#30528;&#33394;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;GPT-4&#30340;&#36845;&#20195;&#33258;&#25105;&#25209;&#35780;&#21644;&#22806;&#37096;&#27491;&#30830;&#25512;&#29702;&#22120;&#39564;&#35777;&#23545;&#26368;&#32456;&#32467;&#26524;&#26377;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#23384;&#22312;&#35768;&#22810;&#24847;&#35265;&#20998;&#27495;&#12290;&#23613;&#31649;&#26368;&#21021;&#23545;&#20110;&#25512;&#29702;&#33021;&#22815;&#38543;&#30528;&#35268;&#27169;&#33258;&#21160;&#20135;&#29983;&#30340;&#20048;&#35266;&#20027;&#20041;&#34987;&#19968;&#31995;&#21015;&#21453;&#20363;&#25152;&#25233;&#21046;&#65292;&#20294;&#20154;&#20204;&#26222;&#36941;&#30456;&#20449;&#23427;&#20204;&#20855;&#26377;&#36845;&#20195;&#33258;&#25105;&#25209;&#35780;&#33021;&#21147;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22312;&#22270;&#30528;&#33394;&#30340;&#19978;&#19979;&#25991;&#20013;&#23545;LLM&#30340;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#20197;&#21450;&#23454;&#36341;&#38382;&#39064;&#65288;&#22914;&#35843;&#24230;&#21644;&#20998;&#37197;&#65289;&#30456;&#20851;&#30340;&#32463;&#20856;NP&#23436;&#20840;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;GPT4&#22312;&#35299;&#20915;&#22270;&#30528;&#33394;&#23454;&#20363;&#25110;&#39564;&#35777;&#20505;&#36873;&#30528;&#33394;&#30340;&#27491;&#30830;&#24615;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#26377;&#21407;&#21017;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#36845;&#20195;&#27169;&#24335;&#19979;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#27169;&#22411;&#25209;&#35780;&#33258;&#24049;&#30340;&#31572;&#26696;&#20197;&#21450;&#22806;&#37096;&#30340;&#27491;&#30830;&#25512;&#29702;&#22120;&#39564;&#35777;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25209;&#35780;&#30340;&#20869;&#23481;&#26159;&#21542;&#23454;&#38469;&#24433;&#21709;&#20102;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12387</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;NP&#38590;&#24615;&#36136;&#65292;&#29615;&#22659;&#30417;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#20248;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#21253;&#25324;&#31934;&#30830;&#12289;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20854;&#20013;&#21551;&#21457;&#24335;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#19987;&#23478;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#29983;&#25104;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#26469;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#22797;&#26434;&#38598;&#25104;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#32447;&#23398;&#20064;&#19982;&#35268;&#21010;&#12290;&#26032;&#30340;&#26694;&#26550;&#36824;&#20801;&#35768;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#25512;&#29702;&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2310.12386</link><description>&lt;p&gt;
&#22312;&#35748;&#30693;&#23618;&#27425;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Planning in Cognitive Hierarchies. (arXiv:2310.12386v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#22797;&#26434;&#38598;&#25104;&#25512;&#29702;&#34892;&#20026;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#32447;&#23398;&#20064;&#19982;&#35268;&#21010;&#12290;&#26032;&#30340;&#26694;&#26550;&#36824;&#20801;&#35768;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#25512;&#29702;&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#25972;&#21512;&#22810;&#31181;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#32452;&#20214;&#12290;&#22312;&#20445;&#35777;&#20840;&#23616;&#24615;&#36136;&#21644;&#34892;&#20026;&#30340;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#19981;&#21516;&#30340;&#32452;&#20214;&#25972;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#31995;&#32479;&#26159;&#35748;&#30693;&#26426;&#22120;&#20154;&#23398;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20351;&#29992;&#24418;&#24335;&#21270;&#26694;&#26550;&#26469;&#24314;&#27169;&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#20197;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#19968;&#20010;&#24050;&#26377;&#30340;&#24418;&#24335;&#21270;&#26694;&#26550;[Clark et al., 2016]&#65292;&#20197;&#27169;&#25311;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#22797;&#26434;&#38598;&#25104;&#25512;&#29702;&#34892;&#20026;&#65307;&#20174;&#31526;&#21495;&#35268;&#21010;&#21040;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#21644;&#36716;&#25442;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;&#26694;&#26550;&#20801;&#35768;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#19981;&#21516;&#25512;&#29702;&#32452;&#20214;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex robot behaviour typically requires the integration of multiple robotic and Artificial Intelligence (AI) techniques and components. Integrating such disparate components into a coherent system, while also ensuring global properties and behaviours, is a significant challenge for cognitive robotics. Using a formal framework to model the interactions between components can be an important step in dealing with this challenge. In this paper we extend an existing formal framework [Clark et al., 2016] to model complex integrated reasoning behaviours of robotic systems; from symbolic planning through to online learning of policies and transition systems. Furthermore the new framework allows for a more flexible modelling of the interactions between different reasoning components.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#24182;&#20851;&#32852;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#20851;&#31995;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12379</link><description>&lt;p&gt;
&#29992;&#20851;&#31995;&#23884;&#20837;&#38142;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Hard Analogy Questions with Relation Embedding Chains. (arXiv:2310.12379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#24182;&#20851;&#32852;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#65292;&#20197;&#33719;&#24471;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#21644;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#20851;&#31995;&#23884;&#20837;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35789;&#27719;&#35821;&#20041;&#23398;&#20013;&#65292;&#24314;&#27169;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;ConceptNet&#65292;&#24182;&#23558;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#24314;&#27169;&#20026;&#19968;&#32452;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;KGs&#20165;&#38480;&#20110;&#22266;&#23450;&#30340;&#20851;&#31995;&#31867;&#22411;&#65292;&#19981;&#23436;&#25972;&#24182;&#19988;&#36890;&#24120;&#22024;&#26434;&#12290;&#21478;&#19968;&#20010;&#31574;&#30053;&#26159;&#20174;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#28860;&#20851;&#31995;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21482;&#38388;&#25509;&#30456;&#20851;&#30340;&#35789;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#22826;&#36866;&#29992;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#23558;&#32467;&#26500;&#21270;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#36827;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;&#20851;&#31995;&#24314;&#27169;&#20026;&#36335;&#24452;&#65292;&#20294;&#23558;&#20854;&#36793;&#32536;&#19982;&#20851;&#31995;&#23884;&#20837;&#30456;&#20851;&#32852;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#35782;&#21035;&#21512;&#36866;&#30340;&#20013;&#38388;&#35789;&#35821;&#26469;&#33719;&#21462;&#36335;&#24452;&#65292;&#28982;&#21518;&#36873;&#25321;&#37027;&#20123;&#21487;&#20197;&#33719;&#24471;&#26377;&#20449;&#24687;&#37327;&#30340;&#20851;&#31995;&#23884;&#20837;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#26041;&#27861;&#23545;&#20110;&#35299;&#20915;&#22256;&#38590;&#30340;&#31867;&#27604;&#38382;&#39064;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.
&lt;/p&gt;</description></item><item><title>ClusT3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12345</link><description>&lt;p&gt;
ClusT3:&#20449;&#24687;&#19981;&#21464;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ClusT3: Information Invariant Test-Time Training. (arXiv:2310.12345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12345
&lt;/p&gt;
&lt;p&gt;
ClusT3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#38388;&#32463;&#24120;&#21463;&#21040;&#22495;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#65288;TTT&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35299;&#20915;&#20102;&#19968;&#20010;&#27425;&#35201;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#20316;&#20026;&#33258;&#30417;&#30563;&#30340;&#20195;&#29702;&#20219;&#21153;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;TTT&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20316;&#20026;&#36741;&#21161;&#32858;&#31867;&#20219;&#21153;&#38598;&#25104;&#21040;&#26631;&#20934;&#35757;&#32451;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#24120;&#35265;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2310.12342</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#19982;&#35268;&#21010;&#28040;&#38500;&#25512;&#29702;&#65306;&#19968;&#31181;&#24341;&#23548;LLMs&#38750;&#32447;&#24615;&#24605;&#32500;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking. (arXiv:2310.12342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#36890;&#36807;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#21487;&#20197;&#27169;&#25311;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thought Chain&#65288;CoT&#65289;&#25552;&#31034;&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#32447;&#24615;&#35748;&#30693;&#21644;&#36923;&#36753;&#65292;&#25506;&#32034;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35013;&#22791;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#24605;&#32500;&#22797;&#26434;&#19988;&#28151;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24605;&#32500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#31216;&#20026;&#25512;&#26029;&#24615;&#25490;&#38500;&#25552;&#31034;&#65288;IEP&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#25490;&#38500;&#21644;&#25512;&#29702;&#30340;&#21407;&#21017;&#65292;&#20197;&#24341;&#23548;LLM&#36827;&#34892;&#38750;&#32447;&#24615;&#24605;&#32771;&#12290;IEP&#25351;&#23548;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25512;&#26029;&#27599;&#20010;&#21487;&#33021;&#35299;&#19982;&#19978;&#19979;&#25991;&#12289;&#24120;&#35782;&#25110;&#20107;&#23454;&#30340;&#25512;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#36890;&#36807;&#22238;&#28335;&#25512;&#29702;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#12290;&#30456;&#27604;&#20854;&#20182;&#22522;&#20110;CoT&#30340;&#26041;&#27861;&#65292;IEP&#30340;&#21069;&#21521;&#35268;&#21010;&#21644;&#21518;&#21521;&#25490;&#38500;&#36807;&#31243;&#26356;&#22909;&#22320;&#27169;&#25311;&#20102;&#22797;&#26434;&#30340;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#21518;&#32773;&#20165;&#21453;&#26144;&#32447;&#24615;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#39564;&#35777;&#20102;IEP&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated tha
&lt;/p&gt;</description></item><item><title>&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.12324</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#25945;&#24072;&#21644;&#30740;&#31350;&#32773;&#30340;&#28608;&#21169;&#65292;&#25506;&#32034;&#36866;&#24212;&#24615;&#23454;&#39564;&#20419;&#36827;&#25345;&#32493;&#25913;&#36827;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12324
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#27604;&#36739;&#19981;&#21516;&#25945;&#23398;&#31574;&#30053;&#30340;&#26426;&#20250;&#21487;&#20197;&#20026;&#25945;&#24072;&#30340;&#20915;&#31574;&#25552;&#20379;&#26377;&#29992;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#23454;&#39564;&#32570;&#20047;&#28165;&#26224;&#31616;&#26126;&#30340;&#20351;&#29992;&#25968;&#25454;&#24555;&#36895;&#22686;&#21152;&#23454;&#39564;&#23398;&#29983;&#33719;&#24471;&#26368;&#20339;&#26465;&#20214;&#26426;&#20250;&#30340;&#36884;&#24452;&#12290;&#21463;&#39046;&#20808;&#31185;&#25216;&#20844;&#21496;&#22312;&#20135;&#21697;&#24320;&#21457;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23454;&#39564;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24212;&#24615;&#23454;&#39564;&#26469;&#25345;&#32493;&#25913;&#36827;&#35838;&#31243;&#12290;&#22312;&#36866;&#24212;&#24615;&#23454;&#39564;&#20013;&#65292;&#19981;&#21516;&#30340;&#26465;&#20214;&#23558;&#34987;&#24212;&#29992;&#20110;&#23398;&#29983;&#36523;&#19978;&#65292;&#25968;&#25454;&#23558;&#34987;&#20998;&#26512;&#24182;&#29992;&#20110;&#25913;&#21464;&#26410;&#26469;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21738;&#20123;&#34892;&#21160;&#21487;&#20197;&#26356;&#26377;&#24076;&#26395;&#25913;&#21892;&#23398;&#29983;&#30340;&#20307;&#39564;&#25110;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#21160;&#24577;&#22320;&#23558;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#23398;&#29983;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;189&#31687;&#35770;&#25991;&#65292;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#34920;&#26684;&#26469;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12318</link><description>&lt;p&gt;
The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis.
&lt;/p&gt;
&lt;p&gt;
The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis. (arXiv:2310.12318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;189&#31687;&#35770;&#25991;&#65292;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;&#20102;&#24773;&#24863;&#20998;&#26512;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#34920;&#26684;&#26469;&#35299;&#20915;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20844;&#24179;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25209;&#21028;&#24615;&#22320;&#30740;&#31350;189&#31687;&#21516;&#34892;&#35780;&#23457;&#30340;&#35770;&#25991;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#30340;&#24212;&#29992;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25506;&#31350;&#24773;&#24863;&#20998;&#26512;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26469;&#28304;&#20110;&#23545;&#24773;&#24863;&#20998;&#26512;&#22312;&#19981;&#21516;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#25104;&#20026;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#30340;&#35748;&#35782;&#65292;&#24182;&#23545;&#31038;&#20250;&#23398;&#21644;&#25216;&#26415;&#25991;&#29486;&#20013;&#30340;&#24773;&#24863;&#27010;&#24565;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#37329;&#34701;&#12289;&#25919;&#24220;&#21644;&#21307;&#30103;&#31561;&#39046;&#22495;&#23545;&#24773;&#24863;&#30340;&#19981;&#21516;&#27010;&#24565;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#22312;&#24773;&#24863;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#26041;&#38754;&#23384;&#22312;&#26126;&#30830;&#19981;&#36275;&#65292;&#21487;&#33021;&#23548;&#33268;&#25361;&#25112;&#21644;&#20559;&#35265;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28085;&#30422;&#20851;&#38190;&#38382;&#39064;&#30340;&#20262;&#29702;&#34920;&#26684;&#65292;&#20197;&#25351;&#23548;&#20174;&#19994;&#32773;&#30830;&#20445;&#24773;&#24863;&#20998;&#26512;&#30340;&#20844;&#24179;&#21033;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#37319;&#29992;&#36328;&#23398;&#31185;&#26041;&#27861;&#26469;&#23450;&#20041;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#26045;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. Our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. To tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of SA. Our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in SA and offer a pragmatic solution for its implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.12309</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework for Learning Argumentation Semantics. (arXiv:2310.12309v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#21040;&#22312;&#20154;&#19982;&#20154;&#25110;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#20013;&#25152;&#20351;&#29992;&#30340;&#35770;&#35777;&#30340;&#34920;&#31034;&#21644;&#35780;&#20272;&#12290;&#27491;&#24335;&#35770;&#35777;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#24615;&#35821;&#20041;&#23450;&#20041;&#20102;&#35770;&#35777;&#30340;&#25509;&#21463;&#25110;&#25298;&#32477;&#30340;&#26631;&#20934;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#31216;&#20026;&#35770;&#35777;&#27714;&#35299;&#22120;&#30340;&#36719;&#20214;&#31995;&#32479;&#65292;&#29992;&#20110;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#35745;&#31639;&#34987;&#25509;&#21463;/&#34987;&#25298;&#32477;&#30340;&#35770;&#35777;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35782;&#21035;&#25509;&#21463;&#30340;&#35770;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22810;&#20010;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;&#30340;&#21487;&#25509;&#21463;&#24615;&#35821;&#20041;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#20174;&#32780;&#24320;&#36767;&#20102;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#30340;&#26032;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.12304</link><description>&lt;p&gt;
&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Optimization for Molecular Language Models. (arXiv:2310.12304v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#29983;&#25104;&#26032;&#39062;&#21270;&#23398;&#32467;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;\emph{&#20808;&#39564;&#22320;}&#32534;&#30721;&#21270;&#23398;&#23478;&#21487;&#33021;&#26399;&#26395;&#30340;&#26576;&#20123;&#20559;&#22909;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular language modeling is an effective approach to generating novel chemical structures. However, these models do not \emph{a priori} encode certain preferences a chemist may desire. We investigate the use of fine-tuning using Direct Preference Optimization to better align generated molecules with chemist preferences. Our findings suggest that this approach is simple, efficient, and highly effective.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.12303</link><description>&lt;p&gt;
&#25991;&#26723;&#32423;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12303
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#30693;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#22312;&#21477;&#32423;&#21035;&#19978;&#36816;&#34892;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#65292;&#22823;&#22810;&#25968;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#21482;&#26377;&#21477;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#27809;&#26377;&#25991;&#26723;&#32423;&#21035;&#30340;&#20803;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#30340;&#21333;&#35821;&#25968;&#25454;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#20351;&#31995;&#32479;&#32452;&#21512;&#26356;&#28789;&#27963;&#12289;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26435;&#37325;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25193;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20063;&#26356;&#20248;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21453;&#21521;&#32763;&#35793;&#30340;&#32467;&#26524;&#26356;&#22909;&#65292;
&lt;/p&gt;
&lt;p&gt;
Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12298</link><description>&lt;p&gt;
Jorge: GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#30340;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19982;&#19968;&#38454;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#20108;&#38454;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#19968;&#30452;&#19981;&#22826;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#20013;&#30340;&#20027;&#35201;&#25928;&#29575;&#29942;&#39048;&#26159;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#30340;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#65292;&#22312;GPU&#19978;&#35745;&#31639;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Jorge&#65292;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#29305;&#24615;&#21644;&#19968;&#38454;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#28040;&#38500;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#29942;&#39048;&#65292;&#29992;&#36817;&#20284;&#30340;&#39044;&#22788;&#29702;&#22120;&#35745;&#31639;&#26367;&#20195;&#12290;&#36825;&#20351;&#24471;Jorge&#22312;&#22681;&#38047;&#26102;&#38388;&#19978;&#22312;GPU&#19978;&#38750;&#24120;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35843;&#25972;&#33391;&#22909;&#30340;SGD&#22522;&#20934;&#20013;&#30830;&#23450;Jorge&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35843;&#21442;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;Jorge&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#26234;&#33021;&#20307;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#12289;&#34892;&#20026;&#21644;&#24847;&#22270;&#36827;&#34892;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#23454;&#29616;&#26234;&#33021;&#20307;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2310.12290</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26234;&#33021;&#20307;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Fact-based Agent modeling for Multi-Agent Reinforcement Learning. (arXiv:2310.12290v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#26234;&#33021;&#20307;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#12289;&#34892;&#20026;&#21644;&#24847;&#22270;&#36827;&#34892;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#23454;&#29616;&#26234;&#33021;&#20307;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#29615;&#22659;&#20013;&#19982;&#20854;&#20182;&#26234;&#33021;&#20307;&#36827;&#34892;&#20114;&#21160;&#21644;&#21512;&#20316;&#12290;&#26234;&#33021;&#20307;&#24314;&#27169;&#23545;&#20110;&#20419;&#36827;&#26234;&#33021;&#20307;&#38388;&#30340;&#20114;&#21160;&#21644;&#23454;&#29616;&#33258;&#36866;&#24212;&#21512;&#20316;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#65292;&#26234;&#33021;&#20307;&#21516;&#26102;&#23398;&#20064;&#25152;&#26377;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#12289;&#34892;&#20026;&#21644;&#24847;&#22270;&#36827;&#34892;&#24314;&#27169;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#23454;&#29616;&#26234;&#33021;&#20307;&#24314;&#27169;&#65292;&#20551;&#35774;&#22312;&#25191;&#34892;&#25110;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#26412;&#22320;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#30693;&#30340;&#22330;&#26223;&#20013;&#65292;&#22914;&#31454;&#20105;&#22242;&#38431;&#12289;&#19981;&#21487;&#38752;&#30340;&#36890;&#20449;&#21644;&#20986;&#20110;&#38544;&#31169;&#32771;&#34385;&#30340;&#32852;&#37030;&#23398;&#20064;&#31561;&#26410;&#30693;&#20195;&#29702;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#20551;&#35774;&#24182;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#23454;&#29616;&#26234;&#33021;&#20307;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20107;&#23454;&#30340;&#26234;&#33021;&#20307;&#24314;&#27169;&#65288;FAM&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#20110;&#20107;&#23454;&#30340;&#20449;&#24565;&#25512;&#29702;&#65288;FBI&#65289;&#32593;&#32476;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent systems, agents need to interact and collaborate with other agents in environments. Agent modeling is crucial to facilitate agent interactions and make adaptive cooperation strategies. However, it is challenging for agents to model the beliefs, behaviors, and intentions of other agents in non-stationary environment where all agent policies are learned simultaneously. In addition, the existing methods realize agent modeling through behavior cloning which assume that the local information of other agents can be accessed during execution or training. However, this assumption is infeasible in unknown scenarios characterized by unknown agents, such as competition teams, unreliable communication and federated learning due to privacy concerns. To eliminate this assumption and achieve agent modeling in unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which fact-based belief inference (FBI) network models other agents in partially observable environment on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25552;&#39640;MOOC&#20013;&#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12281</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640; MOOC &#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning. (arXiv:2310.12281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25552;&#39640;MOOC&#20013;&#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#22686;&#38271;&#30340;&#22312;&#32447;&#23398;&#20064;&#29616;&#35937;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#25945;&#23460;&#19981;&#21516;&#65292;MOOCs&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#20197;&#28385;&#36275;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#21644;&#22320;&#29702;&#20301;&#32622;&#30340;&#21508;&#31181;&#21463;&#20247;&#12290;&#33879;&#21517;&#22823;&#23398;&#21644;&#19987;&#38376;&#25552;&#20379;MOOCs&#30340;&#20379;&#24212;&#21830;&#65292;&#22914;Coursera&#65292;&#22312;&#21508;&#31181;&#20027;&#39064;&#19978;&#25552;&#20379;MOOC&#35838;&#31243;&#12290;&#30001;&#20110;&#39640;&#20837;&#23398;&#29575;&#21644;&#25945;&#24072;&#19982;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#26377;&#38480;&#30452;&#25509;&#20114;&#21160;&#65292;&#33258;&#21160;&#35780;&#20272;&#20219;&#21153;&#22914;&#25104;&#32489;&#21644;&#26089;&#26399;&#36864;&#23398;&#39044;&#27979;&#21464;&#24471;&#24517;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#28041;&#21450;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30340;&#32467;&#26500;&#38142;&#25509;&#65292;&#20363;&#22914;&#23398;&#29983;&#21644;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#36890;&#36807;&#20132;&#20114;&#22270;&#34920;&#29616;&#30340;&#36825;&#20123;&#32467;&#26500;&#20851;&#31995;&#21253;&#21547;&#21487;&#20197;&#25552;&#39640;&#25152;&#38656;&#20219;&#21153;&#24615;&#33021;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20026;&#19968;&#20010;&#22823;&#35268;&#27169;MOOC&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65288;UDIL&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12244</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;: &#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65288;UDIL&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#36866;&#24212;&#19968;&#31995;&#21015;&#39046;&#22495;&#65292;&#20165;&#33021;&#35775;&#38382;&#20808;&#21069;&#39046;&#22495;&#30340;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#65288;&#21363;&#35760;&#24518;&#65289;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#20309;&#26102;&#36873;&#25321;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#32479;&#19968;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;UDIL&#65289;&#65292;&#29992;&#20110;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;UDIL&#23558;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;UDIL&#22987;&#32456;&#23454;&#29616;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#20851;&#38190;&#35266;&#28857;&#26159;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#23545;&#24212;&#20110;&#25105;&#20204;&#30340;&#36793;&#30028;&#20855;&#26377;&#19981;&#21516;&#30340;&#22266;&#23450;&#31995;&#25968;&#65307;&#22522;&#20110;&#36825;&#31181;&#32479;&#19968;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#30340;UDIL&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#20174;&#32780;&#22987;&#32456;&#23454;&#29616;&#26368;&#32039;&#30340;&#30028;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;UDIL&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26032;&#30340;&#29289;&#20307;&#38598;&#19978;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12238</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#22270;&#23545;&#40784;&#36827;&#34892;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot In-Context Imitation Learning via Implicit Graph Alignment. (arXiv:2310.12238v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26032;&#30340;&#29289;&#20307;&#38598;&#19978;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#22312;&#20960;&#20010;&#19981;&#21516;&#29289;&#20307;&#19978;&#36827;&#34892;&#30340;&#20219;&#21153;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;&#26426;&#22120;&#20154;&#22914;&#20309;&#23398;&#20250;&#22312;&#26032;&#30340;&#12289;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65311;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#20869;&#22810;&#26679;&#30340;&#29289;&#20307;&#20351;&#24471;&#25512;&#26029;&#26032;&#29289;&#20307;&#19982;&#28436;&#31034;&#20013;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#20219;&#21153;&#30456;&#20851;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26465;&#20214;&#20801;&#35768;&#32972;&#26223;&#19979;&#30340;&#23398;&#20064;&#65292;&#22312;&#28436;&#31034;&#20043;&#21518;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#31435;&#21363;&#22312;&#19968;&#32452;&#26032;&#29289;&#20307;&#19978;&#25191;&#34892;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#29289;&#20307;&#31867;&#21035;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#25110;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#39029;&#19978;&#35266;&#30475;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#25968;&#25968;&#25454;&#31867;&#22411;&#30340;&#28909;&#20999;&#28385;&#36275;&#27169;&#29702;&#35770;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;ADT&#26597;&#35810;&#31616;&#21270;&#20026;&#38750;&#35299;&#37322;&#20989;&#25968; (UF)&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#26377;&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#38899;&#24615;&#21644;&#23436;&#22791;&#24615;&#19978;&#37117;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12234</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#20195;&#25968;&#25968;&#25454;&#31867;&#22411;&#30340;&#28909;&#20999;&#28385;&#36275;&#27169;&#29702;&#35770;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes. (arXiv:2310.12234v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#25968;&#25968;&#25454;&#31867;&#22411;&#30340;&#28909;&#20999;&#28385;&#36275;&#27169;&#29702;&#35770;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23558;ADT&#26597;&#35810;&#31616;&#21270;&#20026;&#38750;&#35299;&#37322;&#20989;&#25968; (UF)&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#26377;&#27714;&#35299;&#22120;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#38899;&#24615;&#21644;&#23436;&#22791;&#24615;&#19978;&#37117;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#25968;&#25968;&#25454;&#31867;&#22411; (ADTs) &#26159;&#21151;&#33021;&#24615;&#32534;&#31243;&#35821;&#35328;&#20013;&#32463;&#20856;&#30340;&#26500;&#36896;&#65292;&#29992;&#20110;&#25429;&#25417;&#26522;&#20030;&#31867;&#22411;&#12289;&#21015;&#34920;&#21644;&#26641;&#31561;&#25968;&#25454;&#32467;&#26500;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;ADTs&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#12290;&#20363;&#22914;&#65292;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22914;Python&#65292;&#24050;&#32463;&#20026;ADTs&#28155;&#21152;&#20102;&#25903;&#25345;&#12290;&#21487;&#20197;&#20351;&#29992;&#28385;&#36275;&#27169;&#29702;&#35770; (SMT) &#27714;&#35299;&#26469;&#33258;&#21160;&#36827;&#34892;&#20851;&#20110;ADTs&#30340;&#25512;&#29702;&#65292;SMT&#26159;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#22312;&#19968;&#38454;&#32467;&#26500;&#19978;&#30340;&#32422;&#26463;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;ADTs&#30340;SMT&#27714;&#35299;&#22120;&#26080;&#27861;&#25193;&#23637;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#21516;&#26679;\emph{&#25042;&#24816;}&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;SMT&#27714;&#35299;&#22120;&#65292;&#21363;\emph{&#28909;&#20999;}&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#23558;ADT&#26597;&#35810;&#31616;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#36923;&#36753;&#29702;&#35770;&#65292;&#22914;&#38750;&#35299;&#37322;&#20989;&#25968; (UF)&#65292;&#28982;&#21518;&#22312;&#31616;&#21270;&#21518;&#30340;&#26597;&#35810;&#19978;&#20351;&#29992;&#29616;&#26377;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22768;&#38899;&#24615;&#21644;&#23436;&#22791;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algebraic data types (ADTs) are a construct classically found in functional programming languages that capture data structures like enumerated types, lists, and trees. In recent years, interest in ADTs has increased. For example, popular programming languages, like Python, have added support for ADTs. Automated reasoning about ADTs can be done using satisfiability modulo theories (SMT) solving, an extension of the Boolean satisfiability problem with constraints over first-order structures. Unfortunately, SMT solvers that support ADTs do not scale as state-of-the-art approaches all use variations of the same \emph{lazy} approach. In this paper, we present an SMT solver that takes a fundamentally different approach, an \emph{eager} approach. Specifically, our solver reduces ADT queries to a simpler logical theory, uninterpreted functions (UF), and then uses an existing solver on the reduced query. We prove the soundness and completeness of our approach and demonstrate that it outperforms
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#20114;&#21160;&#26159;&#25512;&#21160;Reddit&#19978;&#36793;&#32536;&#31038;&#21306;&#22686;&#38271;&#30340;&#26426;&#21046;&#20043;&#19968;&#65292;&#25910;&#21040;&#36793;&#32536;&#20114;&#21160;&#30340;&#29992;&#25143;&#27604;&#27809;&#26377;&#25910;&#21040;&#20114;&#21160;&#30340;&#29992;&#25143;&#26356;&#26377;&#21487;&#33021;&#21152;&#20837;&#36793;&#32536;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2310.12186</link><description>&lt;p&gt;
&#38476;&#29983;&#20154;&#21361;&#38505;&#65281;&#19982;&#36793;&#32536;&#29992;&#25143;&#30340;&#36328;&#31038;&#21306;&#20114;&#21160;&#22686;&#21152;&#20102;Reddit&#19978;&#36793;&#32536;&#31038;&#21306;&#30340;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stranger Danger! Cross-Community Interactions with Fringe Users Increase the Growth of Fringe Communities on Reddit. (arXiv:2310.12186v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12186
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20114;&#21160;&#26159;&#25512;&#21160;Reddit&#19978;&#36793;&#32536;&#31038;&#21306;&#22686;&#38271;&#30340;&#26426;&#21046;&#20043;&#19968;&#65292;&#25910;&#21040;&#36793;&#32536;&#20114;&#21160;&#30340;&#29992;&#25143;&#27604;&#27809;&#26377;&#25910;&#21040;&#20114;&#21160;&#30340;&#29992;&#25143;&#26356;&#26377;&#21487;&#33021;&#21152;&#20837;&#36793;&#32536;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#27969;&#24179;&#21488;&#19978;&#65292;&#25512;&#24191;&#38452;&#35851;&#35770;&#21644;&#26497;&#31471;&#24847;&#35782;&#24418;&#24577;&#30340;&#36793;&#32536;&#31038;&#21306;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22686;&#38271;&#26426;&#21046;&#30340;&#30097;&#38382;&#12290;&#25105;&#20204;&#20551;&#35774;&#24182;&#30740;&#31350;&#20102;&#21487;&#33021;&#30340;&#26426;&#21046;&#65306;&#26032;&#25104;&#21592;&#21487;&#33021;&#36890;&#36807;&#36793;&#32536;&#20114;&#21160;&#25307;&#21215;&#65306;&#25104;&#21592;&#19982;&#38750;&#25104;&#21592;&#20043;&#38388;&#30340;&#35780;&#35770;&#20132;&#27969;&#12290;&#25105;&#20204;&#24212;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#30740;&#31350;&#20102;&#36793;&#32536;&#20114;&#21160;&#23545;Reddit&#19978;&#19977;&#20010;&#30693;&#21517;&#36793;&#32536;&#31038;&#21306;&#65288;r/Incel&#65292;r/GenderCritical&#21644;r/The_Donald&#65289;&#22686;&#38271;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#32536;&#20114;&#21160;&#21560;&#24341;&#20102;&#26032;&#30340;&#31038;&#21306;&#25104;&#21592;&#12290;&#25910;&#21040;&#36825;&#20123;&#20114;&#21160;&#30340;&#29992;&#25143;&#27604;&#27809;&#26377;&#25910;&#21040;&#20114;&#21160;&#30340;&#30456;&#20284;&#21305;&#37197;&#29992;&#25143;&#26356;&#26377;&#21487;&#33021;&#21152;&#20837;&#36793;&#32536;&#31038;&#21306;&#65292;&#22686;&#21152;&#20102;4.2&#20010;&#30334;&#20998;&#28857;&#12290;&#36825;&#31181;&#24433;&#21709;&#21463;&#21040;&#20197;&#19979;&#22240;&#32032;&#30340;&#24433;&#21709;&#65306;1&#65289;&#20114;&#21160;&#21457;&#29983;&#30340;&#31038;&#21306;&#29305;&#24615;&#65288;&#20363;&#22914;&#65292;&#24038;&#20542;&#21644;&#21491;&#20542;&#31038;&#21306;&#65289;&#21644;2&#65289;&#20114;&#21160;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fringe communities promoting conspiracy theories and extremist ideologies have thrived on mainstream platforms, raising questions about the mechanisms driving their growth. Here, we hypothesize and study a possible mechanism: new members may be recruited through fringe-interactions: the exchange of comments between members and non-members of fringe communities. We apply text-based causal inference techniques to study the impact of fringe-interactions on the growth of three prominent fringe communities on Reddit: r/Incel, r/GenderCritical, and r/The_Donald. Our results indicate that fringe-interactions attract new members to fringe communities. Users who receive these interactions are up to 4.2 percentage points (pp) more likely to join fringe communities than similar, matched users who do not.  This effect is influenced by 1) the characteristics of communities where the interaction happens (e.g., left vs. right-leaning communities) and 2) the language used in the interactions. Interact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12184</link><description>&lt;p&gt;
GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#30340;&#26550;&#26500;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#23545;&#39640;&#25928;GNN&#35745;&#31639;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20026;&#20248;&#21270;GNN&#32858;&#21512;&#32780;&#35774;&#35745;&#30340;&#21508;&#31181;&#32534;&#31243;&#25277;&#35937;&#24212;&#36816;&#32780;&#29983;&#65292;&#20197;&#20419;&#36827;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#25277;&#35937;&#27809;&#26377;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22240;&#27492;&#23545;&#21738;&#31181;&#26041;&#27861;&#26356;&#22909;&#27809;&#26377;&#26126;&#30830;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#32452;&#32455;&#21644;&#20256;&#25773;&#26041;&#27861;&#30340;&#32500;&#24230;&#23545;&#29616;&#26377;&#30340;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#26500;&#24314;&#36825;&#20123;&#25277;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#21644;&#35814;&#32454;&#30340;&#29305;&#24449;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20860;&#39038;&#24211;&#23384;&#24377;&#24615;&#21644;&#25913;&#21892;&#24179;&#22343;&#24615;&#33021;&#65292;&#26469;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.12183</link><description>&lt;p&gt;
&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Optimistic-Robust Approach for Dynamic Positioning of Omnichannel Inventories. (arXiv:2310.12183v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20048;&#35266;-&#40065;&#26834;&#30340;&#20840;&#28192;&#36947;&#24211;&#23384;&#21160;&#24577;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#20860;&#39038;&#24211;&#23384;&#24377;&#24615;&#21644;&#25913;&#21892;&#24179;&#22343;&#24615;&#33021;&#65292;&#26469;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#21644;&#20813;&#20998;&#24067;&#20048;&#35266;-&#40065;&#26834;&#21452;&#27169;&#24335;&#24211;&#23384;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#26377;&#25928;&#20998;&#37197;&#38144;&#21806;&#38142;&#19978;&#30340;&#24211;&#23384;&#65292;&#20197;&#28385;&#36275;&#26102;&#21464;&#30340;&#12289;&#19981;&#30830;&#23450;&#30340;&#20840;&#28192;&#36947;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#20248;&#21270;&#26041;&#27861;&#26356;&#21152;&#27880;&#37325;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#24615;&#38656;&#27714;&#65292;&#32780;&#21452;&#27169;&#24335;&#31574;&#30053;&#19981;&#20165;&#32771;&#34385;&#20102;&#20445;&#25345;&#20687;&#40065;&#26834;&#20248;&#21270;&#19968;&#26679;&#30340;&#24377;&#24615;&#65292;&#36824;&#36890;&#36807;&#20811;&#26381;&#20869;&#29983;&#22855;&#24322;&#20540;&#30340;&#23384;&#22312;&#32780;&#33719;&#24471;&#20102;&#25913;&#21892;&#24179;&#22343;&#24773;&#20917;&#19979;&#24615;&#33021;&#30340;&#22238;&#25253;&#12290;&#36825;&#31181;&#21452;&#27169;&#24335;&#31574;&#30053;&#22312;&#24179;&#34913;&#24215;&#38138;&#22833;&#21435;&#38144;&#21806;&#21644;&#36328;&#28192;&#36947;&#30005;&#23376;&#21830;&#21153;&#23653;&#32422;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#38754;&#29305;&#21035;&#26377;&#20215;&#20540;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#24211;&#23384;&#20248;&#21270;&#27169;&#22411;&#30340;&#26680;&#24515;&#25152;&#22312;&#12290;&#30001;&#20110;&#28192;&#36947;&#30340;&#24322;&#36136;&#34892;&#20026;&#65292;&#36825;&#20123;&#22240;&#32032;&#26159;&#38750;&#23545;&#31216;&#30340;&#65292;&#21069;&#32773;&#22312;&#22833;&#38144;&#21806;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#20559;&#24046;&#65292;&#32780;&#21518;&#32773;&#21017;&#20381;&#36182;&#20110;&#32593;&#32476;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of data-driven and distribution-free optimistic-robust bimodal inventory optimization (BIO) strategy to effectively allocate inventory across a retail chain to meet time-varying, uncertain omnichannel demand. While prior Robust optimization (RO) methods emphasize the downside, i.e., worst-case adversarial demand, BIO also considers the upside to remain resilient like RO while also reaping the rewards of improved average-case performance by overcoming the presence of endogenous outliers. This bimodal strategy is particularly valuable for balancing the tradeoff between lost sales at the store and the costs of cross-channel e-commerce fulfillment, which is at the core of our inventory optimization model. These factors are asymmetric due to the heterogenous behavior of the channels, with a bias towards the former in terms of lost-sales cost and a dependence on network effects for the latter. We provide structural insights about the BIO solution and how it can be tu
&lt;/p&gt;</description></item><item><title>RK-core&#26041;&#27861;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#26412;&#30340;&#26680;&#24515;&#20540;&#65292;&#21457;&#29616;&#26680;&#24515;&#20540;&#39640;&#30340;&#26679;&#26412;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22823;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.12168</link><description>&lt;p&gt;
RK-core: &#19968;&#31181;&#21487;&#29992;&#20110;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#23618;&#27425;&#32467;&#26500;&#30340;&#24050;&#24314;&#31435;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets. (arXiv:2310.12168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12168
&lt;/p&gt;
&lt;p&gt;
RK-core&#26041;&#27861;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#26412;&#30340;&#26680;&#24515;&#20540;&#65292;&#21457;&#29616;&#26680;&#24515;&#20540;&#39640;&#30340;&#26679;&#26412;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22823;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#20174;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#36716;&#21521;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#12290;&#23545;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#36827;&#23637;&#26159;&#30001;&#20110;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#30340;&#31215;&#32047;&#25152;&#25512;&#21160;&#30340;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26356;&#22823;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;RK-core&#65292;&#20197;&#24110;&#21161;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25968;&#25454;&#38598;&#20869;&#22797;&#26434;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#36739;&#20302;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#22312;&#20854;&#30456;&#24212;&#31867;&#21035;&#20013;&#30340;&#20195;&#34920;&#24615;&#36739;&#20302;&#65292;&#30456;&#21453;&#65292;&#20855;&#26377;&#36739;&#39640;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#20195;&#34920;&#24615;&#12290;&#30456;&#24212;&#22320;&#65292;&#20855;&#26377;&#36739;&#39640;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#30456;&#23545;&#20110;&#20855;&#26377;&#36739;&#20302;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#23545;&#24615;&#33021;&#36129;&#29486;&#26356;&#22823;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;RK-core&#20998;&#26512;&#20855;&#26377;&#19981;&#21516;coreness&#20540;&#30340;&#26679;&#26412;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the field of machine learning has undergone a transition from model-centric to data-centric. The advancements in diverse learning tasks have been propelled by the accumulation of more extensive datasets, subsequently facilitating the training of larger models on these datasets. However, these datasets remain relatively under-explored. To this end, we introduce a pioneering approach known as RK-core, to empower gaining a deeper understanding of the intricate hierarchical structure within datasets. Across several benchmark datasets, we find that samples with low coreness values appear less representative of their respective categories, and conversely, those with high coreness values exhibit greater representativeness. Correspondingly, samples with high coreness values make a more substantial contribution to the performance in comparison to those with low coreness values. Building upon this, we further employ RK-core to analyze the hierarchical structure of samples with differen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#36890;&#36807;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#32780;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#34917;&#20805;&#21644;&#36741;&#21161;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#39640;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12162</link><description>&lt;p&gt;
AI&#28508;&#21147;&#19982;&#35748;&#30693;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#31435;&#22330;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity. (arXiv:2310.12162v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12162
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#36890;&#36807;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#32780;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#34917;&#20805;&#21644;&#36741;&#21161;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#39640;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31435;&#22330;&#25991;&#20214;&#22312;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#19979;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#29305;&#21035;&#24378;&#35843;&#20102;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21487;&#33021;&#39118;&#38505;&#22240;&#32032;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23558;&#20154;&#31867;&#19987;&#23478;&#32435;&#20837;&#8220;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#8221;&#21327;&#20316;&#20013;&#36827;&#34892;&#31649;&#29702;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#23558;&#20026;&#25915;&#20987;&#35782;&#21035;&#12289;&#20107;&#20214;&#21709;&#24212;&#21644;&#24674;&#22797;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#23558;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#21040;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#20013;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12289;&#25361;&#25112;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#39118;&#38505;&#22240;&#32032;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#36215;&#26469;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#26497;&#22823;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#34917;&#20805;&#21644;&#36741;&#21161;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11798</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Auction-Based Scheduling. (arXiv:2310.11798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#38656;&#35201;&#28385;&#36275;&#22810;&#20010;&#37096;&#20998;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#26041;&#27861;&#26159;&#25972;&#20307;&#21270;&#30340;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#20989;&#25968;&#26469;&#36873;&#25321;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#28385;&#36275;&#25152;&#26377;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#12290;&#27599;&#20010;&#30446;&#26631;&#37117;&#20351;&#29992;&#21333;&#29420;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#12290;&#21487;&#20197;&#29702;&#35299;&#30340;&#26159;&#65292;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#19981;&#21516;&#31574;&#30053;&#21487;&#33021;&#22312;&#32473;&#23450;&#26102;&#38388;&#36873;&#25321;&#20914;&#31361;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25293;&#21334;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#32473;&#27599;&#20010;&#31574;&#30053;&#20998;&#37197;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;&#65292;&#22312;&#27599;&#19968;&#27493;&#65292;&#31574;&#30053;&#21516;&#26102;&#20174;&#21487;&#29992;&#30340;&#39044;&#31639;&#20013;&#20986;&#20215;&#26469;&#33719;&#21462;&#35843;&#24230;&#21644;&#36873;&#25321;&#21160;&#20316;&#30340;&#29305;&#26435;&#12290;&#31574;&#30053;&#20351;&#29992;&#20854;&#20986;&#20215;&#26469;&#34920;&#36798;&#35843;&#24230;&#30340;&#32039;&#36843;&#24615;&#65292;&#26377;&#38480;&#30340;&#39044;&#31639;&#30830;&#20445;&#20102;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;NFT&#30340;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#65292;&#20026;&#20102;&#24357;&#34917;&#23545;&#26032;&#20852;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#30340;&#32570;&#21475;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;</title><link>http://arxiv.org/abs/2310.11709</link><description>&lt;p&gt;
Live Graph Lab:&#26397;&#21521;&#20855;&#26377;NFT&#30340;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#20132;&#26131;&#22270;
&lt;/p&gt;
&lt;p&gt;
Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT. (arXiv:2310.11709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;NFT&#30340;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#65292;&#20026;&#20102;&#24357;&#34917;&#23545;&#26032;&#20852;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#30340;&#32570;&#21475;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35843;&#26597;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#36825;&#20123;&#22270;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#25972;&#20010;&#23454;&#26102;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#12290;&#20854;&#20013;&#65292;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#25104;&#20026;&#21306;&#22359;&#38142;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#12290;&#36825;&#20010;&#20998;&#25955;&#21270;&#29983;&#24577;&#31995;&#32479;&#20855;&#26377;&#36229;&#36807;400&#20159;&#32654;&#20803;&#30340;&#24066;&#20540;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21311;&#21517;&#21644;&#23454;&#26102;&#20132;&#26131;&#27963;&#21160;&#65292;&#33258;&#28982;&#24418;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20132;&#26131;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20174;&#26102;&#38388;&#22270;&#20998;&#26512;&#30340;&#35282;&#24230;&#23545;&#36825;&#20010;&#26032;&#20852;&#30340;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveAttack&#30340;&#26032;&#22411;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#26469;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2310.11595</link><description>&lt;p&gt;
WaveAttack&#65306;&#22522;&#20110;&#19981;&#23545;&#31216;&#39057;&#29575;&#28151;&#28102;&#30340;&#22522;&#20110;&#32972;&#38376;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks. (arXiv:2310.11595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveAttack&#30340;&#26032;&#22411;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#26469;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#35768;&#22810;&#23545;&#25163;&#35774;&#35745;&#20102;&#32972;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#25805;&#32437;&#35757;&#32451;&#26679;&#26412;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#32972;&#38376;&#25915;&#20987;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#37117;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#34987;&#29616;&#26377;&#30340;&#32972;&#38376;&#26816;&#27979;&#31639;&#27861;&#36731;&#26131;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#34920;&#29616;&#20026;&#27602;&#21270;&#26679;&#26412;&#30340;&#20302;&#20445;&#30495;&#24615;&#21644;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#38750;&#21487;&#24573;&#30053;&#36716;&#25442;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#24369;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;WaveAttack&#65292;&#23427;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#28155;&#21152;&#33258;&#36866;&#24212;&#27531;&#24046;&#65292;&#20197;&#25552;&#39640;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;WaveAttack&#30340;&#26377;&#25928;&#24615;&#12290;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WaveAttack&#19981;&#20165;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11569</link><description>&lt;p&gt;
&#24403;&#21018;&#24615;&#25104;&#20026;&#38382;&#39064;&#65306;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#20307;&#65292;&#20854;&#30446;&#26631;&#26159;&#23545;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#26410;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#20063;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#20013;&#26045;&#21152;&#23618;&#27425;&#20851;&#31995;&#65292;&#20294;&#26410;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20063;&#40664;&#35748;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#23618;&#27425;&#20851;&#31995;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#26410;&#36866;&#24212;&#26174;&#31034;&#20986;&#20559;&#31163;&#27492;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHiT&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#25972;&#20010;&#23618;&#27425;&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;PROFHiT&#20351;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11466</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#19977;&#32500;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#29289;&#23398;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#21644;&#20122;&#32454;&#32990;&#23450;&#20301;&#20272;&#35745;&#65289;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;AlphaFold2&#65289;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26500;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#26126;&#26174;&#19979;&#38477;&#12290;&#34429;&#28982;&#31867;&#20284;&#29616;&#35937;&#24050;&#32463;&#22312;&#19968;&#33324;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#39044;&#27979;&#30340;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.10541</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#19968;&#22823;&#22411;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#21644;&#21442;&#25968;&#35843;&#25972;&#36807;&#31243;&#21464;&#24471;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#23558;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#35757;&#32451;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#33021;&#26377;&#25928;&#26367;&#20195;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#20165;&#20851;&#27880;&#25913;&#36827;&#23398;&#29983;&#25104;&#32489;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#35748;&#35782;&#21040;&#19987;&#23478;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#37325;&#35201;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#21518;&#32493;&#25968;&#25454;&#38598;&#31934;&#28860;&#20013;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#19987;&#23478;&#36712;&#36857;&#26102;&#65292;&#19987;&#23478;&#30340;&#24179;&#28369;&#24615;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#65292;&#20197;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.10537</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#25193;&#23637;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31364;&#20301;&#23485;&#25968;&#25454;&#26684;&#24335;&#23545;&#20110;&#38477;&#20302;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#27599;&#20010;&#22359;&#30340;&#32553;&#25918;&#22240;&#23376;&#19982;&#31364;&#28014;&#28857;&#21644;&#25972;&#25968;&#31867;&#22411;&#30456;&#32467;&#21512;&#30340;&#24494;&#25193;&#23637;&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#65292;&#20197;&#28385;&#36275;&#30828;&#20214;&#25928;&#29575;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#25705;&#25830;&#20043;&#38388;&#30340;&#31454;&#20105;&#38656;&#27714;&#12290;&#23545;&#20110;AI&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;MX&#25968;&#25454;&#26684;&#24335;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#26102;&#29992;&#25143;&#25705;&#25830;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#26080;&#38656;&#20462;&#25913;&#35757;&#32451;&#37197;&#26041;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#35757;&#32451;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#20110;8&#20301;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#28176;&#21464;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09430</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks. (arXiv:2310.09430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#20998;&#24067;&#24335;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#26102;&#37117;&#23384;&#22312;&#22256;&#38590;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#24050;&#32463;&#23558;&#20154;&#24037;&#31995;&#32479;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#21040;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#35780;&#20272;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#21517;&#20026;"ReClor-plus"&#12289;"LogiQA-plus"&#21644;"LogiQAv2-plus"&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#19977;&#20010;&#23376;&#38598;&#65306;&#31532;&#19968;&#20010;&#26159;&#36873;&#39033;&#38543;&#26426;&#25171;&#20081;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#27491;&#30830;&#36873;&#39033;&#26367;&#25442;&#20026;"&#27809;&#26377;&#20854;&#20182;&#36873;&#39033;&#26159;&#27491;&#30830;&#30340;"&#65292;&#31532;&#19977;&#20010;&#26159;&#21069;&#20004;&#20010;&#23376;&#38598;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#37492;&#21035;&#21644;&#29983;&#25104;&#22411;&#30340;LLMs&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#31616;&#21333;&#30340;&#25216;&#24039;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#21407;&#22987;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#37117;&#24456;&#38590;&#22238;&#31572;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25200;&#21160;&#24341;&#20837;&#20219;&#21153;&#21464;&#21270;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3.5 and GPT-4, have greatly advanced the performance of artificial systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness to perform logical reasoning remain under-evaluated. To probe this ability, we propose three new logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and "LogiQAv2-plus", each featuring three subsets: the first with randomly shuffled options, the second with the correct choices replaced by "none of the other options are correct", and a combination of the previous two subsets. We carry out experiments on these datasets with both discriminative and generative LLMs and show that these simple tricks greatly hinder the performance of the language models. Despite their superior performance on the original publicly available datasets, we find that all models struggle to answer our newly constructed datasets. We show that introducing task variations by perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09342</link><description>&lt;p&gt;
&#20026;&#31243;&#24207;&#39564;&#35777;&#23545;LLM&#29983;&#25104;&#30340;&#24490;&#29615;&#19981;&#21464;&#24335;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Ranking LLM-Generated Loop Invariants for Program Verification. (arXiv:2310.09342v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27491;&#30830;&#19981;&#21464;&#37327;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#20943;&#23569;&#31243;&#24207;&#39564;&#35777;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#24402;&#32435;&#24490;&#29615;&#19981;&#21464;&#37327;&#26159;&#33258;&#21160;&#21270;&#31243;&#24207;&#39564;&#35777;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;gpt-3.5&#25110;gpt-4&#65289;&#33021;&#22815;&#22312;0-shot&#29615;&#22659;&#19979;&#20026;&#19968;&#31867;&#31243;&#24207;&#21512;&#25104;&#24490;&#29615;&#19981;&#21464;&#37327;&#65292;&#20294;&#38656;&#35201;&#22810;&#20010;&#26679;&#26412;&#25165;&#33021;&#29983;&#25104;&#27491;&#30830;&#30340;&#19981;&#21464;&#37327;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#35843;&#29992;&#31243;&#24207;&#39564;&#35777;&#22120;&#26469;&#24314;&#31435;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25490;&#21517;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#38382;&#39064;&#23450;&#20041;&#21306;&#20998;&#27491;&#30830;&#30340;&#24402;&#32435;&#19981;&#21464;&#37327;&#21644;&#38169;&#35823;&#30340;&#23581;&#35797;&#12290;&#35813;&#25490;&#21517;&#22120;&#32463;&#36807;&#23545;&#27604;&#25490;&#21517;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#37325;&#26032;&#25490;&#21517;&#26426;&#21046;&#26174;&#33879;&#25552;&#39640;&#20102;&#27491;&#30830;&#19981;&#21464;&#37327;&#22312;&#29983;&#25104;&#30340;&#20505;&#36873;&#39033;&#20013;&#30340;&#25490;&#21517;&#65292;&#20174;&#32780;&#22823;&#24133;&#20943;&#23569;&#20102;&#23545;&#39564;&#35777;&#22120;&#30340;&#35843;&#29992;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing inductive loop invariants is fundamental to automating program verification. In this work, we observe that Large Language Models (such as gpt-3.5 or gpt-4) are capable of synthesizing loop invariants for a class of programs in a 0-shot setting, yet require several samples to generate the correct invariants. This can lead to a large number of calls to a program verifier to establish an invariant. To address this issue, we propose a {\it re-ranking} approach for the generated results of LLMs. We have designed a ranker that can distinguish between correct inductive invariants and incorrect attempts based on the problem definition. The ranker is optimized as a contrastive ranker. Experimental results demonstrate that this re-ranking mechanism significantly improves the ranking of correct invariants among the generated candidates, leading to a notable reduction in the number of calls to a verifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08395</link><description>&lt;p&gt;
&#20351;&#29992;&#24605;&#36335;&#38142;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#29983;&#25104;&#65288;KBQG&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#36923;&#36753;&#24418;&#24335;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#27880;&#37322;&#30340;&#26114;&#36149;&#25104;&#26412;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24613;&#38656;&#24320;&#21457;KBQG&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#20013;&#36807;&#20110;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#24182;&#19981;&#21512;&#36866;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#21360;&#35937;&#21147;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#23558;KBQG&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#23436;&#25972;&#38382;&#39064;&#30340;&#29983;&#25104;&#34987;&#20998;&#20026;&#19968;&#31995;&#21015;&#30340;&#23376;&#38382;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;KQG-CoT&#39318;&#20808;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#26816;&#32034;&#25903;&#25345;&#24615;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#32771;&#34385;&#36923;&#36753;&#24418;&#24335;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#20889;&#19968;&#20010;&#25552;&#31034;&#26469;&#26126;&#30830;&#25512;&#29702;&#38142;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26725;&#25509;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#21644;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;ChatGPT&#30340;&#24110;&#21161;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.07570</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#35745;&#31639;&#25299;&#25169;&#23398;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Computational Topology. (arXiv:2310.07570v2 [math.AT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;ChatGPT&#26725;&#25509;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#21644;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;ChatGPT&#30340;&#24110;&#21161;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#29615;&#22659;&#20013;&#65292;&#23427;&#24448;&#24448;&#21463;&#21040;&#27010;&#24565;&#38169;&#35823;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26159;&#19968;&#20010;&#30456;&#23545;&#36739;&#26032;&#30340;&#23398;&#31185;&#65292;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#31639;&#27861;&#21644;&#32534;&#30721;&#25216;&#33021;&#22312;&#29702;&#35770;&#23478;&#20013;&#30340;&#29702;&#35299;&#36824;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#25299;&#25169;&#23398;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;ChatGPT&#26469;&#24357;&#21512;&#29702;&#35770;&#25299;&#25169;&#27010;&#24565;&#19982;&#35745;&#31639;&#25299;&#25169;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#35745;&#31639;&#32463;&#39564;&#21644;&#32534;&#30721;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#31929;&#30340;&#29702;&#35770;&#23478;&#22914;&#20309;&#20511;&#21161;ChatGPT&#23558;&#25968;&#23398;&#20844;&#24335;&#21644;&#27010;&#24565;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#30340;&#35745;&#31639;&#25299;&#25169;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#27010;&#36848;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT represents a significant milestone in the field of artificial intelligence (AI), finding widespread applications across diverse domains. However, its effectiveness in mathematical contexts has been somewhat constrained by its susceptibility to conceptual errors. Concurrently, topological data analysis (TDA), a relatively new discipline, has garnered substantial interest in recent years. Nonetheless, the advancement of TDA is impeded by the limited understanding of computational algorithms and coding proficiency among theoreticians. This work endeavors to bridge the gap between theoretical topological concepts and their practical implementation in computational topology through the utilization of ChatGPT. We showcase how a pure theoretician, devoid of computational experience and coding skills, can effectively transform mathematical formulations and concepts into functional code for computational topology with the assistance of ChatGPT. Our strategy outlines a productive process
&lt;/p&gt;</description></item><item><title>ROMO&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#24179;&#24248;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#26469;&#35299;&#20915;&#32422;&#26463;MBO&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07560</link><description>&lt;p&gt;
ROMO: &#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
ROMO: Retrieval-enhanced Offline Model-based Optimization. (arXiv:2310.07560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07560
&lt;/p&gt;
&lt;p&gt;
ROMO&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#24179;&#24248;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#26469;&#35299;&#20915;&#32422;&#26463;MBO&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#30418;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#20986;&#29616;&#65292;&#20854;&#30446;&#26631;&#26159;&#22522;&#20110;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#23547;&#25214;&#20351;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#26368;&#22823;&#21270;&#30340;&#25972;&#20010;&#31354;&#38388;&#19978;&#30340;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MBO&#35774;&#32622;&#65292;&#31216;&#20026;&#32422;&#26463;MBO&#65288;CoMBO&#65289;&#65292;&#20854;&#20013;&#21482;&#26377;&#37096;&#20998;&#35774;&#35745;&#31354;&#38388;&#21487;&#20197;&#20248;&#21270;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21463;&#29615;&#22659;&#32422;&#26463;&#12290;CoMBO&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#35266;&#23519;&#35774;&#35745;&#22312;&#35780;&#20272;&#20013;&#26159;&#24179;&#24248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#36825;&#20123;&#24179;&#24248;&#30340;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#22312;&#20256;&#32479;&#30340;MBO&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#26368;&#20339;&#35266;&#23519;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65288;ROMO&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21487;&#23548;&#24615;&#21069;&#21521;&#26041;&#27861;&#65292;&#23427;&#26816;&#32034;&#31163;&#32447;&#25968;&#25454;&#38598;&#24182;&#32858;&#21512;&#30456;&#20851;&#26679;&#26412;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;CoMBO&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven black-box model-based optimization (MBO) problems arise in a great number of practical application scenarios, where the goal is to find a design over the whole space maximizing a black-box target function based on a static offline dataset. In this work, we consider a more general but challenging MBO setting, named constrained MBO (CoMBO), where only part of the design space can be optimized while the rest is constrained by the environment. A new challenge arising from CoMBO is that most observed designs that satisfy the constraints are mediocre in evaluation. Therefore, we focus on optimizing these mediocre designs in the offline dataset while maintaining the given constraints rather than further boosting the best observed design in the traditional MBO setting. We propose retrieval-enhanced offline model-based optimization (ROMO), a new derivable forward approach that retrieves the offline dataset and aggregates relevant samples to provide a trusted prediction, and use it f
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07091</link><description>&lt;p&gt;
Jaeger:&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jaeger: A Concatenation-Based Multi-Transformer VQA Model. (arXiv:2310.07091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07091
&lt;/p&gt;
&lt;p&gt;
Jaeger&#26159;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;&#65292;&#21033;&#29992;RoBERTa large&#21644;GPT2-xl&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#32771;&#34385;&#22810;&#28304;&#20449;&#24687;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#22312;&#35821;&#35328;&#24847;&#20041;&#28040;&#27495;&#21644;&#32454;&#31890;&#24230;&#22810;&#27169;&#24577;&#26816;&#32034;&#20043;&#38388;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30001;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#24320;&#25918;&#19990;&#30028;&#20808;&#39564;&#27169;&#22411;&#30340;&#21033;&#29992;&#65292;&#25991;&#26723;&#38382;&#31572;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#21709;&#24212;&#26102;&#38388;&#24310;&#38271;&#12289;&#25512;&#26029;&#25345;&#32493;&#26102;&#38388;&#24310;&#38271;&#21644;&#21305;&#37197;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#22810;&#21464;&#25442;&#22120;VQA&#27169;&#22411;Jaegar&#12290;&#20026;&#20102;&#25552;&#21462;&#38382;&#39064;&#29305;&#24449;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;RoBERTa large&#21644;GPT2-xl&#31561;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#20004;&#31181;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#36830;&#25509;&#25805;&#20316;&#12290;&#36825;&#20010;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering poses a challenging task between linguistic sense disambiguation and fine-grained multimodal retrieval. Although there has been encouraging progress in document-based question answering due to the utilization of large language and open-world prior models\cite{1}, several challenges persist, including prolonged response times, extended inference durations, and imprecision in matching. In order to overcome these challenges, we propose Jaegar, a concatenation-based multi-transformer VQA model. To derive question features, we leverage the exceptional capabilities of RoBERTa large\cite{2} and GPT2-xl\cite{3} as feature extractors. Subsequently, we subject the outputs from both models to a concatenation process. This operation allows the model to consider information from diverse sources concurrently, strengthening its representational capability. By leveraging pre-trained models for feature extraction, our approach has the potential to amplify the pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.06165</link><description>&lt;p&gt;
CAW-coref: &#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
CAW-coref: Conjunction-Aware Word-level Coreference Resolution. (arXiv:2310.06165v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#27169;&#22411;&#65288;CAW-coref&#65289;&#65292;&#22312;&#22788;&#29702;&#24182;&#21015;&#25552;&#21450;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20849;&#25351;&#28040;&#35299;&#31995;&#32479;&#27599;&#31687;&#25991;&#31456;&#38656;&#35201;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#27492;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#26469;&#35828;&#65288;&#20363;&#22914;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65289;&#65292;&#20195;&#20215;&#22826;&#39640;&#12290;&#32780;&#35789;&#32423;&#20849;&#25351;&#31995;&#32479; (WL-coref) &#22312;&#25928;&#29575;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20808;&#36827;&#31995;&#32479; 96.6% &#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102; WL-coref &#30340;&#19968;&#20010;&#24120;&#35265;&#20294;&#37325;&#35201;&#30340;&#22833;&#36133;&#26696;&#20363;&#65306;&#22788;&#29702;&#8220;Tom &#21644; Mary&#8221;&#20043;&#31867;&#30340;&#24182;&#21015;&#25552;&#21450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312; OntoNotes &#27979;&#35797;&#38598;&#19978;&#23558;&#24615;&#33021;&#25552;&#39640;&#20102; 0.9% F1&#65292;&#23558;&#39640;&#25928;&#30340;&#35789;&#32423;&#20849;&#25351;&#28040;&#35299;&#19982;&#26114;&#36149;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24046;&#36317;&#32553;&#23567;&#20102;34.6%&#12290;&#25105;&#20204;&#30340;&#20851;&#32852;&#35789;&#24863;&#30693;&#30340;&#35789;&#32423;&#20849;&#25351;&#27169;&#22411;&#65288;CAW-coref&#65289;&#21644;&#20195;&#30721;&#21487;&#22312; https://github.com/KarelDO/wl-coref &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art coreference resolutions systems depend on multiple LLM calls per document and are thus prohibitively expensive for many use cases (e.g., information extraction with large corpora). The leading word-level coreference system (WL-coref) attains 96.6% of these SOTA systems' performance while being much more efficient. In this work, we identify a routine yet important failure case of WL-coref: dealing with conjoined mentions such as 'Tom and Mary'. We offer a simple yet effective solution that improves the performance on the OntoNotes test set by 0.9% F1, shrinking the gap between efficient word-level coreference resolution and expensive SOTA approaches by 34.6%. Our Conjunction-Aware Word-level coreference model (CAW-coref) and code is available at https://github.com/KarelDO/wl-coref.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Choco Solver&#36827;&#34892;&#22810;&#37197;&#32622;&#38382;&#39064;&#27714;&#35299;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;&#23545;&#32422;&#26463;&#27714;&#35299;&#22120;&#24615;&#33021;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#30456;&#20851;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02658</link><description>&lt;p&gt;
&#35299;&#20915;&#22810;&#37197;&#32622;&#38382;&#39064;&#65306;&#20351;&#29992;Choco Solver&#30340;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver. (arXiv:2310.02658v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Choco Solver&#36827;&#34892;&#22810;&#37197;&#32622;&#38382;&#39064;&#27714;&#35299;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;&#23545;&#32422;&#26463;&#27714;&#35299;&#22120;&#24615;&#33021;&#20998;&#26512;&#30340;&#30740;&#31350;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#30456;&#20851;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#37197;&#32622;&#22120;&#25903;&#25345;&#37197;&#32622;&#28385;&#36275;&#21333;&#20010;&#29992;&#25143;&#20559;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#37197;&#32622;&#30340;&#27010;&#24565;&#22522;&#20110;&#37197;&#32622;&#19968;&#32452;&#37197;&#32622;&#30340;&#24819;&#27861;&#12290;&#36825;&#31181;&#21151;&#33021;&#22312;&#37197;&#32622;&#20010;&#24615;&#21270;&#32771;&#35797;&#65292;&#37197;&#32622;&#39033;&#30446;&#22242;&#38431;&#21644;&#20026;&#26053;&#28216;&#22242;&#38431;&#30340;&#27599;&#20010;&#25104;&#21592;&#37197;&#32622;&#19981;&#21516;&#30340;&#26053;&#34892;&#65288;&#20363;&#22914;&#65292;&#22312;&#35775;&#38382;&#29305;&#23450;&#22478;&#24066;&#26102;&#65289;&#31561;&#22330;&#26223;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31034;&#20363;&#20102;&#22810;&#37197;&#32622;&#24212;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#32771;&#35797;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#32422;&#26463;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#20998;&#26512;&#65292;&#24110;&#21161;&#25105;&#20204;&#23545;&#30456;&#24212;&#30340;&#24615;&#33021;&#38382;&#39064;&#26377;&#19968;&#20123;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many scenarios, configurators support the configuration of a solution that satisfies the preferences of a single user. The concept of \emph{multi-configuration} is based on the idea of configuring a set of configurations. Such a functionality is relevant in scenarios such as the configuration of personalized exams, the configuration of project teams, and the configuration of different trips for individual members of a tourist group (e.g., when visiting a specific city). In this paper, we exemplify the application of multi-configuration for generating individualized exams. We also provide a constraint solver performance analysis which helps to gain some insights into corresponding performance issues.
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02227</link><description>&lt;p&gt;
SNIP: &#29992;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#36830;&#25509;&#25968;&#23398;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02227
&lt;/p&gt;
&lt;p&gt;
SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26080;&#27861;&#32570;&#23569;&#31526;&#21495;&#25968;&#23398;&#26041;&#31243;&#26469;&#24314;&#27169;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#30340;&#26102;&#20195;&#65292;&#31185;&#23398;&#25506;&#31350;&#24448;&#24448;&#28041;&#21450;&#21040;&#25910;&#38598;&#35266;&#23519;&#25968;&#25454;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29305;&#21270;&#20110;&#25968;&#20540;&#39046;&#22495;&#25110;&#31526;&#21495;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#20026;&#29305;&#23450;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#31526;&#21495;&#26041;&#31243;&#21644;&#20854;&#25968;&#20540;&#23545;&#24212;&#29289;&#20043;&#38388;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#22823;&#22909;&#22788;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SNIP&#65292;&#19968;&#31181;&#31526;&#21495;-&#25968;&#20540;&#38598;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#23884;&#20837;&#20013;&#30340;&#30456;&#20114;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#28508;&#31354;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNIP&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24341;&#20837;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#36816;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;CNN&#27169;&#22411;&#22312;&#20302;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01680</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26377;&#38480;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Keypoint-Augmented Self-Supervised Learning for Medical Image Segmentation with Limited Annotation. (arXiv:2310.01680v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24341;&#20837;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#36816;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;CNN&#27169;&#22411;&#22312;&#20302;&#27880;&#37322;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;CNN&#27169;&#22411;&#65288;&#22914;UNet&#65289;&#24050;&#25104;&#20026;&#22312;&#20302;&#27880;&#37322;&#29615;&#22659;&#19979;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#40723;&#21169;&#30456;&#21516;&#22270;&#20687;&#32463;&#21382;&#19981;&#21516;&#21464;&#25442;&#26102;&#30340;&#31867;&#20284;&#20840;&#23616;&#34920;&#31034;&#65292;&#25110;&#22312;&#26412;&#36136;&#19978;&#30456;&#20851;&#30340;&#19981;&#21516;&#22270;&#20687;/&#34917;&#19969;&#29305;&#24449;&#20043;&#38388;&#23454;&#26045;&#19981;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;CNN&#25552;&#21462;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#22312;&#25429;&#25417;&#29983;&#29289;&#35299;&#21078;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38271;&#31243;&#31354;&#38388;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#28857;&#22686;&#24378;&#30340;&#34701;&#21512;&#23618;&#65292;&#21487;&#20197;&#25552;&#21462;&#26082;&#20445;&#30041;&#30701;&#31243;&#21448;&#20445;&#30041;&#38271;&#31243;&#33258;&#27880;&#24847;&#21147;&#30340;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#36755;&#20837;&#65292;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#22686;&#24378;CNN&#29305;&#24449;&#22270;&#65292;&#35813;&#36755;&#20837;&#23398;&#20064;&#20102;&#23616;&#37096;&#20851;&#38190;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#38271;&#31243;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#20840;&#23616;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20840;&#23616;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining CNN models (i.e., UNet) through self-supervision has become a powerful approach to facilitate medical image segmentation under low annotation regimes. Recent contrastive learning methods encourage similar global representations when the same image undergoes different transformations, or enforce invariance across different image/patch features that are intrinsically correlated. However, CNN-extracted global and local features are limited in capturing long-range spatial dependencies that are essential in biological anatomy. To this end, we present a keypoint-augmented fusion layer that extracts representations preserving both short- and long-range self-attention. In particular, we augment the CNN feature map at multiple scales by incorporating an additional input that learns long-range spatial self-attention among localized keypoint features. Further, we introduce both global and local self-supervised pretraining for the framework. At the global scale, we obtain global repres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.01448</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#20803;&#35821;&#20041;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Meta Semantic Template for Evaluation of Large Language Models. (arXiv:2310.01448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35821;&#35328;&#30340;&#35821;&#20041;&#65292;&#36824;&#26159;&#20165;&#20165;&#35760;&#20303;&#35757;&#32451;&#25968;&#25454;&#65311;&#26368;&#36817;&#23545;LLM&#28508;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#25285;&#24551;&#24341;&#36215;&#20102;&#31038;&#21306;&#23545;LLM&#35780;&#20272;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTemp&#65292;&#19968;&#31181;&#36890;&#36807;&#21019;&#24314;&#20803;&#35821;&#20041;&#27169;&#26495;&#26469;&#35780;&#20272;LLM&#23545;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;MSTemp&#30340;&#26680;&#24515;&#19981;&#26159;&#30452;&#25509;&#22312;&#29616;&#26377;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#26159;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20316;&#20026;&#31181;&#23376;&#29983;&#25104;&#26032;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#35780;&#20272;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#21477;&#23376;&#65292;MSTemp&#21033;&#29992;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35821;&#20041;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#31216;&#20026;&#21407;&#21477;&#23376;&#30340;&#35821;&#20041;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;MSTemp&#36890;&#36807;&#21477;&#23376;&#35299;&#26512;&#21644;&#38543;&#26426;&#26367;&#25442;&#35789;&#35821;&#26469;&#29983;&#25104;&#35780;&#20272;&#26679;&#26412;&#12290;MSTemp&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#12289;&#21160;&#24577;&#21644;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;MSTemp-
&lt;/p&gt;
&lt;p&gt;
Do large language models (LLMs) genuinely understand the semantics of the language, or just memorize the training data? The recent concern on potential data contamination of LLMs has raised awareness of the community to conduct research on LLMs evaluation. In this paper, we propose MSTemp, an approach that creates meta semantic templates to evaluate the semantic understanding ability of LLMs. The core of MSTemp is not to perform evaluation directly on existing benchmark datasets, but to generate new out-of-distribution (OOD) evaluation sets using existing datasets as seeds. Specifically, for a given sentence, MSTemp leverages another language model to generate new samples while preserving its semantics. The new samples are called semantic templates to the original sentence. Then, MSTemp generates evaluation samples via sentence parsing and random word replacement on the semantic templates. MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.00378</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#20215;&#20540;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Measuring Value Understanding in Language Models through Discriminator-Critique Gap. (arXiv:2310.00378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#27979;&#37327;LLMs&#23545;&#20154;&#31867;&#20215;&#20540;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;LLMs&#30340;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#26377;&#36739;&#22823;&#24433;&#21709;&#65292;&#32780;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20043;&#38388;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21644;&#36866;&#24212;&#24615;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#30495;&#27491;&#29702;&#35299;LLMs&#20013;&#30340;&#20215;&#20540;&#35266;&#38656;&#35201;&#32771;&#34385;&#21040;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#20004;&#20010;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#29702;&#35299;&#27979;&#37327;&#65288;VUM&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#37492;&#21035;-&#25209;&#21028;&#24046;&#36317;&#26469;&#23450;&#37327;&#35780;&#20272;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#21644;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#12290;&#21033;&#29992;&#26045;&#29926;&#33576;&#20215;&#20540;&#35266;&#35843;&#26597;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#35780;&#20272;&#20215;&#20540;&#35266;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;GPT-4&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#21315;&#20010;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32771;&#23519;&#20102;LLMs&#30340;&#36755;&#20986;&#19982;&#22522;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20215;&#20540;&#35266;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;LLMs&#30340;&#22238;&#31572;&#19982;GPT-4&#30340;&#27880;&#37322;&#22312;&#20215;&#20540;&#35748;&#30693;&#21407;&#22240;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;&#20195;&#34920;&#24615;LLMs&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#23610;&#24230;&#23450;&#24459;&#23545;&#8220;&#30693;&#36947;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#22823;&#65292;&#20294;&#23545;&#8220;&#30693;&#36947;&#20026;&#20160;&#20040;&#8221;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) have heightened concerns about their potential misalignment with human values. However, evaluating their grasp of these values is complex due to their intricate and adaptable nature. We argue that truly understanding values in LLMs requires considering both "know what" and "know why". To this end, we present the Value Understanding Measurement (VUM) framework that quantitatively assess both "know what" and "know why" by measuring the discriminator-critique gap related to human values. Using the Schwartz Value Survey, we specify our evaluation values and develop a thousand-level dialogue dataset with GPT-4. Our assessment looks at both the value alignment of LLM's outputs compared to baseline answers and how LLM responses align with reasons for value recognition versus GPT-4's annotations. We evaluate five representative LLMs and provide strong evidence that the scaling law significantly impacts "know what" but not much on "know why", 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2309.13218</link><description>&lt;p&gt;
AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#65306;&#19968;&#20010;&#26694;&#26550;&#21644;&#22312;&#29983;&#20135;&#35843;&#24230;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling. (arXiv:2309.13218v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13218
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#38656;&#27714;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#20248;&#21270;&#26159;&#23547;&#25214;&#21644;&#23454;&#26045;&#39640;&#25928;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#36816;&#33829;&#26041;&#24335;&#65292;&#20197;&#20026;&#20225;&#19994;&#24102;&#26469;&#31454;&#20105;&#20248;&#21183;&#30340;&#36807;&#31243;&#12290;&#32508;&#21512;&#38382;&#39064;&#34920;&#36848;&#26159;&#20225;&#19994;&#20248;&#21270;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#22260;&#32469;&#30528;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#23637;&#24320;&#65292;&#22240;&#27492;&#24456;&#26377;&#21487;&#33021;&#25104;&#20026;&#29942;&#39048;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21487;&#20197;&#28508;&#22312;&#22320;&#20943;&#23569;&#38382;&#39064;&#34920;&#36848;&#20013;&#25152;&#38656;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#29992;&#20110;&#38382;&#39064;&#34920;&#36848;&#30340;LLM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#12289;&#20196;&#29260;&#38480;&#21046;&#20197;&#21450;LLM&#20013;&#32570;&#20047;&#36866;&#24403;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20943;&#23569;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#20219;&#21153;&#30340;LLM&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;AI-&#20225;&#19994;&#20248;&#21270;&#30340;&#21327;&#21516;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10444</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#33258;&#25105;&#24378;&#21270;&#20197;&#25913;&#36827;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Reinforcement for Improving Learnersourced Multiple-Choice Question Explanations with Large Language Models. (arXiv:2309.10444v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#23398;&#29983;&#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#29992;&#20110;&#25913;&#36827;&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#20013;&#23398;&#29983;&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#36164;&#28304;&#20849;&#20139;&#28041;&#21450;&#23398;&#29983;&#29983;&#25104;&#21644;&#20998;&#20139;&#23398;&#20064;&#36164;&#28304;&#12290;&#22312;&#23398;&#29983;&#29983;&#25104;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#21019;&#24314;&#35299;&#37322;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#23545;&#30456;&#20851;&#27010;&#24565;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#24448;&#24448;&#30001;&#20110;&#20027;&#39064;&#29702;&#35299;&#26377;&#38480;&#21644;&#20165;&#20165;&#37325;&#30003;&#38382;&#39064;&#12289;&#24178;&#25200;&#22240;&#32032;&#21644;&#27491;&#30830;&#31572;&#26696;&#30340;&#20542;&#21521;&#32780;&#38590;&#20197;&#32534;&#20889;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24110;&#21161;&#25903;&#25745;&#36825;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#24378;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#29983;&#25104;&#19982;&#23398;&#29983;&#23545;&#40784;&#30340;&#35299;&#37322;&#65292;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#20197;&#30830;&#20445;&#20854;&#36136;&#37327;&#65292;&#24182;&#36845;&#20195;&#22686;&#24378;&#35299;&#37322;&#12290;&#22914;&#26524;&#19968;&#20010;&#35299;&#37322;&#30340;&#35780;&#20272;&#20998;&#25968;&#20302;&#20110;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#26694;&#26550;&#20250;&#36845;&#20195;&#22320;&#20248;&#21270;&#21644;&#37325;&#26032;&#35780;&#20272;&#35299;&#37322;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#19968;&#20010;&#23398;&#29983;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing involves students generating and sharing learning resources with their peers. When learnersourcing multiple-choice questions, creating explanations for the generated questions is a crucial step as it facilitates a deeper understanding of the related concepts. However, it is often difficult for students to craft effective explanations due to limited subject understanding and a tendency to merely restate the question stem, distractors, and correct answer. To help scaffold this task, in this work we propose a self-reinforcement large-language-model framework, with the goal of generating and evaluating explanations automatically. Comprising three modules, the framework generates student-aligned explanations, evaluates these explanations to ensure their quality and iteratively enhances the explanations. If an explanation's evaluation score falls below a defined threshold, the framework iteratively refines and reassesses the explanation. Importantly, our framework emulates th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#30340;&#26032;&#22411;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#12290;&#36890;&#36807;BLAST&#25628;&#32034;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#21644;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10170</link><description>&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#65292;&#35774;&#35745;&#21644;&#20998;&#26512;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#20197;&#25552;&#39640;&#26426;&#26800;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties. (arXiv:2309.10170v1 [cond-mat.mtrl-sci] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#30340;&#26032;&#22411;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#12290;&#36890;&#36807;BLAST&#25628;&#32034;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#21644;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34584;&#34523;&#19997;&#26159;&#19968;&#31181;&#25317;&#26377;&#20986;&#33394;&#30340;&#26426;&#26800;&#24615;&#33021;&#65288;&#22914;&#24378;&#24230;&#65292;&#24310;&#23637;&#24615;&#21644;&#36731;&#37327;&#21270;&#65289;&#30340;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20165;&#26377;&#26377;&#38480;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#23436;&#20840;&#25506;&#32034;&#24207;&#21015;-&#24615;&#33021;&#20851;&#31995;&#20197;&#36827;&#34892;&#20998;&#26512;&#21644;&#35774;&#35745;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#20197;&#28385;&#36275;&#22797;&#26434;&#30340;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#34507;&#30333;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#38024;&#23545;&#32422;1,000&#20010;&#20027;&#35201;&#27873;&#33146;&#19997;&#34507;&#30333;&#65288;MaSp&#65289;&#24207;&#21015;&#36827;&#34892;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#29983;&#25104;&#31574;&#30053;&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#12290;&#36890;&#36807;&#20197;&#19979;&#35780;&#20272;&#24615;&#33021;&#65306;&#65288;1&#65289;&#36890;&#36807;BLAST&#25628;&#32034;&#23545;&#29983;&#25104;&#30340;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#26032;&#39062;&#24615;&#20998;&#26512;&#21644;&#34507;&#30333;&#31867;&#22411;&#20998;&#31867;&#65292;&#65288;2&#65289;&#23545;&#27604;&#31867;&#20284;&#24207;&#21015;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#65288;3&#65289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#65292;&#20197;&#21450;&#65288;4&#65289;&#35814;&#32454;&#30340;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spider silks are remarkable materials characterized by superb mechanical properties such as strength, extensibility and lightweightedness. Yet, to date, limited models are available to fully explore sequence-property relationships for analysis and design. Here we propose a custom generative large-language model to enable design of novel spider silk protein sequences to meet complex combinations of target mechanical properties. The model, pretrained on a large set of protein sequences, is fine-tuned on ~1,000 major ampullate spidroin (MaSp) sequences for which associated fiber-level mechanical properties exist, to yield an end-to-end forward and inverse generative strategy. Performance is assessed through: (1), a novelty analysis and protein type classification for generated spidroin sequences through BLAST searches, (2) property evaluation and comparison with similar sequences, (3) comparison of molecular structures, as well as, and (4) a detailed sequence motif analyses. We generate s
&lt;/p&gt;</description></item><item><title>DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.02072</link><description>&lt;p&gt;
DeepVol&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeepVol: A Deep Transfer Learning Approach for Universal Asset Volatility Modeling. (arXiv:2309.02072v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02072
&lt;/p&gt;
&lt;p&gt;
DeepVol&#26159;&#19968;&#31181;&#29992;&#20110;&#36890;&#29992;&#36164;&#20135;&#27874;&#21160;&#24615;&#24314;&#27169;&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21487;&#33021;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27874;&#21160;&#24615;&#27169;&#22411;DeepVol&#65292;&#23427;&#22312;&#27169;&#22411;&#30340;&#24191;&#27867;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35745;&#37327;&#32463;&#27982;&#27169;&#22411;&#12290;DeepVol&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#24314;&#27169;&#25152;&#26377;&#37329;&#34701;&#36164;&#20135;&#30340;&#27874;&#21160;&#24615;&#21160;&#24577;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#36164;&#20135;&#12290;&#36825;&#19982;&#35745;&#37327;&#32463;&#27982;&#23398;&#25991;&#29486;&#20013;&#30340;&#20027;&#27969;&#20570;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#38656;&#35201;&#20026;&#19981;&#21516;&#25968;&#25454;&#38598;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;&#24341;&#20837;DeepVol&#20026;&#37329;&#34701;&#34892;&#19994;&#30340;&#27874;&#21160;&#24615;&#24314;&#27169;&#21644;&#39044;&#27979;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#21487;&#33021;&#20250;&#25913;&#21464;&#23545;&#27874;&#21160;&#24615;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DeepVol, a promising new deep learning volatility model that outperforms traditional econometric models in terms of model generality. DeepVol leverages the power of transfer learning to effectively capture and model the volatility dynamics of all financial assets, including previously unseen ones, using a single universal model. This contrasts to the prevailing practice in econometrics literature, which necessitates training separate models for individual datasets. The introduction of DeepVol opens up new avenues for volatility modeling and forecasting in the finance industry, potentially transforming the way volatility is understood and predicted.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.06368</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#20854;&#25512;&#33616;&#20165;&#36866;&#21512;&#29992;&#25143;&#23545;&#24050;&#28040;&#36153;&#29289;&#21697;&#30340;&#35780;&#32423;&#21382;&#21490;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#27873;&#65292;&#29992;&#25143;&#26080;&#27861;&#20174;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#20013;&#20307;&#39564;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#24418;&#24335;&#65292;&#20197;&#36125;&#21494;&#26031;&#24778;&#21916;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#28040;&#36153;&#24182;&#35780;&#32423;&#21518;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#12290;&#32467;&#21512;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#30340;&#21327;&#21516;&#36807;&#28388;&#32452;&#20214;&#65292;&#21487;&#20197;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#24847;&#22806;&#24615;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#20027;&#39064;&#32423;&#21035;&#30340;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;Goodreads&#20013;&#25552;&#21462;&#30340;&#22270;&#20070;&#38405;&#35835;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;26&#21315;&#20010;&#29992;&#25143;&#21644;&#36817;130&#19975;&#26412;&#20070;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;449&#31687;&#20070;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
&lt;/p&gt;</description></item><item><title>Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09688</link><description>&lt;p&gt;
Amazon-M2: &#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09688
&lt;/p&gt;
&lt;p&gt;
Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#26469;&#35828;&#65292;&#24314;&#27169;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#29702;&#35299;&#23458;&#25143;&#30340;&#20559;&#22909;&#23545;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25216;&#26415;&#21033;&#29992;&#23458;&#25143;&#20250;&#35805;&#25968;&#25454;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#20114;&#21160;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20250;&#35805;&#25968;&#25454;&#38598;&#22312;&#39033;&#30446;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20840;&#38754;&#22320;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20559;&#22909;&#30340;&#35889;&#31995;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Amazon Multilingual Multi-locale Shopping Session Dataset&#65292;&#21363;Amazon-M2&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#30001;&#26469;&#33258;&#20845;&#20010;&#19981;&#21516;&#21306;&#22495;&#30340;&#25968;&#30334;&#19975;&#29992;&#25143;&#20250;&#35805;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20135;&#21697;&#30340;&#20027;&#35201;&#35821;&#35328;&#26159;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#26085;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.03512</link><description>&lt;p&gt;
&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#22312;LiDAR&#25968;&#25454;&#19978;&#35782;&#21035;&#22475;&#34255;&#30340;&#32771;&#21476;&#32467;&#26500;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32771;&#21476;&#30740;&#31350;&#20013;&#30340;&#36965;&#24863;&#25968;&#25454;&#26102;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#38556;&#30861;&#26159;&#36866;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#20256;&#36755;&#23398;&#20064;&#30340;&#24212;&#29992;&#32463;&#24120;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#20010;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#24517;&#35201;&#25506;&#32034;&#22312;&#19981;&#21516;&#32771;&#21476;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20256;&#36755;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#20004;&#20010;&#35821;&#20041;&#20998;&#21106;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;LiDAR&#25968;&#25454;&#38598;&#19978;&#30340;&#21508;&#31181;&#20256;&#36755;&#23398;&#20064;&#37197;&#32622;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20256;&#36755;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#32771;&#21476;&#23398;&#20013;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#23613;&#31649;&#23578;&#26410;&#35266;&#23519;&#21040;&#31995;&#32479;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27492;&#31867;&#25216;&#26415;&#26377;&#25928;&#24615;&#30340;&#20855;&#20307;&#35265;&#35299;&#65292;&#21487;&#20316;&#20026;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying deep learning to remote sensing data in archaeological research, a notable obstacle is the limited availability of suitable datasets for training models. The application of transfer learning is frequently employed to mitigate this drawback. However, there is still a need to explore its effectiveness when applied across different archaeological datasets. This paper compares the performance of various transfer learning configurations using two semantic segmentation deep neural networks on two LiDAR datasets. The experimental results indicate that transfer learning-based approaches in archaeology can lead to performance improvements, although a systematic enhancement has not yet been observed. We provide specific insights about the validity of such techniques that can serve as a baseline for future works.
&lt;/p&gt;</description></item><item><title>MeLM&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22810;&#27169;&#24335;&#21147;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#27491;&#21521;&#21644;&#36870;&#21521;&#21147;&#23398;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.17525</link><description>&lt;p&gt;
MeLM&#65306;&#19968;&#20010;&#35299;&#20915;&#27491;&#21521;&#21644;&#36870;&#21521;&#21147;&#23398;&#38382;&#39064;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. (arXiv:2306.17525v1 [cond-mat.mtrl-sci] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17525
&lt;/p&gt;
&lt;p&gt;
MeLM&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#22810;&#27169;&#24335;&#21147;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#27491;&#21521;&#21644;&#36870;&#21521;&#21147;&#23398;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22810;&#27169;&#24335;&#21147;&#23398;&#35821;&#35328;&#27169;&#22411;MeLM&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#38750;&#32447;&#24615;&#30340;&#27491;&#21521;&#21644;&#36870;&#21521;&#38382;&#39064;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#32452;&#25351;&#20196;&#12289;&#25968;&#23383;&#21644;&#24494;&#32467;&#26500;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#21508;&#20010;&#31034;&#20363;&#65292;&#21253;&#25324;&#20223;&#29983;&#20998;&#23618;&#34562;&#31389;&#35774;&#35745;&#12289;&#30899;&#32435;&#31859;&#31649;&#21147;&#23398;&#21644;&#34507;&#30333;&#36136;&#23637;&#24320;&#12290;&#23613;&#31649;&#27169;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#36731;&#26494;&#22320;&#34701;&#20837;&#19981;&#21516;&#26448;&#26009;&#12289;&#23610;&#24230;&#21644;&#21147;&#23398;&#29305;&#24449;&#65292;&#20294;&#23427;&#22312;&#19981;&#21516;&#30340;&#27491;&#21521;&#21644;&#36870;&#21521;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;MeLM&#26377;&#25928;&#22320;&#34920;&#31034;&#20102;&#19968;&#20010;&#30001;&#25968;&#20159;&#20010;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#24222;&#22823;&#22810;&#31890;&#23376;&#31995;&#32479;&#65292;&#36890;&#36807;&#24418;&#25104;&#22270;&#24418;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21457;&#29616;&#30456;&#20114;&#20316;&#29992;&#21183;&#33021;&#65292;&#28982;&#21518;&#21033;&#29992;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#21327;&#21516;&#25928;&#24212;&#26469;&#35782;&#21035;&#20986;&#29616;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#36864;&#21270;&#21147;&#23398;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the flexible nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13856</link><description>&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#36935;&#35265;&#35821;&#35328;&#65306;&#22686;&#24378;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#40784;&#20197;&#25903;&#25345;&#24207;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification. (arXiv:2306.13856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#24207;&#25968;&#20998;&#31867;&#12290;&#22312;&#24207;&#25968;&#20998;&#31867;&#20013;&#65292;&#26631;&#31614;&#21253;&#21547;&#39069;&#22806;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#22914;&#26524;&#20165;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21551;&#21457;&#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#20219;&#21153;&#36716;&#21270;&#20026;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#26469;&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#20013;&#20016;&#23500;&#30340;&#24207;&#25968;&#20808;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2RCLIP&#65292;&#23427;&#20174;&#20004;&#20010;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20102;&#35821;&#35328;&#20808;&#39564;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#65292;&#26088;&#22312;&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#12290;&#23427;&#22312;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#39118;&#26684;&#25552;&#31034;&#28151;&#21512;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#34701;&#20837;&#35821;&#35328;&#20808;&#39564;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#39321;&#33609;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#36817;&#20284;&#32465;&#23450;&#20248;&#21270;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20869;&#36827;&#34892;&#20102;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;&#65288;CMOCL&#65289;&#65292;&#29992;&#20110;&#35268;&#33539;&#20174;&#35821;&#35328;&#20013;&#23548;&#20986;&#30340;&#24207;&#25968;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#27969;&#34892;&#30340;&#24207;&#25968;&#20998;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11586</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25913;&#36827;&#26041;&#27861;&#21253;&#25324;&#22810;&#22270;&#31471;&#21475;&#32534;&#21495;&#12289;&#20010;&#20307;ID&#21644;&#21453;&#21521;&#28040;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20960;&#20046;&#21487;&#20197;&#24471;&#21040;&#23436;&#32654;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26816;&#27979;&#27927;&#38065;&#20132;&#26131;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#30340;&#23569;&#25968;&#31867;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#39640;&#36798;30%&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#26641;&#21644;GNN&#30340;&#22522;&#20934;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#25552;&#21319;&#20102;&#19977;&#20010;&#26631;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;</title><link>http://arxiv.org/abs/2306.06344</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#22330;&#26223;&#32423;&#20132;&#36890;&#20223;&#30495;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#20223;&#30495;&#26159;&#21152;&#36895;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21457;&#23637;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#23398;&#20064;&#30340;&#20132;&#36890;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#24456;&#38590;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTG++&#65292;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#27169;&#22411;&#39592;&#24178;&#32467;&#26500;&#65292;&#24182;&#19988;&#35201;&#26377;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#35821;&#35328;&#19982;&#20132;&#36890;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#26102;&#31354;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#22330;&#26223;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#29983;&#25104;&#20102;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26397;&#30528;&#26597;&#35810;&#21512;&#35268;&#30340;&#29983;&#25104;&#26041;&#21521;&#21069;&#36827;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; MASCHInE &#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29983;&#25104;&#21407;&#22411;&#22270;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#36827;&#32780;&#35757;&#32451;&#20986;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#30340; KGEs&#12290;</title><link>http://arxiv.org/abs/2306.03659</link><description>&lt;p&gt;
Schema First&#65281;&#36890;&#36807;MASCHInE&#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#36890;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; MASCHInE &#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29983;&#25104;&#21407;&#22411;&#22270;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#36827;&#32780;&#35757;&#32451;&#20986;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#30340; KGEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#30693;&#35782;&#22270;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21363;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGEs&#65289;&#12290;&#23398;&#20064;&#22810;&#21151;&#33021;&#30340;KGEs&#38750;&#24120;&#26377;&#24847;&#20041;&#65292;&#22240;&#20026;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;KGEMs&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#23884;&#20837;&#26159;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;KGEMs&#23454;&#38469;&#19978;&#26159;&#21542;&#21019;&#24314;&#20102;&#24213;&#23618;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#35821;&#20041;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#19968;&#36215;&#65292;&#23558;&#19981;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#19968;&#36215;&#65289;&#30340;&#26222;&#36941;&#20551;&#35774;&#21463;&#21040;&#20102;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#29983;&#25104;&#21407;&#22411;&#22270;-&#19968;&#20010;&#23567;&#22411;&#12289;&#20462;&#25913;&#36807;&#30340;KG&#29256;&#26412;&#65292;&#21033;&#29992;&#20102;RDF/S&#20449;&#24687;&#12290;&#25152;&#23398;&#20064;&#30340;&#22522;&#20110;&#21407;&#22411;&#22270;&#30340;&#23884;&#20837;&#26088;&#22312;&#23553;&#35013;KG&#30340;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#22312;&#23398;&#20064;KGEs&#26102;&#21152;&#20197;&#21033;&#29992;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#12290;&#23545;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16986</link><description>&lt;p&gt;
NavGPT: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#26174;&#24335;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models. (arXiv:2305.16986v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16986
&lt;/p&gt;
&lt;p&gt;
NavGPT&#26159;&#22522;&#20110;LLM&#30340;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#25191;&#34892;&#38646;-shot&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#32423;&#35268;&#21010;&#33021;&#21147;&#65292;&#21487;&#20197;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;NavGPT&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;ChatGPT&#21644;GPT-4&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#27169;&#22411;&#30340;&#25193;&#23637;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#36235;&#21183;&#24378;&#35843;&#20102;&#20351;&#29992;&#26080;&#38480;&#35821;&#35328;&#25968;&#25454;&#35757;&#32451;LLM&#30340;&#28508;&#21147;&#65292;&#25512;&#21160;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;&#31929;&#22522;&#20110;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#23548;&#33322;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#20026;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#25191;&#34892;&#38646;-shot&#30340;&#36830;&#32493;&#21160;&#20316;&#39044;&#27979;&#65292;&#25581;&#31034;&#20102;&#23545;&#20110;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#19979;GPT&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;NavGPT&#23558;&#35270;&#35273;&#35266;&#23519;&#12289;&#23548;&#33322;&#21382;&#21490;&#21644;&#26410;&#26469;&#21487;&#25506;&#32034;&#26041;&#21521;&#30340;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#25512;&#29702;&#20986;&#26234;&#33021;&#20307;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#20915;&#23450;&#22914;&#20309;&#25509;&#36817;&#30446;&#26631;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NavGPT&#21487;&#20197;&#26126;&#30830;&#22320;&#25191;&#34892;&#23548;&#33322;&#30340;&#39640;&#32423;&#35268;&#21010;&#65292;&#21253;&#25324;&#23558;&#25351;&#20196;&#20998;&#35299;&#25104;&#23376;&#30446;&#26631;&#12289;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#36827;&#34892;&#38556;&#30861;&#29289;&#36991;&#20813;&#65292;&#24182;&#21442;&#32771;&#20808;&#21069;&#30340;&#27493;&#39588;&#36827;&#34892;&#28548;&#28165;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#21487;&#33021;&#25104;&#20026;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#27969;&#31243;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#21697;&#65292;&#23637;&#31034;&#20102;&#36890;&#29992;&#20307;&#29616;&#26234;&#33021;&#20307;&#21457;&#23637;&#30340;&#32654;&#22909;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goal, integrating commonsense k
&lt;/p&gt;</description></item><item><title>Voyager&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#12289;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#21644;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#19981;&#26029;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#21644;&#22312;&#29609;Minecraft&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16291</link><description>&lt;p&gt;
Voyager:&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#26426;&#22120;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Voyager: An Open-Ended Embodied Agent with Large Language Models. (arXiv:2305.16291v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16291
&lt;/p&gt;
&lt;p&gt;
Voyager&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#12289;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#21644;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#19981;&#26029;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#21644;&#22312;&#29609;Minecraft&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Voyager&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#25345;&#32493;&#25506;&#32034;&#19990;&#30028;&#12289;&#33719;&#24471;&#22810;&#31181;&#25216;&#33021;&#21644;&#36827;&#34892;&#26032;&#30340;&#21457;&#29616;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31532;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#20840;&#33021;&#20195;&#29702;&#12290;Voyager&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;1)&#26368;&#22823;&#21270;&#25506;&#32034;&#30340;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#65292;2)&#29992;&#20110;&#23384;&#20648;&#21644;&#26816;&#32034;&#22797;&#26434;&#34892;&#20026;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#65292;&#21644;3)&#19968;&#31181;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#12289;&#25191;&#34892;&#38169;&#35823;&#21644;&#33258;&#25105;&#39564;&#35777;&#36827;&#34892;&#31243;&#24207;&#25913;&#36827;&#30340;&#26032;&#30340;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#12290;Voyager&#36890;&#36807;&#40657;&#30418;&#26597;&#35810;&#19982;GPT-4&#20114;&#21160;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#21442;&#25968;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;Voyager&#24320;&#21457;&#30340;&#25216;&#33021;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#36825;&#21152;&#24555;&#20102;&#20195;&#29702;&#30340;&#33021;&#21147;&#22686;&#38271;&#24182;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;Voyager&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#22411;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#29609;Minecraft&#26041;&#38754;&#23637;&#29616;&#20102;&#24322;&#24120;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.14381</link><description>&lt;p&gt;
&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;MCR&#65289;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#23545;&#40784;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#35813;&#33539;&#20363;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#30340;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#22810;&#27169;&#24577;&#19978;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#35757;&#32451;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;C-MCR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#27169;&#24577;&#23545;&#19978;&#39044;&#35757;&#32451;&#20004;&#20010;&#29616;&#26377;&#30340;MCR&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#22312;&#26032;&#31354;&#38388;&#20013;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#27169;&#24577;&#23545;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#22312;&#27599;&#20010;MCR&#20869;&#24050;&#32463;&#23545;&#40784;&#65292;&#22240;&#27492;&#36890;&#36807;&#37325;&#21472;&#27169;&#24577;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#12290;&#20026;&#20102;&#21457;&#25381;C-MCR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;&#30340;int
&lt;/p&gt;
&lt;p&gt;
Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26080;&#27880;&#37322;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#21484;&#22238;&#29575;&#30340;&#35789;&#20041;&#20849;&#23384;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.12818</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#35821;&#35328;&#35789;&#20041;&#20851;&#31995;&#22270;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs. (arXiv:2305.12818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12818
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26080;&#27880;&#37322;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#21484;&#22238;&#29575;&#30340;&#35789;&#20041;&#20849;&#23384;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#35821;&#35328;&#23398;&#20013;&#65292;&#35789;&#20041;&#20849;&#23384;&#25351;&#30340;&#26159;&#19968;&#20010;&#35789;&#27719;&#24418;&#24335;&#20256;&#36798;&#20004;&#20010;&#25110;&#26356;&#22810;&#19981;&#21516;&#30340;&#24847;&#20041;&#30340;&#29616;&#35937;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#24037;&#20316;&#22522;&#20110;&#27880;&#37322;&#30340;&#35789;&#27719;&#34920;&#65292;&#38480;&#21046;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#20174;&#26410;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#35782;&#21035;&#20102;&#36229;&#36807;2,000&#20010;&#27010;&#24565;&#22312;1,335&#31181;&#35821;&#35328;&#20013;&#30340;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24314;&#31435;&#22522;&#20110;&#35789;&#20041;&#20849;&#23384;&#27169;&#24335;&#30340;&#22810;&#35821;&#35328;&#22270;&#65306;ColexNet&#21644;ColexNet+&#12290;ColexNet&#30340;&#33410;&#28857;&#26159;&#27010;&#24565;&#65292;&#36793;&#26159;&#35789;&#20041;&#20849;&#23384;&#20851;&#31995;&#12290;&#22312;ColexNet+&#20013;&#65292;&#27010;&#24565;&#33410;&#28857;&#36890;&#36807;&#20013;&#38388;&#33410;&#28857;&#36827;&#34892;&#38468;&#21152;&#36830;&#25509;&#65292;&#27599;&#20010;&#20013;&#38388;&#33410;&#28857;&#20195;&#34920;1,334&#31181;&#35821;&#35328;&#20013;&#30340;&#19968;&#20010;ngram&#12290;&#25105;&#20204;&#20351;&#29992;ColexNet+&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23884;&#20837;&#65292;&#31216;&#20043;&#20026;$\overrightarrow{\mbox{ColexNet+}}$&#65292;&#38750;&#24120;&#36866;&#21512;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;ColexNet&#22312;&#36328;&#35821;&#35328;&#35789;&#20041;&#20849;&#23384;&#25968;&#25454;&#38598;CLICS&#19978;&#30340;&#39640;&#21484;&#22238;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#24314;&#35758;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. We then propose simple and effective methods to build multilingual graphs from the colexification patterns: ColexNet and ColexNet+. ColexNet's nodes are concepts and its edges are colexifications. In ColexNet+, concept nodes are additionally linked through intermediate nodes, each representing an ngram in one of 1,334 languages. We use ColexNet+ to train $\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that are well-suited for transfer learning. In our experiments, we first show that ColexNet achieves high recall on CLICS, a dataset of crosslingual colexifications. We then evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Logic-LM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21319;&#22312;&#22797;&#26434;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Logic-LM&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#27604;&#20351;&#29992;&#26631;&#20934;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25928;&#26524;&#20998;&#21035;&#25552;&#39640;&#20102;39.2%&#21644;18.4%&#12290;&#36825;&#34920;&#26126;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#26159;&#23454;&#29616;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12295</link><description>&lt;p&gt;
Logic-LM&#65306;&#36890;&#36807;&#31526;&#21495;&#27714;&#35299;&#22120;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning. (arXiv:2305.12295v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Logic-LM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#21319;&#22312;&#22797;&#26434;&#36923;&#36753;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#36827;&#34892;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#36827;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Logic-LM&#22312;&#22810;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#27604;&#20351;&#29992;&#26631;&#20934;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25928;&#26524;&#20998;&#21035;&#25552;&#39640;&#20102;39.2%&#21644;18.4%&#12290;&#36825;&#34920;&#26126;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#26159;&#23454;&#29616;&#20934;&#30830;&#36923;&#36753;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20102;&#20154;&#31867;&#19968;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#36923;&#36753;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;Logic-LM&#65292;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#20197;&#25913;&#36827;&#36923;&#36753;&#38382;&#39064;&#30340;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;LLMs&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#31526;&#21495;&#21270;&#34920;&#36848;&#12290;&#28982;&#21518;&#65292;&#30830;&#23450;&#24615;&#31526;&#21495;&#27714;&#35299;&#22120;&#23545;&#38382;&#39064;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#33258;&#25105;&#23436;&#21892;&#27169;&#22359;&#65292;&#21033;&#29992;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#38169;&#35823;&#20449;&#24687;&#20462;&#27491;&#31526;&#21495;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#22312;&#20116;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;ProofWriter&#12289;PrOntoQA&#12289;FOLIO&#12289;LogicalDeduction&#21644;AR-LSAT&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Logic-LM&#30340;&#26377;&#25928;&#24615;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;Logic-LM&#30456;&#27604;&#20165;&#20351;&#29992;LLMs&#30340;&#26631;&#20934;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;39.2%&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20351;&#29992;LLMs&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;18.4%&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;&#31526;&#21495;&#36923;&#36753;&#30456;&#32467;&#21512;&#65292;Logic-LM&#20026;&#20934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical r
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01328</link><description>&lt;p&gt;
IC3&#65306;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01328
&lt;/p&gt;
&lt;p&gt;
"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20320;&#35831;&#19968;&#20010;&#20154;&#25551;&#36848;&#19968;&#24133;&#22270;&#20687;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#29992;&#19968;&#21315;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;&#12290;&#20256;&#32479;&#19978;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#29983;&#25104;&#19968;&#20010;&#8220;&#26368;&#20339;&#8221;&#65288;&#19982;&#21442;&#32771;&#26368;&#30456;&#20284;&#65289;&#30340;&#22270;&#20687;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#40723;&#21169;&#29983;&#25104;&#8220;&#20449;&#24687;&#36139;&#20047;&#8221;&#30340;&#23383;&#24149;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#21487;&#33021;&#32454;&#33410;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#32780;&#24573;&#30053;&#20102;&#22330;&#26223;&#20013;&#20854;&#20182;&#21487;&#33021;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;"&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;"&#65288;IC3&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#39640;&#23618;&#32454;&#33410;&#30340;&#21333;&#20010;&#23383;&#24149;&#12290;&#20154;&#31867;&#35780;&#20215;IC3&#29983;&#25104;&#30340;&#23383;&#24149;&#33267;&#23569;&#19982;&#22522;&#20934;SOTA&#27169;&#22411;&#19968;&#26679;&#26377;&#24110;&#21161;&#30340;&#24773;&#20917;&#21344;&#20102;&#19977;&#20998;&#20043;&#20108;&#20197;&#19978;&#65292;&#24182;&#19988;IC3&#21487;&#20197;&#23558;SOTA&#33258;&#21160;&#21484;&#22238;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;84%&#65292;&#32988;&#36807;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#30456;&#27604;&#20110;SOTA&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#36890;&#36807;https://davidmchan&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single "best" (most like a reference) image caption. Unfortunately, doing so encourages captions that are "informationally impoverished," and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at https://davidmchan.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09724</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29616;&#26377;&#20107;&#23454;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20351;&#29992;&#33410;&#28857;&#30340;&#22270;&#37051;&#22495;&#25552;&#20379;&#20102;&#27604;&#20165;&#20351;&#29992;&#26597;&#35810;&#20449;&#24687;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;KG&#38142;&#25509;&#39044;&#27979;&#30340;GNNs&#36981;&#24490;&#25972;&#20010;KG&#19978;&#30340;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#35745;&#31639;&#12289;&#33410;&#28857;&#34920;&#31034;&#30340;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22823;&#35268;&#27169;&#19978;&#65292;&#20174;&#25972;&#20010;KG&#20013;&#32858;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#35745;&#31639;&#19978;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;KG&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#23454;&#20363;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;GNN&#20316;&#20026;r&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#35270;&#22270;&#31995;&#32479;&#20013;&#36827;&#34892;&#22810;&#20154;&#19977;&#32500;&#23039;&#21183;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#29289;&#30340;&#21807;&#19968;&#26631;&#35782;&#21644;&#20811;&#26381;&#22122;&#22768;&#21644;&#28508;&#22312;&#36974;&#25377;&#30340;&#25361;&#25112;&#12290;&#27492;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;3D&#26631;&#27880;&#12290;</title><link>http://arxiv.org/abs/2212.08731</link><description>&lt;p&gt;
&#20174;&#26080;&#26631;&#27880;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#20154;&#19977;&#32500;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-person 3D pose estimation from unlabelled data. (arXiv:2212.08731v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#35270;&#22270;&#31995;&#32479;&#20013;&#36827;&#34892;&#22810;&#20154;&#19977;&#32500;&#23039;&#21183;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#29289;&#30340;&#21807;&#19968;&#26631;&#35782;&#21644;&#20811;&#26381;&#22122;&#22768;&#21644;&#28508;&#22312;&#36974;&#25377;&#30340;&#25361;&#25112;&#12290;&#27492;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;3D&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20154;&#19977;&#32500;&#23039;&#21183;&#20272;&#35745;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#22810;&#35270;&#22270;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#30456;&#26426;&#25552;&#20379;&#30340;2D&#20449;&#24687;&#65292;&#38656;&#35201;&#39318;&#20808;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#21807;&#19968;&#26631;&#35782;&#27599;&#20010;&#20154;&#29289;&#12290;&#27492;&#22806;&#65292;&#20174;&#22810;&#35270;&#22270;2D&#20449;&#24687;&#20013;&#20272;&#35745;&#27599;&#20010;&#20154;&#29289;&#30340;&#19977;&#32500;&#23039;&#21183;&#38656;&#35201;&#20811;&#26381;&#22122;&#22768;&#21644;&#28508;&#22312;&#36974;&#25377;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#39044;&#27979;&#22330;&#26223;&#20013;&#20154;&#29289;&#30340;&#36328;&#35270;&#22270;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#20174;2D&#28857;&#29983;&#25104;&#27599;&#20010;&#20154;&#29289;&#30340;&#19977;&#32500;&#23039;&#21183;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#21253;&#21547;3D&#26631;&#27880;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Its numerous applications make multi-human 3D pose estimation a remarkably impactful area of research. Nevertheless, assuming a multiple-view system composed of several regular RGB cameras, 3D multi-pose estimation presents several challenges. First of all, each person must be uniquely identified in the different views to separate the 2D information provided by the cameras. Secondly, the 3D pose estimation process from the multi-view 2D information of each person must be robust against noise and potential occlusions in the scenario. In this work, we address these two challenges with the help of deep learning. Specifically, we present a model based on Graph Neural Networks capable of predicting the cross-view correspondence of the people in the scenario along with a Multilayer Perceptron that takes the 2D points to yield the 3D poses of each person. These two models are trained in a self-supervised manner, thus avoiding the need for large datasets with 3D annotations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2212.07469</link><description>&lt;p&gt;
&#36890;&#36807;"&#31283;&#23450;&#36793;&#32536;"&#23398;&#20064;&#38408;&#20540;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Learning threshold neurons via the "edge of stability". (arXiv:2212.07469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20998;&#26512;&#36890;&#24120;&#22522;&#20110;&#26497;&#23567;&#23398;&#20064;&#29575;&#30340;&#19981;&#29616;&#23454;&#20551;&#35774;&#12290;&#19982;&#23454;&#38469;&#26234;&#24935;&#21644;&#32463;&#39564;&#30740;&#31350;&#30456;&#21453;&#65292;&#20363;&#22914;J. Cohen&#31561;&#20154;&#30340;&#24037;&#20316;&#65288;ICLR 2021&#65289;&#65292;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26032;&#29616;&#35937;&#65288;"&#31283;&#23450;&#36793;&#32536;"&#25110;"&#19981;&#31283;&#23450;&#25910;&#25947;"&#65289;&#65292;&#20197;&#21450;&#22823;&#23398;&#20064;&#29575;&#20307;&#21046;&#19979;&#30340;&#28508;&#22312;&#27867;&#21270;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#21518;&#19968;&#31181;&#25928;&#24212;&#20173;&#28982;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#36808;&#20986;&#20102;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#30495;&#27491;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#27493;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#23574;&#38160;&#30340;&#38454;&#36291;&#36716;&#21464;&#65292;&#24403;&#27493;&#38271;&#23567;&#20110;&#27492;&#20540;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#21040;"&#38408;&#20540;&#26679;&#24335;"&#31070;&#32463;&#20803;&#65288;&#21363;&#20855;&#26377;&#38750;&#38646;&#31532;&#19968;&#23618;&#20559;&#32622;&#30340;&#31070;&#32463;&#20803;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the "edge of stability" or "unstable convergence") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn "threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This elu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08238</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#20010;&#27861;&#24459;&#26696;&#20363;&#30340;&#20107;&#23454;&#25551;&#36848;&#25991;&#26412;&#65292;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#39044;&#27979;&#26696;&#20363;&#30340;&#32618;&#21517;&#12289;&#27861;&#24459;&#26465;&#27454;&#21644;&#22788;&#32602;&#26399;&#38480;&#12290;LJP&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#20854;&#20013;&#21482;&#23384;&#22312;&#24494;&#22937;&#30340;&#25991;&#26412;&#24046;&#24322;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#20998;&#31867;&#25439;&#22833;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#24573;&#30053;&#20102;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#25968;&#23383;&#65292;&#29992;&#20110;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;moco&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#21487;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#26500;&#24314;&#27491;&#20363;&#23545;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#21516;&#26102;&#26377;&#21033;&#20110;LJP&#30340;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#21033;&#29992;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#25968;&#23383;&#26469;&#39044;&#27979;&#26576;&#20123;&#26696;&#20363;&#30340;&#22788;&#32602;&#26399;&#38480;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30001;&#39044;&#35757;&#32451;&#25968;&#20540;&#27169;&#22411;&#32534;&#30721;&#30340;&#25552;&#21462;&#30340;&#29359;&#32618;&#37329;&#39069;&#23545;&#20107;&#23454;&#25551;&#36848;&#30340;&#34920;&#31034;&#12290;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13623</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;: &#25945;&#31243;&#65292;&#22238;&#39038;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#24050;&#32463;&#22312;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#37329;&#34701;&#65292;&#25512;&#33616;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#20197;&#21450;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#20854;&#28789;&#27963;&#30340;&#20248;&#21270;&#23646;&#24615;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#30740;&#31350;&#31354;&#38388;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#12289;&#29366;&#24577;&#34920;&#31034;&#12289;&#26102;&#38388;&#32467;&#26500;&#21644;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#26469;&#35299;&#20915;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05015</link><description>&lt;p&gt;
&#31890;&#23376;&#20449;&#24565;&#36817;&#20284;POMDP&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimality Guarantees for Particle Belief Approximation of POMDPs. (arXiv:2210.05015v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#33324;&#29702;&#35770;&#26469;&#38480;&#23450;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#25552;&#20379;&#20102;&#29616;&#23454;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#28789;&#27963;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;POMDP&#30340;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;&#21644;&#35266;&#27979;&#31354;&#38388;&#26159;&#36830;&#32493;&#25110;&#28151;&#21512;&#30340;&#26102;&#20505;&#65292;&#36825;&#22312;&#29289;&#29702;&#31995;&#32479;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#23613;&#31649;&#26368;&#36817;&#20351;&#29992;&#35266;&#27979;&#20284;&#28982;&#26435;&#37325;&#31574;&#21010;&#30340;&#22312;&#32447;&#37319;&#26679;POMDP&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20808;&#21069;&#24182;&#27809;&#26377;&#25552;&#20986;&#19968;&#33324;&#29702;&#35770;&#26469;&#21051;&#30011;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#30340;&#31890;&#23376;&#28388;&#27874;&#25216;&#26415;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38480;&#23450;&#20219;&#20309;POMDP&#19982;&#20854;&#30456;&#24212;&#30340;&#26377;&#38480;&#26679;&#26412;&#31890;&#23376;&#20449;&#24565;MDP(PB-MDP)&#36924;&#36817;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#36825;&#31181;PB-MDP&#21644;POMDP&#20043;&#38388;&#30340;&#22522;&#30784;&#26725;&#26753;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#30456;&#24212;&#30340;&#31890;&#23376;&#20449;&#24565;MDP&#23558;&#20219;&#20309;&#37319;&#26679;MDP&#31639;&#27861;&#36866;&#24212;&#21040;POMDP&#20013;&#65292;&#20174;&#32780;&#23558;MDP&#31639;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#25193;&#23637;&#21040;POMDP&#20013;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#22312;&#35299;&#20915;&#20855;&#26377;&#22823;&#30340;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;POMDP&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness, a general theory characterizing the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is bounding the error between any POMDP and its corresponding finite sample particle belief MDP (PB-MDP) approximation. This fundamental bridge between PB-MDPs and POMDPs allows us to adapt any sampling-based MDP algorithm to a POMDP by solving the corresponding particle belief MDP, thereby extending the convergence guarantees of the MDP algorithm to the POMDP. Practically, thi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#38544;&#31169;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.04030</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#31169;&#19982;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#27602;&#21270;&#25915;&#20987;&#20013;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#38544;&#31169;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#21033;&#29992;&#20998;&#24067;&#24335;&#29992;&#25143;&#30340;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21487;&#20449;&#20219;&#30340;&#19981;&#21516;&#29992;&#25143;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;FL&#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#20445;&#25252;&#26412;&#22320;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;FL&#36890;&#24120;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65288;DPFL&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#24046;&#20998;&#38544;&#31169;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#20309;&#31181;&#32852;&#31995;&#65311;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;DPFL&#30340;&#38544;&#31169;&#23646;&#24615;&#20026;FL&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#65311;&#25105;&#20204;&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#36827;FL&#30340;&#38544;&#31169;&#20197;&#25552;&#39640;&#36825;&#31181;&#31283;&#23450;&#24615;&#35748;&#35777;&#65311;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;FL&#30340;&#29992;&#25143;&#32423;&#21644;&#23454;&#20363;&#32423;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20998;&#26512;&#20197;&#23454;&#29616;&#25552;&#39640;&#23454;&#20363;&#32423;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31283;&#23450;&#24615;&#35748;&#35777;&#25351;&#26631;&#65306;&#35748;&#35777;&#39044;&#27979;&#21644;&#35748;&#35777;&#25915;&#20987;&#26080;&#25928;&#24615;&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#23454;&#20363;&#32423;DPFL&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) provides an efficient paradigm to jointly train a global model leveraging data from distributed users. As local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the privacy of local users, FL is usually trained in a differentially private way (DPFL). Thus, in this paper, we ask: What are the underlying connections between differential privacy and certified robustness in FL against poisoning attacks? Can we leverage the innate privacy property of DPFL to provide certified robustness for FL? Can we further improve the privacy of FL to improve such robustness certification? We first investigate both user-level and instance-level privacy of FL and provide formal privacy analysis to achieve improved instance-level privacy. We then provide two robustness certification criteria: certified prediction and certified attack inefficacy for DPFL on both user and i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2207.07025</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20132;&#27969;&#26469;&#36827;&#34892;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Learning to translate by learning to communicate. (arXiv:2207.07025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#20219;&#21153;&#28608;&#21169;&#27169;&#22411;&#26469;&#25913;&#36827;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32039;&#24613;&#36890;&#20449;&#65288;EC&#65289;&#21644;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#25913;&#36827;&#20102;&#29616;&#20195;&#38750;&#30417;&#30563;NMT&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#24050;&#26377;&#35266;&#28857;&#35748;&#20026;&#65292;&#24403;&#21069;&#22312;NLP&#39046;&#22495;&#20027;&#23548;&#22320;&#20301;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#27861;&#20135;&#29983;&#31283;&#20581;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#65292;&#24182;&#31361;&#20986;&#20102;&#23545;&#22522;&#20110;&#30446;&#26631;&#12289;&#30446;&#26631;&#23548;&#21521;&#21644;&#20132;&#20114;&#24335;&#35821;&#35328;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBART&#65292;Liu&#31561;&#65292;2020&#65289;&#23884;&#20837;&#21040;&#19968;&#20010;EC&#22270;&#20687;&#21442;&#32771;&#28216;&#25103;&#20013;&#65292;&#27169;&#22411;&#34987;&#28608;&#21169;&#20351;&#29992;&#22810;&#35821;&#35328;&#29983;&#25104;&#26469;&#23436;&#25104;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#36825;&#23558;&#20351;&#22810;&#31181;&#35821;&#35328;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#20219;&#21153;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;EC&#24494;&#35843;&#30340;&#21464;&#20307;&#65288;Steinert-Threlkeld&#31561;&#20154;&#65292;2022&#65289;&#65292;&#20854;&#20013;&#19968;&#31181;&#22312;&#21253;&#25324;&#36164;&#28304;&#21294;&#20047;&#30340;&#23612;&#27850;&#23572;&#35821;&#22312;&#20869;&#30340;&#22235;&#31181;&#35821;&#35328;&#20013;&#37117;&#20248;&#20110;&#20165;&#20351;&#29992;&#22238;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formulate and test a technique to use Emergent Communication (EC) with a pre-trained multilingual model to improve on modern Unsupervised NMT systems, especially for low-resource languages. It has been argued that the current dominant paradigm in NLP of pre-training on text-only corpora will not yield robust natural language understanding systems, and the need for grounded, goal-oriented, and interactive language learning has been high lighted. In our approach, we embed a multilingual model (mBART, Liu et al., 2020) into an EC image-reference game, in which the model is incentivized to use multilingual generations to accomplish a vision-grounded task. The hypothesis is that this will align multiple languages to a shared task space. We present two variants of EC Fine-Tuning (Steinert-Threlkeld et al., 2022), one of which outperforms a backtranslation-only baseline in all four languages investigated, including the low-resource language Nepali.
&lt;/p&gt;</description></item><item><title>PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.07940</link><description>&lt;p&gt;
PROFHIT: &#38754;&#21521;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07940
&lt;/p&gt;
&lt;p&gt;
PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#31181;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#20998;&#23618;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#19978;&#20063;&#24341;&#20837;&#20102;&#20998;&#23618;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#40664;&#40664;&#22320;&#20551;&#35774;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#20998;&#23618;&#20851;&#31995;&#19968;&#33268;&#65292;&#24182;&#19988;&#19981;&#36866;&#24212;&#26174;&#31034;&#19982;&#27492;&#20551;&#35774;&#20559;&#31163;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHIT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#24615;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;PROFHIT&#37319;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#31867;&#21035;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#26469;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#32771;&#34385;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#36827;&#34892;&#20851;&#38190;&#28857;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.07162</link><description>&lt;p&gt;
&#26080;&#20851;&#31867;&#21035;&#30340;&#26377;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#30340;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#31867;&#21035;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#26469;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#32771;&#34385;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#36827;&#34892;&#20851;&#38190;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#19982;&#8220;&#23454;&#20363;&#32423;&#8221;&#21644;&#8220;&#31867;&#21035;&#32423;&#8221;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#26080;&#20851;&#31867;&#21035;&#30340;&#26041;&#24335;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#65292;&#20351;&#20854;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#23545;&#35937;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;RGB-D&#22270;&#20687;&#21644;&#30495;&#23454;&#20851;&#38190;&#28857;&#26469;&#35757;&#32451;&#32534;&#30721;&#22120;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#28508;&#22312;&#34920;&#31034;&#34987;&#21516;&#26102;&#20803;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#29992;&#20110;&#39044;&#27979;&#26032;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20851;&#38190;&#28857;&#39044;&#27979;&#30340;&#26032;&#39062;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26126;&#30830;&#32771;&#34385;&#21040;&#27599;&#20010;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312;linemod&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#23436;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel meta-learning approach for 6D pose estimation on unknown objects. In contrast to ``instance-level" and ``category-level" pose estimation methods, our algorithm learns object representation in a category-agnostic way, which endows it with strong generalization capabilities across object categories. Specifically, we employ a neural process-based meta-learning approach to train an encoder to capture texture and geometry of an object in a latent representation, based on very few RGB-D images and ground-truth keypoints. The latent representation is then used by a simultaneously meta-trained decoder to predict the 6D pose of the object in new images. Furthermore, we propose a novel geometry-aware decoder for the keypoint prediction using a Graph Neural Network (GNN), which explicitly takes geometric constraints specific to each object into consideration. To evaluate our algorithm, extensive experiments are conducted on the \linemod dataset, and on our new fully-annotated s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.14276</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31639;&#27861;&#19981;&#26029;&#31361;&#30772;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#39046;&#22495;&#22806;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#38476;&#29983;&#39046;&#22495;&#30340;&#22810;&#28304;&#36866;&#24212;&#38382;&#39064;&#65306;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#20013;&#27867;&#21270;&#21040;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#24615;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#36866;&#24212;&#65306;&#19968;&#20010;T5&#32534;&#30721;-&#35299;&#30721;&#22120;&#39318;&#20808;&#20174;&#36755;&#20837;&#31034;&#20363;&#20013;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#31614;&#21517;&#34987;&#19968;&#20010;&#36229;&#32593;&#32476;&#21033;&#29992;&#26469;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;29&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#24773;&#24863;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20004;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24050;&#26377;&#31639;&#27861;&#12290;&#22312;&#39640;&#32423;&#29256;&#26412;&#20013;&#65292;&#31614;&#21517;&#36824;&#20016;&#23500;&#20102;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#24494;&#35843;&#26550;&#26500;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains' semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier's weights. We evaluated our method across two tasks - sentiment classification and natural language inference - in 29 adaptation scenarios, where it outpaced established algorithms. In an advanced version, the signature also enriches the input example's representation. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#21160;&#28857;&#29702;&#35770;&#30340;&#33258;&#28982;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#30340;&#22810;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#21442;&#25968;&#21306;&#38388;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2105.09407</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#35843;&#31639;&#23376;&#29702;&#35770;&#36827;&#34892;&#19977;&#23618;&#21644;&#22810;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trilevel and Multilevel Optimization using Monotone Operator Theory. (arXiv:2105.09407v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.09407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#21160;&#28857;&#29702;&#35770;&#30340;&#33258;&#28982;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#30340;&#22810;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#21442;&#25968;&#21306;&#38388;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31867;&#30456;&#24403;&#36890;&#29992;&#30340;&#22810;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20984;&#30446;&#26631;&#20989;&#25968;&#38656;&#35201;&#22312;&#28385;&#36275;&#23884;&#22871;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#24615;&#32422;&#26463;&#19979;&#36827;&#34892;&#26368;&#23567;&#21270;&#12290;&#20316;&#20026;&#29305;&#20363;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#19977;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20004;&#20010;&#36739;&#20302;&#23618;&#30340;&#30446;&#26631;&#21253;&#25324;&#24179;&#28369;&#39033;&#21644;&#38750;&#24179;&#28369;&#39033;&#30340;&#27714;&#21644;&#12290;&#22522;&#20110;&#19981;&#21160;&#28857;&#29702;&#35770;&#21644;&#30456;&#20851;&#35770;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#19968;&#38454;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#20960;&#20010;&#21442;&#25968;&#21306;&#38388;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider rather a general class of multi-level optimization problems, where a convex objective function is to be minimized subject to constraints of optimality of nested convex optimization problems. As a special case, we consider a trilevel optimization problem, where the objective of the two lower layers consists of a sum of a smooth and a non-smooth term.~Based on fixed-point theory and related arguments, we present a natural first-order algorithm and analyze its convergence and rates of convergence in several regimes of parameters.
&lt;/p&gt;</description></item></channel></rss>