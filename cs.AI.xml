<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01351</link><description>&lt;p&gt;
&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#30340;&#21333;&#32431;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25925;&#38556;&#20195;&#29702;&#30340;&#35748;&#30693;&#36923;&#36753;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#21333;&#32431;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20801;&#35768;&#19990;&#30028;&#20013;&#21442;&#19982;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#12290;&#36825;&#23545;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#20316;&#32773;&#19968;&#30452;&#22312;&#30740;&#31350;&#21333;&#32431;&#27169;&#22411;&#65292;&#36825;&#26159;&#22522;&#20110;&#31216;&#20026;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#39640;&#32500;&#32467;&#26500;&#30340;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#12290;&#22312;&#26368;&#21021;&#30340;&#24418;&#24335;&#20013;&#65292;&#21333;&#32431;&#27169;&#22411;&#22987;&#32456;&#34987;&#20551;&#35774;&#20026;&#32431;&#31929;&#30340;&#65292;&#24847;&#21619;&#30528;&#25152;&#26377;&#19990;&#30028;&#20855;&#26377;&#30456;&#21516;&#30340;&#32500;&#24230;&#12290;&#36825;&#30456;&#24403;&#20110;&#22522;&#20110;&#20811;&#37324;&#26222;&#20811;&#27169;&#22411;&#30340;&#26631;&#20934;S5n&#35821;&#20041;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36890;&#36807;&#31227;&#38500;&#27169;&#22411;&#24517;&#39035;&#26159;&#32431;&#31929;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#36229;&#36234;&#24120;&#35268;&#30340;&#20811;&#37324;&#26222;&#20811;&#35821;&#20041;&#65292;&#24182;&#30740;&#31350;&#21442;&#19982;&#19968;&#20010;&#19990;&#30028;&#30340;&#20195;&#29702;&#25968;&#37327;&#21487;&#20197;&#21464;&#21270;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#22312;&#35768;&#22810;&#35770;&#25991;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#20110;&#23481;&#38169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#20854;&#20013;&#22312;&#31995;&#32479;&#25191;&#34892;&#26399;&#38388;&#21487;&#33021;&#20250;&#21457;&#29983;&#36807;&#31243;&#23849;&#28291;&#12290;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22312;&#23450;&#20041;&#19981;&#32431;&#30340;&#21333;&#32431;&#27169;&#22411;&#26102;&#65292;&#24494;&#22937;&#30340;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#24471;&#21040;&#30340;&#36923;&#36753;&#30340;&#19981;&#21516;&#20844;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20844;&#29702;&#21270;&#30456;&#24212;&#30340;&#35748;&#30693;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several authors have been investigating simplicial models, a model of epistemic logic based on higher-dimensional structures called simplicial complexes. In the original formulation, simplicial models were always assumed to be pure, meaning that all worlds have the same dimension. This is equivalent to the standard S5n semantics of epistemic logic, based on Kripke models. By removing the assumption that models must be pure, we can go beyond the usual Kripke semantics and study epistemic logics where the number of agents participating in a world can vary. This approach has been developed in a number of papers, with applications in fault-tolerant distributed computing where processes may crash during the execution of a system. A difficulty that arises is that subtle design choices in the definition of impure simplicial models can result in different axioms of the resulting logic. In this paper, we classify those design choices systematically, and axiomatize the correspon
&lt;/p&gt;</description></item><item><title>AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01305</link><description>&lt;p&gt;
AWEQ&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01305
&lt;/p&gt;
&lt;p&gt;
AWEQ&#26159;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#28608;&#27963;&#26435;&#37325;&#22343;&#34913;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#30340;&#22343;&#34913;&#26041;&#27861;&#20943;&#23567;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#30456;&#23545;&#36739;&#39640;&#12290;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#30828;&#20214;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AWEQ&#65292;&#19968;&#31181;&#21518;&#35757;&#32451;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#24320;&#38144;&#12290;AWEQ&#22312;&#36229;&#20302;&#20301;&#37327;&#21270;&#21644;8-bit&#26435;&#37325;&#21644;&#28608;&#27963;(W8A8)&#37327;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#35266;&#23519;&#21040;&#26435;&#37325;&#37327;&#21270;&#27604;&#28608;&#27963;&#37327;&#21270;&#26356;&#23481;&#26131;&#12290;AWEQ&#36890;&#36807;&#36890;&#36947;&#22343;&#34913;&#23558;&#28608;&#27963;&#37327;&#21270;&#30340;&#38590;&#24230;&#36716;&#31227;&#21040;&#26435;&#37325;&#19978;&#65292;&#23454;&#29616;&#20102;&#20004;&#32773;&#37327;&#21270;&#22256;&#38590;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22343;&#34913;&#26041;&#27861;&#65292;&#20943;&#23567;&#20102;&#37327;&#21270;&#20559;&#24046;&#35823;&#24046;&#65292;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#20687;LLaMA&#36825;&#26679;&#30340;&#27969;&#34892;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2311.00797</link><description>&lt;p&gt;
&#28436;&#21270;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#65306;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#20182;&#20204;&#35782;&#21035;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#25551;&#36848;&#32593;&#32476;&#30340;&#28436;&#21270;&#34892;&#20026;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#32597;&#35265;&#30340;&#22823;&#24133;&#24230;&#38598;&#20307;&#25391;&#33633;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#24335;&#30740;&#31350;&#33258;&#36866;&#24212;&#26131;&#24863;-&#24863;&#26579;-&#26131;&#24863;(SIS)&#27969;&#34892;&#30149;&#23398;&#32593;&#32476;&#30340;&#20020;&#30028;&#36716;&#25240;&#28857;&#38598;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#21463;&#25968;&#20540;&#38543;&#26426;&#31215;&#20998;&#22120;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;ResNet&#26550;&#26500;&#65292;&#35782;&#21035;&#20986;&#19968;&#20010;&#21442;&#25968;&#30456;&#20851;&#30340;&#22522;&#20110;&#29289;&#29702;&#24847;&#20041;&#30340;&#31895;&#31890;&#24230;&#22343;&#22330;&#21464;&#37327;&#30340;&#26377;&#25928;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(eSDE)&#12290;&#25105;&#20204;&#22522;&#20110;eSDE&#30340;&#30830;&#23450;&#20559;&#31227;&#39033;&#26500;&#24314;&#20102;&#19968;&#20010;&#36817;&#20284;&#30340;&#26377;&#25928;&#20998;&#23700;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#22343;&#22330;SIS&#27169;&#22411;&#30340;&#20998;&#23700;&#22270;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#28436;&#21270;&#32593;&#32476;&#30340;&#26377;&#25928;SIS&#21160;&#21147;&#23398;&#20013;&#30340;&#27425;&#20020;&#30028;Hopf&#20998;&#23700;&#65292;&#23427;&#24341;&#36215;&#20102;&#20020;&#30028;&#36716;&#25240;&#34892;&#20026;&#65307;&#36825;&#34920;&#29616;&#20026;&#22823;&#24133;&#24230;&#30340;&#38598;&#20307;&#25391;&#33633;&#65292;&#23427;&#20204;&#20174;(&#22122;&#22768;&#30340;)&#22266;&#23450;&#29366;&#24577;&#30340;&#37051;&#22495;&#20013;&#33258;&#21457;&#22320;&#12289;&#32597;&#35265;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#22797;&#30340;&#26292;&#21147;&#27169;&#25311;&#21644;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#25968;&#23398;&#24037;&#20855;&#30740;&#31350;&#20102;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
&lt;/p&gt;</description></item><item><title>MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00334</link><description>&lt;p&gt;
MetisFL:&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#24037;&#20316;&#27969;&#30340;&#23604;&#23596;&#24182;&#34892;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00334
&lt;/p&gt;
&lt;p&gt;
MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#31995;&#32479;&#36890;&#24120;&#30001;&#20004;&#20010;&#26680;&#24515;&#22788;&#29702;&#23454;&#20307;&#32452;&#25104;:&#32852;&#37030;&#25511;&#21046;&#22120;&#21644;&#23398;&#20064;&#22120;&#12290;&#25511;&#21046;&#22120;&#36127;&#36131;&#31649;&#29702;&#22312;&#23398;&#20064;&#22120;&#20043;&#38388;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#31243;&#65292;&#23398;&#20064;&#22120;&#36127;&#36131;&#22312;&#20854;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#32852;&#37030;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#26102;&#65292;FL&#31995;&#32479;&#23545;&#21442;&#19982;&#23398;&#20064;&#22120;&#30340;&#35745;&#31639;&#36164;&#28304;&#25110;&#25968;&#25454;&#27809;&#26377;&#25511;&#21046;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;FL&#31995;&#32479;&#26469;&#20419;&#36827;FL&#24037;&#20316;&#27969;&#30340;&#24320;&#21457;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#22823;&#22810;&#25968;&#24573;&#35270;&#20102;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MetisFL&#30340;&#26032;&#22411;FL&#31995;&#32479;&#65292;&#20854;&#20013;&#32852;&#37030;&#25511;&#21046;&#22120;&#26159;&#31532;&#19968;&#31561;&#20844;&#27665;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.18122</link><description>&lt;p&gt;
OpinSummEval:&#20877;&#32771;&#33258;&#21160;&#21270;&#35780;&#20272;&#22312;&#24847;&#35265;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization. (arXiv:2310.18122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23545;&#24847;&#35265;&#25688;&#35201;&#36827;&#34892;&#33258;&#21160;&#21270;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#65292;&#20294;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#24847;&#35265;&#25688;&#35201;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25688;&#35201;&#20219;&#21153;&#19981;&#21516;&#65292;&#24847;&#35265;&#25688;&#35201;&#19987;&#27880;&#20110;&#35266;&#28857;&#21644;&#24773;&#24863;&#65292;&#22240;&#27492;&#19982;&#20247;&#19981;&#21516;&#12290;&#34429;&#28982;&#20687;ROUGE&#36825;&#26679;&#30340;&#26576;&#20123;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23545;&#35780;&#20272;&#24847;&#35265;&#25688;&#35201;&#30340;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;OpinSummEval&#65292;&#23427;&#21253;&#25324;&#26469;&#33258;14&#20010;&#24847;&#35265;&#25688;&#35201;&#27169;&#22411;&#30340;&#20154;&#24037;&#21028;&#26029;&#21644;&#36755;&#20986;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;24&#20010;&#33258;&#21160;&#24230;&#37327;&#19982;&#20154;&#24037;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#36890;&#24120;&#20248;&#20110;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#22522;&#20110;&#24378;&#22823;&#27169;&#22411;&#65288;&#22914;BART&#21644;GPT-3/3.5&#65289;&#26500;&#24314;&#30340;&#24230;&#37327;&#20063;&#19981;&#33021;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#22987;&#32456;&#20445;&#25345;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#25913;&#36827;&#24847;&#35265;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#21487;&#29992;&#20110;https://github.com/A-Chicharito-S/OpinSummEval/tree/main&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2310.16452</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#25512;&#33616;&#20013;&#30340;&#24544;&#23454;&#36335;&#24452;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PEARLM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#24320;&#23637;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20381;&#36182;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#65292;&#36824;&#36991;&#20813;&#20102;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36335;&#24452;&#25512;&#29702;&#26041;&#27861;&#22312;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#36879;&#26126;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEARLM&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#26377;&#25928;&#25429;&#33719;&#29992;&#25143;&#34892;&#20026;&#21644;&#20135;&#21697;&#31471;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#36335;&#24452;&#20013;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#24182;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#32479;&#19968;&#22312;&#21516;&#19968;&#20248;&#21270;&#31354;&#38388;&#20013;&#12290;&#24207;&#21015;&#35299;&#30721;&#30340;&#32422;&#26463;&#20445;&#35777;&#20102;&#36335;&#24452;&#23545;&#30693;&#35782;&#22270;&#35889;&#30340;&#24544;&#23454;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11715</link><description>&lt;p&gt;
&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#32454;&#31890;&#24230;NER&#22330;&#26223;&#19979;&#24120;&#24120;&#38754;&#20020;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24212;&#29992;K-shot&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#24403;&#27880;&#37322;&#25968;&#37327;&#36229;&#36807;&#20960;&#21313;&#20010;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#24448;&#24448;&#36798;&#21040;&#39281;&#21644;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#19968;&#31181;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#39044;&#35757;&#32451;&#65292;&#23427;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#26080;&#27861;&#30452;&#25509;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23613;&#31649;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#24456;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#65288;F2C&#65289;&#26144;&#23556;&#30697;&#38453;&#30340;&#32454;&#31890;&#24230;NER&#27169;&#22411;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#19982;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#30340;&#31895;&#31890;&#24230;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10705</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;&#21322;&#23548;&#20307;&#26230;&#22278;&#22320;&#22270;&#20013;&#32570;&#38519;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65306;&#19968;&#39033;&#35843;&#26597;&#12289;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#35782;&#21035;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#20026;&#19981;&#21516;&#31639;&#27861;&#21644;&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20005;&#35880;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#35782;&#21035;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#26230;&#22278;&#32570;&#38519;&#30340;&#26041;&#27861;&#23398;&#12290;&#23613;&#31649;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;ML&#22312;&#26230;&#22278;&#32570;&#38519;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#32570;&#20047;&#20840;&#38754;&#30340;&#32508;&#36848;&#12290;&#26412;&#25991;&#35797;&#22270;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#25991;&#29486;&#65292;&#28145;&#20837;&#20998;&#26512;&#21508;&#31181;ML&#31639;&#27861;&#22312;&#26230;&#22278;&#32570;&#38519;&#26816;&#27979;&#39046;&#22495;&#30340;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#23398;&#20998;&#31867;&#20307;&#31995;&#65292;&#35814;&#32454;&#20998;&#31867;&#20102;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#23376;&#25216;&#26415;&#21010;&#20998;&#12290;&#36825;&#20010;&#20998;&#31867;&#20307;&#31995;&#20174;&#24191;&#27867;&#30340;&#26041;&#27861;&#23398;&#31867;&#21035;&#24320;&#22987;&#65292;&#21040;&#20855;&#20307;&#30340;&#23376;&#25216;&#26415;&#32467;&#26463;&#12290;&#23427;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#19981;&#21516;&#31639;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#25216;&#26415;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#20005;&#35880;&#30340;&#23454;&#35777;&#21644;&#23454;&#39564;&#35780;&#20272;&#26469;&#39564;&#35777;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08660</link><description>&lt;p&gt;
&#22312;&#19981;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;RL&#31574;&#30053;&#65306;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36895;&#29575;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#21151;&#29575;&#25511;&#21046;&#12289;&#27874;&#26463;&#25104;&#24418;&#21644;&#24178;&#25200;&#28040;&#38500;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#22522;&#31449;&#19982;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#36890;&#20449;&#30340;&#24773;&#20917;&#12290;&#30001;&#20110;&#31351;&#20030;&#25628;&#32034;&#30340;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20197;&#20854;&#38590;&#20197;&#20934;&#30830;&#24314;&#27169;&#30340;&#34892;&#20026;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;RL&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#20415;&#20195;&#29702;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22833;&#36133;&#30340;&#39640;&#25104;&#26412;&#65292;&#23558;&#31639;&#27861;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#25506;&#32034;&#21644;&#23398;&#20064;&#26159;&#19981;&#26126;&#26234;&#30340;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25511;&#21046;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#19968;&#31181;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03059</link><description>&lt;p&gt;
Point-PEFT: &#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03059
&lt;/p&gt;
&lt;p&gt;
Point-PEFT&#26159;&#19968;&#31181;&#29992;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#65292;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#65292;&#20197;&#26368;&#23567;&#21270;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#21033;&#29992;&#20869;&#23384;&#24211;&#21644;&#20934;&#30830;&#30340;&#32858;&#21512;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27969;&#34892;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#31561;&#39046;&#22495;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#19979;&#28216;&#20219;&#21153;&#30340;&#36866;&#24212;&#25104;&#26412;&#65292;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#29992;&#20110;&#35821;&#35328;&#21644;2D&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19987;&#38376;PEFT&#26041;&#27861;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-PEFT&#65292;&#19968;&#31181;&#29992;&#20110;&#36866;&#24212;&#28857;&#20113;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20854;&#20855;&#26377;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#30340;3D&#27169;&#22411;&#65292;&#25105;&#20204;&#20923;&#32467;&#22823;&#37096;&#20998;&#21442;&#25968;&#65292;&#21482;&#24494;&#35843;&#26032;&#22686;&#30340;PEFT&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#21253;&#25324;Point-prior Prompt&#21644;Geometry-aware Adapter&#12290;Point-prior Prompt&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20869;&#23384;&#24211;&#26469;&#22686;&#24378;&#25552;&#31034;&#26631;&#35760;&#30340;&#21442;&#25968;&#26080;&#20851;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;Geometry-aware Adapter&#26088;&#22312;&#23545;&#19981;&#21516;&#20219;&#21153;&#25110;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.00023</link><description>&lt;p&gt;
De-SaTE&#65306;&#29992;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#39044;&#27979;&#30340;&#21435;&#22122;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;De-SaTE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#20197;&#21450;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#65292;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#25552;&#20379;&#20851;&#38190;&#25351;&#26631;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#20026;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#20379;&#30005;&#21040;&#25512;&#21160;&#30005;&#21160;&#27773;&#36710;&#21644;&#25903;&#25345;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#26377;&#25928;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#20934;&#30830;&#39044;&#27979;&#20854;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#65292;&#36825;&#26159;&#39044;&#38450;&#24615;&#32500;&#25252;&#21644;&#39044;&#27979;&#24615;&#20998;&#26512;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#20010;&#21435;&#22122;&#27169;&#22359;&#30340;&#33021;&#37327;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#32463;&#36807;&#35757;&#32451;&#26469;&#22788;&#29702;&#30005;&#27744;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22122;&#22768;&#31867;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23567;&#27874;&#21435;&#22122;&#22120;&#26469;&#29983;&#25104;&#32534;&#30721;/&#20998;&#35299;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#36890;&#36807;&#19987;&#29992;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#32534;&#30721;&#22120;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;NASA&#21644;CALCE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#22810;&#31181;&#22122;&#22768;&#27169;&#24335;&#19979;&#30340;&#24191;&#27867;&#20581;&#24247;&#25351;&#26631;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25253;&#21578;&#30340;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10987</link><description>&lt;p&gt;
Spiking NeRF&#65306;&#20351;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#31359;&#36879;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#20855;&#26377;&#28508;&#22312;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#20197;&#22823;&#37327;&#33021;&#37327;&#28040;&#32791;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#28145;&#20837;&#25506;&#32034;&#20197;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#36827;&#34892;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33033;&#20914;NeRF&#65288;SpikingNeRF&#65289;&#65292;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;SNN&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;SNN&#23545;&#36752;&#23556;&#22330;&#30340;&#37325;&#24314;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#20197;&#22522;&#20110;&#33033;&#20914;&#12289;&#26080;&#20056;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;SpikingNeRF&#20013;&#65292;&#20809;&#32447;&#19978;&#30340;&#27599;&#20010;&#37319;&#26679;&#28857;&#21305;&#37197;&#21040;&#29305;&#23450;&#30340;&#26102;&#38388;&#27493;&#65292;&#24182;&#20197;&#28151;&#21512;&#26041;&#24335;&#34920;&#31034;&#65292;&#20854;&#20013;&#20307;&#32032;&#32593;&#26684;&#20063;&#24471;&#21040;&#32500;&#25252;&#12290;&#22522;&#20110;&#20307;&#32032;&#32593;&#26684;&#65292;&#30830;&#23450;&#37319;&#26679;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#34987;&#23631;&#34109;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25805;&#20316;&#20063;&#20250;&#20135;&#29983;&#19981;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15991</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving. (arXiv:2308.15991v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24635;&#26159;&#24314;&#31435;&#22312;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#65288;&#22914;&#35268;&#21010;&#22120;&#21644;&#25511;&#21046;&#22120;&#65289;&#20043;&#19978;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#23545;&#20110;&#36825;&#20123;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#21407;&#22987;&#20363;&#31243;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#23545;&#27169;&#22411;&#65288;&#22914;&#19978;&#19979;&#25991;&#21644;&#21160;&#21147;&#23398;&#65289;&#20570;&#20986;&#20102;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#19981;&#36275;&#20197;&#24212;&#23545;&#30495;&#23454;&#31995;&#32479;&#20013;&#30340;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#12290;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#26082;&#24102;&#26469;&#20102;&#24378;&#20581;&#24615;&#21448;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#36890;&#36807;&#20197;&#26080;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36816;&#34892;&#36712;&#36857;&#36319;&#36394;&#26469;&#22686;&#24378;&#20102;&#36890;&#29992;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#24403;&#21069;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems are always built on motion-related modules such as the planner and the controller. An accurate and robust trajectory tracking method is indispensable for these motion-related modules as a primitive routine. Current methods often make strong assumptions about the model such as the context and the dynamics, which are not robust enough to deal with the changing scenarios in a real-world system. In this paper, we propose a Deep Reinforcement Learning (DRL)-based trajectory tracking method for the motion-related modules in autonomous driving systems. The representation learning ability of DL and the exploration nature of RL bring strong robustness and improve accuracy. Meanwhile, it enhances versatility by running the trajectory tracking in a model-free and data-driven manner. Through extensive experiments, we demonstrate both the efficiency and effectiveness of our method compared to current methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#65288;dQC&#65289;&#24037;&#20855;&#65292;&#29992;&#20110;DNN&#20998;&#21106;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13488</link><description>&lt;p&gt;
&#20020;&#26102;&#19981;&#30830;&#23450;&#24615;&#23450;&#20301;&#20197;&#23454;&#29616;&#20154;&#22312;&#22238;&#36335;&#20998;&#26512;&#21160;&#24577;&#22686;&#24378;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets. (arXiv:2308.13488v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#65288;dQC&#65289;&#24037;&#20855;&#65292;&#29992;&#20110;DNN&#20998;&#21106;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22686;&#24378;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;DCE-CMRI&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#35786;&#26029;&#24515;&#32908;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#24322;&#24120;&#30340;&#27169;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25195;&#25551;&#20013;&#65292;&#20250;&#22312;&#21508;&#31181;&#23545;&#27604;&#24230;&#8220;&#36827;&#20837;/&#31163;&#24320;&#8221;&#38454;&#27573;&#33719;&#21462;&#25509;&#36817;300&#24133;&#24515;&#32908;&#28748;&#27880;&#30340;&#26102;&#24207;&#22270;&#20687;&#12290;&#23545;&#20110;&#27599;&#19968;&#24103;DCE&#22270;&#20687;&#24207;&#21015;&#20013;&#24515;&#32908;&#36718;&#24275;&#30340;&#25163;&#21160;&#20998;&#21106;&#21487;&#33021;&#26159;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38750;&#21018;&#24615;&#36816;&#21160;&#26657;&#27491;&#22833;&#36133;&#25110;&#19981;&#21487;&#29992;&#26102;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26174;&#31034;&#20986;&#23545;DCE-CMRI&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#19968;&#31181;&#21487;&#38752;&#26816;&#27979;&#22833;&#36133;&#20998;&#21106;&#30340;&#8220;&#21160;&#24577;&#36136;&#37327;&#25511;&#21046;&#8221;&#65288;dQC&#65289;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#22522;&#20110;DNN&#30340;&#33258;&#30001;&#21628;&#21560;DCE-CMRI&#25968;&#25454;&#38598;&#20998;&#21106;&#30340;dQC&#24037;&#20855;&#65292;&#24182;&#36890;&#36807;&#22312;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#20154;&#22312;&#22238;&#36335;&#26694;&#26550;&#26469;&#25913;&#21892;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast "wash in/out" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed or is unavailable. While deep neural networks (DNNs) have shown promise for analyzing DCE-CMRI datasets, a "dynamic quality control" (dQC) technique for reliably detecting failed segmentations is lacking. Here we propose a new space-time uncertainty metric as a dQC tool for DNN-based segmentation of free-breathing DCE-CMRI datasets by validating the proposed metric on an external dataset and establishing a human-in-the-loop framework to improve the segmentation results. In the proposed approach,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07336</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#21644;&#24418;&#24335;&#36923;&#36753;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26356;&#27867;&#21270;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20174;&#21512;&#25104;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#20855;&#20307;&#30340;&#28436;&#32462;&#35268;&#21017;&#26469;&#29983;&#25104;&#28436;&#32462;&#31034;&#20363;&#65292;&#20294;&#36825;&#20123;&#35268;&#21017;&#21463;&#38480;&#25110;&#32773;&#26159;&#20219;&#24847;&#30340;&#12290;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#25152;&#33719;&#24471;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#24182;&#37319;&#29992;&#22522;&#20110;&#24418;&#24335;&#36923;&#36753;&#29702;&#35770;&#30340;&#19968;&#32452;&#33391;&#22909;&#22522;&#30784;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#24403;&#36825;&#20123;&#35268;&#21017;&#20197;&#22810;&#27493;&#26041;&#24335;&#32452;&#21512;&#26102;&#65292;&#21487;&#20197;&#25512;&#23548;&#20986;&#20219;&#20309;&#20854;&#20182;&#28436;&#32462;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#25552;&#20986;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LMs&#65292;&#21363;$\textbf{FLD}$&#65288;$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction&#65289;&#65292;&#33719;&#24471;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#28436;&#32462;&#25512;&#29702;&#35821;&#26009;&#24211;&#21487;&#20197;&#22686;&#24378;LMs&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#38754;&#65292;&#20197;&#21450;&#19981;&#21516;&#26041;&#38754;&#26080;&#27861;&#22686;&#24378;&#30340;&#26041;&#38754;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#28436;&#32462;&#35821;&#26009;&#24211;&#25110;&#20854;&#20182;&#26041;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#26041;&#38754;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.04586</link><description>&lt;p&gt;
AIs&#30340;&#21457;&#23637;&#33073;&#38772;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developmental Bootstrapping of AIs. (arXiv:2308.04586v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04586
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#30340;&#25361;&#25112;&#65292;&#28982;&#32780;&#65292;&#21457;&#23637;&#33073;&#38772;&#27861;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#20799;&#31461;&#30340;&#33021;&#21147;&#21457;&#23637;&#36807;&#31243;&#65292;&#20026;&#21019;&#24314;&#31283;&#20581;&#21487;&#38752;&#30340;AI&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#19968;&#20123;AI&#22312;&#23553;&#38381;&#30340;&#19990;&#30028;&#65292;&#22914;&#26827;&#30424;&#28216;&#25103;&#20013;&#36229;&#36234;&#20102;&#20154;&#31867;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#28151;&#20081;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#26377;&#38480;&#12290;&#23427;&#20204;&#20250;&#29359;&#22855;&#24618;&#30340;&#38169;&#35823;&#32780;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#12290;&#23427;&#20204;&#24456;&#38590;&#21463;&#21040;&#25351;&#23548;&#65292;&#19981;&#33021;&#36816;&#29992;&#24120;&#35782;&#65292;&#32570;&#20047;&#22909;&#22855;&#24515;&#12290;&#23427;&#20204;&#19981;&#33021;&#25104;&#20026;&#33391;&#22909;&#30340;&#21512;&#20316;&#32773;&#12290;&#20256;&#32479;&#25163;&#21160;&#26500;&#24314;&#30340;&#31526;&#21495;AI&#26041;&#27861;&#26500;&#24314;&#30340;&#31995;&#32479;&#21644;&#20351;&#29992;&#29983;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;AI&#26041;&#27861;(&#21253;&#25324;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;)&#26500;&#24314;&#30340;&#31995;&#32479;&#37117;&#26080;&#27861;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#20204;&#19981;&#36866;&#21512;&#21019;&#24314;&#24378;&#22823;&#21644;&#21487;&#20449;&#36182;&#30340;AI&#12290;&#23613;&#31649;&#27492;&#26041;&#27861;&#19981;&#23646;&#20110;&#20027;&#27969;&#30340;AI&#26041;&#27861;&#65292;&#20294;&#21457;&#23637;&#33073;&#38772;&#27861;&#26174;&#31034;&#20986;&#24076;&#26395;&#12290;&#22312;&#21457;&#23637;&#33073;&#38772;&#27861;&#20013;&#65292;AI&#20687;&#20154;&#31867;&#20799;&#31461;&#19968;&#26679;&#21457;&#23637;&#33021;&#21147;&#12290;&#23427;&#20204;&#20174;&#20808;&#22825;&#33021;&#21147;&#24320;&#22987;&#12290;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#24182;&#20174;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#36890;&#36807;&#33258;&#25105;&#21457;&#23637;&#30340;&#33021;&#21147;&#36880;&#27493;&#25193;&#23637;&#20808;&#22825;&#33021;&#21147;&#12290;&#23427;&#20204;&#20114;&#21160;&#24182;&#36880;&#28176;&#23558;&#25152;&#23398;&#24212;&#29992;&#20110;&#23454;&#38469;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although some current AIs surpass human abilities especially in closed worlds such as board games, their performance in the messy real world is limited. They make strange mistakes and do not notice them. They cannot be instructed easily, fail to use common sense, and lack curiosity. They do not make good collaborators. Neither systems built using the traditional manually-constructed symbolic AI approach nor systems built using generative and deep learning AI approaches including large language models (LLMs) can meet the challenges. They are not well suited for creating robust and trustworthy AIs. Although it is outside of mainstream AI approaches, developmental bootstrapping shows promise. In developmental bootstrapping, AIs develop competences like human children do. They start with innate competences. Like humans, they interact with the environment and learn from their interactions. They incrementally extend their innate competences with self-developed competences. They interact and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#23454;&#29616;&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#26102;&#38388;&#30028;&#38480;&#21644;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2307.12897</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#30340;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#20219;&#24847;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#23454;&#29616;&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#26102;&#38388;&#30028;&#38480;&#21644;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36172;&#21338;&#20248;&#21270;&#20013;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#38656;&#35201;&#22312;&#34892;&#21160;&#36873;&#25321;&#26041;&#38754;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#65292;&#36824;&#38656;&#35201;&#22312;&#27169;&#22411;&#36873;&#25321;&#26041;&#38754;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#20381;&#36182;&#20110;&#23558;&#19981;&#21516;&#27169;&#22411;&#35270;&#20026;&#19987;&#23478;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36951;&#25022;&#26041;&#38754;&#19982;&#27169;&#22411;&#25968;&#37327;$M$&#30340;&#35268;&#27169;&#65288;$\text{poly}M$&#65289;&#21576;&#19981;&#33391;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#22312;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26377;&#21033;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#26469;&#27169;&#25311;&#20840;&#20449;&#24687;&#21453;&#39304;&#32473;&#22312;&#32447;&#23398;&#20064;&#32773;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24320;&#21457;&#20986;&#20855;&#26377;&#25351;&#25968;&#25913;&#36827;&#65288;$\log M$&#65289;&#22312;&#36951;&#25022;&#26041;&#38754;&#23545;$M$&#20381;&#36182;&#24615;&#30340;ALEXP&#12290;ALEXP&#22312;&#36951;&#25022;&#26041;&#38754;&#20855;&#26377;&#20219;&#24847;&#20445;&#35777;&#65292;&#24182;&#19988;&#26082;&#19981;&#38656;&#35201;&#23545;&#26102;&#38388;&#30028;$n$&#20855;&#26377;&#30693;&#35782;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#21021;&#22987;&#30340;&#32431;&#25506;&#32034;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Lasso&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#22343;&#21248;&#20998;&#26512;&#65292;&#24314;&#31435;&#20102;&#22312;&#32447;&#23398;&#20064;&#21644;&#39640;&#32500;&#32479;&#35745;&#20043;&#38388;&#30340;&#26032;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12166</link><description>&lt;p&gt;
&#27169;&#20223;&#28216;&#25103;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#26816;&#27979;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#38761;&#26032;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19981;&#21516;&#20307;&#35009;&#30340;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65306;&#35770;&#25991;&#12289;&#25925;&#20107;&#12289;&#35799;&#27468;&#21644;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#36825;&#20123;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20998;&#31867;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25925;&#20107;&#20889;&#20316;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;&#22810;&#31867;&#21035;&#20219;&#21153;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#29305;&#23450;LLM&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-ge
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.10455</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#29699;&#29983;&#29289;&#22810;&#26679;&#24615;&#35780;&#20272;&#36808;&#20986;&#30340;&#19968;&#27493;&#65306;BIOSCAN-1M&#26118;&#34411;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;BIOSCAN-Insect&#65292;&#29992;&#20110;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#26118;&#34411;&#29983;&#29289;&#22810;&#26679;&#24615;&#36827;&#34892;&#32534;&#30446;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25163;&#24037;&#26631;&#35760;&#26118;&#34411;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21363;BIOSCAN-Insect&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#30001;&#19987;&#23478;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#30456;&#20851;&#30340;&#36951;&#20256;&#20449;&#24687;&#65292;&#21253;&#25324;&#21407;&#22987;&#26680;&#33527;&#37240;&#26465;&#24418;&#30721;&#24207;&#21015;&#21644;&#20998;&#37197;&#30340;&#26465;&#24418;&#30721;&#32034;&#24341;&#21495;&#65292;&#36825;&#20123;&#26159;&#22522;&#20110;&#36951;&#20256;&#30340;&#29289;&#31181;&#20998;&#31867;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#36873;&#30340;&#30334;&#19975;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#35780;&#20272;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#36824;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#29305;&#24449;&#65292;&#23545;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20063;&#20855;&#26377;&#30740;&#31350;&#20215;&#20540;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22266;&#26377;&#30340;&#29983;&#29289;&#24615;&#36136;&#65292;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#38271;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20998;&#31867;&#26631;&#31614;&#26159;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#26041;&#26696;&#65292;&#22312;&#36739;&#20302;&#32423;&#21035;&#19978;&#21576;&#29616;&#20986;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#38500;&#20102;&#28608;&#21457;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#30740;&#31350;&#30340;&#20852;&#36259;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#36824;&#20419;&#36827;&#20102;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
&lt;/p&gt;</description></item><item><title>FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10317</link><description>&lt;p&gt;
FedBug: &#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10317
&lt;/p&gt;
&lt;p&gt;
FedBug&#26159;&#19968;&#20010;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20923;&#32467;&#21644;&#36880;&#28176;&#35299;&#20923;&#27169;&#22411;&#23618;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#32531;&#35299;&#23458;&#25143;&#31471;&#28418;&#31227;&#29616;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26694;&#26550;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20026;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#24615;&#65292;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#25311;&#21512;&#24182;&#19982;&#24444;&#27492;&#21457;&#25955;&#65292;&#36825;&#34987;&#31216;&#20026;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedBug&#65288;&#20855;&#26377;&#33258;&#24213;&#21521;&#19978;&#36880;&#28176;&#35299;&#20923;&#30340;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20943;&#36731;&#23458;&#25143;&#31471;&#28418;&#31227;&#12290;FedBug&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#27599;&#20010;&#20840;&#23616;&#36718;&#27425;&#26381;&#21153;&#22120;&#20998;&#21457;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#36328;&#23458;&#25143;&#31471;&#23545;&#40784;&#30340;&#21442;&#32771;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#19978;&#65292;FedBug&#20174;&#20923;&#32467;&#25972;&#20010;&#27169;&#22411;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#35299;&#20923;&#23618;&#65292;&#20174;&#36755;&#20837;&#23618;&#21040;&#36755;&#20986;&#23618;&#12290;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#35757;&#32451;&#35299;&#20923;&#30340;&#26032;&#23618;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20998;&#31163;&#36229;&#24179;&#38754;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;FedBug
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.01231</link><description>&lt;p&gt;
&#23545;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;(ER)&#26159;&#35782;&#21035;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#25351;&#21521;&#30456;&#21516;&#23454;&#20307;&#30340;&#35760;&#24405;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#35299;&#20915;ER&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21305;&#37197;&#38454;&#27573;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#23545;&#23454;&#39564;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#36827;&#34892;&#26816;&#26597;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#20010;&#24050;&#24314;&#31435;&#25968;&#25454;&#38598;&#30340;&#38590;&#24230;&#21644;&#36866;&#29992;&#24615;&#65306;&#20004;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#28041;&#21450;&#26032;&#30340;&#32447;&#24615;&#24230;&#37327;&#21644;&#29616;&#26377;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20197;&#21450;&#20004;&#31181;&#23454;&#38469;&#26041;&#27861;&#65306;&#26368;&#20339;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#21305;&#37197;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#20339;&#23398;&#20064;&#21305;&#37197;&#22120;&#21644;&#23436;&#32654;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#25968;&#25454;&#38598;&#37117;&#25552;&#20986;&#20102;&#30456;&#24403;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.16045</link><description>&lt;p&gt;
OpenNDD:&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16045
&lt;/p&gt;
&lt;p&gt;
OpenNDD&#26159;&#19968;&#20010;&#29992;&#20110;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#25216;&#26415;&#65292;&#33021;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#24182;&#35782;&#21035;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;(NDDs)&#26159;&#19968;&#32452;&#39640;&#24739;&#30149;&#29575;&#30340;&#38556;&#30861;&#65292;&#34920;&#29616;&#20986;&#20020;&#24202;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#24471;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#30340;NDDs&#65288;&#22914;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#21644;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#65288;ADHD&#65289;&#65289;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;NDDs&#35786;&#26029;&#24182;&#27809;&#26377;&#21487;&#38752;&#30340;&#29983;&#29702;&#26631;&#24535;&#29289;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#24515;&#29702;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26234;&#33021;&#36741;&#21161;&#35786;&#26029;&#26469;&#38450;&#27490;&#35823;&#35786;&#21644;&#28431;&#35786;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#19982;&#38543;&#21518;&#30340;&#30456;&#24212;&#27835;&#30103;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;NDDs&#31579;&#26597;&#21644;&#26816;&#27979;&#30340;&#24320;&#25918;&#24615;&#35782;&#21035;&#26694;&#26550;&#65292;&#36825;&#26159;&#22312;&#35813;&#39046;&#22495;&#20013;&#39318;&#27425;&#24212;&#29992;&#24320;&#25918;&#24615;&#35782;&#21035;&#12290;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#25239;&#24490;&#29615;&#28857;&#24320;&#25918;&#24615;&#35782;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#33021;&#22815;&#35782;&#21035;&#36807;&#21435;&#26410;&#36935;&#21040;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13948</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#65306;&#20171;&#32461;&#26131;&#20110;&#20351;&#29992;&#30340;PurpleAirSF&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#22810;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#33539;&#22260;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#20154;&#21592;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#24314;&#27169;&#21644;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#30740;&#31350;&#65292;&#21516;&#26102;&#27492;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26410;&#26469;&#24320;&#21457;&#26032;&#22411;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#32473;&#30740;&#31350;&#20154;&#21592;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PurpleAirSF&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20840;&#38754;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;PurpleAir&#32593;&#32476;&#20013;&#25910;&#38598;&#32780;&#26469;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#21508;&#31181;&#31354;&#27668;&#36136;&#37327;&#27979;&#37327;&#25351;&#26631;&#21644;&#24191;&#27867;&#30340;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26032;&#22411;&#39044;&#27979;&#27169;&#22411;&#12289;&#30740;&#31350;&#31354;&#27668;&#27745;&#26579;&#27169;&#24335;&#20197;&#21450;&#35843;&#26597;&#20854;&#23545;&#20581;&#24247;&#21644;&#29615;&#22659;&#30340;&#24433;&#21709;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;PurpleAirSF&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#32463;&#20856;&#21644;&#29616;&#20195;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#21046;&#23450;&#24314;&#31435;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13761</link><description>&lt;p&gt;
CeBed: &#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;CeBed&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20449;&#36947;&#20272;&#35745;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#20844;&#27491;&#21644;&#29616;&#23454;&#30340;&#27604;&#36739;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#24615;&#33021;&#36890;&#24120;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#36827;&#34892;&#27604;&#36739;&#12290;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#24037;&#20855;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#24211;&#65289;&#38459;&#30861;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#20449;&#36947;&#20272;&#35745;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24314;&#31435;&#22522;&#20934;&#27979;&#35797;&#30340;&#20513;&#35758;&#65292;&#32479;&#19968;&#20102;&#20960;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CeBed&#65288;&#20449;&#36947;&#20272;&#35745;&#27979;&#35797;&#24179;&#21488;&#65289;&#65292;&#21253;&#25324;&#28085;&#30422;&#21508;&#31181;&#31995;&#32479;&#27169;&#22411;&#21644;&#20256;&#25773;&#26465;&#20214;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#21313;&#20010;&#28145;&#24230;&#21644;&#20256;&#32479;&#30340;&#22522;&#32447;&#23454;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#39537;&#21160;OFDM&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#39046;&#22495;&#20869;&#23454;&#39564;&#26465;&#20214;&#19981;&#19968;&#33268;&#21644;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.11816</link><description>&lt;p&gt;
&#23398;&#20250;&#29983;&#25104;&#27604;&#20320;&#30340;LMM&#26356;&#22909;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; RLGF&#65292;&#29992;&#20110;&#22312; GPT-3 &#31561;&#21160;&#24577;&#40657;&#21283;&#23376;&#25351;&#23548;&#19979;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; LLM &#30340;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#30456;&#27604;&#36890;&#29992; RL &#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312; IMDB &#21644; CommonGen &#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#20363;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#26368;&#36817;&#30340;LLM&#65292;&#22914;ChatGPT&#21644;GPT - 4&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#24182;&#34701;&#21512;&#20102;RL&#21644;&#20154;&#31867;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#23398;&#20064;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#25506;&#32034;&#20102;&#36229;&#20986;&#36890;&#29992;RL&#31639;&#27861;&#22914;PPO&#20043;&#22806;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;RL&#31639;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21160;&#24577;&#40657;&#21283;&#23376;&#30340;&#25351;&#23548;LLM&#22914;GPT-3&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#24341;&#23548;&#21453;&#39304;&#30340;RL(RLGF)&#65292;&#36825;&#26159;&#19968;&#22871;&#29992;&#20110;LLM&#24494;&#35843;&#30340;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;GRUE&#22522;&#20934;&#27979;&#35797;&#30340;IMDB&#27491;&#21521;&#35780;&#35770;&#21644;CommonGen&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;RL&#31639;&#27861;&#27604;&#30417;&#30563;&#23398;&#20064;(SL)&#21644;&#40664;&#35748;PPO&#22522;&#32447;&#34920;&#29616;&#26356;&#39640;&#65292;&#35777;&#26126;&#20102;&#19982;&#25351;&#23548;LLM&#20114;&#21160;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#21644;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#36816;&#21160;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10065</link><description>&lt;p&gt;
&#39535;&#26381;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#21644;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#36816;&#21160;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20132;&#21709;&#20048;&#20013;&#29983;&#25104;&#25351;&#25381;&#23478;&#30340;&#21160;&#20316;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23398;&#20064;&#35821;&#20041;&#38899;&#20048;&#29305;&#24449;&#24182;&#25429;&#25417;&#30495;&#23454;&#25351;&#25381;&#21160;&#20316;&#30340;&#28508;&#22312;&#20998;&#24067;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24212;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#20294;&#21069;&#26223;&#20809;&#36861;&#36857;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20248;&#21183;&#65292;&#20294;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#36824;&#26410;&#34987;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#20197;&#25552;&#39640;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26045;&#21152;&#38468;&#21152;&#35268;&#21017;&#21270;&#21644;&#22686;&#21152;&#36816;&#21160;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20960;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;Frechet Gesture Distance&#65288;FGD&#65289;&#21644;Beat Consistency Score&#65288;BC&#65289;&#65292;&#20197;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04220</link><description>&lt;p&gt;
&#22312;&#34920;&#38754;&#20043;&#19979;&#23547;&#25214;&#65306;&#21033;&#29992;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;TDM&#65292;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#23567;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#35206;&#30422;&#33539;&#22260;&#12290;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#21644;&#38590;&#20197;&#25511;&#21046;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#23567;&#19988;&#35206;&#30422;&#33539;&#22260;&#29421;&#31364;&#65292;&#20174;&#32780;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#37096;&#32626;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#19979;&#26174;&#33879;&#25552;&#39640;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;(T-symmetry)&#24378;&#21046;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;(TDM)&#65292;&#24314;&#31435;&#20102;&#19968;&#23545;&#27491;&#21521;&#21644;&#21453;&#21521;&#28508;&#22312;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;TDM&#20026;&#23567;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;T-symmetry&#30340;&#31526;&#21512;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;OOD&#26679;&#26412;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.11755</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#65306;&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25512;&#33616;&#25552;&#20379;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#26159;&#23454;&#29616;&#36879;&#26126;&#19988;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#36755;&#20986;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#30784;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30446;&#26631;&#12289;&#35299;&#37322;&#33539;&#22260;&#12289;&#35299;&#37322;&#26679;&#24335;&#21644;&#35299;&#37322;&#26684;&#24335;&#36825;&#22235;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#23457;&#26597;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#35299;&#37322;&#30340;&#25991;&#29486;&#12290;&#35748;&#35782;&#21040;&#21487;&#35270;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20174;&#35299;&#37322;&#24615;&#35270;&#35273;&#26041;&#24335;&#30340;&#35282;&#24230;&#36884;&#24452;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#65292;&#21363;&#20351;&#29992;&#21487;&#35270;&#21270;&#20316;&#20026;&#35299;&#37322;&#30340;&#26174;&#31034;&#26679;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13525</link><description>&lt;p&gt;
&#20113;&#35745;&#31639;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#23545;&#20110;&#31649;&#29702;&#20113;&#25968;&#25454;&#20013;&#24515;&#24182;&#20445;&#35777;&#23458;&#25143;&#26368;&#20302;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#24314;&#27169;&#26410;&#26469;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#30001;&#20110;&#36164;&#28304;&#36807;&#24230;&#20998;&#37197;&#32780;&#24102;&#26469;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36164;&#28304;&#38656;&#27714;&#30340;&#20998;&#24067;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24773;&#26223;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36807;&#31243;&#26159;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#37197;&#32622;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27493;&#39588;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23558;&#21452;&#21464;&#37327;&#27169;&#22411;&#19982;&#20854;&#21333;&#21464;&#37327;&#23545;&#24212;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#24433;&#21709;QoS&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2303.10180</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23433;&#20840;&#30340;&#19993;&#27850;&#37210;&#20840;&#40635;&#21058;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#23454;&#29616;&#20840;&#40635;&#33647;&#29289;&#21058;&#37327;&#25511;&#21046;&#65292;&#28155;&#21152;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#21644;&#31574;&#30053;&#32422;&#26463;&#39033;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#40635;&#37257;&#26377;&#26395;&#23454;&#29616;&#26356;&#31934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#40635;&#37257;&#31649;&#29702;&#65292;&#20351;&#40635;&#37257;&#24072;&#20813;&#20110;&#37325;&#22797;&#24615;&#20219;&#21153;&#65292;&#19987;&#27880;&#20110;&#24739;&#32773;&#25163;&#26415;&#25252;&#29702;&#30340;&#26368;&#20851;&#38190;&#26041;&#38754;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#20110;&#21019;&#24314;&#27169;&#25311;&#29615;&#22659;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20294;&#31163;&#20020;&#24202;&#24212;&#29992;&#36824;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Policy Constraint Q-Learning(PCQL)&#26469;&#35299;&#20915;&#23398;&#20064;&#40635;&#37257;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#24341;&#20837;&#20102;&#20445;&#23432;Q-Learning&#26041;&#27861;&#20197;&#32531;&#35299;&#33073;&#32447;&#24773;&#20917;&#19979;Q&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#22312;&#26234;&#33021;&#20307;&#30340;&#22521;&#35757;&#20013;&#28155;&#21152;&#19968;&#39033;&#31574;&#30053;&#32422;&#26463;&#39033;&#65292;&#20197;&#20445;&#25345;&#26234;&#33021;&#20307;&#21644;&#40635;&#37257;&#24072;&#30340;&#31574;&#30053;&#20998;&#24067;&#19968;&#33268;&#65292;&#20197;&#30830;&#20445;&#26234;&#33021;&#20307;&#22312;&#20840;&#40635;&#24773;&#26223;&#19979;&#20570;&#20986;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PCQL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09447</link><description>&lt;p&gt;
&#20351;&#29992;Prompt-Tuning&#30340;&#21407;&#22411;&#36716;&#21521;&#38024;&#23545;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#24322;&#24615;&#25552;&#31034;&#35843;&#25972;&#26469;&#25552;&#39640;&#21407;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#38382;&#39064;&#12290;&#22522;&#20110;&#27492;&#27169;&#22411;&#30340;CPP&#26041;&#27861;&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#24615;&#33021;&#25509;&#36817;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20316;&#20026;&#31867;&#21035;&#23884;&#20837;&#30340;&#19968;&#31181;&#34920;&#31034;&#65292;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#30340;&#20869;&#23384;&#21344;&#29992;&#25110;&#20943;&#36731;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#35821;&#20041;&#28418;&#31227;&#21644;&#21407;&#22411;&#24178;&#25200;&#23548;&#33268;&#30340;&#24615;&#33021;&#24613;&#21095;&#24694;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21407;&#22411;&#25552;&#31034;&#65288;CPP&#65289;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#35843;&#25972;&#65292;&#24403;&#22312;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20004;&#20010;&#38556;&#30861;&#24182;&#26174;&#30528;&#25552;&#39640;&#21407;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CPP&#22312;&#22235;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;4%&#33267;6%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;CPP&#19981;&#38656;&#35201;&#37325;&#22797;&#35757;&#32451;&#65292;&#23427;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#36830;&#32493;&#23398;&#20064;&#21644;&#31163;&#32447;&#32852;&#21512;&#23398;&#20064;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#19979;&#36830;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11975</link><description>&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31526;&#21495;&#38899;&#20048;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#23383;&#33410;&#23545;&#32534;&#30721;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#24182;&#22686;&#21152;&#35789;&#27719;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19982;&#28145;&#24230;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#31526;&#21495;&#38899;&#20048;&#36890;&#24120;&#19982;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#30456;&#32467;&#21512;&#12290;&#20026;&#27492;&#65292;&#38899;&#20048;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#21363;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#31163;&#25955;&#30340;&#26631;&#35760;&#12290;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#26041;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#22240;&#20026;&#38899;&#20048;&#21487;&#20197;&#30001;&#21516;&#26102;&#23384;&#22312;&#30340;&#36712;&#36947;&#65292;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#30340;&#21516;&#26102;&#38899;&#31526;&#32452;&#25104;&#12290;&#30446;&#21069;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#35760;&#21270;&#20381;&#36182;&#20110;&#25551;&#36848;&#38899;&#31526;&#23646;&#24615;&#21644;&#26102;&#38388;&#20107;&#20214;&#30340;&#23567;&#22411;&#26631;&#35760;&#23383;&#20856;&#65292;&#23548;&#33268;&#26631;&#35760;&#24207;&#21015;&#30456;&#24403;&#38271;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#30340;&#20351;&#29992;&#19981;&#22815;&#20248;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#21512;&#24182;&#23884;&#20837;&#25110;&#32452;&#21512;&#26631;&#35760;&#26469;&#20943;&#23569;&#25972;&#20307;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23383;&#33410;&#23545;&#32534;&#30721;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20854;&#26174;&#33879;&#20943;&#23567;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#23884;&#20837;&#33021;&#21147;&#19982;&#26356;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#32467;&#21512;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.08487</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65306;&#19968;&#31687;&#25945;&#31243;&#20860;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semantics-Empowered Communication: A Tutorial-cum-Survey. (arXiv:2212.08487v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#26041;&#38754;&#30340;&#25945;&#31243;&#20860;&#32508;&#36848;&#65292;&#22238;&#39038;&#20102;&#25991;&#29486;&#65292;&#20171;&#32461;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#23558;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#21551;&#29992;&#25216;&#26415;&#30340;&#20998;&#31867;&#21644;&#26410;&#26469;&#24212;&#29992;&#22330;&#26223;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35821;&#20041;&#30340;&#27807;&#36890;&#65288;SemCom&#65289;&#30740;&#31350;&#30340;&#20852;&#36215;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20854;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#29702;&#35770;&#12289;&#24212;&#29992;&#12289;&#24230;&#37327;&#21644;&#23454;&#29616;&#65289;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#32972;&#26223;&#21644;&#30740;&#31350;&#20998;&#31867;&#65292;&#20197;&#21450;&#35814;&#32454;&#30340;&#25216;&#26415;&#25945;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#25991;&#29486;&#24182;&#22238;&#31572;&#20851;&#20110;&#35821;&#20041;&#20256;&#36755;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemCom&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#21382;&#21490;&#12289;&#29702;&#35770;&#12289;&#24230;&#37327;&#12289;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#65292;&#24182;&#20171;&#32461;&#20102;&#30740;&#31350;&#26041;&#21521;&#30340;&#20998;&#31867;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#22522;&#20110;&#25512;&#29702;&#30340;&#26041;&#27861;&#23545;&#20851;&#38190;&#30340;&#21551;&#29992;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#23427;&#20204;&#22914;&#20309;&#28436;&#21464;&#24182;&#20026;&#29616;&#20195;&#20869;&#23481;&#21644;&#36890;&#36947;&#35821;&#20041;&#39537;&#21160;&#30340;&#36890;&#20449;&#20570;&#20986;&#36129;&#29486;&#12290;&#38500;&#20102;&#22238;&#39038;&#21644;&#24635;&#32467;&#26368;&#26032;&#30340;e&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the springing up of the semantics-empowered communication (SemCom) research, it is now witnessing an unprecedentedly growing interest towards a wide range of aspects (e.g., theories, applications, metrics and implementations) in both academia and industry. In this work, we primarily aim to provide a comprehensive survey on both the background and research taxonomy, as well as a detailed technical tutorial. Specifically, we start by reviewing the literature and answering the "what" and "why" questions in semantic transmissions. Afterwards, we present the ecosystems of SemCom, including history, theories, metrics, datasets and toolkits, on top of which the taxonomy for research directions is presented. Furthermore, we propose to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content &amp; channel semantics-empowered communications. Besides reviewing and summarizing the latest e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;</title><link>http://arxiv.org/abs/2212.01382</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#21033;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;Q-learning&#31639;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#26469;&#20248;&#21270;&#31574;&#30053;&#12290;&#31639;&#27861;&#34987;&#35777;&#26126;&#26159;&#21487;&#25910;&#25947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#22810;&#20010;&#32500;&#24230;&#30340;&#21521;&#37327;&#20540;&#22870;&#21169;&#19978;&#21516;&#26102;&#33719;&#24471;&#39640;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#21463;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#26399;&#26395;&#31119;&#21033;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#38024;&#23545;&#21521;&#37327;&#30340;&#38271;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38750;&#32447;&#24615;&#20844;&#24179;&#31119;&#21033;&#20989;&#25968;&#12290;&#20854;&#20013;&#19968;&#20010;&#32463;&#20856;&#30340;&#20363;&#23376;&#26159;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#20989;&#25968;&#65292;&#25110;&#32773;&#20960;&#20309;&#24179;&#22343;&#25968;&#65292;&#20854;&#23545;&#25968;&#21464;&#25442;&#20063;&#34987;&#31216;&#20026;&#27604;&#20363;&#20844;&#24179;&#30446;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26399;&#26395;&#32435;&#20160;&#31038;&#20250;&#31119;&#21033;&#36827;&#34892;&#36817;&#20284;&#26368;&#20248;&#21270;&#20063;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;Q-learning&#25913;&#36827;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#23398;&#20064;&#26356;&#26032;&#21644;&#38750;&#31283;&#24577;&#21160;&#20316;&#36873;&#25321;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20248;&#21270;&#38750;&#32447;&#24615;&#31119;&#21033;&#20989;&#25968;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21487;&#25910;&#25947;&#30340;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#20013;&#36866;&#37197;&#22120;&#36335;&#30001;&#30340;&#20316;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;Multi-Head Routing (MHR)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#36335;&#30001;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.03831</link><description>&lt;p&gt;
&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#22810;&#22836;&#36866;&#37197;&#22120;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#20219;&#21153;&#27867;&#21270;&#20013;&#36866;&#37197;&#22120;&#36335;&#30001;&#30340;&#20316;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;Multi-Head Routing (MHR)&#65292;&#36890;&#36807;&#31934;&#32454;&#30340;&#36335;&#30001;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#36866;&#37197;&#20043;&#21069;&#36890;&#36807;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#38598;&#19978;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#26469;&#23454;&#29616;&#12290;Ponti&#31561;&#20154;&#30340;Polytropon [Ponti et al., 2023] ($\texttt{Poly}$)&#22312;&#39044;&#35757;&#32451;&#21644;&#23569;&#26679;&#26412;&#36866;&#37197;&#26399;&#38388;&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#32452;&#36866;&#37197;&#22120;&#21644;&#36873;&#25321;&#27599;&#20010;&#20219;&#21153;&#30340;&#36866;&#37197;&#22120;&#23376;&#38598;&#30340;&#36335;&#30001;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#36335;&#30001;&#22312;&#20854;&#25104;&#21151;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#21457;&#29616;&#35774;&#35745;&#20102;&#22522;&#20110;&#27492;&#30340;&#26032;&#21464;&#20307;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#26356;&#31934;&#32454;&#30340;&#36335;&#30001;&#33021;&#25552;&#20379;&#26356;&#22810;&#34920;&#36798;&#24615;&#30340;&#30452;&#35273;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Head Routing (MHR)&#65292;&#23427;&#32467;&#21512;&#20102;&#36866;&#37197;&#22120;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#22312;&#21487;&#27604;&#36739;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#34920;&#29616;&#26356;&#22909;&#65307;&#36890;&#36807;&#20165;&#24494;&#35843;&#36335;&#30001;&#20989;&#25968;&#32780;&#19981;&#26159;&#36866;&#37197;&#22120;(MHR-z)&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#26497;&#39640;&#21442;&#25968;&#25928;&#29575;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;Poly/MHR&#30340;&#20132;&#21449;&#27169;&#22411;&#36866;&#37197;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\texttt{MHR}$ (Multi-Head Routing), which combines $\textit{subsets}$ of adapter parameters and outperforms $\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\texttt{Poly}$/$\texttt{MHR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14750</link><description>&lt;p&gt;
&#38024;&#23545;&#20117;&#27979;&#25968;&#25454;&#30340;&#38750;&#23545;&#27604;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20117;&#27979;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#65292;&#37319;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#38750;&#23545;&#27604;&#24230;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20943;&#23569;&#23545;&#25968;&#25454;&#30340;&#26631;&#27880;&#38656;&#27714;&#65292;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30707;&#27833;&#21644;&#22825;&#28982;&#27668;&#34892;&#19994;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#26681;&#25454;&#38075;&#20117;&#25968;&#25454;&#20026;&#20117;&#27573;&#25552;&#20379;&#34920;&#31034;&#24418;&#24335;&#12290;&#20197;&#24448;&#30340;&#23581;&#35797;&#20027;&#35201;&#26159;&#26377;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#20851;&#27880;&#20110;&#30456;&#20284;&#24615;&#20219;&#21153;&#65292;&#21363;&#20272;&#35745;&#20117;&#27573;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#12290;&#25105;&#20204;&#24076;&#26395;&#22312;&#19981;&#20351;&#29992;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#19982;&#26377;&#30417;&#30563;&#33539;&#24335;&#30456;&#21453;&#65292;&#36825;&#20010;&#26041;&#27861;&#23545;&#25968;&#25454;&#38656;&#35201;&#24456;&#23569;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#29616;&#20170;&#65292;&#22823;&#22810;&#25968;SSL&#26041;&#27861;&#35201;&#20040;&#26159;&#23545;&#27604;&#30340;&#65292;&#35201;&#20040;&#26159;&#38750;&#23545;&#27604;&#30340;&#12290;&#23545;&#27604;&#26041;&#27861;&#20351;&#30456;&#20284;&#30340;&#65288;&#27491;&#65289;&#23545;&#35937;&#30340;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#25509;&#36817;&#65292;&#24182;&#23558;&#19981;&#21516;&#30340;&#65288;&#36127;&#65289;&#23545;&#35937;&#19982;&#20043;&#36317;&#31163;&#12290;&#30001;&#20110;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#30340;&#27491;&#36127;&#26631;&#27880;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#25552;&#20379;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#38750;&#23545;&#27604;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#27492;&#31867;&#26631;&#27880;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#23481;&#26131;&#35782;&#21035;&#30340;&#30456;&#20284;&#23545;&#35937;&#23545;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The representation learning problem in the oil &amp; gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item></channel></rss>