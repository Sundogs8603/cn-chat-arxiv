<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30446;&#26631;&#33021;&#26174;&#33879;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#19982;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#30456;&#20284;&#65292;&#36825;&#20123;&#36741;&#21161;&#30446;&#26631;&#20063;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.06089</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#27169;&#20223;&#22823;&#33041;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predictive auxiliary objectives in deep RL mimic learning in the brain. (arXiv:2310.06089v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;&#34920;&#31034;&#23398;&#20064;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30446;&#26631;&#33021;&#26174;&#33879;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#19982;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#30456;&#20284;&#65292;&#36825;&#20123;&#36741;&#21161;&#30446;&#26631;&#20063;&#27169;&#25311;&#20102;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#20107;&#20214;&#30340;&#33021;&#21147;&#34987;&#20551;&#35774;&#20026;&#33258;&#28982;&#21644;&#26426;&#22120;&#35748;&#30693;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#36825;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#25903;&#25345;&#65292;&#20854;&#20013;&#33258;&#30417;&#30563;&#36741;&#21161;&#30446;&#26631;&#65288;&#22914;&#39044;&#27979;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#34920;&#31034;&#23398;&#20064;&#21644;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#36741;&#21161;&#30446;&#26631;&#23545;RL&#31995;&#32479;&#20013;&#19981;&#21516;&#27169;&#22359;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#36825;&#20123;&#27169;&#25311;&#22823;&#33041;&#35266;&#23519;&#21040;&#30340;&#34920;&#24449;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26550;&#26500;&#20013;&#65292;&#39044;&#27979;&#30446;&#26631;&#29305;&#21035;&#25552;&#39640;&#21644;&#31283;&#23450;&#23398;&#20064;&#65292;&#24182;&#19988;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#38271;&#30340;&#39044;&#27979;&#26102;&#27573;&#22312;&#25903;&#25345;&#34920;&#24449;&#36801;&#31227;&#26041;&#38754;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;RL&#31995;&#32479;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#19982;&#22823;&#33041;&#20013;&#35266;&#23519;&#21040;&#30340;&#31070;&#32463;&#27963;&#21160;&#21464;&#21270;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#36741;&#21161;&#39044;&#27979;&#27169;&#22411;&#21644;&#22823;&#33041;&#20013;&#30340;&#34920;&#24449;&#21464;&#21270;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.16620</link><description>&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36229;&#21442;&#25968;&#36716;&#31227;&#65306;&#21160;&#24577;&#21644;&#32553;&#25918;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#20419;&#20351;&#20174;&#19994;&#32773;&#23547;&#25214;&#20351;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#20195;&#29702;&#26041;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;&#20854;&#20013;&#19968;&#20010;&#24314;&#35758;&#20351;&#29992;$\mu$P&#21442;&#25968;&#21270;&#32593;&#32476;&#65292;&#20854;&#20013;&#23567;&#23485;&#24230;&#32593;&#32476;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#21040;&#20219;&#24847;&#23485;&#24230;&#30340;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#36229;&#21442;&#25968;&#19981;&#20250;&#22312;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;$1/\sqrt{\text{depth}}$&#30340;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#21442;&#25968;&#21270;&#35757;&#32451;&#30340;&#27531;&#24046;&#32467;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;ResNet&#21644;Vision Transformer&#65292;&#22312;CIFAR-10&#21644;ImageNet&#19978;&#23637;&#31034;&#20102;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#21457;&#29616;&#24471;&#21040;&#20102;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#21160;&#26426;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.03886</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21151;&#33021;&#35299;&#37322;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#35813;&#22871;&#20214;&#21253;&#25324;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21487;&#35835;&#30340;&#25551;&#36848;&#26631;&#35760;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22359;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#36825;&#20123;&#25551;&#36848;&#21487;&#20197;&#26292;&#38706;&#22833;&#36133;&#12289;&#24341;&#23548;&#24178;&#39044;&#65292;&#29978;&#33267;&#21487;&#20197;&#35299;&#37322;&#37325;&#35201;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#26800;&#21407;&#29702;&#30340;&#24050;&#35757;&#32451;&#32593;&#32476;&#25551;&#36848;&#37117;&#28041;&#21450;&#21040;&#23567;&#27169;&#22411;&#12289;&#29421;&#20041;&#29616;&#35937;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#12290;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#24615;&#20013;&#26631;&#35760;&#20986;&#25152;&#26377;&#20154;&#21487;&#35299;&#37322;&#30340;&#23376;&#35745;&#31639;&#20960;&#20046;&#32943;&#23450;&#38656;&#35201;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#21644;&#39564;&#35777;&#25551;&#36848;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#30340;&#25216;&#26415;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#26377;&#38480;&#19988;&#20020;&#26102;&#12290;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#39564;&#35777;&#21644;&#27604;&#36739;&#24320;&#25918;&#24335;&#26631;&#35760;&#24037;&#20855;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FIND&#65288;&#20989;&#25968;&#35299;&#37322;&#21644;&#25551;&#36848;&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35299;&#37322;&#26041;&#27861;&#26500;&#24314;&#27169;&#22359;&#30340;&#22522;&#20934;&#22871;&#20214;&#12290;FIND&#21253;&#21547;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#31995;&#32479;&#30340;&#32452;&#20214;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03251</link><description>&lt;p&gt;
&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#65292;&#37319;&#29992;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#25193;&#23637;&#65292;&#34701;&#20837;&#20102;&#26102;&#38388;&#32500;&#24230;&#12290;&#22312;TKGs&#19978;&#36827;&#34892;&#25512;&#29702;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#22522;&#20110;&#21382;&#21490;&#20107;&#20214;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#12290;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#25581;&#31034;&#21382;&#21490;&#23376;&#22270;&#21644;&#26102;&#38388;&#27169;&#24335;&#20013;&#30340;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#38752;&#23454;&#20307;&#24314;&#27169;&#26469;&#27169;&#25311;TKGs&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#33410;&#28857;&#22312;&#30693;&#35782;&#34920;&#31034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#20250;&#20986;&#29616;&#26032;&#23454;&#20307;&#12290;&#36825;&#20351;&#24471;&#20381;&#36182;&#20110;&#23454;&#20307;&#30340;&#26041;&#27861;&#24456;&#38590;&#24212;&#23545;&#22823;&#37327;&#23454;&#20307;&#65292;&#24182;&#19988;&#26377;&#25928;&#22788;&#29702;&#26032;&#20986;&#29616;&#30340;&#23454;&#20307;&#20063;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#31070;&#32463;&#32593;&#32476;&#65288;TiPNN&#65289;&#65292;&#23427;&#20197;&#23454;&#20307;&#29420;&#31435;&#30340;&#35282;&#24230;&#23545;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TiPNN&#37319;&#29992;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#65292;&#21517;&#20026;&#21382;&#21490;&#26102;&#38388;&#22270;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20020;&#26102;&#24402;&#32435;&#36335;&#24452;&#25552;&#21462;&#32467;&#26500;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph (KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial task that aims to predict future facts based on historical occurrences. The key challenge lies in uncovering structural dependencies within historical subgraphs and temporal patterns. Most existing approaches model TKGs relying on entity modeling, as nodes in the graph play a crucial role in knowledge representation. However, the real-world scenario often involves an extensive number of entities, with new entities emerging over time. This makes it challenging for entity-dependent methods to cope with extensive volumes of entities, and effectively handling newly emerging entities also becomes a significant challenge. Therefore, we propose Temporal Inductive Path Neural Network (TiPNN), which models historical information in an entity-independent perspective. Specifically, TiPNN adopts a unified graph, namely history temporal graph, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.11471</link><description>&lt;p&gt;
&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#65288;DOVESEI&#65289;
&lt;/p&gt;
&lt;p&gt;
Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24320;&#25918;&#35789;&#27719;&#22686;&#24378;&#30340;&#26234;&#33021;&#23433;&#20840;&#30528;&#38470;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#35270;&#35273;&#20282;&#26381;&#65292;&#36866;&#24212;&#19981;&#21516;&#22330;&#26223;&#19988;&#26080;&#38656;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#36827;&#34892;&#27169;&#22411;&#25913;&#36827;&#65292;&#21487;&#20197;&#22788;&#29702;100&#31859;&#39640;&#24230;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22478;&#24066;&#31354;&#20013;&#26426;&#22120;&#20154;&#30340;&#22522;&#30784;&#27493;&#39588;&#20043;&#19968;&#65292;&#21363;&#23433;&#20840;&#30528;&#38470;&#12290;&#25105;&#20204;&#20851;&#27880;&#23433;&#20840;&#30528;&#38470;&#24863;&#30693;&#22534;&#26632;&#20013;&#26368;&#20851;&#38190;&#30340;&#26041;&#38754;&#20043;&#19968;&#65292;&#21363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#21453;&#24212;&#24335;&#26080;&#20154;&#26426;&#31995;&#32479;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#22270;&#20687;&#20998;&#21106;&#30340;&#33021;&#21147;&#23454;&#29616;&#35270;&#35273;&#20282;&#26381;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20854;&#24320;&#25918;&#35789;&#27719;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#35843;&#25972;&#38656;&#27714;&#65292;&#32469;&#36807;&#23545;&#20869;&#37096;&#27169;&#22411;&#36827;&#34892;&#22823;&#37327;&#25968;&#25454;&#31215;&#32047;&#20197;&#36827;&#34892;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#24403;&#22320;&#24403;&#23616;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#20174;100&#31859;&#39640;&#24230;&#36215;&#39134;&#30340;&#25805;&#20316;&#12290;&#36825;&#20010;&#36873;&#25321;&#26159;&#26377;&#24847;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#22788;&#29702;&#30340;&#39640;&#24230;&#20165;&#38480;&#20110;30&#31859;&#65292;&#19982;&#23567;&#22411;&#31435;&#20307;&#30456;&#26426;&#30340;&#33021;&#21147;&#30456;&#21563;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#32479;&#30340;&#19977;&#32500;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#26469;&#23548;&#33322;&#21097;&#19979;&#30340;20&#31859;&#12290;&#21033;&#29992;&#21333;&#30446;&#30456;&#26426;&#21644;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30340;&#26399;&#26395;&#21464;&#24418;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#24182;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.15657</link><description>&lt;p&gt;
&#25237;&#31080;&#20013;&#20108;&#39033;&#24335;&#21464;&#24418;&#20986;&#20046;&#24847;&#26009;
&lt;/p&gt;
&lt;p&gt;
The Distortion of Binomial Voting Defies Expectation. (arXiv:2306.15657v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30340;&#26399;&#26395;&#21464;&#24418;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#24182;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31038;&#20250;&#36873;&#25321;&#20013;&#65292;&#25237;&#31080;&#35268;&#21017;&#30340;&#21464;&#24418;&#24230;&#37327;&#20102;&#35268;&#21017;&#22312;&#20811;&#26381;&#26377;&#38480;&#30340;&#20559;&#22909;&#20449;&#24687;&#20197;&#36873;&#25321;&#31038;&#20250;&#19978;&#29702;&#24819;&#32467;&#26524;&#30340;&#31243;&#24230;&#12290;&#36825;&#20010;&#27010;&#24565;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20165;&#20165;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#30340;&#35270;&#35282;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25237;&#31080;&#35268;&#21017;&#30456;&#23545;&#20110;&#24213;&#23618;&#36164;&#20135;&#32773;&#25928;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#35268;&#21017;-&#20108;&#39033;&#24335;&#25237;&#31080;&#65292;&#23427;&#20026;&#25152;&#26377;&#20998;&#24067;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26399;&#26395;&#21464;&#24418;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational social choice, the distortion of a voting rule quantifies the degree to which the rule overcomes limited preference information to select a socially desirable outcome. This concept has been investigated extensively, but only through a worst-case lens. Instead, we study the expected distortion of voting rules with respect to an underlying distribution over voter utilities. Our main contribution is the design and analysis of a novel and intuitive rule, binomial voting, which provides strong expected distortion guarantees for all distributions.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.05668</link><description>&lt;p&gt;
RePaint-NeRF&#65306;&#22522;&#20110;&#35821;&#20041;&#25513;&#30721;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340; NeRF &#32534;&#36753;.
&lt;/p&gt;
&lt;p&gt;
RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models. (arXiv:2306.05668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#65292;&#24182;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#20986;&#29616;&#20419;&#36827;&#20102;&#23545;&#22797;&#26434;&#30495;&#23454;&#19990;&#30028;&#30340;&#39640;&#20445;&#30495;&#35270;&#22270;&#30340;&#21512;&#25104;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312; NeRF &#20013;&#37325;&#26032;&#32472;&#21046;&#20869;&#23481;&#20173;&#28982;&#26159;&#19968;&#39033;&#38750;&#24120;&#33499;&#21051;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25509;&#21463; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#24182;&#25913;&#21464;&#31070;&#32463;&#22330;&#26223;&#20013;&#30340; 3D &#20869;&#23481;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25351;&#23548;&#25351;&#23450;&#30340; 3D &#20869;&#23481;&#30340;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35821;&#20041;&#22320;&#36873;&#25321;&#30446;&#26631;&#23545;&#35937;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#23558;&#25351;&#23548; NeRF &#27169;&#22411;&#29983;&#25104;&#26032;&#30340; 3D &#23545;&#35937;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640; NeRF &#30340;&#21487;&#32534;&#36753;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#20110;&#22312;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#32534;&#36753; NeRF &#20013;&#30340; 3D &#23545;&#35937;&#26159;&#26377;&#25928;&#30340;&#65292;&#21253;&#25324;&#32534;&#36753;&#22806;&#35266;&#12289;&#24418;&#29366;&#31561;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#23436;&#25104;&#36825;&#20123;&#32534;&#36753;&#20219;&#21153;&#12290;&#35831;&#35775;&#38382; https://repaintnerf.github.io &#20197;&#26597;&#30475;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of Neural Radiance Fields (NeRF) has promoted the development of synthesized high-fidelity views of the intricate real world. However, it is still a very demanding task to repaint the content in NeRF. In this paper, we propose a novel framework that can take RGB images as input and alter the 3D content in neural scenes. Our work leverages existing diffusion models to guide changes in the designated 3D content. Specifically, we semantically select the target object and a pre-trained diffusion model will guide the NeRF model to generate new 3D objects, which can improve the editability, diversity, and application range of NeRF. Experiment results show that our algorithm is effective for editing 3D objects in NeRF under different text prompts, including editing appearance, shape, and more. We validate our method on both real-world datasets and synthetic-world datasets for these editing tasks. Please visit https://repaintnerf.github.io for a better view of our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#24674;&#22797;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05415</link><description>&lt;p&gt;
&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#65306;&#20174;&#29702;&#35770;&#21040;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Causal normalizing flows: from theory to practice. (arXiv:2306.05415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#21487;&#20197;&#24674;&#22797;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#21033;&#29992;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#22240;&#26524;&#25512;&#35770;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#38750;&#32447;&#24615;ICA&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#22312;&#32473;&#23450;&#22240;&#26524;&#25490;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#37492;&#21035;&#20986;&#26469;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#33258;&#22238;&#24402;&#24402;&#19968;&#21270;&#27969;&#36827;&#34892;&#24674;&#22797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#25429;&#25417;&#28508;&#22312;&#22240;&#26524;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#35774;&#35745;&#21644;&#23398;&#20064;&#36873;&#25321;&#30340;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#20013;&#23454;&#29616;do-operator&#65292;&#20174;&#32780;&#22238;&#31572;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#23545;&#27604;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#36873;&#25321;&#65307;&#23558;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#19982;&#20854;&#20182;&#36924;&#36817;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65307;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#22240;&#26524;&#24402;&#19968;&#21270;&#27969;&#21487;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#37096;&#20998;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20195;&#30721;&#21487;&#20197;&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we deepen on the use of normalizing flows for causal reasoning. Specifically, we first leverage recent results on non-linear ICA to show that causal models are identifiable from observational data given a causal ordering, and thus can be recovered using autoregressive normalizing flows (NFs). Second, we analyze different design and learning choices for causal normalizing flows to capture the underlying causal data-generating process. Third, we describe how to implement the do-operator in causal NFs, and thus, how to answer interventional and counterfactual questions. Finally, in our experiments, we validate our design and training choices through a comprehensive ablation study; compare causal NFs to other approaches for approximating causal models; and empirically demonstrate that causal NFs can be used to address real-world problems, where the presence of mixed discrete-continuous data and partial knowledge on the causal graph is the norm. The code for this work can be f
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02204</link><description>&lt;p&gt;
&#24490;&#29615;&#19968;&#33268;&#24615;&#39537;&#21160;&#30340;&#29289;&#20307;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#30340;&#24341;&#20837;&#65292;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#29289;&#20307;&#24212;&#26144;&#23556;&#21040;&#19981;&#21516;&#27133;&#20301;&#30340;&#32422;&#26463;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#31867;&#20284;&#20110;&#20154;&#31867;&#35748;&#30693;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#26550;&#26500;&#20808;&#39564;&#25110;&#36741;&#21161;&#20449;&#24687;&#65288;&#20363;&#22914;&#28145;&#24230;&#22270;&#25110;&#27969;&#22330;&#22270;&#65289;&#26469;&#25506;&#32034;&#22522;&#20110;&#27133;&#20301;&#30340;&#26041;&#27861;&#65292;&#20197;&#34920;&#31034;&#23545;&#35937;&#20026;&#31216;&#20026;&#8220;&#27133;&#20301;&#8221;&#25110;&#8220;&#23545;&#35937;&#25991;&#20214;&#8221;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#20307;&#21457;&#29616;&#12290; &#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#26550;&#26500;&#20808;&#39564;&#20250;&#24341;&#20837;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#38656;&#35201;&#31934;&#24515;&#35774;&#35745;&#25165;&#33021;&#35782;&#21035;&#27491;&#30830;&#30340;&#23545;&#35937;&#12290; &#21516;&#26679;&#65292;&#20381;&#36182;&#36741;&#21161;&#20449;&#24687;&#30340;&#26041;&#27861;&#20063;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#36825;&#31181;&#20449;&#24687;&#36890;&#24120;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26126;&#30830;&#20248;&#21270;&#22330;&#26223;&#20013;&#27599;&#20010;&#23545;&#35937;&#24212;&#26144;&#23556;&#21040;&#19968;&#20010;&#19981;&#21516;&#27133;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#24418;&#24335;&#21270;&#36825;&#20010;&#32422;&#26463;&#65292;&#31216;&#20043;&#20026;&#24490;&#29615;&#19968;&#33268;&#24615;&#30446;&#26631;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#21457;&#29616;&#29289;&#20307;&#12290; &#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#29289;&#20307;&#21457;&#29616;&#21644;&#23569;&#26679;&#26412;&#29289;&#20307;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches have explored slot-based methods utilizing architectural priors or auxiliary information such as depth maps or flow maps to facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. However, reliance on architectural priors introduces unreliability and requires meticulous engineering to identify the correct objects. Likewise, methods relying on auxiliary information are suboptimal as such information is often unavailable for most natural scenes. To address these limitations, we propose a method that explicitly optimizes the constraint that each object in a scene should be mapped to a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. We refer to them as the \textit{cycle-consistency} objectives. By applying these con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13877</link><description>&lt;p&gt;
Narrative XL: &#19968;&#20010;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Narrative XL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#38271;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#33258;&#28982;&#25968;&#25454;&#38598;&#65292;&#20197;&#24110;&#21161;&#25913;&#36827;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25968;&#25454;&#38598;&#30001; GPT 3.5 &#29983;&#25104;&#65292;&#25688;&#35201;&#21253;&#25324;&#26469;&#33258; Project Gutenberg &#30340; 1500 &#26412;&#20070;&#20013;&#27599;&#20010;&#22330;&#26223;&#30340;&#24635;&#32467;&#65292;&#20197;&#21450;&#37197;&#22871;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#20219;&#20309;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#35201;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#19981;&#20165;&#38656;&#35201;&#23545;&#20856;&#22411;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#25110;&#35757;&#32451;&#31243;&#24207;&#36827;&#34892;&#26356;&#25913;&#65292;&#36824;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#36825;&#20123;&#26032;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#36164;&#28304;&#32570;&#23569;&#19968;&#20123;&#20851;&#38190;&#23646;&#24615;&#65292;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#35268;&#27169;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#35780;&#20272;&#65289;&#38271;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#30701;&#26399;&#35760;&#24518;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#21019;&#24314;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992; GPT 3.5&#65292;&#25105;&#20204;&#24635;&#32467;&#20102; Project Gutenberg &#20013; 1500 &#26412;&#25163;&#24037;&#31579;&#36873;&#30340;&#20070;&#31821;&#20013;&#30340;&#27599;&#20010;&#22330;&#26223;&#65292;&#27599;&#26412;&#20070;&#24471;&#21040;&#22823;&#32422; 150 &#20010;&#22330;&#26223;&#32423;&#21035;&#30340;&#25688;&#35201;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20123;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65292;&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#22330;&#26223;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Despite their tremendous successes, most large language models do not have any long-term memory mechanisms, which restricts their applications. Overcoming this limitation would not only require changes to the typical transformer architectures or training procedures, but also a dataset on which these new models could be trained and evaluated. We argue that existing resources lack a few key properties, and that at present, there are no naturalistic datasets of sufficient scale to train (and not only evaluate) long-term memory language models. We then present our solution that capitalizes on the advances in short-term memory language models to create such a dataset. Using GPT 3.5, we summarized each scene in 1500 hand-curated books from Project Gutenberg, which resulted in approximately 150 scene-level summaries per book. We then created a number of reading comprehension questions based on these summaries, including three types of multiple-choice scene recognition questions, as well as fr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09424</link><description>&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26657;&#20934;&#21487;&#26368;&#23567;&#21270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26657;&#20934;&#26159;&#19968;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#36328;&#22823;&#37327;&#22242;&#20307;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#20989;&#25968;&#65292;&#22810;&#26657;&#20934;&#20063;&#34987;&#35748;&#20026;&#26159;&#19982;&#26368;&#23567;&#21270;&#25439;&#22833;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#65288;&#20960;&#20046;&#25152;&#26377;&#30340;&#65289;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#26041;&#38754;&#65292;&#32780;&#19981;&#26159;&#20851;&#20110;&#31639;&#27861;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#32771;&#34385;&#12290;&#20197;&#21069;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#39044;&#27979;&#22120;&#65292;&#22240;&#27492;&#26159;&#34920;&#24449;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#22914; SGD&#65292;&#24182;&#19988;&#19981;&#24212;&#35299;&#37322;&#20026;&#8220;&#20844;&#24179;&#24615;&#20174;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#20813;&#36153;&#30340;&#22909;&#22788;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
&lt;/p&gt;</description></item><item><title>&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#24182;&#36807;&#28388;&#25481;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#23545;&#23453;&#36149;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#36827;&#34892;&#35299;&#32544;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#22686;&#24378;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05066</link><description>&lt;p&gt;
&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05066
&lt;/p&gt;
&lt;p&gt;
&#25197;&#26354;-&#35299;&#32544;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#24182;&#36807;&#28388;&#25481;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#12290;&#35813;&#26041;&#27861;&#36824;&#35299;&#20915;&#20102;&#23545;&#23453;&#36149;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#36827;&#34892;&#35299;&#32544;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#23545;&#22686;&#24378;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20197;&#20854;&#22312;&#34920;&#31034;&#23398;&#20064;&#21644;&#21508;&#31181;&#19979;&#28216;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#26368;&#36817;&#65292;&#27491;&#23545;&#23545;&#27604;&#23398;&#20064;&#65288;POCL&#65289;&#22312;&#26080;&#38656;&#26500;&#24314;&#27491;&#36127;&#35757;&#32451;&#38598;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#24615;&#33021;&#12290;&#23427;&#36890;&#36807;&#38477;&#20302;&#23545;&#25209;&#27425;&#22823;&#23567;&#30340;&#20381;&#36182;&#26469;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#12290;POCL&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#25439;&#22833;&#20989;&#25968;&#25552;&#21462;&#22833;&#30495;&#19981;&#21464;&#34920;&#31034;&#65288;DIR&#65289;&#65292;&#35813;&#34920;&#31034;&#25551;&#36848;&#20102;&#21463;&#19981;&#21516;&#22833;&#30495;&#24433;&#21709;&#30340;&#27491;&#23545;&#34920;&#31034;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#38544;&#24335;&#22320;&#20351;&#27169;&#22411;&#33021;&#22815;&#28388;&#38500;&#25110;&#24573;&#30053;&#21463;&#19981;&#21516;&#22833;&#30495;&#24433;&#21709;&#30340;&#22833;&#30495;&#21464;&#20307;&#34920;&#31034;&#65288;DVR&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;POCL&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#24378;&#21046;&#25191;&#34892;&#26377;&#20215;&#20540;&#30340;DVR&#30340;&#35299;&#32544;&#21644;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;POCL&#26041;&#27861;&#23545;&#22686;&#24378;&#31574;&#30053;&#24456;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is well known for its remarkable performance in representation learning and various downstream computer vision tasks. Recently, Positive-pair-Only Contrastive Learning (POCL) has achieved reliable performance without the need to construct positive-negative training sets. It reduces memory requirements by lessening the dependency on the batch size. The POCL method typically uses a single loss function to extract the distortion invariant representation (DIR) which describes the proximity of positive-pair representations affected by different distortions. This loss function implicitly enables the model to filter out or ignore the distortion variant representation (DVR) affected by different distortions. However, existing POCL methods do not explicitly enforce the disentanglement and exploitation of the actually valuable DVR. In addition, these POCL methods have been observed to be sensitive to augmentation strategies. To address these limitations, we propose a nov
&lt;/p&gt;</description></item><item><title>RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2211.03408</link><description>&lt;p&gt;
RITA:&#36890;&#36807;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
RITA: Boost Autonomous Driving Simulators with Realistic Interactive Traffic Flow. (arXiv:2211.03408v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03408
&lt;/p&gt;
&lt;p&gt;
RITA&#26159;&#19968;&#20010;&#38598;&#25104;&#32452;&#20214;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#65292;&#29992;&#20110;&#27979;&#35797;&#21644;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#12290;&#23427;&#30001;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65292;&#25903;&#25345;&#30495;&#23454;&#20132;&#20114;&#24335;&#20132;&#36890;&#27969;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#25511;&#21046;&#20132;&#36890;&#27969;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#29983;&#25104;&#26159;&#26500;&#24314;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#26680;&#24515;&#27169;&#22359;&#12290; &#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#21487;&#29992;&#30340;&#27169;&#25311;&#22120;&#26080;&#27861;&#22797;&#21046;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#21508;&#31181;&#29305;&#24449;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#24182;&#21516;&#26102;&#27169;&#25311;&#23545;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30340;&#20154;&#31867;&#21453;&#24212;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Realistic Interactive TrAffic flow (RITA)&#20316;&#20026;&#29616;&#26377;&#39550;&#39542;&#27169;&#25311;&#22120;&#30340;&#38598;&#25104;&#32452;&#20214;&#65292;&#20026;&#27979;&#35797;&#21644;&#20248;&#21270;&#39550;&#39542;&#31574;&#30053;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20132;&#36890;&#27969;&#12290; RITA&#30340;&#24320;&#21457;&#32771;&#34385;&#20102;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65292;&#21363;&#30495;&#23454;&#24230;&#65292;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#30001;&#31216;&#20026;RITABackend&#21644;RITAKit&#30340;&#20004;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#12290; RITABackend&#25903;&#25345;&#36710;&#36742;&#25511;&#21046;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20132;&#36890;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;RITAKit&#21017;&#24320;&#21457;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#20197;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#29983;&#25104;&#21487;&#25511;&#30340;&#20132;&#36890;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality traffic flow generation is the core module in building simulators for autonomous driving. However, the majority of available simulators are incapable of replicating traffic patterns that accurately reflect the various features of real-world data while also simulating human-like reactive responses to the tested autopilot driving strategies. Taking one step forward to addressing such a problem, we propose Realistic Interactive TrAffic flow (RITA) as an integrated component of existing driving simulators to provide high-quality traffic flow for the evaluation and optimization of the tested driving strategies. RITA is developed with consideration of three key features, i.e., fidelity, diversity, and controllability, and consists of two core modules called RITABackend and RITAKit. RITABackend is built to support vehicle-wise control and provide traffic generation models from real-world datasets, while RITAKit is developed with easy-to-use interfaces for controllable traffic gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01842</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#20998;&#23618;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#24182;&#20419;&#36827;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#30340;&#26500;&#24314;&#22359;&#20013;&#21457;&#29616;&#31070;&#32463;&#32467;&#26500;&#26159;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#25628;&#32034;&#19968;&#20123;&#38480;&#23450;&#26041;&#38754;&#30340;&#26550;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#30340;&#32479;&#19968;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#32780;&#32039;&#20945;&#22320;&#29983;&#25104;&#34920;&#36798;&#21147;&#24378;&#22823;&#30340;&#20998;&#23618;&#25628;&#32034;&#31354;&#38388;&#65292;&#27604;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#31354;&#38388;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36890;&#36807;&#22686;&#24378;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20307;&#31995;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32467;&#26500;&#30340;&#35268;&#24459;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#23618;&#26680;&#35774;&#35745;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#25628;&#32034;&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#25628;&#32034;&#22914;&#27492;&#24222;&#22823;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#26694;&#26550;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#25628;&#32034;&#31574;&#30053;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item></channel></rss>