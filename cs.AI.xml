<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02587</link><description>&lt;p&gt;
&#35757;&#32451;&#25193;&#23637;&#26597;&#35810;&#30340;&#25490;&#24207;&#22120;&#30340;&#20986;&#20046;&#24847;&#26009;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Surprising Effectiveness of Rankers Trained on Expanded Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02587
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25193;&#23637;&#21644;&#22256;&#38590;&#26597;&#35810;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25490;&#24207;&#31995;&#32479;&#20013;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22788;&#29702;&#26597;&#35810;&#20998;&#24067;&#23614;&#37096;&#30340;&#22256;&#38590;&#26597;&#35810;&#12290;&#36825;&#31181;&#22256;&#38590;&#21487;&#33021;&#28304;&#20110;&#23384;&#22312;&#19981;&#24120;&#35265;&#12289;&#26410;&#26126;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#26597;&#35810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#23545;&#35757;&#32451;&#26597;&#35810;&#36827;&#34892;&#20102;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#25193;&#23637;&#26469;&#25552;&#39640;&#22256;&#38590;&#26597;&#35810;&#30340;&#25490;&#24207;&#24615;&#33021;&#65292;&#32780;&#19981;&#25439;&#23475;&#20854;&#20182;&#26597;&#35810;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;LLM&#36827;&#34892;&#26597;&#35810;&#20016;&#23500;&#21270;&#65292;&#20351;&#29992;&#30456;&#20851;&#25991;&#26723;&#36827;&#34892;&#35757;&#32451;&#12290;&#25509;&#19979;&#26469;&#65292;&#19987;&#38376;&#30340;&#25490;&#24207;&#22120;&#20165;&#22312;&#20016;&#23500;&#30340;&#22256;&#38590;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#26159;&#22312;&#21407;&#22987;&#26597;&#35810;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#19987;&#38376;&#25490;&#24207;&#22120;&#21644;&#22522;&#26412;&#25490;&#24207;&#22120;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#20197;&#21450;&#20026;&#27599;&#20010;&#26597;&#35810;&#20272;&#35745;&#30340;&#26597;&#35810;&#24615;&#33021;&#24471;&#20998;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#36890;&#24120;&#23545;&#25152;&#26377;&#26597;&#35810;&#20351;&#29992;&#21333;&#20010;&#25490;&#24207;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#26131;&#26597;&#35810;&#26377;&#20559;&#35265;&#65292;&#26131;&#26597;&#35810;&#26500;&#25104;&#26597;&#35810;&#20998;&#24067;&#30340;&#22823;&#22810;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02587v1 Announce Type: cross  Abstract: An important problem in text-ranking systems is handling the hard queries that form the tail end of the query distribution. The difficulty may arise due to the presence of uncommon, underspecified, or incomplete queries. In this work, we improve the ranking performance of hard or difficult queries without compromising the performance of other queries. Firstly, we do LLM based query enrichment for training queries using relevant documents. Next, a specialized ranker is fine-tuned only on the enriched hard queries instead of the original queries. We combine the relevance scores from the specialized ranker and the base ranker, along with a query performance score estimated for each query. Our approach departs from existing methods that usually employ a single ranker for all queries, which is biased towards easy queries, which form the majority of the query distribution. In our extensive experiments on the DL-Hard dataset, we find that a p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18910</link><description>&lt;p&gt;
&#23545;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#24726;&#35770;&#30340;&#20284;&#28982;&#20960;&#20309;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Geometric Explanation of the Likelihood OOD Detection Paradox
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18910
&lt;/p&gt;
&lt;p&gt;
&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#36890;&#24120;&#34920;&#29616;&#20986;&#20196;&#20154;&#22256;&#24785;&#30340;&#34892;&#20026;&#65306;&#24403;&#22312;&#30456;&#23545;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#20250;&#32473;&#26469;&#33258;&#26356;&#31616;&#21333;&#26469;&#28304;&#30340;&#31163;&#32676;&#25968;&#25454;&#36171;&#20104;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#12290;&#26356;&#20351;&#20154;&#24863;&#21040;&#31070;&#31192;&#30340;&#26159;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#65292;&#20294;&#36825;&#20123;DGMs&#20174;&#26410;&#29983;&#25104;&#36807;&#31163;&#32676;&#26679;&#26412;&#12290;&#36825;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#24726;&#35770;&#23578;&#26410;&#24471;&#21040;&#26368;&#32456;&#35299;&#37322;&#65292;&#20351;&#24471;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22914;&#26524;&#39640;&#20284;&#28982;&#21306;&#22495;&#20013;&#21253;&#21547;&#20102;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#37027;&#20040;&#36825;&#20123;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22320;&#26041;&#21487;&#33021;&#20986;&#29616;&#22823;&#23494;&#24230;&#20294;&#20302;&#27010;&#29575;&#36136;&#37327;&#30340;&#30475;&#20284;&#30683;&#30462;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;(LID)&#20272;&#35745;&#21487;&#20197;&#35782;&#21035;&#36825;&#31181;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;DGM&#33719;&#24471;&#30340;&#20284;&#28982;&#21644;LID&#20272;&#35745;&#30456;&#37197;&#23545;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.16554</link><description>&lt;p&gt;
PE&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#25991;&#26412;&#23618;&#27425;&#29983;&#25104;&#30340;Poincar&#233;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PE: A Poincare Explanation Method for Fast Text Hierarchy Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16554
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: NLP&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#36716;&#31227;&#21040;&#23618;&#27425;&#23646;&#24615;&#65288;HA&#65289;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32791;&#26102;&#30340;&#36138;&#23146;&#25628;&#32034;&#26469;&#24314;&#27169;&#38750;&#36830;&#32493;&#32452;&#21512;&#65292;&#24573;&#30053;&#20102;&#29305;&#24449;&#34920;&#31034;&#20013;&#28508;&#22312;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Poincar&#233;&#35299;&#37322;&#65288;PE&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(n^2logn)$&#12290;&#21463;Poincar&#233;&#27169;&#22411;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#23884;&#20837;&#25237;&#24433;&#21040;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#36825;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#23545;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25237;&#24433;&#31354;&#38388;&#20013;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 Announce Type: cross  Abstract: The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#24212;&#29992;ChatGPT&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#20854;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15274</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#65306;&#31532;&#19968;&#24180;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#24212;&#29992;ChatGPT&#30340;&#24773;&#20917;&#65292;&#24635;&#32467;&#20102;&#20854;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#26631;&#24535;&#30528;&#22312;&#21508;&#20010;&#23398;&#31185;&#39046;&#22495;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#25506;&#32034;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#23398;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#28085;&#30422;&#32452;&#23398;&#12289;&#36951;&#20256;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#12289;&#33647;&#29289;&#21457;&#29616;&#12289;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29702;&#35299;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#32534;&#31243;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#25945;&#32946;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25551;&#32472;&#20102;&#35813;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#24403;&#21069;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#21457;&#23637;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15274v1 Announce Type: cross  Abstract: The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in various sectors of bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future development.
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08337</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20154;&#31867;&#20223;&#29983;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#30340;&#25317;&#22581;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#24212;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#31649;&#29702;&#22478;&#24066;&#20132;&#36890;&#27969;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30340;&#19981;&#36275;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;TSC&#20013;&#65292;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#19968;&#22871;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#26377;&#21161;&#20110;&#25506;&#35752;&#38745;&#24577;&#21644;&#21160;&#24577;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08337v1 Announce Type: cross  Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#27169;&#25311;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#23384;&#20648;I/O&#30165;&#36857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#65292;&#20026;&#21457;&#23637;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.07540</link><description>&lt;p&gt;
WannaLaugh: &#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#21202;&#32034;&#36719;&#20214;&#27169;&#25311;&#22120; -- &#23398;&#20064;&#27169;&#20223;&#24694;&#24847;&#23384;&#20648;&#30165;&#36857;
&lt;/p&gt;
&lt;p&gt;
WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic Malicious Storage Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#27169;&#25311;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#30340;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#23384;&#20648;I/O&#30165;&#36857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#65292;&#20026;&#21457;&#23637;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21202;&#32034;&#36719;&#20214;&#20316;&#20026;&#19968;&#31181;&#21487;&#24597;&#19988;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#25345;&#32493;&#23545;&#20840;&#29699;&#20010;&#20154;&#21644;&#32452;&#32455;&#36896;&#25104;&#20005;&#37325;&#21518;&#26524;&#12290;&#20256;&#32479;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#38745;&#24577;&#31614;&#21517;&#21644;&#24212;&#29992;&#31243;&#24207;&#34892;&#20026;&#27169;&#24335;&#65292;&#21463;&#21040;&#36825;&#20123;&#23041;&#32961;&#21160;&#24577;&#26412;&#36136;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21202;&#32034;&#36719;&#20214;&#27169;&#25311;&#22120;&#12290;&#35813;&#24037;&#20855;&#26088;&#22312;&#23433;&#20840;&#22320;&#27169;&#20223;&#21202;&#32034;&#36719;&#20214;&#25915;&#20987;&#65292;&#32780;&#19981;&#24341;&#36215;&#23454;&#38469;&#20260;&#23475;&#25110;&#20256;&#25773;&#24694;&#24847;&#36719;&#20214;&#65292;&#20351;&#20854;&#25104;&#20026;&#30740;&#31350;&#21202;&#32034;&#36719;&#20214;&#34892;&#20026;&#30340;&#29420;&#29305;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#27169;&#25311;&#22120;&#21019;&#24314;&#23384;&#20648;I/O&#30165;&#36857;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22312;&#26816;&#27979;&#21202;&#32034;&#36719;&#20214;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#22312;&#24320;&#21457;&#36127;&#36131;&#20219;&#30340;&#32593;&#32476;&#23433;&#20840;&#24037;&#20855;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07540v1 Announce Type: cross  Abstract: Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues to inflict severe consequences on individuals and organizations worldwide. Traditional detection methods, reliant on static signatures and application behavioral patterns, are challenged by the dynamic nature of these threats. This paper introduces three primary contributions to address this challenge. First, we introduce a ransomware emulator. This tool is designed to safely mimic ransomware attacks without causing actual harm or spreading malware, making it a unique solution for studying ransomware behavior. Second, we demonstrate how we use this emulator to create storage I/O traces. These traces are then utilized to train machine-learning models. Our results show that these models are effective in detecting ransomware, highlighting the practical application of our emulator in developing responsible cybersecurity tools. Third, we show how our emulator can be
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02567</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Eliciting Better Multilingual Structured Reasoning from LLMs through Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02567
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#22312;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#25110;&#31616;&#21333;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;xSTREET&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#35821;&#35328;&#30340;&#22235;&#20010;&#20219;&#21153;&#12290;xSTREET&#26292;&#38706;&#20102;&#22522;&#26412;LLM&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#25512;&#29702;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24314;&#31435;&#22312;LLM&#22312;&#20195;&#30721;&#19978;&#35757;&#32451;&#26356;&#22909;&#30340;&#25512;&#29702;&#36825;&#19968;&#35266;&#28857;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#22312;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#20195;&#30721;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#22810;&#35821;&#35328;&#27880;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#31243;&#24207;&#20195;&#30721;&#19981;&#21464;&#12290;&#20854;&#27425;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#21253;&#21547;&#36880;&#27493;&#20195;&#30721;&#21407;&#35821;&#30340;&#25552;&#31034;&#32467;&#26500;&#26469;&#24357;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#23548;&#20986;&#26032;&#20107;&#23454;&#24182;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;xSTREET&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31185;&#23398;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02567v1 Announce Type: cross  Abstract: Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reaso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01193</link><description>&lt;p&gt;
RAGged Edges: Retrieval-Augmented Chatbots&#30340;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25269;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#32467;&#26524;&#34920;&#26126;RAG&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273; - &#29983;&#25104;&#30475;&#20284;&#27491;&#30830;&#20294;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#24456;&#20851;&#38190;&#65292;&#23601;&#20687;&#26368;&#36817;&#30340;&#27861;&#38498;&#26696;&#20363;&#20013;&#30475;&#21040;&#30340;&#37027;&#26679;&#65292;ChatGPT&#30340;&#20351;&#29992;&#23548;&#33268;&#20102;&#19981;&#23384;&#22312;&#30340;&#27861;&#24459;&#35009;&#20915;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#25552;&#31034;&#38598;&#25104;&#26469;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26469;&#25269;&#21046;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26088;&#22312;&#35825;&#23548;&#24187;&#35273;&#30340;&#25552;&#31034;&#26469;&#23545;RAG&#19982;&#26631;&#20934;LLMs&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;RAG&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24403;&#25552;&#31034;&#30452;&#25509;&#19982;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#29702;&#35299;&#30456;&#30683;&#30462;&#26102;&#65292;RAG&#20173;&#28982;&#20250;&#34987;&#35823;&#23548;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#24187;&#35273;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#30830;&#20445;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RAG&#37096;&#32626;&#30340;&#23454;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01193v1 Announce Type: cross  Abstract: Large language models (LLMs) like ChatGPT demonstrate the remarkable progress of artificial intelligence. However, their tendency to hallucinate -- generate plausible but false information -- poses a significant challenge. This issue is critical, as seen in recent court cases where ChatGPT's use led to citations of non-existent legal rulings. This paper explores how Retrieval-Augmented Generation (RAG) can counter hallucinations by integrating external knowledge with prompts. We empirically evaluate RAG against standard LLMs using prompts designed to induce hallucinations. Our results show that RAG increases accuracy in some cases, but can still be misled when prompts directly contradict the model's pre-trained understanding. These findings highlight the complex nature of hallucinations and the need for more robust solutions to ensure LLM reliability in real-world applications. We offer practical recommendations for RAG deployment and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18150</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#26080;&#30417;&#30563;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#35282;&#33394;&#23450;&#20041;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#25972;&#21512;&#26816;&#32034;&#20449;&#24687;&#20197;&#29983;&#25104;&#26356;&#21152;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#23558;&#26469;&#33258;&#26816;&#32034;&#30340;&#39069;&#22806;&#20449;&#24687;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#26377;&#25928;&#21033;&#29992;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#26377;&#26102;&#20250;&#24573;&#35270;&#25110;&#34987;&#38169;&#35823;&#24341;&#23548;&#12290;&#20854;&#20851;&#38190;&#21407;&#22240;&#22312;&#20110;LLMs&#30340;&#35757;&#32451;&#27809;&#26377;&#28165;&#26224;&#22320;&#35753;LLMs&#23398;&#20250;&#22914;&#20309;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#26816;&#32034;&#25991;&#26412;&#36755;&#20837;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#22312;RAG&#20013;&#30340;&#35282;&#33394;&#35270;&#20026;&#8220;&#20449;&#24687;&#32454;&#21270;&#32773;&#8221;&#65292;&#36825;&#24847;&#21619;&#30528;&#26080;&#35770;&#26816;&#32034;&#25991;&#26412;&#30340;&#27491;&#30830;&#24615;&#12289;&#23436;&#25972;&#24615;&#25110;&#26377;&#29992;&#24615;&#22914;&#20309;&#65292;LLMs&#37117;&#33021;&#19968;&#33268;&#22320;&#25972;&#21512;&#26816;&#32034;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#29983;&#25104;&#27604;&#26816;&#32034;&#25991;&#26412;&#26356;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#25991;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InFO-RAG&#30340;&#20449;&#24687;&#32454;&#21270;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;LLMs&#29992;&#20110;RAG&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18150v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG i
&lt;/p&gt;</description></item><item><title>BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14758</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65306;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch and match: black-box variational inference with a score-based divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14758
&lt;/p&gt;
&lt;p&gt;
BaM&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#65292;&#38024;&#23545;&#39640;&#26041;&#24046;&#26799;&#24230;&#20272;&#35745;&#24930;&#25910;&#25947;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#21464;&#20998;&#26063;&#20013;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#26102;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#23558;&#25351;&#25968;&#24555;&#36895;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#65292;BaM&#22312;&#22810;&#31181;&#29983;&#25104;&#27169;&#22411;&#25512;&#26029;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20027;&#35201;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#23454;&#29616;&#37117;&#26159;&#22522;&#20110;&#20248;&#21270;&#38543;&#26426;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;BBVI&#26041;&#27861;&#36890;&#24120;&#30001;&#20110;&#20854;&#26799;&#24230;&#20272;&#35745;&#30340;&#39640;&#26041;&#24046;&#32780;&#25910;&#25947;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;&#21644;&#21305;&#37197;&#65288;BaM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#30340;BBVI&#26367;&#20195;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#31163;&#25955;&#21487;&#20197;&#36890;&#36807;&#23545;&#20855;&#26377;&#20840;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#21464;&#20998;&#26063;&#20351;&#29992;&#23553;&#38381;&#24418;&#24335;&#30340;&#36817;&#31471;&#26356;&#26032;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#26102;BaM&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#22312;&#25209;&#37327;&#22823;&#23567;&#36235;&#20110;&#26080;&#31351;&#26102;&#21464;&#20998;&#21442;&#25968;&#26356;&#26032;&#20250;&#25351;&#25968;&#25910;&#25947;&#21040;&#30446;&#26631;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;BaM&#22312;&#28304;&#33258;&#23618;&#27425;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21518;&#39564;&#25512;&#26029;&#30340;&#39640;&#26031;&#21644;&#38750;&#39640;&#26031;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;BaM&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14758v1 Announce Type: cross  Abstract: Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM ty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13254</link><description>&lt;p&gt;
CounterCurate: &#36890;&#36807;&#23545;&#29031;&#20363;&#23376;&#22686;&#24378;&#29289;&#29702;&#21644;&#35821;&#20041;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;CounterCurate&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#20363;&#23376;&#21644;&#29983;&#25104;&#24335;&#24494;&#35843;&#65292;&#20840;&#38754;&#25552;&#21319;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#29289;&#29702;&#25512;&#29702;&#21644;&#35821;&#20041;&#23545;&#29031;&#24494;&#35843;&#26041;&#38754;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CounterCurate&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#20840;&#38754;&#25552;&#21319;&#23545;&#27604;&#21644;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#38382;&#39064;&#65306;&#24573;&#35270;&#20102;&#22522;&#20110;&#29289;&#29702;&#30340;&#25512;&#29702;&#65288;&#35745;&#25968;&#21644;&#20301;&#32622;&#29702;&#35299;&#65289;&#65292;&#20197;&#21450;&#21033;&#29992;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21453;&#20107;&#23454;&#24494;&#35843;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21019;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20123;&#31354;&#30333;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#31361;&#20986;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;LLaVA&#65289;&#22312;&#22522;&#20110;&#29289;&#29702;&#30340;&#32452;&#21512;&#25512;&#29702;&#20013;&#20960;&#20046;&#26080;&#27861;&#32988;&#20219;&#30340;&#34920;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;GLIGEN&#29983;&#25104;&#24494;&#35843;&#25968;&#25454;&#65292;&#20351;&#24471;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65306;&#22312;&#25105;&#20204;&#26032;&#30340;&#31574;&#21010;&#30340;Flickr30k-Positions&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;CLIP&#21644;LLaVA&#30340;&#24615;&#33021;&#20998;&#21035;&#25552;&#39640;&#20102;+33%&#21644;+37%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39640;&#24615;&#33021;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13254v1 Announce Type: cross  Abstract: We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of hig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.09712</link><description>&lt;p&gt;
&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Cross Attention as an Inductive Bias for Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#33268;&#21147;&#20110;&#25552;&#21462;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#22240;&#32032;&#12290;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#22240;&#24335;&#20998;&#35299;&#36825;&#20123;&#34920;&#31034;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#29305;&#23450;&#32467;&#26500;&#35774;&#35745;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#21644;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20419;&#36827;&#35299;&#32544;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#19968;&#32452;&#27010;&#24565;&#20196;&#29260;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#22270;&#20687;&#37325;&#26500;&#30340;&#28508;&#22312;&#25193;&#25955;&#30340;&#26465;&#20214;&#65292;&#20854;&#20013;&#36890;&#36807;&#27010;&#24565;&#20196;&#29260;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#25193;&#25955;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#23601;&#33021;&#36798;&#21040;&#26356;&#20248;&#31168;&#30340;&#35299;&#32544;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25152;&#26377;&#20808;&#21069;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09712v1 Announce Type: cross  Abstract: Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive abl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09398</link><description>&lt;p&gt;
&#20351;&#29992;KV&#32531;&#23384;&#21387;&#32553;&#21512;&#25104;&#24490;&#29615;&#20197;&#25552;&#39640;LLM&#25512;&#26029;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#22240;&#32032;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30001;&#38190;&#20540;(KV)&#32531;&#23384;&#24341;&#36215;&#30340;&#20869;&#23384;&#29942;&#39048;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#24555;&#25463;&#26041;&#24335;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;KV&#23545;&#12290;&#29616;&#26377;&#30340;KV&#32531;&#23384;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25110;&#39537;&#36880;&#30456;&#23545;&#19981;&#37325;&#35201;&#30340;KV&#23545;&#30340;&#22823;&#29255;&#21306;&#22495;&#65292;&#26174;&#33879;&#20943;&#23569;&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#22312;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#22823;&#22810;&#25968;&#21069;&#19968;&#20010;&#26631;&#35760;&#30340;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#23427;&#23558;&#19968;&#20010;&#65288;&#20960;&#20046;&#20813;&#36153;&#30340;&#65289;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#19982;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#31616;&#21333;&#22320;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#20415;&#25152;&#26377;&#30340;&#26631;&#35760;&#21487;&#20197;&#22312;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#20013;&#26597;&#35810;&#12290;&#23427;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#20445;&#30041;&#20449;&#24687;&#65292;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LESS&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#32531;&#23384;&#25152;&#26377;&#20869;&#23481;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#19982;&#20854;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09398v1 Announce Type: cross Abstract: Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;</title><link>https://arxiv.org/abs/2402.04978</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21327;&#20316;&#30340;&#22686;&#24378;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#25512;&#29702;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#33258;&#30001;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#20854;&#20013;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;LLMs&#20043;&#38388;&#23494;&#20999;&#21512;&#20316;&#12290;&#35813;&#26041;&#26696;&#39318;&#20808;&#20351;&#29992;LLMs&#36845;&#20195;&#22320;&#25506;&#32034;KG&#65292;&#36873;&#25321;&#24615;&#22320;&#26816;&#32034;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#23376;&#22270;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;&#28982;&#21518;&#24341;&#23548;LLMs&#36827;&#19968;&#27493;&#32452;&#21512;&#20869;&#22312;&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26126;&#30830;&#38416;&#36848;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#65292;&#24182;&#20415;&#20110;&#36861;&#36394;&#25512;&#29702;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07359</link><description>&lt;p&gt;
&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reliability and Interpretability in Science and Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07359
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24378;&#35843;&#20102;&#31185;&#23398;&#19982;&#28145;&#24230;&#23398;&#20064;&#20013;&#27169;&#22411;&#20551;&#35774;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#20551;&#35774;&#35748;&#35782;&#35770;&#22797;&#26434;&#24615;&#30340;&#20998;&#26512;&#65292;&#21516;&#26102;&#32467;&#21512;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#29305;&#28857;&#65292;&#26469;&#35780;&#20272;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#26085;&#30410;&#37325;&#35201;&#65292;&#24182;&#19988;&#19982;&#27492;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#24050;&#32463;&#28608;&#21457;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#20165;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#26377;&#25152;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#26631;&#20934;&#38169;&#35823;&#20998;&#26512;&#19982;&#23545;DNN&#27169;&#22411;&#19982;&#26631;&#20934;&#31185;&#23398;&#24314;&#27169;&#30340;&#21487;&#33021;&#24046;&#24322;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#30340;&#26356;&#28145;&#23618;&#27425;&#30340;&#35748;&#35782;&#35770;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20960;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#20551;&#35774;&#65288;&#22312;ML&#21644;&#20256;&#32479;&#31185;&#23398;&#20013;&#22343;&#23384;&#22312;&#65289;&#22312;&#26080;&#29702;&#35770;&#31185;&#23398;&#30340;&#38169;&#35273;&#19979;&#30340;&#26222;&#36941;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#20174;&#65288;&#35748;&#35782;&#35770;&#30340;&#65289;&#22797;&#26434;&#24615;&#35282;&#24230;&#20998;&#26512;&#20102;&#27169;&#22411;&#20551;&#35774;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#20551;&#35774;&#22312;&#21487;&#38752;&#24615;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the question of the reliability of Machine Learning (ML) methods has acquired significant importance, and the analysis of the associated uncertainties has motivated a growing amount of research. However, most of these studies have applied standard error analysis to ML models, and in particular Deep Neural Network (DNN) models, which represent a rather significant departure from standard scientific modelling. It is therefore necessary to integrate the standard error analysis with a deeper epistemological analysis of the possible differences between DNN models and standard scientific modelling and the possible implications of these differences in the assessment of reliability. This article offers several contributions. First, it emphasises the ubiquitous role of model assumptions (both in ML and traditional Science) against the illusion of theory-free science. Secondly, model assumptions are analysed from the point of view of their (epistemic) complexity, which is shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15842</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#30340;LLM&#30340;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#65288;&#20316;&#20026;&#36719;&#26631;&#31614;&#65289;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;LLM&#20805;&#24403;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#20026;&#20102;&#20174;LLM&#30340;&#36755;&#20986;&#27010;&#29575;&#20013;&#23398;&#20064;&#32780;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#20197;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#20026;&#20102;&#39564;&#35777;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;6,684&#20010;&#23398;&#29983;&#23545;&#31185;&#23398;&#38382;&#39064;&#30340;&#20889;&#20316;&#22238;&#31572;&#21644;&#19977;&#20010;&#20154;&#24037;&#19987;&#23478;&#35780;&#20998;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;7T&#12290;&#25105;&#20204;&#23558;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;TinyBERT&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2401.05054</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding. (arXiv:2401.05054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#22810;&#26679;&#24615;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#21152;&#20837;&#22810;&#26679;&#24615;&#30446;&#26631;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#20135;&#29983;&#19981;&#20165;&#27491;&#30830;&#32780;&#19988;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#22312;&#29983;&#25104;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#20135;&#29983;&#26368;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#29983;&#25104;&#22810;&#26679;&#21270;&#36755;&#20986;&#32780;&#25552;&#20986;&#30340;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#25110;&#38543;&#26426;&#25277;&#26679;&#65292;&#22240;&#27492;&#20854;&#36755;&#20986;&#36136;&#37327;&#21463;&#38480;&#20110;&#36825;&#20123;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;--&#36890;&#36807;&#23558;&#22810;&#26679;&#24615;&#30446;&#26631;&#24378;&#21152;&#21040;MBR&#35299;&#30721;&#20013;&#26469;&#24320;&#21457;&#20419;&#36827;&#22810;&#26679;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;MBR&#30340;&#21464;&#20307;&#65292;&#21363;&#22810;&#26679;&#24615;MBR&#65288;DMBR&#65289;&#21644;k-medoids MBR&#65288;KMBR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#19968;&#32452;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21508;&#31181;&#23450;&#21521;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;DMBR&#21644;KMBR&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
One of the most important challenges in text generation systems is to produce outputs that are not only correct but also diverse. Recently, Minimum Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the highest quality among the decoding algorithms. However, existing algorithms proposed for generating diverse outputs are predominantly based on beam search or random sampling, thus their output quality is capped by these underlying methods. In this paper, we investigate an alternative approach -- we develop diversity-promoting decoding algorithms by enforcing diversity objectives to MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and $k$-medoids MBR (KMBR), methods to generate a set of sentences with high quality and diversity. We evaluate DMBR and KMBR on a variety of directed text generation tasks using encoder-decoder models and a large language model with prompting. The experimental results show that the proposed method achieves a better trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.02749</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#26356;&#24555;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding. (arXiv:2401.02749v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#24555;&#22320;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36845;&#20195;&#28040;&#38500;&#27861;&#31639;&#27861;&#26469;&#35299;&#20915;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#65292;&#20197;&#36798;&#21040;&#21152;&#36895;&#35299;&#30721;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22312;&#24191;&#27867;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MBR&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#35745;&#31639;MBR&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#38656;&#35201;&#21709;&#24212;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#21098;&#26525;(CBP)&#26041;&#27861;&#26469;&#38477;&#20302;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#23427;&#33021;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#20294;&#26159;&#23427;&#38656;&#35201;&#20351;&#29992;&#24320;&#21457;&#38598;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#20248;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#30340;&#36817;&#20284;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;AMBR&#65289;&#35299;&#30721;&#26041;&#27861;&#12290;AMBR&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#24471;&#20986;&#65306;&#35745;&#31639;&#22522;&#20110;&#26679;&#26412;&#30340;MBR&#30446;&#26631;&#30340;&#38382;&#39064;&#26159;&#20013;&#20301;&#25968;&#35782;&#21035;&#38382;&#39064;&#12290;AMBR&#20351;&#29992;&#20102;&#36845;&#20195;&#28040;&#38500;&#27861;&#65288;CSH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to beam search decoding for a wide range of text generation tasks. However, MBR requires a huge amount of time for inference to compute the MBR objective, which makes the method infeasible in many situations where response time is critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently been proposed to reduce the inference time in machine translation tasks. Although it is shown to significantly reduce the amount of computation, it requires hyperparameter tuning using a development set to be effective. To this end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a hyperparameter-free method to run MBR decoding approximately. AMBR is derived from the observation that the problem of computing the sample-based MBR objective is the medoid identification problem. AMBR uses the Correlated Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best approximation algorithm to date
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#22522;&#20110;&#30340;&#31354;&#38388;&#29305;&#24449;RIR-SF&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#38477;&#20302;&#20102;21.3&#65285;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#24182;&#19988;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00146</link><description>&lt;p&gt;
RIR-SF: &#22522;&#20110;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#30340;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#30340;&#31354;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
RIR-SF: Room Impulse Response Based Spatial Feature for Multi-channel Multi-talker ASR. (arXiv:2311.00146v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#22522;&#20110;&#30340;&#31354;&#38388;&#29305;&#24449;RIR-SF&#65292;&#36890;&#36807;&#19982;&#24050;&#26377;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#34920;&#26126;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#23545;&#38477;&#20302;&#20102;21.3&#65285;&#30340;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#65292;&#24182;&#19988;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#35821;&#38899;&#39046;&#22495;&#20013;&#38754;&#20020;&#25345;&#32493;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#28151;&#21709;&#25928;&#26524;&#26102;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#37325;&#21472;&#30340;&#35821;&#38899;&#20449;&#21495;&#19982;&#30446;&#26631;&#35828;&#35805;&#20154;&#20256;&#36755;&#21040;&#40614;&#20811;&#39118;&#38453;&#21015;&#30340;&#25151;&#38388;&#20914;&#28608;&#21709;&#24212;&#65288;RIR&#65289;&#36827;&#34892;&#21367;&#31215;&#12290;&#36825;&#31181;&#21019;&#26032;&#25216;&#26415;&#20135;&#29983;&#20102;&#19968;&#31181;&#21517;&#20026;RIR-SF&#30340;&#26032;&#22411;&#31354;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#19982;&#20808;&#21069;&#24314;&#31435;&#30340;3D&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;RIR-SF&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RIR-SF&#22312;&#22810;&#36890;&#36947;&#22810;&#35828;&#35805;&#20154;ASR&#31995;&#32479;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#23548;&#33268;&#20102;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#30340;&#26174;&#33879;&#30456;&#23545;&#38477;&#20302;21.3&#65285;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#23545;&#24378;&#28151;&#21709;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-channel multi-talker automatic speech recognition (ASR) presents ongoing challenges within the speech community, particularly when confronted with significant reverberation effects. In this study, we introduce a novel approach involving the convolution of overlapping speech signals with the room impulse response (RIR) corresponding to the target speaker's transmission to a microphone array. This innovative technique yields a novel spatial feature known as the RIR-SF. Through a comprehensive comparison with the previously established state-of-the-art 3D spatial feature, both theoretical analysis and experimental results substantiate the superiority of our proposed RIR-SF. We demonstrate that the RIR-SF outperforms existing methods, leading to a remarkable 21.3\% relative reduction in the Character Error Rate (CER) in multi-channel multi-talker ASR systems. Importantly, this novel feature exhibits robustness in the face of strong reverberation, surpassing the limitations of previou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2310.16560</link><description>&lt;p&gt;
&#22270;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#24212;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Label Propagation for Graph Label Noise. (arXiv:2310.16560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#65292;&#20197;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#23427;&#20250;&#26174;&#33879;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#65292;&#28982;&#32780;&#65292;&#22270;&#27169;&#22411;&#23558;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26356;&#23481;&#26131;&#21463;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36817;&#26399;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#20013;&#26631;&#31614;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#22270;&#26159;&#21516;&#26500;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#26159;&#24179;&#28369;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#21487;&#33021;&#21253;&#21547;&#19981;&#21516;&#31243;&#24230;&#30340;&#24322;&#36136;&#24615;&#29978;&#33267;&#26159;&#24322;&#36136;&#24615;&#30340;&#20027;&#23548;&#65292;&#23548;&#33268;&#24403;&#21069;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20219;&#24847;&#24322;&#36136;&#24615;&#26465;&#20214;&#19979;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#26088;&#22312;&#32416;&#27491;&#22122;&#22768;&#26631;&#31614;&#24182;&#20026;&#20043;&#21069;&#26410;&#26631;&#35760;&#30340;&#33410;&#28857;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#35777;&#20998;&#26512;&#65292;&#25506;&#35752;&#22270;&#21516;&#36136;&#24615;&#23545;&#22270;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#20256;&#25773;&#30340;&#31639;&#27861;&#26469;&#22788;&#29702;&#20219;&#24847;&#24322;&#36136;&#24615;&#30340;&#22270;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise is a common challenge in large datasets, as it can significantly degrade the generalization ability of deep neural networks. Most existing studies focus on noisy labels in computer vision; however, graph models encompass both node features and graph topology as input, and become more susceptible to label noise through message-passing mechanisms. Recently, only a few works have been proposed to tackle the label noise on graphs. One major limitation is that they assume the graph is homophilous and the labels are smoothly distributed. Nevertheless, real-world graphs may contain varying degrees of heterophily or even be heterophily-dominated, leading to the inadequacy of current methods. In this paper, we study graph label noise in the context of arbitrary heterophily, with the aim of rectifying noisy labels and assigning labels to previously unlabeled nodes. We begin by conducting two empirical analyses to explore the impact of graph homophily on graph label noise. Following o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.09605</link><description>&lt;p&gt;
&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#20351;LLMs&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Penetrative AI: Making LLMs Comprehend the Physical World. (arXiv:2310.09605v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#19982;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#12290;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#36827;&#34892;&#29289;&#29702;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#30340;&#24615;&#36136;&#20197;&#21450;&#23427;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#25972;&#21512;&#24120;&#35782;&#20154;&#31867;&#30693;&#35782;&#30340;&#28508;&#21147;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;LLMs&#22914;&#20309;&#36890;&#36807;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#25512;&#29702;&#26469;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#19968;&#27010;&#24565;&#31216;&#20026;&#8220;&#28183;&#36879;&#24335;&#20154;&#24037;&#26234;&#33021;&#8221;&#12290;&#35770;&#25991;&#22312;LLMs&#33021;&#22815;&#36879;&#36807;&#22788;&#29702;&#24863;&#30693;&#20449;&#21495;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#25506;&#32034;&#20102;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#65288;ChatGPT&#26159;&#25105;&#20204;&#30740;&#31350;&#20013;&#30340;&#20195;&#34920;&#24615;&#20363;&#23376;&#65289;&#22312;&#24212;&#29992;&#20869;&#23884;&#30340;&#19990;&#30028;&#30693;&#35782;&#35299;&#37322;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#23545;&#29289;&#29702;&#39046;&#22495;&#30340;&#20219;&#21153;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#30456;&#24403;&#29420;&#29305;&#30340;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#20026;LLMs&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks. Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world. This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term "Penetrative AI". The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals. Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the embedded world knowledge for interpreting IoT sensor data and reasoning over them about tasks in the physical realm. Not only this opens up new applications for LLMs 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14348</link><description>&lt;p&gt;
&#36890;&#36807;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#25269;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#29992;&#20110;&#38450;&#24481;&#21487;&#33021;&#21457;&#29983;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#26500;&#24314;&#65292;&#24182;&#36890;&#36807;&#31283;&#20581;&#30340;&#23545;&#40784;&#26816;&#26597;&#20989;&#25968;&#26469;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;LLMs&#21487;&#33021;&#34987;&#28389;&#29992;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#24694;&#24847;&#20869;&#23481;&#12290;&#23613;&#31649;&#26377;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#40784;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#24182;&#38450;&#27490;&#23427;&#20204;&#29983;&#25104;&#19981;&#36866;&#24403;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#23545;&#40784;&#36890;&#24120;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#20248;&#21270;&#25110;&#25163;&#24037;&#26500;&#24314;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#23545;&#40784;&#30340;LLM&#65288;RA-LLM&#65289;&#65292;&#20197;&#38450;&#33539;&#28508;&#22312;&#30340;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#12290;RA-LLM&#21487;&#20197;&#30452;&#25509;&#26500;&#24314;&#22312;&#29616;&#26377;&#30340;&#23545;&#40784;LLM&#19978;&#65292;&#36890;&#36807;&#20855;&#26377;&#31283;&#20581;&#23545;&#40784;&#26816;&#26597;&#21151;&#33021;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;LLM&#36827;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;RA-LLM&#22312;&#38450;&#24481;&#23545;&#40784;&#30772;&#22351;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.02726</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#24320;&#25918;&#39046;&#22495;&#31185;&#23398;&#20551;&#35774;&#21457;&#29616;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. (arXiv:2309.02726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#31532;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#31995;&#32479;&#65292;&#33021;&#22815;&#22522;&#20110;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31185;&#23398;&#23478;&#35266;&#23519;&#19990;&#30028;&#24182;&#35797;&#22270;&#25552;&#20986;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#20551;&#35774;&#26102;&#65292;&#20551;&#35774;&#24402;&#32435;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#29702;&#31867;&#22411;&#12290;&#36807;&#21435;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#30340;&#35266;&#23519;&#27880;&#37322;&#19981;&#26159;&#21407;&#22987;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#32780;&#26159;&#25163;&#21160;&#36873;&#25321;&#30340;&#21477;&#23376;&#65288;&#23548;&#33268;&#20102;&#19968;&#20010;&#23553;&#38381;&#39046;&#22495;&#30340;&#35774;&#32622;&#65289;&#65307;&#65288;2&#65289;&#23454;&#38469;&#30340;&#20551;&#35774;&#27880;&#37322;&#20027;&#35201;&#26159;&#24120;&#35782;&#30693;&#35782;&#65292;&#20351;&#24471;&#20219;&#21153;&#19981;&#22826;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#31038;&#20250;&#31185;&#23398;&#23398;&#26415;&#20551;&#35774;&#21457;&#29616;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;50&#31687;&#21457;&#34920;&#22312;&#39030;&#32423;&#31038;&#20250;&#31185;&#23398;&#26399;&#21002;&#19978;&#30340;&#26368;&#26032;&#35770;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#25910;&#38598;&#20102;&#24320;&#21457;&#35770;&#25991;&#20013;&#30340;&#20551;&#35774;&#25152;&#38656;&#30340;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#20165;&#36890;&#36807;&#19968;&#22534;&#21407;&#22987;&#32593;&#32476;&#35821;&#26009;&#24211;&#23601;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#12289;&#26032;&#39062;&#19988;&#23545;&#20154;&#31867;&#30740;&#31350;&#32773;&#26377;&#24110;&#21161;&#30340;&#20551;&#35774;&#12290;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#21487;&#20197;&#35299;&#20915;&#20197;&#21069;&#20851;&#20110;&#20551;&#35774;&#24402;&#32435;&#30340;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction has a limited setting that (1) the observation annotations of the dataset are not raw web corpus but are manually selected sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses annotations are mostly commonsense knowledge, making the task less challenging. In this work, we propose the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent papers published in top social science journals. Raw web corpora that are necessary for developing hypotheses in the published papers are also collected in the dataset, with the final goal of creating a system that automatically generates valid, novel, and helpful (to human researchers) hypotheses, given only a pile of raw web corpora. The new dataset can tackle the previou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22522;&#20110;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07429</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Loss for Neural Source Code Summarization. (arXiv:2308.07429v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#30340;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35780;&#20272;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#20013;&#22522;&#20110;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#12290;&#20195;&#30721;&#25688;&#35201;&#26159;&#32534;&#20889;&#28304;&#20195;&#30721;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#12290;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36825;&#20123;&#25551;&#36848;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#12290;&#20960;&#20046;&#25152;&#26377;&#30446;&#21069;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#65292;&#20363;&#22914;GPT&#12289;Codex&#12289;LLaMA&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#26041;&#27861;&#37117;&#20351;&#29992;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;CCE&#65289;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32593;&#32476;&#20248;&#21270;&#12290;CCE&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#23427;&#19968;&#27425;&#35745;&#31639;&#27599;&#20010;&#21333;&#35789;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#32780;&#19981;&#26159;&#35780;&#20272;&#25972;&#20010;&#21477;&#23376;&#65307;2&#65289;&#23427;&#35201;&#27714;&#23436;&#32654;&#39044;&#27979;&#65292;&#19981;&#20801;&#35768;&#23545;&#21516;&#20041;&#35789;&#32473;&#20104;&#37096;&#20998;&#20449;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#35745;&#31639;&#25972;&#20010;&#36755;&#20986;&#21477;&#23376;&#39044;&#27979;&#30340;&#25439;&#22833;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#27599;&#20010;&#35757;&#32451;&#25209;&#27425;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each 
&lt;/p&gt;</description></item><item><title>DualCross&#26159;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;&#21333;&#30446;BEV&#24863;&#30693;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.03724</link><description>&lt;p&gt;
DualCross: &#21333;&#30446;BEV&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#19982;&#36328;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception. (arXiv:2305.03724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03724
&lt;/p&gt;
&lt;p&gt;
DualCross&#26159;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#26088;&#22312;&#20351;&#21333;&#30446;BEV&#24863;&#30693;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#32553;&#23567;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#24182;&#32467;&#21512;&#22810;&#31181;&#20256;&#24863;&#22120;&#27169;&#24577;&#26159;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#21482;&#20851;&#27880;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#65292;&#24573;&#35270;&#20102;&#23454;&#38469;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#21516;&#26102;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DualCross&#65292;&#19968;&#20010;&#36328;&#27169;&#24577;&#21644;&#36328;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;&#23398;&#20064;&#26356;&#20026;&#31283;&#20581;&#30340;&#21333;&#30446;&#40479;&#30640;&#24863;&#30693;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#35757;&#32451;&#38454;&#27573;&#20174;&#19968;&#20010;&#39046;&#22495;&#30340;LiDAR&#20256;&#24863;&#22120;&#20013;&#36716;&#31227;&#28857;&#20113;&#30693;&#35782;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#20165;&#25668;&#20687;&#22836;&#30340;&#27979;&#35797;&#22330;&#26223;&#20013;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#27425;&#23545;&#21333;&#30446;3D&#20219;&#21153;&#30340;&#36328;&#39046;&#22495;&#36328;&#20256;&#24863;&#22120;&#24863;&#30693;&#21644;&#36866;&#24212;&#36827;&#34892;&#20102;&#24320;&#25918;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.04914</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models. (arXiv:2302.04914v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#12289;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#24211;&#23545;&#20110;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#26469;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20174;&#30740;&#31350;&#35770;&#25991;&#30340;&#20840;&#25991;&#20013;&#25552;&#21462;&#26448;&#26009;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#24555;&#36895;&#24320;&#21457;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#24211;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#19981;&#38656;&#35201;&#32534;&#30721;&#65292;&#19981;&#38656;&#35201;&#20851;&#20110;&#25552;&#21462;&#23646;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#25110;&#27169;&#22411;&#35757;&#32451;&#65292;&#19988;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#24211;&#20013;&#20855;&#26377;&#39640;&#21484;&#22238;&#29575;&#21644;&#20960;&#20046;&#23436;&#32654;&#30340;&#31934;&#30830;&#24230;&#12290;&#35813;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#65292;&#38500;&#20102;&#19968;&#20010;&#38656;&#35201;&#20154;&#24037;&#36741;&#21161;&#30340;&#27493;&#39588;&#65292;&#36890;&#24120;&#21482;&#38656;&#35201;&#20960;&#20010;&#23567;&#26102;&#30340;&#20154;&#21147;&#21171;&#21160;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20960;&#20046;&#21487;&#20197;&#19982;&#20219;&#20309;&#27492;&#31867;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#36825;&#37324;&#35780;&#20272;&#20102;GPT-3/3.5&#12289;bart&#21644;DeBERTaV3&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#25552;&#21462;&#20307;&#27169;&#37327;&#25968;&#25454;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#33719;&#24471;&#20102;&#39640;&#36798;90%&#30340;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and comprehensive material databases extracted from research papers are critical for materials science and engineering but require significant human effort to develop. In this paper we present a simple method of extracting materials data from full texts of research papers suitable for quickly developing modest-sized databases. The method requires minimal to no coding, prior knowledge about the extracted property, or model training, and provides high recall and almost perfect precision in the resultant database. The method is fully automated except for one human-assisted step, which typically requires just a few hours of human labor. The method builds on top of natural language processing and large general language models but can work with almost any such model. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for comparison. We provide a detailed detailed analysis of the methods performance in extracting bulk modulus data, obtaining up to 90% precision at 9
&lt;/p&gt;</description></item></channel></rss>