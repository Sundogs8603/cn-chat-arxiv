<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01331</link><description>&lt;p&gt;
&#36890;&#36807;&#21407;&#22987;Wasserstein&#29366;&#24577;&#21344;&#29992;&#21305;&#37197;&#23454;&#29616;&#30340;&#31163;&#32447;&#35266;&#23519;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation from Observation via Primal Wasserstein State Occupancy Matching. (arXiv:2311.01331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;Wasserstein&#36317;&#31163;&#26469;&#21305;&#37197;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#27169;&#20223;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#19982;&#29615;&#22659;&#30340;&#20219;&#24847;&#20132;&#20114;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#19987;&#23478;&#31034;&#33539;&#30340;&#34892;&#20026;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20004;&#32773;&#30340;&#38656;&#27714;&#65292;&#31163;&#32447;&#23398;&#20064;&#20174;&#35266;&#23519;&#65288;LfO&#65289;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#21482;&#26377;&#19987;&#23478;&#29366;&#24577;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#38750;&#19987;&#23478;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#26657;&#27491;&#20272;&#35745;&#65288;DICE&#65289;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#31574;&#30053;&#20043;&#38388;&#30340;&#29366;&#24577;&#21344;&#29992;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#38480;&#20110;$f$-divergences&#65288;KL&#21644;$\chi^2$&#65289;&#25110;&#24102;&#26377;Rubinstein&#23545;&#20598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21518;&#32773;&#38480;&#21046;&#20102;&#23545;&#24615;&#33021;&#20851;&#38190;&#30340;&#22522;&#30784;&#36317;&#31163;&#24230;&#37327;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22987;Wasserstein DICE&#65288;PW-DICE&#65289;&#65292;&#23427;&#36890;&#36807;&#24754;&#35266;&#27491;&#21017;&#21270;&#22120;&#26368;&#23567;&#21270;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#21344;&#29992;&#20043;&#38388;&#30340;&#21407;&#22987;Wasserstein&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;dis
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, arbitrary interactions with the environment can often be costly, and actions of expert demonstrations are not always available. To reduce the need for both, Offline Learning from Observations (LfO) is extensively studied, where the agent learns to solve a task with only expert states and \textit{task-agnostic} non-expert state-action pairs. The state-of-the-art DIstribution Correction Estimation (DICE) methods minimize the state occupancy divergence between the learner and expert policies. However, they are limited to either $f$-divergences (KL and $\chi^2$) or Wasserstein distance with Rubinstein duality, the latter of which constrains the underlying distance metric crucial to the performance of Wasserstein-based solutions. To address this problem, we propose Primal Wasserstein DICE (PW-DICE), which minimizes the primal Wasserstein distance between the expert and learner state occupancies with a pessimistic regularizer and leverages a contrastively learned dis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.19805</link><description>&lt;p&gt;
SERA&#65306;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SERA&#30340;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#35774;&#35745;&#20869;&#22312;&#22870;&#21169;&#26469;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#28508;&#22312;&#24212;&#29992;&#26159;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#21021;&#22987;&#21270;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#36827;&#34892;&#21518;&#32493;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23545;&#31163;&#32447;&#39044;&#35757;&#32451;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#31163;&#32447;&#20445;&#23432;&#26041;&#27861;&#38477;&#20302;&#20102;agent&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#22312;&#32447;&#24494;&#35843;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26679;&#26412;&#39640;&#25928;&#22870;&#21169;&#22686;&#24378;&#65288;SERA&#65289;&#30340;&#36890;&#29992;&#22870;&#21169;&#22686;&#24378;&#26694;&#26550;&#12290;SERA&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#40723;&#21169;agent&#36827;&#34892;&#25506;&#32034;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#25913;&#21892;&#22312;&#32447;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#38544;&#24335;&#22320;&#23454;&#29616;&#20102;&#29366;&#24577;&#36793;&#32536;&#21305;&#37197;&#65288;SMM&#65289;&#24182;&#24809;&#32602;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#29366;&#24577;&#34892;&#21160;&#65292;&#20174;&#32780;&#40723;&#21169;agent&#35206;&#30422;&#30446;&#26631;&#29366;&#24577;&#23494;&#24230;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#22312;&#32447;&#24494;&#35843;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.13230</link><description>&lt;p&gt;
&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Absolute Policy Optimization. (arXiv:2310.13230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#19979;&#30028;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#21644;Atari&#28216;&#25103;&#20013;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20449;&#20219;&#22495;&#30340;&#22312;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#21644;&#28216;&#25103;&#22330;&#26223;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#31867;&#21035;&#20013;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#20027;&#35201;&#24378;&#35843;&#23545;&#39044;&#26399;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#32570;&#20047;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#24615;&#33021;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65307;&#36890;&#36807;&#20248;&#21270;&#35813;&#20989;&#25968;&#65292;&#21487;&#20197;&#30830;&#20445;&#36817;&#20046;&#24635;&#20307;&#24615;&#33021;&#26679;&#26412;&#30340;&#19979;&#30028;&#65288;&#32477;&#23545;&#24615;&#33021;&#65289;&#21576;&#29616;&#21333;&#35843;&#25913;&#36827;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#23545;&#36825;&#20010;&#29702;&#35770;&#22522;&#30784;&#31639;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#31216;&#20026;&#32477;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;APO&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#25484;&#25569;Atari&#28216;&#25103;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;APO&#22312;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20063;&#26174;&#33879;&#25913;&#21892;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, trust region on-policy reinforcement learning has achieved impressive results in addressing complex control tasks and gaming scenarios. However, contemporary state-of-the-art algorithms within this category primarily emphasize improvement in expected performance, lacking the ability to control over the worst-case performance outcomes. To address this limitation, we introduce a novel objective function; by optimizing which, it will lead to guaranteed monotonic improvement in the lower bound of near-total performance samples (absolute performance). Considering this groundbreaking theoretical advancement, we then refine this theoretically grounded algorithm through a series of approximations, resulting in a practical solution called Absolute Policy Optimization (APO). Our experiments demonstrate the effectiveness of our approach across challenging continuous control benchmark tasks and extend its applicability to mastering Atari games. Our findings reveal that APO signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#65288;TDL&#65289;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#33021;&#26377;&#25928;&#25429;&#25417;&#38899;&#39057;&#30340;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.03036</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#30340;&#39640;&#25928;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Efficient Temporary Deepfake Location Approach Based Embeddings for Partially Spoofed Audio Detection. (arXiv:2309.03036v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#20020;&#26102;Deepfake&#20301;&#32622;&#26041;&#27861;&#65288;TDL&#65289;&#29992;&#20110;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#65292;&#36890;&#36807;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#33021;&#26377;&#25928;&#25429;&#25417;&#38899;&#39057;&#30340;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22312;&#24103;&#32423;&#21035;&#20934;&#30830;&#22320;&#23450;&#20301;&#38899;&#39057;&#30340;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#21270;&#30340;&#37096;&#20998;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#20020;&#26102;Deepfake&#20301;&#32622;&#65288;TDL&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#29305;&#24449;&#21644;&#20301;&#32622;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#37096;&#20998;&#65306;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#21644;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#12290;&#20026;&#20102;&#22686;&#24378;&#30495;&#23454;&#29305;&#24449;&#21644;&#20266;&#36896;&#29305;&#24449;&#20043;&#38388;&#30340;&#21306;&#20998;&#24230;&#65292;&#23884;&#20837;&#30456;&#20284;&#24615;&#27169;&#22359;&#34987;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#23558;&#30495;&#23454;&#24103;&#19982;&#20266;&#36896;&#24103;&#20998;&#31163;&#24320;&#26469;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20851;&#27880;&#20301;&#32622;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#21367;&#31215;&#25805;&#20316;&#65292;&#29992;&#20110;&#35745;&#31639;&#30456;&#37051;&#24103;&#20043;&#38388;&#30340;&#29305;&#23450;&#24103;&#30456;&#20284;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#37051;&#23621;&#36827;&#34892;&#21367;&#31215;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ASVspoof2019 Partial Spoof&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially spoofed audio detection is a challenging task, lying in the need to accurately locate the authenticity of audio at the frame level. To address this issue, we propose a fine-grained partially spoofed audio detection method, namely Temporal Deepfake Location (TDL), which can effectively capture information of both features and locations. Specifically, our approach involves two novel parts: embedding similarity module and temporal convolution operation. To enhance the identification between the real and fake features, the embedding similarity module is designed to generate an embedding space that can separate the real frames from fake frames. To effectively concentrate on the position information, temporal convolution operation is proposed to calculate the frame-specific similarities among neighboring frames, and dynamically select informative neighbors to convolution. Extensive experiments show that our method outperform baseline models in ASVspoof2019 Partial Spoof dataset and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.00018</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#38750;&#19987;&#23478;&#29992;&#25143;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35832;&#22914;&#38598;&#25104;&#26799;&#24230;&#31561;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20135;&#29983;&#20102;&#21253;&#21547;&#22823;&#37327;&#20449;&#24687;&#20294;&#38590;&#20197;&#35299;&#37322;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#26368;&#22823;&#28608;&#27963;&#32452;&#25552;&#21462;&#65288;MAGE&#65289;&#21644;&#22810;&#23610;&#24230;&#21487;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#65288;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#25214;&#21040;&#32473;&#23450;CNN&#20013;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23558;&#36825;&#20123;&#30456;&#20284;&#29305;&#24449;&#27169;&#24335;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65288;&#21253;&#25324;&#22240;&#26524;&#20851;&#31995;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#31867;&#21035;&#24863;&#30693;&#39034;&#24207;&#30456;&#20851;&#24615;&#65288;CaOC&#65289;&#65292;&#20840;&#23616;&#35780;&#20272;&#26681;&#25454;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, enhancing global interpretability. MAGE finds, for a given CNN, combinations of features which, globally, form a semantic meaning, that we call concepts. We group these similar feature patterns by clustering in ``concepts'', that we visualize through Ms-IV. This last method is inspired by Occlusion and Sensitivity analysis (incorporating causality), and uses a novel metric, called Class-aware Order Correlation (CaOC), to globally evaluate the most important image regions according to the model's 
&lt;/p&gt;</description></item><item><title>survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;</title><link>http://arxiv.org/abs/2308.16113</link><description>&lt;p&gt;
survex&#65306;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#29983;&#23384;&#27169;&#22411;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
survex: an R package for explaining machine learning survival models. (arXiv:2308.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16113
&lt;/p&gt;
&lt;p&gt;
survex&#26159;&#19968;&#20010;R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#65292;&#21487;&#20197;&#25913;&#36827;&#27169;&#22411;&#65292;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#20986;&#33394;&#24615;&#33021;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#29992;&#20110;&#34917;&#20805;&#21644;&#36229;&#36234;&#20256;&#32479;&#30340;&#32479;&#35745;&#29983;&#23384;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#25805;&#20316;&#21644;&#39044;&#27979;&#21407;&#29702;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;survex R&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#24212;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#20219;&#20309;&#29983;&#23384;&#27169;&#22411;&#12290;&#25152;&#25552;&#36719;&#20214;&#30340;&#21151;&#33021;&#21253;&#25324;&#29702;&#35299;&#21644;&#35786;&#26029;&#29983;&#23384;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#20197;&#25913;&#36827;&#23427;&#20204;&#12290;&#36890;&#36807;&#25581;&#31034;&#21464;&#37327;&#25928;&#24212;&#21644;&#37325;&#35201;&#24615;&#31561;&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#65292;survex&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24182;&#26816;&#27979;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#21307;&#30103;&#24212;&#29992;&#31561;&#25935;&#24863;&#39046;&#22495;&#21487;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their flexibility and superior performance, machine learning models frequently complement and outperform traditional statistical survival models. However, their widespread adoption is hindered by a lack of user-friendly tools to explain their internal operations and prediction rationales. To tackle this issue, we introduce the survex R package, which provides a cohesive framework for explaining any survival model by applying explainable artificial intelligence techniques. The capabilities of the proposed software encompass understanding and diagnosing survival models, which can lead to their improvement. By revealing insights into the decision-making process, such as variable effects and importances, survex enables the assessment of model reliability and the detection of biases. Thus, transparency and responsibility may be promoted in sensitive areas, such as biomedical research and healthcare applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11494</link><description>&lt;p&gt;
&#39044;&#27979;&#12289;&#25913;&#36827;&#12289;&#21512;&#25104;&#65306;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting. (arXiv:2307.11494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33258;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;TSDiff&#12290;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#36741;&#21161;&#32593;&#32476;&#25110;&#35757;&#32451;&#36807;&#31243;&#30340;&#25913;&#21464;&#65292;&#22312;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39044;&#27979;&#25110;&#22635;&#34917;&#20219;&#21153;&#30340;&#26465;&#20214;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38754;&#21521;&#22810;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TSDiff&#65292;&#19968;&#31181;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#26465;&#20214;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#24341;&#23548;&#26426;&#21046;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#24471;TSDiff&#33021;&#22815;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#26465;&#20214;&#35774;&#32622;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32593;&#32476;&#25110;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#39044;&#27979;&#12289;&#25913;&#36827;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;TSDiff&#19982;&#20960;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#39044;&#27979;&#26041;&#27861;&#30456;&#31454;&#20105;&#65288;&#39044;&#27979;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;TSDiff&#23398;&#21040;&#30340;&#38544;&#24615;&#27010;&#29575;&#23494;&#24230;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;p
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved state-of-the-art performance in generative modeling tasks across various domains. Prior works on time series diffusion models have primarily focused on developing conditional models tailored to specific forecasting or imputation tasks. In this work, we explore the potential of task-agnostic, unconditional diffusion models for several time series applications. We propose TSDiff, an unconditionally trained diffusion model for time series. Our proposed self-guidance mechanism enables conditioning TSDiff for downstream tasks during inference, without requiring auxiliary networks or altering the training procedure. We demonstrate the effectiveness of our method on three different time series tasks: forecasting, refinement, and synthetic data generation. First, we show that TSDiff is competitive with several task-specific conditional forecasting methods (predict). Second, we leverage the learned implicit probability density of TSDiff to iteratively refine the p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2307.08927</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Stage Cable Routing through Hierarchical Imitation Learning. (arXiv:2307.08927v3 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#38454;&#27573;&#30005;&#32518;&#24067;&#32447;&#20219;&#21153;&#20013;&#30340;&#23618;&#27425;&#21270;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#12289;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#21644;&#25193;&#23637;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;&#25104;&#21151;&#25511;&#21046;&#22120;&#38656;&#35201;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#30005;&#32518;&#24067;&#32447;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#36890;&#36807;&#19968;&#31995;&#21015;&#22841;&#23376;&#26469;&#24067;&#32447;&#12290;&#36825;&#20010;&#35774;&#32622;&#20195;&#34920;&#20102;&#22797;&#26434;&#22810;&#38454;&#27573;&#26426;&#22120;&#20154;&#25805;&#20316;&#22330;&#26223;&#30340;&#25361;&#25112;&#65306;&#22788;&#29702;&#21487;&#21464;&#24418;&#29289;&#20307;&#65292;&#23545;&#35270;&#35273;&#24863;&#30693;&#38381;&#29615;&#65292;&#22788;&#29702;&#30001;&#22810;&#20010;&#27493;&#39588;&#32452;&#25104;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#24517;&#39035;&#25104;&#21151;&#25191;&#34892;&#25165;&#33021;&#23436;&#25104;&#25972;&#20010;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#23398;&#20064;&#25104;&#21151;&#29575;&#36275;&#22815;&#39640;&#30340;&#21333;&#20010;&#22522;&#20803;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65306;&#22914;&#26524;&#27599;&#20010;&#38454;&#27573;&#24517;&#39035;&#25104;&#21151;&#23436;&#25104;&#24182;&#19988;&#26377;&#36739;&#22823;&#30340;&#22833;&#36133;&#27010;&#29575;&#65292;&#25972;&#20010;&#20219;&#21153;&#25104;&#21151;&#23436;&#25104;&#30340;&#27010;&#29575;&#21464;&#24471;&#24494;&#19981;&#36275;&#36947;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#22810;&#38454;&#27573;&#20219;&#21153;&#30340;&#25104;&#21151;&#25511;&#21046;&#22120;&#24517;&#39035;&#33021;&#22815;&#20174;&#22833;&#36133;&#20013;&#24674;&#22797;&#65292;&#24182;&#36890;&#36807;&#32874;&#26126;&#22320;&#36873;&#25321;&#20174;&#32780;&#32416;&#27491;&#20302;&#32423;&#25511;&#21046;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning to perform multi-stage robotic manipulation tasks, with applications to cable routing, where the robot must route a cable through a series of clips. This setting presents challenges representative of complex multi-stage robotic manipulation scenarios: handling deformable objects, closing the loop on visual perception, and handling extended behaviors consisting of multiple steps that must be executed successfully to complete the entire task. In such settings, learning individual primitives for each stage that succeed with a high enough rate to perform a complete temporally extended task is impractical: if each stage must be completed successfully and has a non-negligible probability of failure, the likelihood of successful completion of the entire task becomes negligible. Therefore, successful controllers for such multi-stage tasks must be able to recover from failure and compensate for imperfections in low-level controllers by smartly choosing which con
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2307.01452</link><description>&lt;p&gt;
&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01452
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#33539;&#24335;&#12290;&#23613;&#31649;&#36817;&#20960;&#21313;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#22240;&#27492;&#24517;&#39035;&#36890;&#36807;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#23398;&#20064;&#12290;&#20182;&#20204;&#21487;&#33021;&#22312;&#35299;&#37322;&#33258;&#24049;&#30340;&#20915;&#31574;&#20197;&#21450;&#25512;&#24191;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#23427;&#21487;&#20197;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#19981;&#21464;&#24615;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#36825;&#23548;&#33268;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#23427;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22240;&#26524;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#26469;&#22686;&#24378;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcemen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2306.16772</link><description>&lt;p&gt;
&#20174;&#21512;&#25104;&#30340;&#20154;&#31867;&#22242;&#38431;&#27963;&#21160;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Synthetic Human Group Activities. (arXiv:2306.16772v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16772
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;Unity&#24341;&#25806;&#39537;&#21160;&#23454;&#29616;&#12290;&#35813;&#29983;&#25104;&#22120;&#20855;&#26377;&#22823;&#35268;&#27169;&#25968;&#25454;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#23545;&#22797;&#26434;&#30340;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#30340;&#29702;&#35299;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#20219;&#21153;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#33719;&#21462;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3Act&#65292;&#19968;&#20010;&#22810;&#35270;&#22270;&#22810;&#22242;&#38431;&#22810;&#20154;&#30340;&#20154;&#31867;&#21407;&#23376;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;M3Act&#37319;&#29992;Unity&#24341;&#25806;&#39537;&#21160;&#65292;&#21253;&#21547;&#21487;&#20379;&#20223;&#30495;&#20351;&#29992;&#30340;&#19977;&#32500;&#22330;&#26223;&#21644;&#20154;&#29289;&#36164;&#28304;&#65292;&#21487;&#37197;&#32622;&#30340;&#29031;&#26126;&#21644;&#25668;&#20687;&#31995;&#32479;&#65292;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22359;&#21270;&#22242;&#38431;&#27963;&#21160;&#65292;&#20197;&#21450;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20855;&#26377;&#22823;&#37327;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#12289;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#12289;2D&#23039;&#21183;&#12289;3D&#21160;&#20316;&#65289;&#21644;&#39640;&#36136;&#37327;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;2D&#36793;&#30028;&#26694;&#12289;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#12289;&#20010;&#20307;&#21160;&#20316;&#21644;&#22242;&#38431;&#27963;&#21160;&#31867;&#21035;&#65289;&#12290;&#21033;&#29992;M3Act&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#27963;&#21160;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#20114;&#21160;&#21644;&#22242;&#38431;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of complex human interactions and group activities has garnered attention in human-centric computer vision. However, the advancement of the related tasks is hindered due to the difficulty of obtaining large-scale labeled real-world datasets. To mitigate the issue, we propose M3Act, a multi-view multi-group multi-person human atomic action and group activity data generator. Powered by the Unity engine, M3Act contains simulation-ready 3D scenes and human assets, configurable lighting and camera systems, highly parameterized modular group activities, and a large degree of domain randomization during the data generation process. Our data generator is capable of generating large-scale datasets of human activities with multiple viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality annotations for individual persons and multi-person groups (2D bounding boxes, instance segmentation masks, individual actions and group activity categories). Using M3Act, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.10229</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65306;&#20174;&#32858;&#31867;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#25968;&#25454;&#32452;&#32455;&#30340;&#24046;&#24322;&#65292;&#37325;&#28857;&#20851;&#27880;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#30456;&#23545;&#23616;&#37096;&#23494;&#24230;&#65288;RLD&#65289;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#31751;&#20869;&#30340;&#23616;&#37096;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35270;&#35273;&#31034;&#20363;&#65292;&#20197;&#31361;&#20986;&#23616;&#37096;&#23494;&#38598;&#31751;&#21644;&#20840;&#23616;&#23494;&#38598;&#31751;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#24418;&#25104;&#30340;&#31751;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#32780;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35777;&#26126;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
&lt;/p&gt;</description></item><item><title>DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07061</link><description>&lt;p&gt;
DroidBot-GPT&#65306;&#22522;&#20110;GPT&#30340;Android UI&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT: GPT-powered UI Automation for Android. (arXiv:2304.07061v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07061
&lt;/p&gt;
&lt;p&gt;
DroidBot-GPT&#26159;&#19968;&#27454;&#21033;&#29992;GPT&#27169;&#22411;&#33258;&#21160;&#21270;Android&#24212;&#29992;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#26681;&#25454;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#26377;&#26395;&#25552;&#39640;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DroidBot-GPT&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31867;&#20284;GPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#19982;Android&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#30340;&#24037;&#20855;&#12290;&#32473;&#23450;&#25152;&#38656;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;DroidBot-GPT&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#24182;&#25191;&#34892;&#25805;&#20316;&#65292;&#23548;&#33322;&#24212;&#29992;&#31243;&#24207;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#23558;&#24212;&#29992;&#31243;&#24207;GUI&#29366;&#24577;&#20449;&#24687;&#21644;&#26234;&#33021;&#25163;&#26426;&#23631;&#24149;&#19978;&#21487;&#29992;&#30340;&#25805;&#20316;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#24182;&#35201;&#27714;LLM&#36873;&#25321;&#21160;&#20316;&#26469;&#23454;&#29616;&#12290;&#30001;&#20110;LLM&#36890;&#24120;&#21463;&#36807;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#21253;&#25324;&#21508;&#31181;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#22240;&#27492;&#23427;&#20855;&#26377;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#20316;&#20986;&#21512;&#29702;&#21160;&#20316;&#36873;&#25321;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#33258;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;DroidBot-GPT&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;10&#20010;&#31867;&#21035;&#30340;17&#20010;Android&#24212;&#29992;&#31243;&#24207;&#30340;33&#20010;&#20219;&#21153;&#12290;&#23427;&#21487;&#20197;&#25104;&#21151;&#23436;&#25104;39.39%&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#24179;&#22343;&#37096;&#20998;&#23436;&#25104;&#36827;&#24230;&#32422;&#20026;66.76%&#12290;&#37492;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#26159;&#24191;&#27867;&#21487;&#29992;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;DroidBot-GPT&#22312;&#25913;&#21892;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#27979;&#35797;&#21644;&#24320;&#21457;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large language models (LLMs) to automate the interactions with Android mobile applications. Given a natural language description of a desired task, DroidBot-GPT can automatically generate and execute actions that navigate the app to complete the task. It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions. Since the LLM is typically trained on a large amount of data including the how-to manuals of diverse software applications, it has the ability to make reasonable choices of actions based on the provided information. We evaluate DroidBot-GPT with a self-created dataset that contains 33 tasks collected from 17 Android applications spanning 10 categories. It can successfully complete 39.39% of the tasks, and the average partial completion progress is about 66.76%. Given the fact that our method is f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03898</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#22312;&#24191;&#21578;&#25628;&#32034;&#21644;&#25512;&#33616;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#38271;&#24230;&#30701;&#65292;&#35821;&#20041;&#20449;&#24687;&#21294;&#20047;&#21644;&#21333;&#35789;&#27495;&#20041;&#38382;&#39064;&#25104;&#20026;&#27492;&#31867;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#25991;&#26412;&#34917;&#20805;&#21477;&#23376;&#25110;&#30693;&#35782;&#24211;&#26469;&#25552;&#20379;&#38468;&#21152;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#22320;&#20132;&#20114;&#21407;&#22987;&#21477;&#23376;&#21644;&#34917;&#20805;&#21477;&#23376;&#65292;&#20063;&#27809;&#26377;&#32771;&#34385;&#21040;&#22806;&#37096;&#30693;&#35782;&#24211;&#24341;&#20837;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#34917;&#20805;&#21477;&#23376;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#33719;&#24471;&#26356;&#20855;&#35821;&#20041;&#21305;&#37197;&#24615;&#30340;&#21407;&#22987;&#21477;&#23376;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#38190;&#35789;&#20316;&#20026;&#21407;&#22987;&#21477;&#23376;&#30340;&#20027;&#35201;&#35821;&#20041;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, short Text Matching tasks have been widely applied in the fields ofadvertising search and recommendation. The difficulty lies in the lack of semantic information and word ambiguity caused by the short length of the text. Previous works have introduced complement sentences or knowledge bases to provide additional feature information. However, these methods have not fully interacted between the original sentence and the complement sentence, and have not considered the noise issue that may arise from the introduction of external knowledge bases. Therefore, this paper proposes a short Text Matching model that combines contrastive learning and external knowledge. The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence. In addition, to avoid noise, we use keywords as the main semantics of the original sentence to retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.02536</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#21464;&#37327;&#21644;&#20998;&#24067;&#24335;&#31070;&#32463;&#34920;&#31034;&#20043;&#38388;&#23547;&#25214;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25277;&#35937;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#23450;&#20041;&#20102;&#21487;&#35299;&#37322;&#30340;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#20309;&#26102;&#26159;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#20449;&#31616;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#38656;&#35201;&#22312;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#26292;&#21147;&#25628;&#32034;&#23545;&#40784;&#65292;&#24182;&#19988;&#23427;&#20204;&#39044;&#35774;&#39640;&#23618;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#23558;&#19982;&#20302;&#23618;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20132;&#30340;&#31070;&#32463;&#20803;&#38598;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;DAS&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#20801;&#35768;&#20010;&#20307;&#31070;&#32463;&#20803;&#22312;&#38750;&#20256;&#32479;&#22522;&#24213;&#20998;&#24067;&#34920;&#31034;&#20013;&#21457;&#25381;&#22810;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;DAS&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS 
&lt;/p&gt;</description></item><item><title>Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04589</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04589
&lt;/p&gt;
&lt;p&gt;
Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#33829;&#38144;&#21644;&#21307;&#23398;&#20013;&#65292;&#22522;&#20110;&#30701;&#26399;&#20195;&#29702;&#26469;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26576;&#20123;&#39046;&#22495;&#20013;&#24050;&#26377;&#25152;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20197;&#19968;&#31181;&#29702;&#24819;&#21270;&#21644;&#31616;&#21270;&#30340;&#26041;&#24335;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24573;&#30053;&#20102;&#30701;&#26399;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#20840;&#37096;&#35270;&#20026;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#19982;&#23427;&#20204;&#22312;&#30701;&#26399;&#32467;&#26524;&#20013;&#30340;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Laser&#65292;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#65292;&#20854;&#20013;&#35266;&#23519;&#21040;&#20195;&#29702;&#25110;&#20855;&#26377;&#35266;&#23519;&#20195;&#29702;&#12290;&#37492;&#20110;&#20195;&#29702;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#19981;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;iVAE&#65289;&#22312;&#19981;&#38656;&#35201;&#21306;&#20998;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#25110;&#20808;&#39564;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25152;&#26377;&#26377;&#25928;&#20195;&#29702;&#20505;&#36873;&#32773;&#19978;&#30340;&#25972;&#20010;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating long-term causal effects based on short-term surrogates is a significant but challenging problem in many real-world applications, e.g., marketing and medicine. Despite its success in certain domains, most existing methods estimate causal effects in an idealistic and simplistic way - ignoring the causal structure among short-term outcomes and treating all of them as surrogates. However, such methods cannot be well applied to real-world scenarios, in which the partially observed surrogates are mixed with their proxies among short-term outcomes. To this end, we develop our flexible method, Laser, to estimate long-term causal effects in the more realistic situation that the surrogates are observed or have observed proxies.Given the indistinguishability between the surrogates and proxies, we utilize identifiable variational auto-encoder (iVAE) to recover the whole valid surrogates on all the surrogates candidates without the need of distinguishing the observed surrogates or the p
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item></channel></rss>