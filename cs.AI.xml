<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.12451</link><description>&lt;p&gt;
INSIGHT: &#24102;&#26377;&#35821;&#35328;&#35299;&#37322;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#31526;&#21495;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#65292;&#24182;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#24378;&#21270;&#23398;&#20064;&#65288;NS-RL&#65289;&#24050;&#25104;&#20026;&#21487;&#35299;&#37322;&#20915;&#31574;&#21046;&#23450;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#65292;&#20854;&#29305;&#28857;&#26159;&#31526;&#21495;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#35270;&#35273;&#35266;&#27979;&#30340;&#20219;&#21153;&#65292;NS-RL&#28041;&#21450;&#23545;&#29366;&#24577;&#36827;&#34892;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25928;&#29575;&#65292;&#20808;&#21069;&#30340;&#31639;&#27861;&#26080;&#27861;&#21033;&#29992;&#22870;&#21169;&#20449;&#21495;&#26469;&#32454;&#21270;&#32467;&#26500;&#21270;&#29366;&#24577;&#12290;&#21487;&#35775;&#38382;&#24615;&#20063;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#37322;&#24403;&#21069;&#30340;&#31526;&#21495;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21270;&#29366;&#24577;&#21644;&#31526;&#21495;&#31574;&#30053;&#30340;&#26694;&#26550;&#65292;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#25552;&#28860;&#25104;&#21487;&#25193;&#23637;&#30340;&#24863;&#30693;&#27169;&#22359;&#65292;&#20811;&#26381;&#25928;&#29575;&#29942;&#39048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#25919;&#31574;&#21644;&#20915;&#31574;&#29983;&#25104;&#31616;&#27905;&#26131;&#35835;&#30340;&#35821;&#35328;&#35299;&#37322;&#12290;&#22312;&#20061;&#20010;Atari&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12451v1 Announce Type: new  Abstract: Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising paradigm for explainable decision-making, characterized by the interpretability of symbolic policies. For tasks with visual observations, NS-RL entails structured representations for states, but previous algorithms are unable to refine the structured states with reward signals due to a lack of efficiency. Accessibility is also an issue, as extensive domain knowledge is required to interpret current symbolic policies. In this paper, we present a framework that is capable of learning structured states and symbolic policies simultaneously, whose key idea is to overcome the efficiency bottleneck by distilling vision foundation models into a scalable perception module. Moreover, we design a pipeline that uses large language models to generate concise and readable language explanations for policies and decisions. In experiments on nine Atari tasks, our approach demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12093</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#23398;&#20064;&#65306;&#19968;&#31181;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#22330;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;Stackelberg Mean Field Game&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#65292;&#24182;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#26080;&#27169;&#22411;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#22312;&#20419;&#36827;&#32463;&#27982;&#22686;&#38271;&#21644;&#31038;&#20250;&#31283;&#23450;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22522;&#20110;Stackelberg Mean Field Game&#65288;SMFG&#65289;&#27169;&#22411;&#65292;&#23558;&#26368;&#20248;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#38382;&#39064;&#24314;&#27169;&#65292;&#20854;&#20013;&#25919;&#24220;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#30340;&#39046;&#23548;&#32773;&#65292;&#22823;&#35268;&#27169;&#23478;&#24237;&#21160;&#24577;&#21709;&#24212;&#20026;&#36861;&#38543;&#32773;&#12290;&#36825;&#31181;&#24314;&#27169;&#26041;&#27861;&#25429;&#25417;&#20102;&#25919;&#24220;&#21644;&#22823;&#35268;&#27169;&#23478;&#24237;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#21160;&#24577;&#21338;&#24328;&#65292;&#24182;&#21487;&#20197;&#35299;&#37322;&#22320;&#35780;&#20272;&#22522;&#20110;&#24494;&#35266;&#22522;&#30784;&#30340;&#23439;&#35266;&#32463;&#27982;&#25919;&#31574;&#25928;&#26524;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;SMFG&#30340;&#26041;&#27861;&#65292;&#23558;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;Stackelberg&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#65288;SMFRL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#29420;&#31435;&#20110;&#20808;&#21069;&#30340;&#29615;&#22659;&#30693;&#35782;&#21644;&#36716;&#21464;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;SMFG&#26041;&#27861;&#22312;&#32463;&#27982;&#25919;&#31574;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12093v1 Announce Type: cross  Abstract: Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms 
&lt;/p&gt;</description></item><item><title>QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11886</link><description>&lt;p&gt;
QueryAgent&#65306;&#19968;&#31181;&#20855;&#26377;&#29615;&#22659;&#21453;&#39304;&#30340;&#21487;&#38752;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#21450;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11886
&lt;/p&gt;
&lt;p&gt;
QueryAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#65292;&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#39640;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#22312;&#36935;&#21040;&#24187;&#35273;&#26102;&#29616;&#26377;&#26041;&#27861;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;QueryAgent&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#24182;&#36827;&#34892;&#36880;&#27493;&#33258;&#25105;&#26657;&#27491;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#21453;&#39304;&#30340;&#33258;&#25105;&#26657;&#27491;&#26041;&#27861;ERASER&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ERASER&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#20013;&#30340;&#20016;&#23500;&#29615;&#22659;&#21453;&#39304;&#65292;&#22312;&#24517;&#35201;&#26102;&#20165;&#36827;&#34892;&#36873;&#25321;&#24615;&#21644;&#24046;&#24322;&#21270;&#30340;&#33258;&#25105;&#26657;&#27491;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QueryAgent&#22312;GrailQA&#21644;GraphQ&#19978;&#20165;&#20351;&#29992;&#19968;&#20010;&#20363;&#23376;&#23601;&#27604;&#25152;&#26377;&#20808;&#21069;&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#21462;&#24471;&#20102;7.0&#21644;15.0&#30340;F1&#20540;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#21253;&#25324;&#36816;&#34892;&#26102;&#38388;&#12289;&#26597;&#35810;&#24320;&#38144;&#21644;API&#35843;&#29992;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;ERASER&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11886v1 Announce Type: cross  Abstract: Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11432</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#30340;&#25581;&#31192;
&lt;/p&gt;
&lt;p&gt;
Demystifying Deep Reinforcement Learning-Based Autonomous Vehicle Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#20027;&#36710;&#36742;&#20915;&#31574;&#20013;&#65292;&#36890;&#36807;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#28155;&#21152;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20986;&#29616;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#25968;&#37327;&#28608;&#22686;&#12290;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#24050;&#25104;&#20026;&#20854;&#20013;&#19968;&#39033;&#20027;&#35201;&#24212;&#29992;&#65292;&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#25110;&#39640;&#38454;&#36816;&#21160;&#23398;&#21464;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#31163;&#25955;&#36873;&#25321;&#25110;&#36830;&#32493;&#25511;&#21046;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;DRL&#22312;&#33258;&#20027;&#36710;&#36742;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;DRL&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;AV&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#20102;&#22522;&#20110;&#36830;&#32493;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;DRL&#31639;&#27861;&#20316;&#20026;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#28155;&#21152;&#20102;&#19968;&#20010;&#22810;&#22836;&#27880;&#24847;&#21147;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20998;&#26512;&#25216;&#26415;&#26469;&#35752;&#35770;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11432v1 Announce Type: cross  Abstract: With the advent of universal function approximators in the domain of reinforcement learning, the number of practical applications leveraging deep reinforcement learning (DRL) has exploded. Decision-making in automated driving tasks has emerged as a chief application among them, taking the sensor data or the higher-order kinematic variables as the input and providing a discrete choice or continuous control output. However, the black-box nature of the models presents an overwhelming limitation that restricts the real-world deployment of DRL in autonomous vehicles (AVs). Therefore, in this research work, we focus on the interpretability of an attention-based DRL framework. We use a continuous proximal policy optimization-based DRL algorithm as the baseline model and add a multi-head attention framework in an open-source AV simulation environment. We provide some analytical techniques for discussing the interpretability of the trained mode
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03744</link><description>&lt;p&gt;
&#20026;&#21307;&#33647;&#39046;&#22495;&#25171;&#36896;&#23433;&#20840;&#21644;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Safe and Aligned Large Language Models for Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03744
&lt;/p&gt;
&lt;p&gt;
&#23545;&#21307;&#23398;LLMs&#36827;&#34892;&#20102;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23450;&#20041;&#21307;&#23398;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#65292;&#24320;&#21457;&#20102;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#27491;&#22312;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#36827;&#27493;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#32773;&#20063;&#23545;&#23427;&#20204;&#30340;&#28508;&#21147;&#21644;&#39118;&#38505;&#30340;&#28145;&#24230;&#24863;&#21040;&#22256;&#24785;&#12290;&#23613;&#31649;&#24050;&#32463;&#37319;&#21462;&#20102;&#21021;&#27493;&#27493;&#39588;&#35780;&#20272;&#36890;&#29992;&#30693;&#35782;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#24369;&#28857;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#22312;&#20010;&#20154;&#20581;&#24247;&#21644;&#23433;&#20840;&#12289;&#20844;&#20849;&#20581;&#24247;&#21644;&#23433;&#20840;&#20197;&#21450;&#20154;&#26435;&#26041;&#38754;&#23384;&#22312;&#39118;&#38505;&#65292;&#21307;&#23398;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#21307;&#23398;LLMs&#30340;&#39318;&#27425;&#23433;&#20840;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#30340;&#23450;&#20041;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#23475;&#21307;&#23398;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;LLM&#30340;&#21307;&#23398;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#35780;&#20272;&#20102;&#21307;&#23398;LLMs&#30340;&#36890;&#29992;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#23637;&#31034;&#20102;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26356;&#24191;&#27867;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03744v1 Announce Type: new  Abstract: The capabilities of large language models (LLMs) have been progressing at a breathtaking speed, leaving even their own developers grappling with the depth of their potential and risks. While initial steps have been taken to evaluate the safety and alignment of general-knowledge LLMs, exposing some weaknesses, to our knowledge, the safety and alignment of medical LLMs has not been evaluated despite their risks for personal health and safety, public health and safety, and human rights. To this end, we carry out the first safety evaluation for medical LLMs. Specifically, we set forth a definition of medical safety and alignment for medical artificial intelligence systems, develop a dataset of harmful medical questions to evaluate the medical safety and alignment of an LLM, evaluate both general and medical safety and alignment of medical LLMs, demonstrate fine-tuning as an effective mitigation strategy, and discuss broader, large-scale appr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;</title><link>https://arxiv.org/abs/2403.02167</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#35782;&#21035;&#35821;&#38899;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition from voice messages recorded in the wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02167
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Emotional Voice Messages&#25968;&#25454;&#24211;&#65292;&#32467;&#21512;eGeMAPS&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#37326;&#22806;&#24405;&#21046;&#30340;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#34920;&#28436;&#25110;&#24341;&#21457;&#30340;&#35821;&#38899;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Emotional Voice Messages&#65288;EMOVOME&#65289;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#22312;&#28040;&#24687;&#24212;&#29992;&#20013;&#30340;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#30001;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#26631;&#27880;&#32773;&#20197;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#24773;&#24863;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;eGeMAPS&#29305;&#24449;&#12289;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#35762;&#35805;&#32773;&#26080;&#20851;&#30340;SER&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#21442;&#32771;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20998;&#26512;&#20102;&#26631;&#27880;&#32773;&#21644;&#24615;&#21035;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#39044;&#35757;&#32451;&#30340;Unispeech-L&#27169;&#22411;&#21450;&#20854;&#19982;eGeMAPS&#30340;&#32452;&#21512;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#22312;3&#31867;valence&#21644;arousal&#39044;&#27979;&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;61.64%&#21644;55.57%&#30340;Unweighted Accuracy&#65288;UA&#65289;&#65292;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;10%&#12290;&#23545;&#20110;&#24773;&#24863;&#31867;&#21035;&#65292;&#33719;&#24471;&#20102;42.58%&#30340;UA&#12290;EMOVOME&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02167v1 Announce Type: cross  Abstract: Emotion datasets used for Speech Emotion Recognition (SER) often contain acted or elicited speech, limiting their applicability in real-world scenarios. In this work, we used the Emotional Voice Messages (EMOVOME) database, including spontaneous voice messages from conversations of 100 Spanish speakers on a messaging app, labeled in continuous and discrete emotions by expert and non-expert annotators. We created speaker-independent SER models using the eGeMAPS features, transformer-based models and their combination. We compared the results with reference databases and analyzed the influence of annotators and gender fairness. The pre-trained Unispeech-L model and its combination with eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10% improvement over baseline models. For the emotion categories, 42.58% UA was obtained. EMOVOME performed low
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.01924</link><description>&lt;p&gt;
&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#65311;&#20851;&#20110;&#20154;&#24037;&#29615;&#22659;&#22312;&#21307;&#23398;&#24320;&#25918;&#22495;&#38382;&#31572;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#30340;&#31532;&#19968;&#20010;&#29983;&#25104;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#22495;&#38382;&#31572;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#30340;&#25903;&#25345;&#12290;&#36817;&#26399;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#23558;&#30693;&#35782;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#23545;&#25239;&#26550;&#26500;&#35268;&#27169;&#21270;&#65292;&#24182;&#20801;&#35768;&#22312;&#24120;&#35265;&#30340;&#20302;&#36164;&#28304;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26816;&#32034;&#28982;&#21518;&#38405;&#35835;&#30340;&#33539;&#24335;&#24050;&#21464;&#24471;&#26222;&#36941;&#65292;&#27169;&#22411;&#39044;&#27979;&#20381;&#36182;&#20110;&#26469;&#33258;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;PubMed&#12289;&#25945;&#31185;&#20070;&#21644;UMLS&#65289;&#30340;&#30456;&#20851;&#30693;&#35782;&#29255;&#27573;&#12290;&#21478;&#19968;&#26465;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#20294;&#30001;&#20110;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21464;&#24471;&#21487;&#33021;&#30340;&#36335;&#24452;&#26159;&#36890;&#36807;&#25552;&#31034;&#26500;&#24314;&#20154;&#24037;&#29615;&#22659;&#12290;&#22240;&#27492;&#65292;&#8220;&#29983;&#25104;&#36824;&#26159;&#26816;&#32034;&#8221;&#25104;&#20026;&#20102;&#29616;&#20195;&#29256;&#30340;&#21704;&#22982;&#38647;&#29305;&#22256;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MedGENIE&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#29983;&#25104;&#28982;&#21518;&#38405;&#35835;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;MedQA-USMLE&#12289;MedMCQA&#21644;MMLU&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#20174;&#23454;&#36341;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20551;&#35774;&#26368;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01924v1 Announce Type: cross  Abstract: Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maxim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17496</link><description>&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65306;&#33258;&#21457;&#24773;&#24863;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#26159;&#19968;&#20010;&#33258;&#21457;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#12289;&#30007;&#22899;&#24615;&#24179;&#34913;&#30340;999&#26465;&#30495;&#23454;&#20250;&#35805;&#20013;&#30340;&#38899;&#39057;&#28040;&#24687;&#65292;&#36825;&#20123;&#28040;&#24687;&#36890;&#36807;&#19968;&#20010;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#20135;&#29983;&#65292;&#22312;&#21442;&#19982;&#32773;&#34987;&#25307;&#21215;&#20043;&#21069;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#21046;&#20316;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#23454;&#39564;&#23460;&#29615;&#22659;&#32780;&#20135;&#29983;&#30340;&#20219;&#20309;&#24847;&#35782;&#20559;&#35265;&#12290;&#38899;&#39057;&#25353;&#29031;&#19977;&#20010;&#38750;&#19987;&#23478;&#21644;&#20004;&#20010;&#19987;&#23478;&#30340;&#35748;&#21487;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32467;&#21512;&#20197;&#33719;&#24471;&#27599;&#20010;&#32500;&#24230;&#30340;&#26368;&#32456;&#26631;&#31614;&#12290;&#19987;&#23478;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#20110;&#19971;&#31181;&#24773;&#24863;&#31867;&#21035;&#30340;&#39069;&#22806;&#26631;&#31614;&#12290;&#20026;&#20102;&#20026;&#23558;&#26469;&#20351;&#29992;EMOVOME&#36827;&#34892;&#35843;&#26597;&#35774;&#23450;&#22522;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35821;&#38899;&#21644;&#38899;&#39057;&#36716;&#24405;&#26469;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;&#23545;&#20110;&#35821;&#38899;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30340;eGeMAPS&#29305;&#24449;&#38598;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;49.27%&#21644;44.71%&#30340;valence&#21644;arousal&#26410;&#21152;&#26435;&#20934;&#30830;&#24230;&#12290;&#23545;&#20110;&#25991;&#26412;&#37096;&#20998;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#23454;&#29616;&#20102;61%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
&lt;/p&gt;</description></item><item><title>&#27431;&#30431;AI&#27861;&#26696;&#24378;&#35843;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20154;&#26412;AI&#31995;&#32479;&#30340;&#27665;&#20027;&#21628;&#21505;&#65292;&#21516;&#26102;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#36991;&#20813;&#37325;&#22797;GDPR&#30340;&#38169;&#35823;&#24182;&#36991;&#20813;&#23454;&#26045;&#28151;&#20081;&#12290;</title><link>https://arxiv.org/abs/2402.14728</link><description>&lt;p&gt;
&#27431;&#27954;&#23545;&#20154;&#26412;&#31185;&#25216;&#30340;&#25215;&#35834;&#65306;HCI&#22312;&#27431;&#30431;AI&#27861;&#26696;&#25104;&#21151;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The European Commitment to Human-Centered Technology: The Integral Role of HCI in the EU AI Act's Success
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14728
&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;AI&#27861;&#26696;&#24378;&#35843;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20154;&#31867;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20154;&#26412;AI&#31995;&#32479;&#30340;&#27665;&#20027;&#21628;&#21505;&#65292;&#21516;&#26102;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20197;&#36991;&#20813;&#37325;&#22797;GDPR&#30340;&#38169;&#35823;&#24182;&#36991;&#20813;&#23454;&#26045;&#28151;&#20081;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#21457;&#23637;&#23558;&#28145;&#21051;&#37325;&#22609;&#26410;&#26469;&#12290;&#27431;&#30431;&#35748;&#35782;&#21040;&#36825;&#19968;&#21363;&#23558;&#21040;&#26469;&#30340;&#37325;&#35201;&#24615;&#65292;&#24050;&#32463;&#36890;&#36807;&#20102;AI&#27861;&#26696;&#65292;&#23545;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#30340;&#24066;&#22330;&#20934;&#20837;&#36827;&#34892;&#30417;&#31649;&#12290;&#35813;&#27861;&#26696;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#26159;&#36890;&#36807;&#19987;&#27880;&#20110;&#36879;&#26126;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20154;&#31867;&#29702;&#35299;&#21644;&#25511;&#21046;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#32500;&#25252;&#27665;&#20027;&#21644;&#20154;&#36947;&#20027;&#20041;&#20215;&#20540;&#35266;&#12290;&#22240;&#27492;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#19981;&#20165;&#20165;&#35268;&#23450;&#20102;AI&#31995;&#32479;&#30340;&#25216;&#26415;&#35201;&#27714;&#12290;&#27431;&#30431;&#21457;&#20986;&#20102;&#19968;&#20010;&#27665;&#20027;&#21495;&#21484;&#65292;&#35201;&#27714;&#20154;&#26412;AI&#31995;&#32479;&#65292;&#36827;&#32780;&#21046;&#23450;&#20102;&#20154;&#26412;&#21019;&#26032;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#35758;&#31243;&#65292;&#20419;&#36827;AI&#21457;&#23637;&#20013;&#30340;&#20154;&#26412;&#21019;&#26032;&#12290;&#22914;&#26524;&#27809;&#26377;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#31995;&#32479;&#21450;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#24433;&#21709;&#65292;&#27431;&#30431;AI&#27861;&#26696;&#21487;&#33021;&#20250;&#23548;&#33268;&#37325;&#22797;&#27431;&#30431;&#12298;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#30340;&#38169;&#35823;&#65292;&#23548;&#33268;&#20179;&#20419;&#12289;&#28151;&#20081;&#12289;&#20020;&#26102;&#21644;&#27169;&#31946;&#30340;&#23454;&#26045;&#65292;&#24102;&#26469;&#26356;&#22810;&#30340;&#22256;&#24785;&#32780;&#19981;&#26159;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14728v1 Announce Type: cross  Abstract: The evolution of AI is set to profoundly reshape the future. The European Union, recognizing this impending prominence, has enacted the AI Act, regulating market access for AI-based systems. A salient feature of the Act is to guard democratic and humanistic values by focusing regulation on transparency, explainability, and the human ability to understand and control AI systems. Hereby, the EU AI Act does not merely specify technological requirements for AI systems. The EU issues a democratic call for human-centered AI systems and, in turn, an interdisciplinary research agenda for human-centered innovation in AI development. Without robust methods to assess AI systems and their effect on individuals and society, the EU AI Act may lead to repeating the mistakes of the General Data Protection Regulation of the EU and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more confusion than lending guidance. Moreover, determine
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09871</link><description>&lt;p&gt;
MuChin&#65306;&#29992;&#20110;&#35780;&#20272;&#38899;&#20048;&#39046;&#22495;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09871
&lt;/p&gt;
&lt;p&gt;
MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#32479;&#19968;&#35780;&#20272;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#20197;&#25991;&#23383;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#31639;&#27861;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#19987;&#19994;&#20154;&#22763;&#21644;&#20844;&#20247;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#27880;&#37322;&#30340;&#20302;&#31934;&#24230;&#65292;&#29616;&#26377;&#30340;&#38899;&#20048;&#25551;&#36848;&#25968;&#25454;&#38598;&#26080;&#27861;&#20316;&#20026;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuChin&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#30340;&#24320;&#28304;&#38899;&#20048;&#25551;&#36848;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;LLMs&#22312;&#29702;&#35299;&#21644;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#34411;&#38899;&#20048;&#27880;&#37322;&#24179;&#21488;&#65288;CaiMAP&#65289;&#65292;&#37319;&#29992;&#21019;&#26032;&#30340;&#22810;&#20154;&#12289;&#22810;&#38454;&#27573;&#20445;&#35777;&#26041;&#27861;&#65292;&#24182;&#25307;&#21215;&#20102;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#65292;&#20197;&#30830;&#20445;&#27880;&#37322;&#30340;&#31934;&#24230;&#21644;&#19982;&#27969;&#34892;&#35821;&#20041;&#30340;&#23545;&#40784;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09871v1 Announce Type: cross  Abstract: The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06784</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with generative models for object detection on limited datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#26377;&#38480;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#38656;&#35201;&#27491;&#30830;&#26631;&#35760;&#27599;&#20010;&#30446;&#26631;&#21608;&#22260;&#30340;&#36793;&#30028;&#26694;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#22312;&#28023;&#27915;&#29983;&#29289;&#23398;&#39046;&#22495;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#26816;&#27979;&#28023;&#27915;&#29289;&#31181;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38480;&#21046;&#38382;&#39064;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#37319;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#20855;&#20307;&#30340;&#39046;&#22495;&#12290;&#31532;&#20108;&#31181;&#31574;&#30053;&#26159;&#20351;&#29992;copy-paste&#25216;&#26415;&#25110;ad-hoc&#27169;&#25311;&#22120;&#31561;&#26041;&#27861;&#21019;&#24314;&#29305;&#23450;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#30340;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#35774;&#35745;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36890;&#29992;&#24773;&#26223;&#19979;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03293</link><description>&lt;p&gt;
Flora: &#20302;&#31209;&#36866;&#37197;&#22120;&#26159;&#24708;&#24708;&#30340;&#26799;&#24230;&#21387;&#32553;&#22120;
&lt;/p&gt;
&lt;p&gt;
Flora: Low-Rank Adapters Are Secretly Gradient Compressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#30340;&#26041;&#27861;Flora&#65292;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#30340;&#26174;&#30528;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#20351;&#29992;&#26469;&#23384;&#20648;&#35757;&#32451;&#30340;&#20248;&#21270;&#29366;&#24577;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#26469;&#36890;&#36807;&#35757;&#32451;&#26356;&#23569;&#30340;&#21442;&#25968;&#26469;&#20943;&#23569;&#20248;&#21270;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;LoRA&#23558;&#25972;&#20307;&#26435;&#37325;&#26356;&#26032;&#30697;&#38453;&#38480;&#21046;&#20026;&#20302;&#31209;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#30830;&#23450;&#23427;&#21487;&#20197;&#36817;&#20284;&#20026;&#38543;&#26426;&#25237;&#24433;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Flora&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#37319;&#26679;&#25237;&#24433;&#30697;&#38453;&#23454;&#29616;&#39640;&#31209;&#26356;&#26032;&#65292;&#21516;&#26102;&#20139;&#21463;&#20248;&#21270;&#29366;&#24577;&#30340;&#27425;&#32447;&#24615;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large neural networks demonstrating remarkable abilities to complete different tasks, they require excessive memory usage to store the optimization states for training. To alleviate this, the low-rank adaptation (LoRA) is proposed to reduce the optimization states by training fewer parameters. However, LoRA restricts overall weight update matrices to be low-rank, limiting the model performance. In this work, we investigate the dynamics of LoRA and identify that it can be approximated by a random projection. Based on this observation, we propose Flora, which is able to achieve high-rank updates by resampling the projection matrices while enjoying the sublinear space complexity of optimization states. We conduct experiments across different tasks and model architectures to verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>ESPnet-SPK&#26159;&#19968;&#20010;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#12289;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#24182;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#38598;&#25104;&#12290;&#20854;&#36890;&#36807;&#20248;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17230</link><description>&lt;p&gt;
ESPnet-SPK:&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#65292;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#30340;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17230
&lt;/p&gt;
&lt;p&gt;
ESPnet-SPK&#26159;&#19968;&#20010;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#12289;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#24182;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#38598;&#25104;&#12290;&#20854;&#36890;&#36807;&#20248;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ESPnet-SPK&#65292;&#19968;&#20010;&#19987;&#20026;&#35757;&#32451;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#35774;&#35745;&#30340;&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#20379;&#35828;&#35805;&#20154;&#35782;&#21035;&#31038;&#21306;&#30340;&#30740;&#31350;&#20154;&#21592;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#20010;&#27169;&#22411;&#65292;&#20174;x-vector&#21040;&#26368;&#36817;&#30340;SKA-TDNN&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#36731;&#26494;&#24320;&#21457;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#24076;&#26395;&#23558;&#24320;&#21457;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36830;&#25509;&#65292;&#20174;&#32780;&#20351;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36731;&#26494;&#22320;&#25972;&#21512;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#25552;&#21462;&#22120;&#12290;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#25552;&#21462;&#22120;&#21487;&#20197;&#20197;&#29616;&#25104;&#30340;&#26041;&#24335;&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#20854;&#19982;&#20004;&#20010;&#20219;&#21153;&#30340;&#38598;&#25104;&#26469;&#23637;&#31034;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#21478;&#19968;&#20010;&#30446;&#26631;&#26159;&#19982;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#38598;&#25104;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#65292;&#20351;&#29992;WavLM-Large&#21644;ECAPA-TDNN&#22312;Vox1-O&#35780;&#20272;&#21327;&#35758;&#19978;&#23454;&#29616;&#20102;0.39%&#30340;&#31561;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors. First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models. We provide several models, ranging from x-vector to recent SKA-TDNN. Through the modularized architecture design, variants can be developed easily. We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors. Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks. Another goal is to integrate with diverse self-supervised learning features. We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.17095</link><description>&lt;p&gt;
&#20174;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;&#32039;&#24613;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#65292;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#23398;&#20064;&#38544;&#24335;&#23558;&#22270;&#20687;&#21306;&#22495;&#19982;&#35789;&#27719;&#20851;&#32852;&#36215;&#26469;&#65292;&#36825;&#23545;&#20110;&#35832;&#22914;&#35270;&#35273;&#38382;&#31572;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20851;&#32852;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS)&#12290;PnP-OVSS&#21033;&#29992;&#20855;&#26377;&#30452;&#25509;&#25991;&#26412;&#21040;&#22270;&#20687;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25439;&#22833;&#30340;VLM&#12290;&#20026;&#20102;&#22312;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#33879;&#24615;&#20002;&#24323;&#65288;Salience Dropout&#65289;&#65307;&#36890;&#36807;&#36845;&#20195;&#20002;&#24323;&#27169;&#22411;&#26368;&#20851;&#27880;&#30340;&#34917;&#19969;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#25972;&#20010;&#20998;&#21106;&#25513;&#27169;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17095v2 Announce Type: replace-cross  Abstract: From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. \shortname{} does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.08894</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#34701;&#21512;&#30417;&#30563;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;KBQA&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#22495;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#22312;&#28304;&#22495;&#20013;&#26377;&#22823;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuSIC-KBQA&#30340;&#26032;&#22411;KBQA&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#28304;&#22521;&#35757;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#20351;&#29992;LLM&#37325;&#26032;&#25490;&#24207;&#65292;&#23558;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#20197;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#34892;&#32454;&#21270;&#12290;&#22312;&#22235;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#28304;-&#30446;&#26631;KBQA&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FuSIC-KBQA&#26126;&#26174;&#20248;&#20110;&#20026;&#27492;&#35774;&#32622;&#35843;&#25972;&#30340;SoTA KBQA&#27169;&#22411;&#12290;&#22312;&#39046;&#22495;&#20869;&#35774;&#32622;&#30340;&#39069;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;FuSIC-KBQA&#20063;&#20248;&#20110;SoTA KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2306.01931</link><description>&lt;p&gt;
&#25506;&#32034;&#30142;&#30149;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65306;&#29992;&#20110;&#20013;&#25991;&#30142;&#30149;&#35268;&#33539;&#21270;&#30340;&#31616;&#21333;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring semantic information in disease: Simple Data Augmentation Techniques for Chinese Disease Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01931
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23427;&#23558;&#20197;&#21508;&#31181;&#26684;&#24335;&#32534;&#20889;&#30340;&#30142;&#30149;&#21517;&#31216;&#20998;&#31867;&#20026;&#26631;&#20934;&#21270;&#21517;&#31216;&#65292;&#20316;&#20026;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#21508;&#31181;&#19982;&#30142;&#30149;&#30456;&#20851;&#21151;&#33021;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#26159;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#19981;&#36275;&#12290;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36890;&#24120;&#20250;&#38459;&#30861;&#20219;&#21153;&#24615;&#33021;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#30142;&#30149;&#21517;&#31216;&#30340;&#22810;&#36724;&#21644;&#22810;&#31890;&#24230;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#22266;&#26377;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01931v2 Announce Type: replace  Abstract: Disease name normalization is an important task in the medical domain. It classifies disease names written in various formats into standardized names, serving as a fundamental component in smart healthcare systems for various disease-related functions. Nevertheless, the most significant obstacle to existing disease name normalization systems is the severe shortage of training data. While data augmentation is a powerful approach for addressing data scarcity, our findings reveal that conventional data augmentation techniques often impede task performance, primarily due to the multi-axis and multi-granularity nature of disease names. Consequently, we introduce a set of customized data augmentation techniques designed to leverage the semantic information inherent in disease names. These techniques aim to enhance the model's understanding of the semantic intricacies and classification structure of disease names. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.10718</link><description>&lt;p&gt;
&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Whole Page Unbiased Learning to Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.10718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;&#65288;WP-ULTR&#65289;&#26041;&#27861;&#22788;&#29702;&#25972;&#39029; SERP &#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#30340;&#25361;&#25112;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#39029;&#38754;&#21576;&#29616;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#28857;&#20987;&#34892;&#20026;&#26041;&#38754;&#30340;&#20559;&#24046;&#65292;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#20351;&#29992;&#38544;&#24335;&#29992;&#25143;&#21453;&#39304;&#26469;&#25913;&#36827;&#25490;&#24207;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(ULTR)&#31639;&#27861;&#65292;&#36890;&#36807;&#20559;&#24046;&#28857;&#20987;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#26080;&#20559;&#30340;&#25490;&#24207;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20943;&#36731;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#20449;&#20219;&#20559;&#24046;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#25628;&#32034;&#32467;&#26524;&#39029;&#38754;&#21576;&#29616;(SERP)&#20013;&#20854;&#20182;&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#65292;&#20363;&#22914;&#30001;&#22810;&#23186;&#20307;&#24341;&#21457;&#30340;&#21560;&#24341;&#20559;&#24046;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#20559;&#24046;&#22312;&#24037;&#19994;&#31995;&#32479;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#25628;&#32034;&#20307;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#25972;&#39029;&#26080;&#20559;&#23398;&#20064;&#25490;&#24207;(WP-ULTR)&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#25972;&#39029;SERP&#29305;&#24449;&#24341;&#21457;&#30340;&#20559;&#24046;&#12290;&#36825;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65306;(1) &#24456;&#38590;&#25214;&#21040;&#36866;&#21512;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411; (&#29992;&#25143;&#34892;&#20026;&#20551;&#35774;)&#65307;(2) &#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The page presentation biases in the information retrieval system, especially on the click behavior, is a well-known challenge that hinders improving ranking models' performance with implicit user feedback. Unbiased Learning to Rank~(ULTR) algorithms are then proposed to learn an unbiased ranking model with biased click data. However, most existing algorithms are specifically designed to mitigate position-related bias, e.g., trust bias, without considering biases induced by other features in search result page presentation(SERP), e.g. attractive bias induced by the multimedia. Unfortunately, those biases widely exist in industrial systems and may lead to an unsatisfactory search experience. Therefore, we introduce a new problem, i.e., whole-page Unbiased Learning to Rank(WP-ULTR), aiming to handle biases induced by whole-page SERP features simultaneously. It presents tremendous challenges: (1) a suitable user behavior model (user behavior hypothesis) can be hard to find; and (2) complex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.15241</link><description>&lt;p&gt;
&#21453;&#23398;&#20064;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlearning Reveals the Influential Training Data of Language Models. (arXiv:2401.15241v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;UnTrac&#65292;&#36890;&#36807;&#21453;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UnTrac&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#35782;&#21035;&#21738;&#20123;&#35757;&#32451;&#25968;&#25454;&#38598;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#35757;&#32451;&#20013;&#31227;&#38500;&#27599;&#20010;&#25968;&#25454;&#38598;&#26469;&#34913;&#37327;&#20854;&#24433;&#21709;;&#28982;&#32780;&#65292;&#22810;&#27425;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;UnTrac&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21462;&#28040;&#23398;&#20064;&#26469;&#20272;&#35745;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;UnTrac&#38750;&#24120;&#31616;&#21333;; &#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26469;&#21462;&#28040;&#23398;&#20064;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#22312;&#21462;&#28040;&#23398;&#20064;&#21518;&#27169;&#22411;&#30340;&#39044;&#27979;&#21457;&#29983;&#20102;&#22810;&#22823;&#21464;&#21270;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21542;&#33021;&#35780;&#20272;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#29983;&#25104;&#26377;&#27602;&#12289;&#26377;&#20559;&#35265;&#21644;&#19981;&#30495;&#23454;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#31354;&#38388;&#25110;&#22810;&#20010;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to enhance the performance of language models while mitigating the risks of generating harmful content, it is crucial to identify which training dataset affects the model's outputs. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac, which estimates the influence of a training dataset by unlearning it from the trained model. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning. We empirically examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Experimental results demonstrate that our method estimates their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple model checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04206</link><description>&lt;p&gt;
&#20174;AI&#25945;&#32451;&#23398;&#20064;&#36187;&#36710;&#65306;&#22810;&#27169;&#24577;&#33258;&#21160;&#39550;&#39542;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#39033;&#21069;&#21518;&#23454;&#39564;&#20013;&#65288;n=41&#65289;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#19987;&#23478;&#30340;&#25351;&#23548;&#35828;&#26126;&#30340;AI&#25945;&#32451;&#30340;&#35299;&#37322;&#27807;&#36890;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#20449;&#24515;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#21442;&#19982;&#32773;&#34987;&#20998;&#20026;&#22235;&#20010;&#32452;&#65292;&#35780;&#20272;&#20102;AI&#25945;&#32451;&#35299;&#37322;&#30340;&#20004;&#20010;&#32500;&#24230;&#65306;&#20449;&#24687;&#31867;&#22411;&#65288;'what'&#21644;'why'-type&#35299;&#37322;&#65289;&#21644;&#21576;&#29616;&#26041;&#24335;&#65288;&#21548;&#35273;&#21644;&#35270;&#35273;&#65289;&#12290;&#36890;&#36807;&#37319;&#35775;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#27604;&#36739;&#21508;&#32452;&#20043;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#31867;&#22411;&#21644;&#26041;&#24335;&#23545;&#24615;&#33021;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#24046;&#24322;&#24402;&#22240;&#20110;&#20449;&#24687;&#22914;&#20309;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24433;&#21709;&#21442;&#19982;&#32773;&#32463;&#21382;&#30340;&#36127;&#33655;&#36807;&#36733;&#12290;&#36825;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#20449;&#20219;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.09868</link><description>&lt;p&gt;
INTERVENOR: &#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09868
&lt;/p&gt;
&lt;p&gt;
INTERVENOR&#27169;&#22411;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65292;&#20351;&#29992;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#26469;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;INTERVENOR&#30340;&#20132;&#20114;&#24335;&#20462;&#22797;&#38142;&#26465;&#65288;INTERactiVE chaiN Of Repairing&#65289;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#20462;&#22797;&#20195;&#30721;&#30340;&#34892;&#20026;&#65288;&#36845;&#20195;&#21028;&#26029;&#12289;&#37325;&#26032;&#24605;&#32771;&#21644;&#20462;&#22797;&#65289;&#65292;&#24182;&#20419;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;INTERVENOR&#37319;&#29992;&#20102;&#20004;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#21363;Code Learner&#21644;Code Teacher&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#20462;&#22797;&#20013;&#25198;&#28436;&#19981;&#21516;&#30340;&#35282;&#33394;&#65292;&#24182;&#36890;&#36807;&#20114;&#21160;&#26469;&#20462;&#22797;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;Code Learner&#26681;&#25454;Code Teacher&#30340;&#25351;&#23548;&#29983;&#25104;&#21644;&#20462;&#22797;&#20195;&#30721;&#65292;&#32780;Code Teacher&#26681;&#25454;&#32534;&#35793;&#22120;&#30340;&#21453;&#39304;&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#38169;&#35823;&#65292;&#24182;&#36845;&#20195;&#29983;&#25104;&#20462;&#22797;&#38142;&#26465;&#65288;CoR&#65289;&#20197;&#24341;&#23548;Code Learner&#30340;&#20195;&#30721;&#20462;&#22797;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;INTERVENOR&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#36716;&#25442;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110;GPT-3.5&#27169;&#22411;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;13%&#21644;4.5%&#30340;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;CoR&#33021;&#22815;&#25581;&#31034;bug&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01373</link><description>&lt;p&gt;
&#35748;&#35777;&#20219;&#20309;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21508;&#20010;&#21306;&#22495;&#25110;&#22359;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#22312;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#26816;&#27979;&#20013;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#22312;&#22522;&#20110;&#24378;&#22823;&#30340;&#22270;&#20687;&#32423;&#35270;&#35273;&#35821;&#35328;&#65288;ViL&#65289;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#21162;&#21147;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;&#24191;&#27867;&#30340;&#21306;&#22495;-&#26631;&#31614;&#23545;&#38598;&#21512;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#65292;&#35201;&#20040;&#23558;&#26816;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#21306;&#22495;&#24314;&#35758;&#30340;&#22270;&#20687;&#32423;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#21463;&#21040;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#35757;&#32451;&#38656;&#27714;&#12289;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#20197;&#21450;&#29615;&#22659;&#20449;&#24687;&#30340;&#19981;&#36275;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#23450;&#20301;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#31216;&#20026;RegionSpot&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.02409</link><description>&lt;p&gt;
Nugget 2D&#65306;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models. (arXiv:2310.02409v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02409
&lt;/p&gt;
&lt;p&gt;
Nugget 2D&#26159;&#19968;&#31181;&#29992;&#20110;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#30041;&#20219;&#21153;&#33021;&#21147;&#30340;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#35299;&#30721;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#32553;&#25918;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;Qin&#65286;Van Durme&#65288;2023&#24180;&#65289;&#30340;Nugget&#26041;&#27861;&#20174;BERT&#31867;&#26694;&#26550;&#25193;&#23637;&#21040;&#20165;&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21382;&#21490;&#24314;&#27169;&#20026;&#21387;&#32553;&#30340;&#8220;nuggets&#8221;&#65292;&#36825;&#20123;&#8220;nuggets&#8221;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36827;&#34892;&#37325;&#24314;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#35832;&#22914;LLaMA&#20043;&#31867;&#30340;&#29616;&#25104;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Nugget2D&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#20445;&#30041;&#20102;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#24320;&#38144;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#32534;&#30721;&#23454;&#39564;&#20013;&#65292;Nugget2D&#21487;&#20197;&#20197;20&#20493;&#30340;&#21387;&#32553;&#27604;&#25910;&#32553;&#19978;&#19979;&#25991;&#65292;&#37325;&#24314;&#26102;&#30340;BLEU&#24471;&#20998;&#20026;98&#65285;&#65292;&#23454;&#29616;&#20102;&#36817;&#20046;&#26080;&#25439;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin &amp; Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;XAIstories&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#26041;&#24335;&#35299;&#37322;AI&#39044;&#27979;&#65292;&#20854;&#20013;SHAPstories&#22522;&#20110;SHAP&#35299;&#37322;&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;CFstories&#22522;&#20110;CF&#35299;&#37322;&#35299;&#37322;&#20915;&#31574;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;90%&#30340;&#26222;&#36890;&#35835;&#32773;&#35748;&#21487;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#30340;&#35828;&#26381;&#21147;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#35748;&#20026;SHAPstories&#33021;&#22815;&#25552;&#39640;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2309.17057</link><description>&lt;p&gt;
&#35762;&#32473;&#25105;&#21548;&#65281;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21465;&#20107;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Tell Me a Story! Narrative-Driven XAI with Large Language Models. (arXiv:2309.17057v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;XAIstories&#26694;&#26550;&#65292;&#36890;&#36807;&#21465;&#20107;&#26041;&#24335;&#35299;&#37322;AI&#39044;&#27979;&#65292;&#20854;&#20013;SHAPstories&#22522;&#20110;SHAP&#35299;&#37322;&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;CFstories&#22522;&#20110;CF&#35299;&#37322;&#35299;&#37322;&#20915;&#31574;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#36807;90%&#30340;&#26222;&#36890;&#35835;&#32773;&#35748;&#21487;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#30340;&#35828;&#26381;&#21147;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#35748;&#20026;SHAPstories&#33021;&#22815;&#25552;&#39640;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#37325;&#35201;&#39046;&#22495;&#20013;&#65292;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30427;&#34892;&#21152;&#22823;&#20102;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#38656;&#27714;&#12290;&#24191;&#27867;&#20351;&#29992;&#30340;SHAP&#20540;&#34429;&#28982;&#37327;&#21270;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#20294;&#24448;&#24448;&#36807;&#20110;&#22797;&#26434;&#65292;&#32570;&#20047;&#20154;&#24615;&#21270;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#21453;&#20107;&#23454;&#65288;CF&#65289;&#35299;&#37322;&#23637;&#31034;&#20102;&#8220;&#22914;&#26524;&#8221;&#20294;&#27809;&#26377;&#35299;&#37322;&#8220;&#20026;&#20160;&#20040;&#8221;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;XAIstories&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;XAIstories&#25552;&#20379;&#20102;&#21465;&#20107;&#26469;&#38416;&#26126;AI&#39044;&#27979;&#65306;&#22522;&#20110;SHAP&#35299;&#37322;&#30340;SHAPstories&#35299;&#37322;&#39044;&#27979;&#24471;&#20998;&#65292;&#32780;&#22522;&#20110;CF&#35299;&#37322;&#30340;CFstories&#35299;&#37322;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20196;&#20154;&#38663;&#24778;&#65306;&#36229;&#36807;90%&#30340;&#35843;&#26597;&#26222;&#36890;&#35835;&#32773;&#35748;&#20026;SHAPstories&#29983;&#25104;&#30340;&#21465;&#20107;&#26159;&#26377;&#35828;&#26381;&#21147;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#20027;&#35201;&#35748;&#20026;SHAPstories&#22312;&#21521;&#26222;&#36890;&#35835;&#32773;&#20256;&#36798;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20215;&#20540;&#65292;92%&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#34920;&#31034;&#36825;&#23558;&#26377;&#21161;&#20110;&#38750;&#19987;&#19994;&#20154;&#22763;&#30340;&#26131;&#29992;&#24615;&#21644;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in under
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00237</link><description>&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#30340;&#20844;&#24320;&#21487;&#20849;&#20139;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00237
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#26500;&#24314;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20811;&#26381;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21512;&#25104;&#30340;&#20020;&#24202;&#26696;&#20363;&#25253;&#21578;&#65292;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#65292;&#20197;&#35299;&#20915;&#20020;&#24202;&#35760;&#24405;&#30340;&#26377;&#38480;&#21487;&#21450;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#35760;&#24405;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#19987;&#38376;&#30340;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;Asclepius&#12290;&#34429;&#28982;Asclepius&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;Asclepius&#19982;&#21253;&#25324;GPT-3.5-turbo&#21644;&#20854;&#20182;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#22312;&#20869;&#30340;&#20960;&#31181;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#23558;Asclepius&#19982;&#20854;&#22312;&#30495;&#23454;&#20020;&#24202;&#35760;&#24405;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;&#21512;&#25104;&#20020;&#24202;&#35760;&#24405;&#22312;&#26500;&#24314;&#20020;&#24202;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#21487;&#20197;&#20316;&#20026;&#21487;&#34892;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13669</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment. (arXiv:2305.13669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MixAlign&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#65292;&#23454;&#29616;&#33258;&#21160;&#30340;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22240;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#38382;&#39064;&#21644;&#30693;&#35782;&#32780;&#23548;&#33268;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#36817;&#26399;&#36827;&#23637;&#26174;&#33879;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#35823;&#23548;&#24615;&#21644;&#19981;&#25903;&#25345;&#30340;&#22238;&#31572;&#12290;&#19968;&#31181;&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20174;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#21644;&#25972;&#21512;&#25903;&#25345;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#38382;&#39064;&#36890;&#24120;&#19982;&#23384;&#20648;&#30340;&#30693;&#35782;&#19981;&#22826;&#23545;&#40784;&#65292;&#22240;&#20026;&#20182;&#20204;&#22312;&#25552;&#38382;&#21069;&#19981;&#30693;&#36947;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#21487;&#33021;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#23450;&#20301;&#21644;&#21033;&#29992;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#36843;&#20351;&#20854;&#36890;&#36807;&#24573;&#30053;&#25110;&#35206;&#30422;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#32780;&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MixAlign&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#19982;&#29992;&#25143;&#21644;&#30693;&#35782;&#24211;&#20132;&#20114;&#20197;&#33719;&#24471;&#24182;&#25972;&#21512;&#20851;&#20110;&#29992;&#25143;&#38382;&#39064;&#19982;&#23384;&#20648;&#20449;&#24687;&#30456;&#20851;&#24615;&#30340;&#28548;&#28165;&#20449;&#24687;&#12290; MixAlign &#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;-&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#20154;&#24037;&#29992;&#25143;&#28548;&#28165;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable recent advances in language models, they still struggle with the hallucination problem and can generate misleading and unsupported responses. A common approach to mitigate the hallucination issue is retrieving and incorporating supporting evidence from a knowledge base. However, user questions usually do not align well with the stored knowledge, as they are unaware of the information available before asking questions. This misalignment can limit the language model's ability to locate and utilize the knowledge, potentially forcing it to hallucinate by ignoring or overriding the retrieved evidence. To address this issue, we introduce MixAlign, a framework that interacts with both the user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic question-knowledge alignment and, if necessary, further enhances this alignment through human user clari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#21452;&#37325;&#21512;&#21516;&#38382;&#39064;&#20013;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#28608;&#21169;&#30456;&#23481;&#30340;&#21512;&#21516;&#65292;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12350</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#21452;&#37325;&#21512;&#21516;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence and Dual Contract. (arXiv:2303.12350v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#21452;&#37325;&#21512;&#21516;&#38382;&#39064;&#20013;&#33021;&#22815;&#33258;&#20027;&#35774;&#35745;&#28608;&#21169;&#30456;&#23481;&#30340;&#21512;&#21516;&#65292;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#36890;&#20449;&#65292;&#24182;&#19988;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#24555;&#36895;&#36827;&#27493;&#65292;&#20154;&#20204;&#24076;&#26395;&#31639;&#27861;&#24456;&#24555;&#23601;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#20195;&#20154;&#31867;&#20915;&#31574;&#32773;&#65292;&#20363;&#22914;&#21512;&#21516;&#35774;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#30740;&#31350;&#20102;&#30001;&#20154;&#24037;&#26234;&#33021;&#65288;&#22810;&#26234;&#33021;&#20307;Q&#23398;&#20064;&#65289;&#39537;&#21160;&#30340;&#31639;&#27861;&#22312;&#21452;&#37325;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#30340;&#32463;&#20856;&#8220;&#21452;&#37325;&#21512;&#21516;&#8221;&#27169;&#22411;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;AI&#31639;&#27861;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#35774;&#35745;&#21512;&#36866;&#30340;&#28608;&#21169;&#30456;&#23481;&#21512;&#21516;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#24341;&#23548;&#25110;&#32773;&#23427;&#20204;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#30001;&#19981;&#21516;AI&#31639;&#27861;&#25903;&#25345;&#30340;&#22996;&#25176;&#20154;&#21487;&#20197;&#37319;&#29992;&#28151;&#21512;&#21644;&#38646;&#21644;&#21338;&#24328;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26356;&#20855;&#26234;&#33021;&#30340;&#22996;&#25176;&#20154;&#24448;&#24448;&#20250;&#21464;&#24471;&#21512;&#20316;&#65292;&#32780;&#26234;&#33021;&#36739;&#20302;&#30340;&#22996;&#25176;&#20154;&#21017;&#20250;&#20986;&#29616;&#20869;&#29983;&#24615;&#36817;&#35270;&#24182;&#20542;&#21521;&#20110;&#31454;&#20105;&#12290;&#22312;&#26368;&#20248;&#21512;&#21516;&#19979;&#65292;&#20195;&#29702;&#30340;&#36739;&#20302;&#21512;&#21516;&#28608;&#21169;&#30001;&#22996;&#25176;&#20154;&#20043;&#38388;&#30340;&#21246;&#32467;&#31574;&#30053;&#32500;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the dramatic progress of artificial intelligence algorithms in recent times, it is hoped that algorithms will soon supplant human decision-makers in various fields, such as contract design. We analyze the possible consequences by experimentally studying the behavior of algorithms powered by Artificial Intelligence (Multi-agent Q-learning) in a workhorse \emph{dual contract} model for dual-principal-agent problems. We find that the AI algorithms autonomously learn to design incentive-compatible contracts without external guidance or communication among themselves. We emphasize that the principal, powered by distinct AI algorithms, can play mixed-sum behavior such as collusion and competition. We find that the more intelligent principals tend to become cooperative, and the less intelligent principals are endogenizing myopia and tend to become competitive. Under the optimal contract, the lower contract incentive to the agent is sustained by collusive strategies between the principals
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2302.05534</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Robust Knowledge Transfer in Tiered Reinforcement Learning. (arXiv:2302.05534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#32423;&#22686;&#24378;&#23398;&#20064;&#35774;&#32622;&#65292;&#36825;&#26159;&#19968;&#20010;&#24182;&#34892;&#20256;&#36755;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#26159;&#23558;&#30693;&#35782;&#20174;&#20302;&#23618;&#65288;&#28304;&#65289;&#20219;&#21153;&#20256;&#36755;&#21040;&#39640;&#23618;&#65288;&#30446;&#26631;&#65289;&#20219;&#21153;&#65292;&#20197;&#20943;&#23569;&#21518;&#32773;&#30340;&#25506;&#32034;&#39118;&#38505;&#65292;&#21516;&#26102;&#24182;&#34892;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#20302;&#23618;&#21644;&#39640;&#23618;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#24577;&#25110;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#19987;&#27880;&#20110;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26368;&#20248;&#20540;&#25903;&#37197;&#8221;&#30340;&#33258;&#28982;&#32780;&#24517;&#35201;&#30340;&#26465;&#20214;&#65292;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#24471;&#23545;&#20110;&#39640;&#23618;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#29366;&#24577;&#19978;&#21487;&#20197;&#23454;&#29616;&#24658;&#23450;&#30340;&#36951;&#25022;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19981;&#30456;&#20284;&#26102;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;&#65307;&#32780;&#23545;&#20110;&#20302;&#23618;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20570;&#20986;&#29306;&#29298;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#20302;&#23618;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the ``Optimal Value Dominance'' for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks
&lt;/p&gt;</description></item></channel></rss>