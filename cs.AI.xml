<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2309.05030</link><description>&lt;p&gt;
&#21435;&#27542;&#27665;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#65306;&#23041;&#33394;&#36798;&#23572;&#29595;&#12289;&#35770;&#35777;&#21644;&#33402;&#26415;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#27542;&#27665;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#23545;&#40784;&#65306;&#21363;&#22522;&#20110;&#32454;&#33268;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#19982;&#26399;&#26395;&#20540;&#19968;&#33268;&#12290;&#38500;&#20102;&#20854;&#20182;&#23454;&#36341;&#65292;&#27542;&#27665;&#20027;&#20041;&#36824;&#26377;&#19968;&#37096;&#20998;&#26159;&#25913;&#21464;&#34987;&#27542;&#27665;&#27665;&#26063;&#30340;&#20449;&#20208;&#21644;&#20215;&#20540;&#35266;&#30340;&#21382;&#21490;&#65307;&#32780;&#24403;&#21069;&#30340;LLM&#23545;&#40784;&#23454;&#36341;&#27491;&#26159;&#36825;&#19968;&#21382;&#21490;&#30340;&#22797;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19977;&#20010;&#25552;&#35758;&#23545;AI&#23545;&#40784;&#36827;&#34892;&#21435;&#27542;&#27665;&#21270;&#65306;&#65288;a&#65289;&#23558;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20174;&#35199;&#26041;&#21746;&#23398;&#36716;&#21464;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#65288;b&#65289;&#22312;&#23545;&#40784;&#25216;&#26415;&#20013;&#20801;&#35768;&#35770;&#35777;&#21644;&#22810;&#20803;&#20027;&#20041;&#30340;&#20256;&#32479;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#20215;&#20540;&#30340;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#25110;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
&lt;/p&gt;</description></item><item><title>VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.05027</link><description>&lt;p&gt;
VoiceFlow: &#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#30340;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. (arXiv:2309.05027v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05027
&lt;/p&gt;
&lt;p&gt;
VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#22240;&#20854;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20294;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#25439;&#23475;&#20102;&#20854;&#25928;&#29575;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoiceFlow&#65292;&#19968;&#31181;&#21033;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;&#21512;&#25104;&#36136;&#37327;&#30340;&#22768;&#23398;&#27169;&#22411;&#65292;&#21482;&#38656;&#26377;&#38480;&#27425;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#12290;VoiceFlow&#23558;&#29983;&#25104;mel-spectrograms&#30340;&#36807;&#31243;&#36716;&#21270;&#20026;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#25991;&#26412;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#20272;&#35745;&#20986;&#20854;&#21521;&#37327;&#22330;&#12290;&#28982;&#21518;&#65292;&#30699;&#27491;&#27969;&#25216;&#26415;&#26377;&#25928;&#22320;&#20351;&#20854;&#37319;&#26679;&#36712;&#36857;&#30452;&#32447;&#21270;&#65292;&#23454;&#29616;&#39640;&#25928;&#21512;&#25104;&#12290;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#35828;&#35805;&#32773;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#26174;&#31034;&#65292;VoiceFlow&#30456;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;VoiceFlow&#20013;&#30699;&#27491;&#27969;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.05007</link><description>&lt;p&gt;
FOLLOWUPQG:&#38754;&#21521;&#20449;&#24687;&#33719;&#21462;&#30340;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation. (arXiv:2309.05007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26469;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;FOLLOWUPQG&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20986;&#20110;&#22909;&#22855;&#24515;&#32780;&#25552;&#20986;&#36319;&#36827;&#38382;&#39064;&#65292;&#36825;&#21453;&#26144;&#20102;&#20154;&#31867;&#21019;&#36896;&#24615;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20449;&#24687;&#33719;&#21462;&#36319;&#36827;&#38382;&#39064;&#29983;&#25104;&#65288;FQG&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#33021;&#22815;&#26356;&#28145;&#20837;&#29702;&#35299;&#21021;&#22987;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;FOLLOWUPQG&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;Reddit&#35770;&#22363;&#30340;&#36229;&#36807;3K&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#65288;&#21021;&#22987;&#38382;&#39064;&#65292;&#31572;&#26696;&#65292;&#36319;&#36827;&#38382;&#39064;&#65289;&#20803;&#32452;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#21451;&#22909;&#30340;&#35299;&#37322;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;FOLLOWUPQG&#20013;&#30340;&#38382;&#39064;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#30340;&#23454;&#29992;&#31574;&#30053;&#26469;&#23547;&#27714;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#23618;&#27425;&#30340;&#35748;&#30693;&#25216;&#33021;&#65288;&#22914;&#24212;&#29992;&#21644;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#30340;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36319;&#36827;&#38382;&#39064;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#25506;&#32034;&#22914;&#20309;&#22522;&#20110;&#36880;&#27493;&#28436;&#31034;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;FOLLOWUPQG&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04977</link><description>&lt;p&gt;
RGAT&#65306;&#26356;&#28145;&#20837;&#25506;&#32034;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution. (arXiv:2309.04977v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21477;&#27861;&#20449;&#24687;&#23545;&#24456;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23558;&#20854;&#19982;&#35789;&#35821;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#25351;&#20195;&#28040;&#35299;&#38382;&#39064;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#39044;&#35757;&#32451;BERT&#21644;&#21477;&#27861;&#20851;&#31995;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;RGAT&#65289;&#30340;&#31471;&#21040;&#31471;&#35299;&#26512;&#22120;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#21477;&#27861;&#20381;&#36182;&#20449;&#24687;&#22312;&#25351;&#20195;&#28040;&#35299;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;RGAT&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#29702;&#35299;&#21477;&#27861;&#20381;&#36182;&#22270;&#24182;&#23398;&#20064;&#26356;&#22909;&#30340;&#20219;&#21153;&#29305;&#23450;&#21477;&#27861;&#23884;&#20837;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#32467;&#26500;&#65292;&#23558;BERT&#23884;&#20837;&#21644;&#21477;&#27861;&#23884;&#20837;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#30340;Gendered Ambiguous Pronouns&#65288;GAP&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#23545;&#21477;&#27861;&#20381;&#36182;&#22270;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#21516;&#26102;&#65292;&#19981;&#38656;&#35201;&#23545;&#25972;&#20010;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#20808;&#21069;&#26368;&#20339;&#27169;&#22411;&#65288;RGCN-wi&#65289;&#30340;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Although syntactic information is beneficial for many NLP tasks, combining it with contextual information between words to solve the coreference resolution problem needs to be further explored. In this paper, we propose an end-to-end parser that combines pre-trained BERT with a Syntactic Relation Graph Attention Network (RGAT) to take a deeper look into the role of syntactic dependency information for the coreference resolution task. In particular, the RGAT model is first proposed, then used to understand the syntactic dependency graph and learn better task-specific syntactic embeddings. An integrated architecture incorporating BERT embeddings and syntactic embeddings is constructed to generate blending representations for the downstream task. Our experiments on a public Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision learning of the syntactic dependency graph and without fine-tuning the entire BERT, we increased the F1-score of the previous best model (RGCN-wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26080;&#20154;&#26426;&#26469;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31995;&#32479;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#38382;&#39064;&#65292;&#26080;&#20154;&#26426;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.04976</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs. (arXiv:2309.04976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26080;&#20154;&#26426;&#26469;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31995;&#32479;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#38382;&#39064;&#65292;&#26080;&#20154;&#26426;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#30001;&#36884;&#20013;&#20107;&#20214;&#65288;&#20363;&#22914;&#36335;&#27573;&#20851;&#38381;&#65292;&#36710;&#31096;&#31561;&#65289;&#24341;&#36215;&#30340;&#24847;&#22806;&#22478;&#24066;&#20132;&#36890;&#25317;&#22581;&#36890;&#24120;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20132;&#36890;&#20449;&#21495;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#65292;&#22914;SCATS&#21644;SCOOT&#65292;&#30001;&#24863;&#24212;&#32447;&#22280;&#25552;&#20379;&#30340;&#20132;&#36890;&#25968;&#25454;&#26356;&#26032;&#39057;&#29575;&#36739;&#20302;&#65288;&#21363;&#36229;&#36807;1&#20998;&#38047;&#65289;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#30340;&#20132;&#36890;&#20449;&#21495;&#28783;&#35745;&#21010;&#26159;&#20107;&#21457;&#21069;&#39044;&#20808;&#32534;&#31243;&#30340;&#20505;&#36873;&#35745;&#21010;&#30340;&#26377;&#38480;&#38598;&#21512;&#20013;&#36873;&#25321;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25511;&#21046;&#30340;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#20132;&#36890;&#20449;&#21495;&#31995;&#32479;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#25668;&#20687;&#26426;&#21487;&#20197;&#25552;&#20379;&#39640;&#39057;&#39640;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#38656;&#35201;&#36807;&#22810;&#30340;&#28508;&#22312;&#21319;&#32423;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#37096;&#32626;&#25104;&#26412;&#22312;&#22823;&#22478;&#24066;&#20013;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#21487;&#20197;&#22312;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#26041;&#38754;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing unexpected urban traffic congestion caused by en-route events (e.g., road closures, car crashes, etc.) often requires fast and accurate reactions to choose the best-fit traffic signals. Traditional traffic light control systems, such as SCATS and SCOOT, are not efficient as their traffic data provided by induction loops has a low update frequency (i.e., longer than 1 minute). Moreover, the traffic light signal plans used by these systems are selected from a limited set of candidate plans pre-programmed prior to unexpected events' occurrence. Recent research demonstrates that camera-based traffic light systems controlled by deep reinforcement learning (DRL) algorithms are more effective in reducing traffic congestion, in which the cameras can provide high-frequency high-resolution traffic data. However, these systems are costly to deploy in big cities due to the excessive potential upgrades required to road infrastructure. In this paper, we argue that Unmanned Aerial Vehicles (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#20197;&#21450;&#34892;&#20026;&#23884;&#20837;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04974</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#36827;&#34892;&#36830;&#32493;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Robot Learning using Self-Supervised Task Inference. (arXiv:2309.04974v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#20197;&#21450;&#34892;&#20026;&#23884;&#20837;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36171;&#20104;&#26426;&#22120;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#19981;&#26029;&#21457;&#23637;&#30340;&#25216;&#33021;&#32780;&#19981;&#26159;&#25484;&#25569;&#21333;&#19968;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20182;&#20204;&#24456;&#23569;&#20851;&#27880;&#20219;&#21153;&#25512;&#26029;&#12290;&#20026;&#20102;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#38656;&#35201;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#26410;&#26631;&#35760;&#31034;&#33539;&#30340;&#36816;&#21160;&#21644;&#25928;&#26524;&#37096;&#20998;&#30340;&#33258;&#32452;&#32455;&#26469;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#21160;&#20316;-&#24847;&#22270;&#23884;&#20837;&#30340;&#33258;&#32452;&#32455;&#26469;&#23398;&#20064;&#39640;&#23618;&#34892;&#20026;&#23884;&#20837;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#34892;&#20026;&#21305;&#37197;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#26469;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#25512;&#26029;&#32593;&#32476;&#65288;TINet&#65289;&#23558;&#26410;&#26631;&#35760;&#31034;&#33539;&#26144;&#23556;&#21040;&#20854;&#26368;&#36817;&#30340;&#34892;&#20026;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#20219;&#21153;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multi-task learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this paper, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher-level behavior embedding from self-organization of the joint action-intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel Task Inference Network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multi-task p
&lt;/p&gt;</description></item><item><title>Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04965</link><description>&lt;p&gt;
&#21069;&#32512;&#25193;&#25955;&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#26679;&#21270;&#22270;&#20687;&#23383;&#24149;&#30340;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning. (arXiv:2309.04965v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04965
&lt;/p&gt;
&lt;p&gt;
Prefix-diffusion&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#27880;&#20837;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#26469;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#26469;&#20943;&#23569;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#30340;&#23383;&#24149;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#21442;&#25968;&#35268;&#27169;&#36739;&#22823;&#20173;&#28982;&#26159;&#36825;&#20123;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#23383;&#24149;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#25193;&#25955;&#65292;&#31216;&#20026;&#21069;&#32512;&#25193;&#25955;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#21069;&#32512;&#22270;&#20687;&#23884;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#20943;&#23569;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26144;&#23556;&#32593;&#32476;&#12290;&#21069;&#32512;&#25193;&#25955;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23383;&#24149;&#65292;&#21516;&#26102;&#20445;&#25345;&#23383;&#24149;&#30340;&#27969;&#30021;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25193;&#23637;&#22270;&#20687;&#23383;&#24149;&#30340;&#25193;&#25955;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While impressive performance has been achieved in image captioning, the limited diversity of the generated captions and the large parameter scale remain major barriers to the real-word application of these systems. In this work, we propose a lightweight image captioning network in combination with continuous diffusion, called Prefix-diffusion. To achieve diversity, we design an efficient method that injects prefix image embeddings into the denoising process of the diffusion model. In order to reduce trainable parameters, we employ a pre-trained model to extract image features and further design an extra mapping network. Prefix-diffusion is able to generate diverse captions with relatively less parameters, while maintaining the fluency and relevance of the captions benefiting from the generative capabilities of the diffusion model. Our work paves the way for scaling up diffusion models for image captioning, and achieves promising performance compared with recent approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04951</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#25688;&#35201;&#65306;&#19968;&#39033;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-document Summarization: A Comparative Evaluation. (arXiv:2309.04951v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#25991;&#26723;&#25688;&#35201;&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#20026;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#21442;&#32771;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#39046;&#22495;&#30340;&#26368;&#26032;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#30830;&#23450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25991;&#29486;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#26368;&#26032;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;BigSurvey-MDS&#21644;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;PRIMERA&#21644;PEGASUS&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30001;&#20110;&#39046;&#22495;&#30340;&#19981;&#21516;&#32780;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;LED&#22312;MS$^2$&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;PRIMERA&#21644;PEGASUS&#12290;&#25105;&#20204;&#20351;&#29992;ROUGE&#20998;&#25968;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#25351;&#26631;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20102;&#35299;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#20013;&#20934;&#30830;&#12289;&#40065;&#26834;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#26410;&#26469;&#30340;MDS&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is aimed at evaluating state-of-the-art models for Multi-document Summarization (MDS) on different types of datasets in various domains and investigating the limitations of existing models to determine future research directions. To address this gap, we conducted an extensive literature review to identify state-of-the-art models and datasets. We analyzed the performance of PRIMERA and PEGASUS models on BigSurvey-MDS and MS$^2$ datasets, which posed unique challenges due to their varied domains. Our findings show that the General-Purpose Pre-trained Model LED outperforms PRIMERA and PEGASUS on the MS$^2$ dataset. We used the ROUGE score as a performance metric to evaluate the identified models on different datasets. Our study provides valuable insights into the models' strengths and weaknesses, as well as their applicability in different domains. This work serves as a reference for future MDS research and contributes to the development of accurate and robust models which can 
&lt;/p&gt;</description></item><item><title>MFPNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#27531;&#24046;&#22359;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#65292;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04914</link><description>&lt;p&gt;
MFPNet: &#36731;&#37327;&#32423;&#35821;&#20041;&#20998;&#21106;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MFPNet: Multi-scale Feature Propagation Nwtwork For Lightweight Semantic Segmentation. (arXiv:2309.04914v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04914
&lt;/p&gt;
&lt;p&gt;
MFPNet&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#27531;&#24046;&#22359;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#65292;&#33719;&#24471;&#20102;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20016;&#23500;&#30740;&#31350;&#30456;&#27604;&#65292;&#36731;&#37327;&#32423;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#23637;&#20284;&#20046;&#36827;&#23637;&#36739;&#24930;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32039;&#20945;&#26041;&#27861;&#30001;&#20110;&#32593;&#32476;&#30340;&#27973;&#23618;&#32780;&#23548;&#33268;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#20998;&#21106;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#32593;&#32476;&#65288;MFPNet&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#37319;&#29992;&#23545;&#31216;&#30340;&#27531;&#24046;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#28789;&#27963;&#30340;&#29942;&#39048;&#27531;&#24046;&#27169;&#22359;&#65288;BRM&#65289;&#65292;&#20197;&#25506;&#32034;&#28145;&#24230;&#21644;&#20016;&#23500;&#30340;&#22810;&#23610;&#24230;&#35821;&#20041;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#24314;&#27169;&#28508;&#22312;&#30340;&#38271;&#31243;&#19978;&#19979;&#25991;&#20851;&#31995;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;BRM&#22359;&#20043;&#38388;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#20256;&#25773;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to the abundant research focusing on large-scale models, the progress in lightweight semantic segmentation appears to be advancing at a comparatively slower pace. However, existing compact methods often suffer from limited feature representation capability due to the shallowness of their networks. In this paper, we propose a novel lightweight segmentation architecture, called Multi-scale Feature Propagation Network (MFPNet), to address the dilemma. Specifically, we design a robust Encoder-Decoder structure featuring symmetrical residual blocks that consist of flexible bottleneck residual modules (BRMs) to explore deep and rich muti-scale semantic context. Furthermore, taking benefit from their capacity to model latent long-range contextual relationships, we leverage Graph Convolutional Networks (GCNs) to facilitate multi-scale feature propagation between the BRM blocks. When evaluated on benchmark datasets, our proposed approach shows superior segmentation results.
&lt;/p&gt;</description></item><item><title>&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04911</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20113;&#35745;&#31639;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning-based Security in Cloud Computing. (arXiv:2309.04911v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04911
&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#65288;CC&#65289;&#27491;&#22312;&#25913;&#21464;&#21521;&#29992;&#25143;&#25552;&#20379;IT&#36164;&#28304;&#30340;&#26041;&#24335;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#31616;&#21270;&#30340;&#22522;&#30784;&#35774;&#26045;&#26469;&#35775;&#38382;&#21644;&#31649;&#29702;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;CC&#30340;&#22686;&#38271;&#65292;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#21487;&#29992;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#26426;&#23494;&#24615;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;CSPs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#20943;&#23569;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;ML&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#25913;&#21464;CSPs&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#20113;&#35745;&#31639;&#23433;&#20840;&#39046;&#22495;&#30340;&#19968;&#20123;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#19968;&#31995;&#21015;ML&#31639;&#27861;&#30340;&#29305;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#20851;&#20110;ML&#22312;&#20113;&#35745;&#31639;&#20013;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AIDI&#30340;&#21152;&#36895;&#36845;&#20195;&#25193;&#25955;&#36870;&#36716;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#23548;&#21521;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#26377;&#25928;&#30340;&#35745;&#31639;&#25928;&#29575;&#19979;&#26174;&#33879;&#25552;&#39640;&#30495;&#23454;&#22270;&#20687;&#32534;&#36753;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.04907</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#36895;&#36845;&#20195;&#25193;&#25955;&#36870;&#36716;&#36827;&#34892;&#26377;&#25928;&#30340;&#30495;&#23454;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Effective Real Image Editing with Accelerated Iterative Diffusion Inversion. (arXiv:2309.04907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AIDI&#30340;&#21152;&#36895;&#36845;&#20195;&#25193;&#25955;&#36870;&#36716;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#23548;&#21521;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#26377;&#25928;&#30340;&#35745;&#31639;&#25928;&#29575;&#19979;&#26174;&#33879;&#25552;&#39640;&#30495;&#23454;&#22270;&#20687;&#32534;&#36753;&#30340;&#37325;&#24314;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#21644;&#25805;&#32437;&#33258;&#28982;&#22270;&#20687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#12290;&#22312;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#65292;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#23558;&#30495;&#23454;&#22270;&#20687;&#26144;&#23556;&#21040;&#20854;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#22122;&#22768;&#21521;&#37327;&#30340;&#21453;&#21521;&#36807;&#31243;&#65292;&#22240;&#20026;&#38656;&#35201;&#33021;&#22815;&#37325;&#24314;&#22270;&#20687;&#25165;&#33021;&#32534;&#36753;&#20854;&#20869;&#23481;&#12290;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#65292;&#27599;&#20010;&#21453;&#36716;&#27493;&#39588;&#20013;&#30340;&#32447;&#24615;&#21270;&#20551;&#35774;&#20351;&#24471;&#25972;&#20010;&#30830;&#23450;&#24615;&#21453;&#36716;&#36807;&#31243;&#19981;&#21487;&#38752;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#21453;&#36716;&#31283;&#23450;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20135;&#29983;&#26174;&#33879;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21152;&#36895;&#36845;&#20195;&#25193;&#25955;&#21453;&#36716;&#26041;&#27861;&#65288;AIDI&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#39069;&#22806;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#24320;&#38144;&#26174;&#33879;&#25552;&#39640;&#37325;&#24314;&#31934;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#23548;&#21521;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#33539;&#22260;&#19978;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite all recent progress, it is still challenging to edit and manipulate natural images with modern generative models. When using Generative Adversarial Network (GAN), one major hurdle is in the inversion process mapping a real image to its corresponding noise vector in the latent space, since its necessary to be able to reconstruct an image to edit its contents. Likewise for Denoising Diffusion Implicit Models (DDIM), the linearization assumption in each inversion step makes the whole deterministic inversion process unreliable. Existing approaches that have tackled the problem of inversion stability often incur in significant trade-offs in computational efficiency. In this work we propose an Accelerated Iterative Diffusion Inversion method, dubbed AIDI, that significantly improves reconstruction accuracy with minimal additional overhead in space and time complexity. By using a novel blended guidance technique, we show that effective results can be obtained on a large range of image
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;ViTScore&#26469;&#35780;&#20272;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#22312;&#35821;&#20041;&#36890;&#20449;&#20013;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04891</link><description>&lt;p&gt;
&#22914;&#20309;&#29992;ViTScore&#24230;&#37327;&#22270;&#20687;&#30340;&#35821;&#20041;&#36890;&#20449;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Evaluate Semantic Communications for Images with ViTScore Metric?. (arXiv:2309.04891v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;ViTScore&#26469;&#35780;&#20272;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#22312;&#35821;&#20041;&#36890;&#20449;&#20013;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#34987;&#26399;&#26395;&#25104;&#20026;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#25512;&#21160;&#19979;&#19968;&#20195;&#36890;&#20449;&#30340;&#21457;&#23637;&#65292;&#20854;&#20027;&#35201;&#20851;&#27880;&#28857;&#20174;&#31934;&#30830;&#30340;&#27604;&#29305;&#20256;&#36755;&#36716;&#31227;&#21040;&#20102;&#22312;&#36890;&#20449;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#35821;&#20041;&#20449;&#24687;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#19981;&#36866;&#29992;&#20110;&#35780;&#20272;SC&#20013;&#30340;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#29992;&#20110;&#34913;&#37327;&#20004;&#20010;&#22270;&#20687;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#32463;&#20856;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#25110;&#32467;&#26500;&#32423;&#65292;&#20363;&#22914;PSNR&#21644;MS-SSIM&#12290;&#22312;SC&#20013;&#30452;&#25509;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26576;&#20123;&#23450;&#21046;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;LPIPS&#65289;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;NLP&#39046;&#22495;&#30340;BERTScore&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;Vision Transformer Score&#65288;ViTScore&#65289;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ViTScore&#20855;&#26377;&#23545;&#31216;&#24615;&#12289;&#26377;&#30028;&#24615;&#21644;&#24402;&#19968;&#21270;&#31561;&#19977;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;ViTScore&#22312;&#22270;&#20687;&#34913;&#37327;&#20013;&#26041;&#20415;&#21644;&#30452;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communications (SC) have been expected to be a new paradigm shifting to catalyze the next generation communication, whose main concerns shift from accurate bit transmission to effective semantic information exchange in communications. However, the previous and widely-used metrics for images are not applicable to evaluate the image semantic similarity in SC. Classical metrics to measure the similarity between two images usually rely on the pixel level or the structural level, such as the PSNR and the MS-SSIM. Straightforwardly using some tailored metrics based on deep-learning methods in CV community, such as the LPIPS, is infeasible for SC. To tackle this, inspired by BERTScore in NLP community, we propose a novel metric for evaluating image semantic similarity, named Vision Transformer Score (ViTScore). We prove theoretically that ViTScore has 3 important properties, including symmetry, boundedness, and normalization, which make ViTScore convenient and intuitive for image mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#12290;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#25913;&#36827;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;EDA&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04862</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributional Data Augmentation Methods for Low Resource Language. (arXiv:2309.04862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#12290;&#23427;&#20204;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#25913;&#36827;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;EDA&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22686;&#24378;&#26159;&#19968;&#31181;&#20174;&#19981;&#36275;&#36164;&#28304;&#30340;&#35821;&#26009;&#24211;&#20013;&#26500;&#36896;&#21512;&#25104;&#25968;&#25454;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#65292;&#25991;&#26412;&#22686;&#24378;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20986;&#29616;&#65292;&#20197;&#25552;&#21319;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#20043;&#19968;&#26159;&#26131;&#20110;&#25968;&#25454;&#22686;&#24378;&#65288;EDA&#65289;&#65292;&#23427;&#36890;&#36807;&#27880;&#20837;&#21644;&#26367;&#25442;&#21516;&#20041;&#35789;&#20197;&#21450;&#38543;&#26426;&#25490;&#21015;&#21477;&#23376;&#26469;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#12290;EDA&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#38656;&#35201;&#22810;&#21151;&#33021;&#21644;&#23436;&#25972;&#30340;&#21516;&#20041;&#35789;&#35789;&#20856;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24456;&#38590;&#25214;&#21040;&#12290;&#20026;&#20102;&#25552;&#39640;EDA&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#26041;&#27861;&#65306;&#26131;&#20110;&#20998;&#24067;&#24335;&#25968;&#25454;&#22686;&#24378;&#65288;EDDA&#65289;&#21644;&#31867;&#22411;&#29305;&#23450;&#30340;&#30456;&#20284;&#35789;&#26367;&#25442;&#65288;TSSR&#65289;&#65292;&#23427;&#20351;&#29992;&#35821;&#20041;&#35789;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35789;&#24615;&#26631;&#35760;&#26469;&#36827;&#34892;&#35789;&#26367;&#25442;&#21644;&#22686;&#24378;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Text augmentation is a technique for constructing synthetic data from an under-resourced corpus to improve predictive performance. Synthetic data generation is common in numerous domains. However, recently text augmentation has emerged in natural language processing (NLP) to improve downstream tasks. One of the current state-of-the-art text augmentation techniques is easy data augmentation (EDA), which augments the training data by injecting and replacing synonyms and randomly permuting sentences. One major obstacle with EDA is the need for versatile and complete synonym dictionaries, which cannot be easily found in low-resource languages. To improve the utility of EDA, we propose two extensions, easy distributional data augmentation (EDDA) and type specific similar word replacement (TSSR), which uses semantic word context information and part-of-speech tags for word replacement and augmentation. In an extensive empirical evaluation, we show the utility of the proposed methods, measure
&lt;/p&gt;</description></item><item><title>AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04856</link><description>&lt;p&gt;
AmbientFlow: &#26469;&#33258;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#30340;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04856
&lt;/p&gt;
&lt;p&gt;
AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#37325;&#24314;&#12289;&#21518;&#39564;&#37319;&#26679;&#21644;&#25968;&#25454;&#20849;&#20139;&#12290;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#30830;&#30340;&#23494;&#24230;&#20272;&#35745;&#20197;&#21450;&#24555;&#36895;&#12289;&#24265;&#20215;&#21644;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#25104;&#20687;&#31561;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#38271;&#26102;&#38388;&#33719;&#21462;&#25110;&#39640;&#36752;&#23556;&#21058;&#37327;&#65292;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#23545;&#35937;&#30340;&#22122;&#22768;&#25110;&#37096;&#20998;&#35266;&#27979;&#27979;&#37327;&#26356;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AmbientFlow&#65292;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#25968;&#25454;&#24314;&#31435;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>RHPG&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26222;&#36890;&#30340;PG&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20840;&#23616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2309.04831</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#20272;&#35745;&#22120;&#35774;&#35745;&#20013;&#65292;&#36882;&#20943;&#26102;&#22495;&#31574;&#30053;&#25628;&#32034;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04831
&lt;/p&gt;
&lt;p&gt;
RHPG&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26222;&#36890;&#30340;PG&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20840;&#23616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36882;&#20943;&#26102;&#22495;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#65288;&#21363;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RHPG&#31639;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#20063;&#19981;&#38656;&#35201;&#30446;&#26631;&#31995;&#32479;&#26159;&#24320;&#29615;&#31283;&#23450;&#30340;&#12290;RHPG&#30340;&#20851;&#38190;&#26159;&#23558;&#26222;&#36890;&#30340;PG&#65288;&#25110;&#20854;&#20182;&#31574;&#30053;&#25628;&#32034;&#26041;&#21521;&#65289;&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#22312;&#31574;&#30053;&#21442;&#25968;&#20013;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;RHPG&#30340;&#20248;&#21270;&#36335;&#32447;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#38024;&#23545;&#25511;&#21046;&#22120;&#35774;&#35745;&#24320;&#23637;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04806</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#30340;&#21450;&#26102;&#34701;&#21512;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Timely Fusion of Surround Radar/Lidar for Object Detection in Autonomous Driving Systems. (arXiv:2309.04806v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#65292;&#20197;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#30340;&#21709;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21608;&#22260;&#29615;&#22659;&#37325;&#24314;&#12290;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#21487;&#20197;&#20197;&#26368;&#23567;&#30340;&#25104;&#26412;&#25552;&#20379;360&#24230;&#35270;&#37326;&#37319;&#26679;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#26377;&#21069;&#26223;&#30340;&#24863;&#30693;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#29289;&#29702;&#38480;&#21046;&#65292;&#29615;&#32469;&#38647;&#36798;&#30340;&#26059;&#36716;&#36895;&#24230;&#21450;&#29983;&#25104;&#38647;&#36798;&#25968;&#25454;&#24103;&#30340;&#39057;&#29575;&#36828;&#20302;&#20110;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#12290;&#29616;&#26377;&#30340;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#34701;&#21512;&#26041;&#27861;&#24517;&#39035;&#20197;&#29615;&#32469;&#38647;&#36798;&#30340;&#20302;&#39057;&#29575;&#24037;&#20316;&#65292;&#32780;&#26080;&#27861;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#21709;&#24212;&#24615;&#35201;&#27714;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;MVDNet&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#24037;&#20316;&#39057;&#29575;&#20165;&#21463;&#38480;&#20110;&#26356;&#24555;&#30340;&#29615;&#32469;&#28608;&#20809;&#38647;&#36798;&#32780;&#19981;&#26159;&#36739;&#24930;&#30340;&#29615;&#32469;&#38647;&#36798;&#34701;&#21512;&#29615;&#32469;&#38647;&#36798;/&#28608;&#20809;&#38647;&#36798;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#24605;&#36335;&#24456;&#31616;&#21333;&#65306;&#35753;MVDNet&#22788;&#29702;&#26242;&#26102;&#19981;&#23545;&#40784;&#30340;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#24103;&#65292;&#28982;&#21518;&#26681;&#25454;&#24103;&#26102;&#38388;&#20449;&#24687;&#23545;&#34701;&#21512;&#21518;&#30340;&#32467;&#26524;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing Radar and Lidar sensor data can fully utilize their complementary advantages and provide more accurate reconstruction of the surrounding for autonomous driving systems. Surround Radar/Lidar can provide 360-degree view sampling with the minimal cost, which are promising sensing hardware solutions for autonomous driving systems. However, due to the intrinsic physical constraints, the rotating speed of surround Radar, and thus the frequency to generate Radar data frames, is much lower than surround Lidar. Existing Radar/Lidar fusion methods have to work at the low frequency of surround Radar, which cannot meet the high responsiveness requirement of autonomous driving systems.This paper develops techniques to fuse surround Radar/Lidar with working frequency only limited by the faster surround Lidar instead of the slower surround Radar, based on the state-of-the-art object detection model MVDNet. The basic idea of our approach is simple: we let MVDNet work with temporally unaligned d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#19990;&#30028;&#29190;&#21457;&#36229;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;RealBSR&#65292;&#36890;&#36807;&#24341;&#20837;&#32852;&#21512;&#29190;&#21457;&#20851;&#32852;&#32593;&#32476;&#65288;FBAnet&#65289;&#65292;&#20174;&#22810;&#24103;&#20013;&#24544;&#23454;&#22320;&#37325;&#24314;&#22270;&#20687;&#32454;&#33410;&#12290;FBAnet&#20351;&#29992;&#20174;&#32467;&#26500;&#20960;&#20309;&#35282;&#24230;&#30340;&#31616;&#21333;&#21333;&#24212;&#23545;&#40784;&#21644;&#32852;&#21512;&#20146;&#21644;&#21147;&#34701;&#21512;&#65288;FAF&#65289;&#31574;&#30053;&#26469;&#32858;&#21512;&#24103;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#29190;&#21457;&#34920;&#31034;&#35299;&#30721;&#27169;&#22359;&#26469;&#35299;&#30721;&#34701;&#21512;&#30340;&#20449;&#24687;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.04803</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#29190;&#21457;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65306;&#22522;&#20934;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Real-World Burst Image Super-Resolution: Benchmark and Method. (arXiv:2309.04803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#19990;&#30028;&#29190;&#21457;&#36229;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;RealBSR&#65292;&#36890;&#36807;&#24341;&#20837;&#32852;&#21512;&#29190;&#21457;&#20851;&#32852;&#32593;&#32476;&#65288;FBAnet&#65289;&#65292;&#20174;&#22810;&#24103;&#20013;&#24544;&#23454;&#22320;&#37325;&#24314;&#22270;&#20687;&#32454;&#33410;&#12290;FBAnet&#20351;&#29992;&#20174;&#32467;&#26500;&#20960;&#20309;&#35282;&#24230;&#30340;&#31616;&#21333;&#21333;&#24212;&#23545;&#40784;&#21644;&#32852;&#21512;&#20146;&#21644;&#21147;&#34701;&#21512;&#65288;FAF&#65289;&#31574;&#30053;&#26469;&#32858;&#21512;&#24103;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#29190;&#21457;&#34920;&#31034;&#35299;&#30721;&#27169;&#22359;&#26469;&#35299;&#30721;&#34701;&#21512;&#30340;&#20449;&#24687;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#21333;&#19968;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#22312;&#20174;&#19968;&#24352;&#36755;&#20837;&#22270;&#20687;&#20013;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#26102;&#24635;&#26159;&#38519;&#20837;&#22256;&#22659;&#65292;&#23588;&#20854;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#19990;&#30028;&#29190;&#21457;&#36229;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;RealBSR&#65292;&#20197;&#25506;&#32034;&#20174;&#22810;&#24103;&#20013;&#24544;&#23454;&#22320;&#37325;&#24314;&#22270;&#20687;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32852;&#21512;&#29190;&#21457;&#20851;&#32852;&#32593;&#32476;&#65288;FBAnet&#65289;&#65292;&#26469;&#30740;&#31350;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#36864;&#21270;&#26465;&#20214;&#19979;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#20687;&#32032;&#20301;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;FBAnet&#19981;&#20351;&#29992;&#20687;&#32032;&#32423;&#23545;&#40784;&#65292;&#32780;&#26159;&#20174;&#32467;&#26500;&#20960;&#20309;&#35282;&#24230;&#20351;&#29992;&#31616;&#21333;&#30340;&#21333;&#24212;&#23545;&#40784;&#21644;&#32852;&#21512;&#20146;&#21644;&#21147;&#34701;&#21512;&#65288;FAF&#65289;&#31574;&#30053;&#26469;&#32858;&#21512;&#24103;&#38388;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#36825;&#20123;&#34701;&#21512;&#30340;&#20449;&#24687;&#34920;&#31034;&#34987;&#39304;&#36865;&#21040;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#29190;&#21457;&#34920;&#31034;&#35299;&#30721;&#27169;&#22359;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#29256;&#26412;&#65292;&#21363;RealBSR&#21644;...
&lt;/p&gt;
&lt;p&gt;
Despite substantial advances, single-image super-resolution (SISR) is always in a dilemma to reconstruct high-quality images with limited information from one input image, especially in realistic scenarios. In this paper, we establish a large-scale real-world burst super-resolution dataset, i.e., RealBSR, to explore the faithful reconstruction of image details from multiple frames. Furthermore, we introduce a Federated Burst Affinity network (FBAnet) to investigate non-trivial pixel-wise displacements among images under real-world image degradation. Specifically, rather than using pixel-wise alignment, our FBAnet employs a simple homography alignment from a structural geometry aspect and a Federated Affinity Fusion (FAF) strategy to aggregate the complementary information among frames. Those fused informative representations are fed to a Transformer-based module of burst representation decoding. Besides, we have conducted extensive experiments on two versions of our datasets, i.e., Rea
&lt;/p&gt;</description></item><item><title>CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.04802</link><description>&lt;p&gt;
CPMR: &#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#19982;&#20266;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04802
&lt;/p&gt;
&lt;p&gt;
CPMR&#26159;&#19968;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22686;&#37327;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#21019;&#24314;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#30340;&#19977;&#20010;&#34920;&#31034;&#65292;&#20934;&#30830;&#22320;&#24314;&#27169;&#20102;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#21644;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#30340;&#21160;&#26426;&#21487;&#20197;&#20998;&#20026;&#38745;&#24577;&#20559;&#22909;&#21644;&#21160;&#24577;&#20852;&#36259;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#24314;&#27169;&#29992;&#25143;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#26368;&#36817;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#21644;&#28436;&#21270;&#20174;&#25209;&#37327;&#21040;&#36798;&#30340;&#20114;&#21160;&#20013;&#36827;&#34892;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#22312;&#19978;&#19979;&#25991;&#22330;&#26223;&#20013;&#20154;&#20204;&#24456;&#23481;&#26131;&#21463;&#21040;&#20854;&#20182;&#29992;&#25143;&#30340;&#26368;&#36817;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21382;&#21490;&#20114;&#21160;&#20013;&#24212;&#29992;&#28436;&#21270;&#20250;&#31232;&#37322;&#26368;&#36817;&#20114;&#21160;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#26080;&#27861;&#20934;&#30830;&#22320;&#24314;&#27169;&#20852;&#36259;&#21160;&#24577;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20266;&#22810;&#20219;&#21153;&#25512;&#33616;&#31995;&#32479;&#65288;CPMR&#65289;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#29992;&#25143;&#21644;&#39033;&#30446;&#21019;&#24314;&#19977;&#20010;&#34920;&#31034;&#65288;&#38745;&#24577;&#23884;&#20837;&#12289;&#21382;&#21490;&#26102;&#38388;&#29366;&#24577;&#21644;&#19978;&#19979;&#25991;&#26102;&#38388;&#29366;&#24577;&#65289;&#65292;&#26469;&#24314;&#27169;&#21382;&#21490;&#21644;&#19978;&#19979;&#25991;&#24773;&#22659;&#20013;&#30340;&#28436;&#21270;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#26102;&#38388;&#29366;&#24577;&#28436;&#21270;&#21644;&#22686;&#37327;&#25512;&#33616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The motivations of users to make interactions can be divided into static preference and dynamic interest. To accurately model user representations over time, recent studies in sequential recommendation utilize information propagation and evolution to mine from batches of arriving interactions. However, they ignore the fact that people are easily influenced by the recent actions of other users in the contextual scenario, and applying evolution across all historical interactions dilutes the importance of recent ones, thus failing to model the evolution of dynamic interest accurately. To address this issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR) to model the evolution in both historical and contextual scenarios by creating three representations for each user and item under different dynamics: static embedding, historical temporal states, and contextual temporal states. To dually improve the performance of temporal states evolution and incremental recommenda
&lt;/p&gt;</description></item><item><title>TMComposites&#26159;&#19968;&#31181;&#25554;&#25300;&#24335;&#21327;&#20316;&#26041;&#24335;&#65292;&#36890;&#36807;&#29305;&#21270;&#21644;&#35780;&#20272;&#25104;&#21592;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21487;&#20197;&#22312;&#36739;&#22797;&#26434;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04801</link><description>&lt;p&gt;
TMComposites: &#19987;&#29992;Tsetlin&#26426;&#22120;&#20043;&#38388;&#30340;&#21363;&#25554;&#21363;&#29992;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines. (arXiv:2309.04801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04801
&lt;/p&gt;
&lt;p&gt;
TMComposites&#26159;&#19968;&#31181;&#25554;&#25300;&#24335;&#21327;&#20316;&#26041;&#24335;&#65292;&#36890;&#36807;&#29305;&#21270;&#21644;&#35780;&#20272;&#25104;&#21592;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#21487;&#20197;&#22312;&#36739;&#22797;&#26434;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#20174;&#22522;&#20110;&#31639;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#36716;&#21464;&#20026;&#22522;&#20110;&#36923;&#36753;&#30340;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26681;&#26412;&#24615;&#30340;&#25913;&#21464;&#12290;&#25903;&#25345;&#21367;&#31215;&#65292;TM&#25104;&#21151;&#22320;&#22788;&#29702;&#20687;MNIST&#65292;Fashion-MNIST&#21644;CIFAR-2&#36825;&#26679;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;TM&#22312;&#34920;&#31034;&#26356;&#22797;&#26434;&#20219;&#21153;&#30340;CIFAR-10&#21644;CIFAR-100&#19978;&#38590;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19987;&#29992;TM&#20043;&#38388;&#30340;&#21363;&#25554;&#21363;&#29992;&#21327;&#20316;&#65292;&#31216;&#20026;TM Composites&#12290;&#21327;&#20316;&#20381;&#36182;&#20110;TM&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#29305;&#21270;&#30340;&#33021;&#21147;&#21644;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35780;&#20272;&#33258;&#36523;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;&#24403;&#22242;&#38431;&#32452;&#21512;&#26102;&#65292;&#26368;&#33258;&#20449;&#30340;TM&#20570;&#20986;&#20915;&#31574;&#65292;&#35299;&#20915;&#20102;&#19981;&#30830;&#23450;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;TM&#32452;&#21512;&#27604;&#20854;&#25104;&#21592;&#26356;&#26377;&#33021;&#21147;&#65292;&#20174;&#20182;&#20204;&#30340;&#19987;&#19994;&#21270;&#20013;&#21463;&#30410;&#12290;&#21327;&#20316;&#30340;&#29305;&#28857;&#26159;&#21363;&#25554;&#21363;&#29992;&#65292;&#21363;&#25104;&#21592;&#21487;&#20197;&#22312;&#20219;&#20309;&#26102;&#38388;&#12289;&#20219;&#20309;&#26041;&#24335;&#19979;&#32452;&#21512;&#65292;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;TM&#19987;&#19994;&#21270;&#65306;&#26799;&#24230;&#30452;&#26041;&#22270;&#65292;&#33258;&#36866;&#24212;&#39640;&#26031;
&lt;/p&gt;
&lt;p&gt;
Tsetlin Machines (TMs) provide a fundamental shift from arithmetic-based to logic-based machine learning. Supporting convolution, they deal successfully with image classification datasets like MNIST, Fashion-MNIST, and CIFAR-2. However, the TM struggles with getting state-of-the-art performance on CIFAR-10 and CIFAR-100, representing more complex tasks. This paper introduces plug-and-play collaboration between specialized TMs, referred to as TM Composites. The collaboration relies on a TM's ability to specialize during learning and to assess its competence during inference. When teaming up, the most confident TMs make the decisions, relieving the uncertain ones. In this manner, a TM Composite becomes more competent than its members, benefiting from their specializations. The collaboration is plug-and-play in that members can be combined in any way, at any time, without fine-tuning. We implement three TM specializations in our empirical evaluation: Histogram of Gradients, Adaptive Gauss
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#25552;&#20132;&#28040;&#24687;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#30340;&#26816;&#26597;&#12290;</title><link>http://arxiv.org/abs/2309.04797</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#30340;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Full-fledged Commit Message Quality Checker Based on Machine Learning. (arXiv:2309.04797v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04797
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#25552;&#20132;&#28040;&#24687;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20132;&#28040;&#24687;&#65288;CMs&#65289;&#26159;&#29256;&#26412;&#25511;&#21046;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#26356;&#25913;&#20869;&#23481;&#21644;&#21407;&#22240;&#30340;&#37325;&#35201;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#26497;&#22823;&#22320;&#25903;&#25345;&#36719;&#20214;&#32500;&#25252;&#21644;&#28436;&#36827;&#12290;&#20294;&#26159;&#25776;&#20889;&#33391;&#22909;&#30340;CMs&#26159;&#22256;&#38590;&#30340;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#24573;&#35270;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#36866;&#21512;&#23454;&#36341;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;CM&#30340;&#32534;&#20889;&#36136;&#37327;&#65292;&#21253;&#25324;&#20854;&#21547;&#20041;&#21644;&#19978;&#19979;&#25991;&#12290;&#37492;&#20110;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#38382;&#39064;&#65306;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22914;&#20309;&#34913;&#37327;CM&#36136;&#37327;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#65311;&#36890;&#36807;&#32771;&#34385;&#26368;&#27969;&#34892;&#30340;CM&#36136;&#37327;&#25351;&#21335;&#30340;&#25152;&#26377;&#35268;&#21017;&#65292;&#21019;&#24314;&#36825;&#20123;&#35268;&#21017;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#21644;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#26597;&#36825;&#20123;&#35268;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;&#23545;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23454;&#36341;&#20013;&#20855;&#26377;82.9&#65285;&#30340;&#26368;&#20302;F1&#20998;&#25968;&#36275;&#22815;&#22909;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#26816;&#26597;&#25152;&#26377;&#36825;&#20123;CM&#36136;&#37327;&#35268;&#21017;&#12290;&#23427;&#21487;&#20197;&#29992;&#26469;&#36741;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#26356;&#22909;&#30340;CMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Commit messages (CMs) are an essential part of version control. By providing important context in regard to what has changed and why, they strongly support software maintenance and evolution. But writing good CMs is difficult and often neglected by developers. So far, there is no tool suitable for practice that automatically assesses how well a CM is written, including its meaning and context. Since this task is challenging, we ask the research question: how well can the CM quality, including semantics and context, be measured with machine learning methods? By considering all rules from the most popular CM quality guideline, creating datasets for those rules, and training and evaluating state-of-the-art machine learning models to check those rules, we can answer the research question with: sufficiently well for practice, with the lowest F$_1$ score of 82.9\%, for the most challenging task. We develop a full-fledged open-source framework that checks all these CM quality rules. It is use
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#24369;&#28857;&#26469;&#25913;&#36827;&#27169;&#22411;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36817;&#37051;&#20013;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.04777</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#30340;&#24369;&#28857; &#25913;&#36827;&#31283;&#20581;&#27169;&#22411;&#25968;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Model Watermark via Reducing Parametric Vulnerability. (arXiv:2309.04777v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#24369;&#28857;&#26469;&#25913;&#36827;&#27169;&#22411;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36817;&#37051;&#20013;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#20854;&#21830;&#19994;&#20215;&#20540;&#21644;&#23545;&#36164;&#28304;&#30340;&#24040;&#22823;&#38656;&#27714;&#32780;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#28982;&#32780;&#20026;&#20102;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29256;&#26435;&#65292;&#26368;&#36817;&#22522;&#20110;&#21518;&#38376;&#30340;&#25317;&#26377;&#26435;&#39564;&#35777;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#22312;&#36825;&#31181;&#39564;&#35777;&#26041;&#24335;&#20013;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#20197;&#22312;&#21457;&#24067;&#20043;&#21069;&#36890;&#36807;&#23884;&#20837;&#29305;&#23450;&#30340;&#21518;&#38376;&#34892;&#20026;&#23545;&#27169;&#22411;&#36827;&#34892;&#27700;&#21360;&#26631;&#35760;&#12290;&#38450;&#24481;&#26041;&#65288;&#36890;&#24120;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#65289;&#21487;&#20197;&#26681;&#25454;&#34892;&#20026;&#30340;&#23384;&#22312;&#26469;&#21028;&#26029;&#21487;&#30097;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#26159;&#21542;&#26159;&#20174;&#20182;&#20204;&#37027;&#37324;&#8220;&#20599;&#8221;&#26469;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27700;&#21360;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#31227;&#38500;&#25915;&#20987;&#65288;&#29978;&#33267;&#22914;&#24494;&#35843;&#65289;&#38750;&#24120;&#33030;&#24369;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#23545;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#35768;&#22810;&#22312;&#27700;&#21360;&#27169;&#22411;&#38468;&#36817;&#30340;&#21435;&#27700;&#21360;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24456;&#23481;&#26131;&#34987;&#29992;&#20110;&#31227;&#38500;&#25915;&#20987;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36855;&#20320;&#26368;&#22823;&#21270;&#26368;&#23567;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#36825;&#20123;&#21435;&#27700;&#21360;&#27169;&#22411;&#24182;&#24674;&#22797;&#23427;&#20204;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that o
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>AudRandAug&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#38899;&#39057;&#25968;&#25454;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04762</link><description>&lt;p&gt;
AudRandAug&#65306;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudRandAug: Random Image Augmentations for Audio Classification. (arXiv:2309.04762v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04762
&lt;/p&gt;
&lt;p&gt;
AudRandAug&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#38899;&#39057;&#25968;&#25454;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RandAug&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#23450;&#20041;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;RandAug&#22312;&#22270;&#20687;&#30456;&#20851;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#38468;&#21152;&#30340;&#35745;&#31639;&#24320;&#38144;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#23578;&#26410;&#25506;&#32034;&#23558;RandAug&#24212;&#29992;&#20110;&#23558;&#38899;&#39057;&#36716;&#21270;&#20026;&#22270;&#20687;&#27169;&#24335;&#30340;&#38899;&#39057;&#25968;&#25454;&#22686;&#24378;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudRandAug&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#38899;&#39057;&#25968;&#25454;&#30340;RandAug&#25913;&#36827;&#26041;&#27861;&#12290;AudRandAug&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;AudRandAug&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AudRandAug&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.
&lt;/p&gt;</description></item><item><title>RR-CP &#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#19979;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04760</link><description>&lt;p&gt;
RR-CP: &#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#21487;&#20449;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#19968;&#31181;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification. (arXiv:2309.04760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04760
&lt;/p&gt;
&lt;p&gt;
RR-CP &#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#19979;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#39044;&#27979;&#65288;CP&#65289;&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#29983;&#25104;&#19968;&#32452;&#39044;&#27979;&#65292;&#20351;&#24471;&#39044;&#27979;&#38598;&#20960;&#20046;&#24635;&#26159;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;99.5&#65285;&#30340;&#26102;&#38388;&#65289;&#12290; CP&#23545;&#32473;&#23450;&#27979;&#35797;&#26679;&#26412;&#30340;&#21487;&#33021;&#26631;&#31614;&#25552;&#20379;&#20840;&#38754;&#30340;&#39044;&#27979;&#65292;&#32780;&#38598;&#21512;&#30340;&#22823;&#23567;&#34920;&#31034;&#39044;&#27979;&#30340;&#30830;&#23450;&#31243;&#24230;&#65288;&#20363;&#22914;&#65292;&#22823;&#20110;&#19968;&#30340;&#38598;&#21512;&#26159;&#8220;&#19981;&#30830;&#23450;&#30340;&#8221;&#65289;&#12290; CP&#30340;&#36825;&#20123;&#29420;&#29305;&#23646;&#24615;&#20351;&#20154;&#31867;&#19987;&#23478;&#21644;&#21307;&#23398;AI&#27169;&#22411;&#20043;&#38388;&#33021;&#22815;&#26377;&#25928;&#21512;&#20316;&#65292;&#22312;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#65288;RR-CP&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#20197;&#20415;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#36798;&#21040;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#65288;&#20363;&#22914;&#65292;0.5&#65285;&#65289;&#65292;&#24182;&#22312;&#27492;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65288;&#23613;&#37327;&#23567;&#65289;&#12290; &#24403;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#36798;&#21040;&#26102;&#65292;&#25105;&#20204;&#35748;&#20026;&#39044;&#27979;&#38598;&#22823;&#23567;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction (CP) generates a set of predictions for a given test sample such that the prediction set almost always contains the true label (e.g., 99.5\% of the time). CP provides comprehensive predictions on possible labels of a given test sample, and the size of the set indicates how certain the predictions are (e.g., a set larger than one is `uncertain'). Such distinct properties of CP enable effective collaborations between human experts and medical AI models, allowing efficient intervention and quality check in clinical decision-making. In this paper, we propose a new method called Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a stronger statistical guarantee so that the user-specified error rate (e.g., 0.5\%) can be achieved in the test time, and under this constraint, the size of the prediction set is optimized (to be small). We consider a small prediction set size an important measure only when the user-specified error rate is achieved. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#65292;&#20197;&#23454;&#29616;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#12290;&#35813;&#26694;&#26550;&#23558;Navier-Stokes&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;</title><link>http://arxiv.org/abs/2309.04755</link><description>&lt;p&gt;
&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Real-time Training of Physics-informed Neural Networks: Applications in Ultrafast Ultrasound Blood Flow Imaging. (arXiv:2309.04755v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#65292;&#20197;&#23454;&#29616;&#36229;&#24555;&#36895;&#36229;&#22768;&#34880;&#27969;&#25104;&#20687;&#12290;&#35813;&#26694;&#26550;&#23558;Navier-Stokes&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#26041;&#31243;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#30340;&#26368;&#26480;&#20986;&#27714;&#35299;&#22120;&#20043;&#19968;&#65292;&#32780;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#24191;&#27867;&#24212;&#29992;&#20110;&#34880;&#27969;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#23436;&#25972;&#30340;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#23545;&#20110;&#36229;&#24555;&#36895;&#22810;&#26222;&#21202;&#36229;&#22768;&#65292;&#36825;&#19968;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#20110;\emph{&#20307;&#20869;}&#22797;&#26434;&#34880;&#27969;&#21160;&#21147;&#23398;&#30340;&#23637;&#31034;&#65292;&#27599;&#31186;&#33719;&#21462;&#25968;&#21315;&#24103;&#65288;&#25110;&#26102;&#38388;&#25139;&#65289;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PINN&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#31163;&#25955;&#21270;&#20026;&#31283;&#24577;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#39034;&#24207;&#27714;&#35299;&#31283;&#24577;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#20026;&#35299;&#20915;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#31216;&#20026;SeqPINN&#12290;&#22312;SeqPINN&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24179;&#22343;&#24658;&#23450;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20316;&#20026;&#21021;&#22987;&#21270;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#35757;&#32451;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#25139;&#12290;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#27867;&#21270;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Neural Network (PINN) is one of the most preeminent solvers of Navier-Stokes equations, which are widely used as the governing equation of blood flow. However, current approaches, relying on full Navier-Stokes equations, are impractical for ultrafast Doppler ultrasound, the state-of-the-art technique for depiction of complex blood flow dynamics \emph{in vivo} through acquired thousands of frames (or, timestamps) per second. In this article, we first propose a novel training framework of PINN for solving Navier-Stokes equations by discretizing Navier-Stokes equations into steady state and sequentially solving steady-state Navier-Stokes equations with transfer learning. The novel training framework is coined as SeqPINN. Upon the success of SeqPINN, we adopt the idea of averaged constant stochastic gradient descent (SGD) as initialization and propose a parallel training scheme for all timestamps. To ensure an initialization that generalizes well, we borrow the concept of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#22810;&#26102;&#31354;&#32593;&#32476;&#27169;&#22411;(MHSTN)&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20135;&#29983;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04733</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction. (arXiv:2309.04733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#22810;&#26102;&#31354;&#32593;&#32476;&#27169;&#22411;(MHSTN)&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20135;&#29983;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#36895;&#21644;&#39118;&#21521;&#30340;&#39044;&#27979;&#23545;&#20110;&#33322;&#31354;&#21644;&#39118;&#33021;&#21457;&#30005;&#31561;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#30001;&#20110;&#22825;&#27668;&#25968;&#25454;&#20013;&#30340;&#39640;&#38543;&#26426;&#24615;&#21644;&#22797;&#26434;&#30456;&#20851;&#24615;&#65292;&#36825;&#19968;&#39044;&#27979;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#19968;&#37096;&#20998;&#24433;&#21709;&#22240;&#32032;&#65292;&#22240;&#27492;&#32570;&#20047;&#23545;&#38382;&#39064;&#30340;&#31995;&#32479;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;&#32454;&#31890;&#24230;&#39044;&#27979;&#30340;&#20851;&#27880;&#36739;&#23569;&#65292;&#32780;&#32454;&#31890;&#24230;&#39044;&#27979;&#23545;&#20110;&#39640;&#25928;&#30340;&#34892;&#19994;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#21363;&#22810;&#26102;&#31354;&#39118;&#21147;&#32593;&#32476;(MHSTN)&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#32454;&#31890;&#24230;&#39118;&#21147;&#39044;&#27979;&#12290;MHSTN&#23558;&#38024;&#23545;&#19981;&#21516;&#22240;&#32032;&#30340;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;(Seq2Seq)&#39592;&#26550;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#21508;&#31181;&#25968;&#25454;&#28304;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20026;&#32473;&#23450;&#21306;&#22495;&#20869;&#30340;&#25152;&#26377;&#31449;&#28857;&#20135;&#29983;&#22810;&#26102;&#31354;&#30340;&#39044;&#27979;&#12290;MHSTN&#30001;&#22235;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, Multi-Horizon SpatioTemporal Network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal
&lt;/p&gt;</description></item><item><title>TCGAN&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#25239;&#21338;&#24328;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#26080;&#38656;&#26631;&#35760;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.04732</link><description>&lt;p&gt;
TCGAN: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering. (arXiv:2309.04732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04732
&lt;/p&gt;
&lt;p&gt;
TCGAN&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#25239;&#21338;&#24328;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#26080;&#38656;&#26631;&#35760;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#23618;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#21487;&#29992;&#20110;&#25104;&#21151;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#31283;&#23450;&#23398;&#20064;&#65292;&#20294;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20063;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#22686;&#24378;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GANs&#22914;&#20309;&#26377;&#25928;&#22320;&#20316;&#20026;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#65288;&#21363;&#20998;&#31867;&#21644;&#32858;&#31867;&#65289;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19978;&#36848;&#32771;&#34385;&#28608;&#21457;&#20102;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#21367;&#31215;GAN&#65288;TCGAN&#65289;&#12290;TCGAN&#36890;&#36807;&#22312;&#27809;&#26377;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20004;&#20010;&#19968;&#32500;CNN&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#65289;&#20043;&#38388;&#36827;&#34892;&#23545;&#25239;&#21338;&#24328;&#26469;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#35757;&#32451;&#30340;TCGAN&#30340;&#19968;&#37096;&#20998;&#34987;&#37325;&#22797;&#21033;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the superiority of supervised Convolutional Neural Networks (CNNs) in learning hierarchical representations from time series data for successful classification. These methods require sufficiently large labeled data for stable learning, however acquiring high-quality labeled time series data can be costly and potentially infeasible. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. Nonetheless, to our best knowledge, it remains unclear how effectively GANs can serve as a general-purpose solution to learn representations for time series recognition, i.e., classification and clustering. The above considerations inspire us to introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a rep
&lt;/p&gt;</description></item><item><title>&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#23545;&#20381;&#36182;&#20110;&#21442;&#25968;&#20197;&#21450;&#36755;&#20837;&#37325;&#22797;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.04728</link><description>&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#30340;&#36716;&#21464;&#21450;&#23545;&#36755;&#20837;&#37325;&#22797;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Transitions in echo index and dependence on input repetitions. (arXiv:2309.04728v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04728
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#23545;&#20381;&#36182;&#20110;&#21442;&#25968;&#20197;&#21450;&#36755;&#20837;&#37325;&#22797;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#65288;&#21363;&#21463;&#36755;&#20837;&#39537;&#21160;&#65289;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#12290;&#23427;&#25512;&#24191;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#22768;&#29366;&#24577;&#24615;&#36136;&#65292;&#36825;&#23545;&#24212;&#20110;&#22238;&#22768;&#25351;&#25968;&#31561;&#20110;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#22914;&#20309;&#20381;&#36182;&#20110;&#25511;&#21046;&#31995;&#32479;&#23545;&#24378;&#36843;&#21160;&#21147;&#23398;&#30340;&#26377;&#38480;&#29366;&#24577;&#38543;&#26426;&#22806;&#37096;&#36755;&#20837;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#26377;&#38480;&#19968;&#32452;&#26144;&#23556;&#20043;&#38388;&#20999;&#25442;&#30340;&#38750;&#33258;&#27835;&#31995;&#32479;&#30340;&#22238;&#22768;&#25351;&#25968;&#65292;&#20854;&#20013;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#26144;&#23556;&#20855;&#26377;&#26377;&#38480;&#19968;&#32452;&#21452;&#26354;&#22411;&#24179;&#34913;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#27599;&#20010;&#26144;&#23556;&#30340;&#26368;&#23567;&#21644;&#26368;&#22823;&#37325;&#22797;&#23545;&#20110;&#24471;&#21040;&#30340;&#22238;&#22768;&#25351;&#25968;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#29992;RNN&#35745;&#31639;&#26694;&#26550;&#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#23545;&#20110;&#23567;&#24133;&#24378;&#36843;&#65292;&#22238;&#22768;&#25351;&#25968;&#23545;&#24212;&#20110;&#26080;&#36755;&#20837;&#31995;&#32479;&#30340;&#21560;&#24341;&#23376;&#25968;&#37327;&#65292;&#32780;&#23545;&#20110;&#22823;&#24133;&#24378;&#36843;&#65292;&#22238;&#22768;&#25351;&#25968;&#20943;&#23569;&#21040;&#19968;&#12290;&#20013;&#38388;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
The echo index counts the number of simultaneously stable asymptotic responses of a nonautonomous (i.e. input-driven) dynamical system. It generalizes the well-known echo state property for recurrent neural networks this corresponds to the echo index being equal to one. In this paper, we investigate how the echo index depends on parameters that govern typical responses to a finite-state ergodic external input that forces the dynamics. We consider the echo index for a nonautonomous system that switches between a finite set of maps, where we assume that each map possesses a finite set of hyperbolic equilibrium attractors. We find the minimum and maximum repetitions of each map are crucial for the resulting echo index. Casting our theoretical findings in the RNN computing framework, we obtain that for small amplitude forcing the echo index corresponds to the number of attractors for the input-free system, while for large amplitude forcing, the echo index reduces to one. The intermediate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04716</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#65292;&#37325;&#29616;&#30740;&#31350;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26368;&#20339;&#23454;&#36341;&#36890;&#24120;&#26377;&#19977;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23547;&#25214;&#20844;&#24320;&#21487;&#29992;&#30340;&#21407;&#22411;&#65307;&#65288;2&#65289;&#32852;&#31995;&#20316;&#32773;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#25163;&#21160;&#23454;&#29616;&#21407;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#32593;&#32476;&#30740;&#31350;&#27809;&#26377;&#20844;&#24320;&#21407;&#22411;&#65292;&#32780;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#20063;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22823;&#37096;&#20998;&#37325;&#29616;&#24037;&#20316;&#37117;&#33457;&#36153;&#22312;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#36827;&#34892;&#25163;&#21160;&#23454;&#29616;&#19978;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#36153;&#21147;&#65292;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#25552;&#20986;&#20351;&#29992;&#26032;&#20852;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#22235;&#21517;&#20855;&#22791;&#24517;&#35201;&#32593;&#32476;&#30693;&#35782;&#30340;&#23398;&#29983;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#30340;&#37325;&#29616;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT
&lt;/p&gt;</description></item><item><title>Jade&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#20851;&#33410;&#21018;&#20307;&#29289;&#29702;&#24341;&#25806;&#65292;&#20855;&#26377;&#26080;&#37325;&#21472;&#25705;&#25830;&#25509;&#35302;&#30340;&#29305;&#28857;&#12290;&#23427;&#37319;&#29992;&#32447;&#24615;&#20114;&#34917;&#38382;&#39064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26080;&#37325;&#21472;&#30896;&#25758;&#27169;&#25311;&#21644;&#31283;&#23450;&#30340;&#22810;&#20010;&#25705;&#25830;&#25509;&#35302;LCP&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36824;&#20351;&#29992;&#36830;&#32493;&#30896;&#25758;&#26816;&#27979;&#21644;&#22238;&#28335;&#31574;&#30053;&#26469;&#38450;&#27490;&#29289;&#20307;&#37325;&#21472;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#35745;&#31639;&#21644;&#20462;&#25913;Dantzig&#31639;&#27861;&#26469;&#20445;&#35777;&#25972;&#20010;&#27169;&#25311;&#36807;&#31243;&#30340;&#21487;&#24494;&#20998;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Jade&#22312;&#25509;&#35302;&#20016;&#23500;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04710</link><description>&lt;p&gt;
Jade:&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#20851;&#33410;&#21018;&#20307;&#29289;&#29702;&#24341;&#25806;&#65292;&#20855;&#26377;&#26080;&#37325;&#21472;&#25705;&#25830;&#25509;&#35302;.
&lt;/p&gt;
&lt;p&gt;
Jade: A Differentiable Physics Engine for Articulated Rigid Bodies with Intersection-Free Frictional Contact. (arXiv:2309.04710v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04710
&lt;/p&gt;
&lt;p&gt;
Jade&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#20851;&#33410;&#21018;&#20307;&#29289;&#29702;&#24341;&#25806;&#65292;&#20855;&#26377;&#26080;&#37325;&#21472;&#25705;&#25830;&#25509;&#35302;&#30340;&#29305;&#28857;&#12290;&#23427;&#37319;&#29992;&#32447;&#24615;&#20114;&#34917;&#38382;&#39064;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26080;&#37325;&#21472;&#30896;&#25758;&#27169;&#25311;&#21644;&#31283;&#23450;&#30340;&#22810;&#20010;&#25705;&#25830;&#25509;&#35302;LCP&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36824;&#20351;&#29992;&#36830;&#32493;&#30896;&#25758;&#26816;&#27979;&#21644;&#22238;&#28335;&#31574;&#30053;&#26469;&#38450;&#27490;&#29289;&#20307;&#37325;&#21472;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#35745;&#31639;&#21644;&#20462;&#25913;Dantzig&#31639;&#27861;&#26469;&#20445;&#35777;&#25972;&#20010;&#27169;&#25311;&#36807;&#31243;&#30340;&#21487;&#24494;&#20998;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;Jade&#22312;&#25509;&#35302;&#20016;&#23500;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jade&#65292;&#19968;&#31181;&#29992;&#20110;&#20851;&#33410;&#21018;&#20307;&#30340;&#21487;&#24494;&#20998;&#29289;&#29702;&#24341;&#25806;&#12290;Jade&#23558;&#25509;&#35302;&#24314;&#27169;&#20026;&#32447;&#24615;&#20114;&#34917;&#38382;&#39064;&#65288;LCP&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#30456;&#27604;&#65292;Jade&#20855;&#26377;&#26080;&#37325;&#21472;&#30896;&#25758;&#27169;&#25311;&#21644;&#22810;&#20010;&#25705;&#25830;&#25509;&#35302;&#30340;&#31283;&#23450;LCP&#35299;&#20915;&#26041;&#26696;&#31561;&#29305;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#30896;&#25758;&#26816;&#27979;&#26469;&#26816;&#27979;&#30896;&#25758;&#26102;&#38388;&#65292;&#24182;&#37319;&#29992;&#22238;&#28335;&#31574;&#30053;&#26469;&#38450;&#27490;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26799;&#24230;&#35745;&#31639;&#65292;&#20197;&#30830;&#20445;&#25972;&#20010;&#27169;&#25311;&#36807;&#31243;&#22312;&#22238;&#28335;&#26426;&#21046;&#19979;&#26159;&#21487;&#24494;&#20998;&#30340;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#24120;&#35265;&#30340;Dantzig&#31639;&#27861;&#65292;&#20197;&#33719;&#24471;&#22312;&#22810;&#20010;&#25705;&#25830;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22312;&#21508;&#31181;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Jade, a differentiable physics engine for articulated rigid bodies. Jade models contacts as the Linear Complementarity Problem (LCP). Compared to existing differentiable simulations, Jade offers features including intersection-free collision simulation and stable LCP solutions for multiple frictional contacts. We use continuous collision detection to detect the time of impact and adopt the backtracking strategy to prevent intersection between bodies with complex geometry shapes. We derive the gradient calculation to ensure the whole simulation process is differentiable under the backtracking mechanism. We modify the popular Dantzig algorithm to get valid solutions under multiple frictional contacts. We conduct extensive experiments to demonstrate the effectiveness of our differentiable physics simulation over a variety of contact-rich tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;</title><link>http://arxiv.org/abs/2309.04707</link><description>&lt;p&gt;
&#20174;&#25506;&#32034;&#24615;&#35270;&#35282;&#35299;&#37322;&#20195;&#29702;&#34892;&#20026;&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;
&lt;/p&gt;
&lt;p&gt;
Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective. (arXiv:2309.04707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#30340;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#22312;&#20915;&#31574;&#20855;&#26377;&#23454;&#38469;&#21518;&#26524;&#30340;&#39046;&#22495;&#20013;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#21592;&#30340;RL&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;A2CR&#30001;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#32593;&#32476;&#32452;&#25104;&#65306;&#31574;&#30053;&#32593;&#32476;&#65292;&#20215;&#20540;&#32593;&#32476;&#21644;Reasoner&#32593;&#32476;&#12290;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;A2CR&#33258;&#21160;&#29983;&#25104;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;&#23427;&#25552;&#20379;&#20102;&#35832;&#22914;&#22522;&#20110;&#30446;&#30340;&#30340;&#26174;&#33879;&#24615;&#12289;&#26089;&#26399;&#22833;&#36133;&#26816;&#27979;&#21644;&#27169;&#22411;&#30417;&#31649;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;RL&#12290;&#22312;&#21160;&#20316;&#20016;&#23500;&#30340;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;Reasoner-&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage Actor-Critic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor's actions, A2CR automatically generates a more comprehensive and interpretable paradigm for understanding the agent's decision-making process. It offers a range of functionalities such as purpose-based saliency, early failure detection, and model supervision, thereby promoting responsible and trustworthy RL. Evaluations conducted in action-rich Super Mario Bros environments yield intriguing findings: Reasoner-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.04704</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;LLM&#65288;Llama 2&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36890;&#36807;&#32454;&#35843;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;PEFT/LoRA&#30340;&#32454;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#20013;&#65292;&#35813;&#27169;&#22411;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#32454;&#35843;&#65306;&#25581;&#31034;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#21465;&#20107;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#20107;&#23454;&#26680;&#26597;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#25805;&#32437;&#20998;&#26512;&#20197;&#21450;&#25552;&#21462;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#32454;&#35843;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#23545;&#25991;&#26412;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#12290;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#32930;&#22806;&#39592;&#39612;&#30340;&#21069;&#39304;&#25511;&#21046;&#31995;&#32479;&#65292;&#23454;&#29616;&#20027;&#21160;&#37325;&#21147;&#34917;&#20607;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20869;&#37096;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#25968;&#25454;&#35745;&#31639;&#21147;&#30697;&#65292;&#24182;&#37319;&#29992;&#20998;&#26512;&#25511;&#21046;&#26041;&#31243;&#12290;&#19982;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#30456;&#27604;&#65292;&#21069;&#39304;&#25511;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#30828;&#20214;&#22797;&#26434;&#24615;&#21644;&#26356;&#31215;&#26497;&#30340;&#21709;&#24212;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#20102;&#25705;&#25830;&#21644;&#19981;&#24076;&#26395;&#30340;&#20559;&#36716;&#12290;</title><link>http://arxiv.org/abs/2309.04698</link><description>&lt;p&gt;
&#19978;&#32930;&#22806;&#39592;&#39612;&#30340;&#36827;&#23637;&#65306;&#20351;&#29992;&#21069;&#39304;&#25511;&#21046;&#22120;&#23454;&#29616;&#20027;&#21160;&#37325;&#21147;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Advancements in Upper Body Exoskeleton: Implementing Active Gravity Compensation with a Feedforward Controller. (arXiv:2309.04698v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#32930;&#22806;&#39592;&#39612;&#30340;&#21069;&#39304;&#25511;&#21046;&#31995;&#32479;&#65292;&#23454;&#29616;&#20027;&#21160;&#37325;&#21147;&#34917;&#20607;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20869;&#37096;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#25968;&#25454;&#35745;&#31639;&#21147;&#30697;&#65292;&#24182;&#37319;&#29992;&#20998;&#26512;&#25511;&#21046;&#26041;&#31243;&#12290;&#19982;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#30456;&#27604;&#65292;&#21069;&#39304;&#25511;&#21046;&#20855;&#26377;&#26356;&#20302;&#30340;&#30828;&#20214;&#22797;&#26434;&#24615;&#21644;&#26356;&#31215;&#26497;&#30340;&#21709;&#24212;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#20102;&#25705;&#25830;&#21644;&#19981;&#24076;&#26395;&#30340;&#20559;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#19978;&#32930;&#22806;&#39592;&#39612;&#30340;&#20027;&#21160;&#37325;&#21147;&#34917;&#20607;&#30340;&#21069;&#39304;&#25511;&#21046;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20165;&#21033;&#29992;&#26469;&#33258;&#20869;&#37096;&#30005;&#26426;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;&#25968;&#25454;&#26469;&#35745;&#31639;&#21147;&#30697;&#65292;&#37319;&#29992;&#22522;&#20110;&#29275;&#39039;-&#27431;&#25289;&#21453;&#21521;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#25511;&#21046;&#26041;&#31243;&#12290;&#19982;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#30456;&#27604;&#65292;&#21069;&#39304;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#23427;&#28040;&#38500;&#20102;&#23545;&#22806;&#37096;&#21147;&#30697;&#20256;&#24863;&#22120;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30828;&#20214;&#22797;&#26434;&#24615;&#21644;&#37325;&#37327;&#12290;&#27492;&#22806;&#65292;&#21069;&#39304;&#25511;&#21046;&#20855;&#26377;&#26356;&#31215;&#26497;&#30340;&#21709;&#24212;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#22806;&#39592;&#39612;&#36731;&#24039;&#65292;&#30001;4&#20010;&#33258;&#30001;&#24230;&#32452;&#25104;&#65292;&#23494;&#20999;&#27169;&#25311;&#20102;&#20154;&#20307;&#19978;&#32930;&#36816;&#21160;&#23398;&#21644;&#19977;&#32500;&#27963;&#21160;&#33539;&#22260;&#12290;&#25105;&#20204;&#23545;&#22806;&#39592;&#39612;&#36827;&#34892;&#20102;&#30828;&#20214;&#21644;&#27169;&#25311;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#33021;&#12290;&#31995;&#32479;&#22312;&#24310;&#38271;&#30340;&#26102;&#38388;&#20869;&#20445;&#25345;&#20301;&#32622;&#31283;&#23450;&#65292;&#34920;&#29616;&#20986;&#26368;&#23569;&#30340;&#25705;&#25830;&#21644;&#36991;&#20813;&#19981;&#24076;&#26395;&#30340;&#20559;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a feedforward control system designed for active gravity compensation on an upper body exoskeleton. The system utilizes only positional data from internal motor sensors to calculate torque, employing analytical control equations based on Newton-Euler Inverse Dynamics. Compared to feedback control systems, the feedforward approach offers several advantages. It eliminates the need for external torque sensors, resulting in reduced hardware complexity and weight. Moreover, the feedforward control exhibits a more proactive response, leading to enhanced performance. The exoskeleton used in the experiments is lightweight and comprises 4 Degrees of Freedom, closely mimicking human upper body kinematics and three-dimensional range of motion. We conducted tests on both hardware and simulations of the exoskeleton, demonstrating stable performance. The system maintained its position over an extended period, exhibiting minimal friction and avoiding undesired slewing.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04695</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#20197;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Code-Style In-Context Learning for Knowledge-Based Question Answering. (arXiv:2309.04695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32534;&#31243;&#39118;&#26684;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#20013;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38024;&#23545;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;(KBQA)&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#21644;&#27169;&#22411;&#26694;&#26550;&#65292;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#30340;&#20986;&#29616;&#20026;KBQA&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26080;&#38656;&#35757;&#32451;&#30340;&#35821;&#20041;&#35299;&#26512;&#33539;&#24335;&#65306;&#32473;&#23450;&#23569;&#37327;&#38382;&#39064;&#21450;&#20854;&#26631;&#35760;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#20219;&#21153;&#24847;&#22270;&#24182;&#20026;&#26032;&#38382;&#39064;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24378;&#22823;&#30340;LLMs&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#20102;&#35299;&#24456;&#23569;&#65292;&#23548;&#33268;&#26684;&#24335;&#38169;&#35823;&#29575;&#36739;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;KBQA&#30340;&#20195;&#30721;&#39118;&#26684;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#38476;&#29983;&#36923;&#36753;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#26356;&#20026;&#29087;&#24713;&#30340;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#12290;&#23545;&#19977;&#20010;&#20027;&#27969;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#29983;&#25104;&#36923;&#36753;&#34920;&#36798;&#24335;&#20013;&#30340;&#26684;&#24335;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for Knowledge-Based Question Answering (KBQA) usually rely on complex training techniques and model frameworks, leading to many limitations in practical applications. Recently, the emergence of In-Context Learning (ICL) capabilities in Large Language Models (LLMs) provides a simple and training-free semantic parsing paradigm for KBQA: Given a small number of questions and their labeled logical forms as demo examples, LLMs can understand the task intent and generate the logic form for a new question. However, current powerful LLMs have little exposure to logic forms during pre-training, resulting in a high format error rate. To solve this problem, we propose a code-style in-context learning method for KBQA, which converts the generation process of unfamiliar logical form into the more familiar code generation process for LLMs. Experimental results on three mainstream datasets show that our method dramatically mitigated the formatting error problem in generating logic for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEMSP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38480;&#21046;&#24322;&#24120;&#29305;&#24449;&#30340;&#21464;&#21270;&#20540;&#26469;&#25552;&#20379;&#28789;&#27963;&#19988;&#20581;&#22766;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.04676</link><description>&lt;p&gt;
&#28789;&#27963;&#32780;&#20581;&#22766;&#30340;&#20855;&#26377;&#26368;&#23567;&#28385;&#36275;&#25200;&#21160;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations. (arXiv:2309.04676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEMSP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38480;&#21046;&#24322;&#24120;&#29305;&#24449;&#30340;&#21464;&#21270;&#20540;&#26469;&#25552;&#20379;&#28789;&#27963;&#19988;&#20581;&#22766;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#20462;&#25913;&#29305;&#24449;&#21521;&#37327;&#26469;&#23454;&#29616;&#19981;&#21516;&#23454;&#20363;&#30340;&#39044;&#27979;&#12290;CFEs&#21487;&#20197;&#22686;&#24378;&#20449;&#24687;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20026;&#25910;&#21040;&#19981;&#21033;&#39044;&#27979;&#30340;&#29992;&#25143;&#25552;&#20379;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#30456;&#21516;&#23454;&#20363;&#25110;&#31245;&#26377;&#24046;&#24322;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#25552;&#20379;&#22810;&#20010;CFEs&#12290;&#22810;&#20010;CFEs&#20026;&#29992;&#25143;&#36873;&#25321;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36820;&#22238;&#19981;&#31283;&#23450;&#30340;CFEs&#19988;&#25104;&#26412;&#19981;&#21516;&#65292;&#23558;&#20250;&#25439;&#23475;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#28789;&#27963;&#24615;&#24182;&#35299;&#20915;&#38750;&#20581;&#22766;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21629;&#21517;&#20026;&#20855;&#26377;&#26368;&#23567;&#28385;&#36275;&#25200;&#21160;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#65288;CEMSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations (CFEs) exemplify how to minimally modify a feature vector to achieve a different prediction for an instance. CFEs can enhance informational fairness and trustworthiness, and provide suggestions for users who receive adverse predictions. However, recent research has shown that multiple CFEs can be offered for the same instance or instances with slight differences. Multiple CFEs provide flexible choices and cover diverse desiderata for user selection. However, individual fairness and model reliability will be damaged if unstable CFEs with different costs are returned. Existing methods fail to exploit flexibility and address the concerns of non-robustness simultaneously. To address these issues, we propose a conceptually simple yet effective solution named Counterfactual Explanations with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains changing values of abnormal features with the help of their semantically meaningful normal ranges. Fo
&lt;/p&gt;</description></item><item><title>FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04663</link><description>&lt;p&gt;
FIAT: &#23558;&#23398;&#20064;&#33539;&#24335;&#19982;&#25351;&#20196;&#21152;&#36895;&#35843;&#20248;&#30456;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning. (arXiv:2309.04663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04663
&lt;/p&gt;
&lt;p&gt;
FIAT&#26159;&#19968;&#31181;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#34701;&#21512;&#30340;&#26032;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#36827;&#34892;&#25351;&#20196;&#21644;&#25512;&#29702;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#27169;&#22411;&#19978;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#65292;&#32463;&#36807;&#22810;&#35821;&#35328;&#20219;&#21153;&#27979;&#35797;&#65292;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23398;&#20064;&#33539;&#24335;&#36890;&#24120;&#20998;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#12290;&#27599;&#31181;&#33539;&#24335;&#37117;&#26377;&#20854;&#33258;&#36523;&#30340;&#21462;&#33293;&#65292;&#36825;&#21462;&#20915;&#20110;&#21487;&#29992;&#25968;&#25454;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35745;&#31639;&#25104;&#26412;&#12289;&#26131;&#29992;&#24615;&#21644;&#26368;&#32456;&#36136;&#37327;&#65292;&#20294;&#26080;&#27861;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20197;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#33258;&#28982;&#32852;&#31995;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;ICL&#21644;&#24494;&#35843;&#33539;&#24335;&#12290;&#22522;&#20110;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIAT&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#36825;&#20123;&#33539;&#24335;&#30340;&#20248;&#28857;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#24471;&#22312;&#26368;&#22823;&#27169;&#22411;&#19978;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#24037;&#31243;&#25351;&#20196;&#21644;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#25928;&#29575;&#35843;&#20248;&#30340;&#36739;&#23567;&#27169;&#22411;&#19978;&#20351;&#29992;&#31867;&#20284;&#30340;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;FIAT&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;FIAT&#22312;100-10,000&#20010;&#35757;&#32451;&#26679;&#26412;&#35268;&#27169;&#19979;&#22343;&#27604;ICL&#21644;&#24494;&#35843;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24076;&#26395;FIAT&#33021;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#20110;3D&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.04651</link><description>&lt;p&gt;
&#35270;&#39057;&#21644;&#21512;&#25104;&#30913;&#20849;&#25391;&#25104;&#20687;&#39044;&#35757;&#32451;&#30340;3D&#35270;&#35273;&#26550;&#26500;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Video and Synthetic MRI Pre-training of 3D Vision Architectures for Neuroimage Analysis. (arXiv:2309.04651v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#23545;&#20110;3D&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#22312;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#20195;&#34920;&#20102;&#25105;&#20204;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#19968;&#31181;&#26368;&#26032;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#36716;&#31227;&#23398;&#20064;&#28041;&#21450;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#26368;&#23567;&#38480;&#24230;&#22320;&#24494;&#35843;&#23427;&#20204;&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;3D&#21307;&#23398;&#24433;&#20687;&#20219;&#21153;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26159;&#22312;&#33258;&#28982;&#22270;&#20687;&#12289;&#21307;&#23398;&#22270;&#20687;&#36824;&#26159;&#21512;&#25104;&#30340;MRI&#25195;&#25551;&#25110;&#35270;&#39057;&#25968;&#25454;&#19978;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#19978;&#28216;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViTs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#19977;&#20010;&#29420;&#29305;&#30340;&#19979;&#28216;&#31070;&#32463;&#24433;&#20687;&#20219;&#21153;&#65292;&#38590;&#26131;&#31243;&#24230;&#21508;&#24322;&#65306;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#21644;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#20998;&#31867;&#65292;&#8220;&#33041;&#40836;&#8221;&#39044;&#27979;&#12290;&#23454;&#39564;&#27979;&#35797;&#24471;&#20986;&#20197;&#19979;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#65306;1.&#39044;&#35757;&#32451;&#25913;&#21892;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#19968;&#20010;&#25552;&#21319;.
&lt;/p&gt;
&lt;p&gt;
Transfer learning represents a recent paradigm shift in the way we build artificial intelligence (AI) systems. In contrast to training task-specific models, transfer learning involves pre-training deep learning models on a large corpus of data and minimally fine-tuning them for adaptation to specific tasks. Even so, for 3D medical imaging tasks, we do not know if it is best to pre-train models on natural images, medical images, or even synthetically generated MRI scans or video data. To evaluate these alternatives, here we benchmarked vision transformers (ViTs) and convolutional neural networks (CNNs), initialized with varied upstream pre-training approaches. These methods were then adapted to three unique downstream neuroimaging tasks with a range of difficulty: Alzheimer's disease (AD) and Parkinson's disease (PD) classification, "brain age" prediction. Experimental tests led to the following key observations: 1. Pre-training improved performance across all tasks including a boost of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#21335;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#31561;&#24320;&#28304;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35757;&#32451;&#20102;&#22235;&#20010;&#27169;&#22411;&#65292;&#27492;&#20026;&#36234;&#21335;&#35821;&#30340;&#39318;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.04646</link><description>&lt;p&gt;
&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#36234;&#21335;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Finetuning Large Language Models For Vietnamese Chatbot. (arXiv:2309.04646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36234;&#21335;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#31561;&#24320;&#28304;&#39033;&#30446;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#25104;&#21151;&#35757;&#32451;&#20102;&#22235;&#20010;&#27169;&#22411;&#65292;&#27492;&#20026;&#36234;&#21335;&#35821;&#30340;&#39318;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#12289;PaLM&#21644;LLaMa&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#36817;&#30340;&#25351;&#20196;&#35843;&#20248;&#36827;&#23637;&#20351;&#24471;LLMs&#33021;&#22815;&#25353;&#29031;&#29992;&#25143;&#25351;&#20196;&#24182;&#20135;&#29983;&#31867;&#20284;&#20154;&#31867;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#23454;&#29616;LLMs&#25152;&#38656;&#30340;&#39640;&#25104;&#26412;&#23545;&#23398;&#26415;&#30740;&#31350;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#36234;&#21335;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;LLMs&#21644;&#25351;&#20196;&#35843;&#35856;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24320;&#28304;&#39033;&#30446;&#65288;Alpaca&#12289;GPT4All&#21644;Chat-Doctor&#65289;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#36890;&#29992;&#21644;&#29305;&#23450;&#30340;&#21307;&#23398;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22312;&#20004;&#20010;&#24320;&#25918;&#30340;LLMs&#19978;&#65306;Bloomz&#65288;&#22810;&#35821;&#35328;&#65289;&#21644;GPTJ-6B&#65288;&#36234;&#21335;&#35821;&#65289;&#65292;&#24471;&#21040;&#22235;&#20010;&#27169;&#22411;&#65306;Bloomz-Chat&#65292;Blo
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-4, PaLM, and LLaMa, have been shown to achieve remarkable performance across a variety of natural language tasks. Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses. However, the high costs associated with training and implementing LLMs pose challenges to academic research. Furthermore, the availability of pretrained LLMs and instruction-tune datasets for Vietnamese language is limited. To tackle these concerns, we leverage large-scale instruction-following datasets from open-source projects, namely Alpaca, GPT4All, and Chat-Doctor, which cover general domain and specific medical domain. To the best of our knowledge, these are the first instructional dataset for Vietnamese. Subsequently, we utilize parameter-efficient tuning through Low-Rank Adaptation (LoRA) on two open LLMs: Bloomz (Multilingual) and GPTJ-6B (Vietnamese), resulting four models: Bloomz-Chat, Blo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#23569;&#37327;&#31034;&#33539;&#20013;&#23398;&#20064;&#21147;&#22522;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35302;&#35273;&#34920;&#31034;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#26469;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04640</link><description>&lt;p&gt;
&#20174;&#31034;&#33539;&#20013;&#39044;&#35757;&#32451;&#35302;&#35273;&#34920;&#31034;&#65292;&#23454;&#29616;&#21147;&#22522;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation. (arXiv:2309.04640v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#23569;&#37327;&#31034;&#33539;&#20013;&#23398;&#20064;&#21147;&#22522;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35302;&#35273;&#34920;&#31034;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#26469;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#20013;&#65292;&#21147;&#20256;&#24863;&#22312;&#36866;&#24212;&#25805;&#32437;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#30340;&#25805;&#32437;&#20219;&#21153;&#27867;&#21270;&#21040;&#26410;&#30693;&#23545;&#35937;&#25152;&#38656;&#30340;&#23545;&#35937;&#23646;&#24615;&#30340;&#22522;&#26412;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#31034;&#33539;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#26114;&#36149;&#30340;&#20154;&#31867;&#31034;&#33539;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21322;&#30417;&#30563;&#31034;&#33539;&#23398;&#20064;&#26041;&#27861;&#23558;&#23398;&#20064;&#27169;&#22411;&#20998;&#35299;&#20026;&#35302;&#35273;&#34920;&#31034;&#32534;&#30721;&#22120;&#21644;&#21160;&#20316;&#29983;&#25104;&#35299;&#30721;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#39044;&#20808;&#35757;&#32451;&#35302;&#35273;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#33539;&#23398;&#20064;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#35299;&#30721;&#22120;&#65292;&#20174;&#20154;&#31867;&#23398;&#20064;&#25216;&#33021;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#19981;&#21516;&#21018;&#24230;&#21644;&#34920;&#38754;&#25705;&#25830;&#30340;&#28023;&#32501;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#25830;&#25325;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#31034;&#33539;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#25830;&#25325;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wipin
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#37319;&#29992;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#24212;&#29992;&#20110;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04626</link><description>&lt;p&gt;
&#20302;&#31209;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#21644;&#21453;&#21521;&#27979;&#37327;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning. (arXiv:2309.04626v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04626
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#37319;&#29992;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#24212;&#29992;&#20110;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#31216;&#20026;&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#12290;PAQ&#37319;&#29992;&#20102;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20449;&#24687;&#37327;&#21448;&#36731;&#37327;&#32423;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#23637;&#31034;&#22312;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#39640;&#32500;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#65292;&#26080;&#27861;&#24212;&#29992;&#26631;&#20934;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20174;PAQ&#20013;&#23398;&#20064;&#24230;&#37327;&#30340;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#23637;&#31034;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04615</link><description>&lt;p&gt;
&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#20998;&#35299;&#22312;&#22522;&#20110;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Value Decomposition Framework with Disentangled World Model&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#21516;&#30446;&#26631;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26080;&#27169;&#22411;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21253;&#25324;&#21160;&#20316;&#26465;&#20214;&#12289;&#26080;&#21160;&#20316;&#21644;&#38745;&#24577;&#20998;&#25903;&#65292;&#26469;&#35299;&#24320;&#29615;&#22659;&#21160;&#24577;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20135;&#29983;&#24819;&#35937;&#20013;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#30495;&#23454;&#29615;&#22659;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#23558;&#20854;&#19982;&#22522;&#20110;&#20540;&#30340;&#26694;&#26550;&#21512;&#24182;&#65292;&#20197;&#39044;&#27979;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#25972;&#20307;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#26469;&#38142;&#25509;&#19981;&#21516;&#30340;&#30151;&#29366;&#28165;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#20013;&#39044;&#27979;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.04607</link><description>&lt;p&gt;
&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#38142;&#25509;&#30151;&#29366;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Linking Symptom Inventories using Semantic Textual Similarity. (arXiv:2309.04607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04607
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#26469;&#38142;&#25509;&#19981;&#21516;&#30340;&#30151;&#29366;&#28165;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#20013;&#39044;&#27979;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#30340;&#30151;&#29366;&#28165;&#21333;&#26469;&#34913;&#37327;&#20020;&#24202;&#30151;&#29366;&#65292;&#20294;&#36825;&#31181;&#22810;&#26679;&#24615;&#23548;&#33268;&#20102;&#20960;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#29615;&#22659;&#21644;&#30740;&#31350;&#30340;&#32467;&#26524;&#19981;&#21487;&#27604;&#36739;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;Semantic Textual Similarity&#65292;STS&#65289;&#26469;&#38142;&#25509;&#20808;&#21069;&#19981;&#30456;&#23481;&#30340;&#30151;&#29366;&#28165;&#21333;&#20013;&#30340;&#30151;&#29366;&#21644;&#35780;&#20998;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22235;&#20010;&#39044;&#35757;&#32451;&#30340;STS&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23545;&#26469;&#33258;16&#20010;&#22269;&#38469;&#25968;&#25454;&#28304;&#30340;6,607&#21517;&#21442;&#19982;&#32773;&#30340;&#22235;&#20010;&#19981;&#21516;&#28165;&#21333;&#20013;&#30340;&#25968;&#21315;&#20010;&#30151;&#29366;&#25551;&#36848;&#23545;&#36827;&#34892;&#30456;&#20851;&#20869;&#23481;&#30340;&#31579;&#26597; - &#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#38656;&#35201;&#19987;&#23478;&#23567;&#32452;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#20845;&#39033;&#20219;&#21153;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#28165;&#21333;&#20013;&#30340;&#30151;&#29366;&#20005;&#37325;&#31243;&#24230;&#12290;STS&#26041;&#27861;&#22312;&#20116;&#20010;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;74.8%&#30340;&#20934;&#30830;&#29575;&#65292;&#32988;&#36807;&#20102;&#20854;&#20182;&#34987;&#27979;&#35797;&#30340;&#27169;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#32467;&#21512;&#35821;&#22659;&#21644;&#35821;&#20041;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#19987;&#23478;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#20135;&#29983;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An extensive library of symptom inventories has been developed over time to measure clinical symptoms, but this variety has led to several long standing issues. Most notably, results drawn from different settings and studies are not comparable, which limits reproducibility. Here, we present an artificial intelligence (AI) approach using semantic textual similarity (STS) to link symptoms and scores across previously incongruous symptom inventories. We tested the ability of four pre-trained STS models to screen thousands of symptom description pairs for related content - a challenging task typically requiring expert panels. Models were tasked to predict symptom severity across four different inventories for 6,607 participants drawn from 16 international data sources. The STS approach achieved 74.8% accuracy across five tasks, outperforming other models tested. This work suggests that incorporating contextual, semantic information can assist expert decision-making processes, yielding gain
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04579</link><description>&lt;p&gt;
EGOFALLS:&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65288;arXiv:2309.04579v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#36827;&#34892;&#25684;&#20498;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#21548;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33030;&#24369;&#20154;&#32676;&#65292;&#22914;&#32769;&#24180;&#20154;&#65292;&#25684;&#20498;&#24448;&#24448;&#26159;&#20005;&#37325;&#19988;&#24120;&#23548;&#33268;&#27515;&#20129;&#30340;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#20381;&#36182;&#21333;&#20010;&#20256;&#24863;&#22120;&#65288;&#22270;&#20687;&#25110;&#21152;&#36895;&#24230;&#35745;&#65289;&#25429;&#25417;&#25968;&#25454;&#26469;&#35299;&#20915;&#25684;&#20498;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#20174;&#33258;&#25105;&#20013;&#24515;&#25668;&#20687;&#22836;&#25429;&#25417;&#30340;&#35270;&#39057;&#20013;&#25552;&#21462;&#30340;&#22810;&#27169;&#24577;&#25551;&#36848;&#31526;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22312;&#25552;&#21462;&#30340;&#25551;&#36848;&#31526;&#20043;&#19978;&#26500;&#24314;&#30340;&#36831;&#20915;&#31574;&#34701;&#21512;&#23618;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#36825;&#26159;&#25105;&#20204;&#35748;&#20026;&#30340;&#31532;&#19968;&#20010;&#20844;&#20849;&#21516;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;14&#20010;&#21463;&#35797;&#32773;&#30340;10,948&#20010;&#35270;&#39057;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#20197;&#35780;&#20272;&#21333;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24615;&#33021;&#65292;&#35270;&#35273;&#20449;&#24687;&#34701;&#21512;&#20197;&#21450;&#35270;&#35273;&#21644;&#38899;&#39057;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#36831;&#20915;&#31574;&#34701;&#21512;&#23558;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls are significant and often fatal for vulnerable populations such as the elderly. Previous works have addressed the detection of falls by relying on data capture by a single sensor, images or accelerometers. In this work, we rely on multimodal descriptors extracted from videos captured by egocentric cameras. Our proposed method includes a late decision fusion layer that builds on top of the extracted descriptors. Furthermore, we collect a new dataset on which we assess our proposed approach. We believe this is the first public dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects. We conducted ablation experiments to assess the performance of individual feature extractors, fusion of visual information, and fusion of both visual and audio information. Moreover, we experimented with internal and external cross-validation. Our results demonstrate that the fusion of audio and visual information through late decision fusion improves detection performance, making
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04565</link><description>&lt;p&gt;
&#35299;&#25918;&#22270;&#23398;&#20064;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#21644;&#24212;&#29992;&#65292;&#20294;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21644;&#22312;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#20219;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#38754;&#23545;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#23478;&#20204;&#22312;&#36817;&#24180;&#26469;&#35774;&#35745;&#20102;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#20182;&#20204;&#36824;&#23454;&#26045;&#20102;&#22270;&#20013;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;AutoGraph&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20182;&#20204;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#31649;&#29702;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#65292;&#65288;2&#65289;&#22788;&#29702;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#30340;&#27969;&#31243;&#65288;&#36229;&#36807;&#26550;&#26500;&#35774;&#35745;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;AutoGraph&#26102;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#26469;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29992;&#25143;&#35831;&#27714;&#65288;&#35813;&#35831;&#27714;&#21487;&#33021;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#32536;&#25110;&#22270;&#32423;&#21035;&#30340;&#19981;&#21516;&#25968;&#25454;&#21644;&#23398;&#20064;&#30446;&#26631;&#65289;&#65292;&#22797;&#26434;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#23558;&#30001;LLM&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.04522</link><description>&lt;p&gt;
&#36830;&#25509;NTK&#21644;NNGP&#65306;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#22312;&#26680;&#21306;&#22495;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#32570;&#20047;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#23545;&#20110;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#25551;&#36848;&#32593;&#32476;&#30340;&#36755;&#20986;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26694;&#26550;&#65292;&#20551;&#35774;&#20102;&#32447;&#24615;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#65307;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21644;&#23398;&#20064;&#21518;&#30340;&#32593;&#32476;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#30340;&#31934;&#30830;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#31070;&#32463;&#21160;&#24577;&#26680;&#65288;NDK&#65289;&#65292;&#36825;&#20010;&#26680;&#21487;&#20197;&#21516;&#26102;&#20135;&#29983;NTK&#21644;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#20102;PRECODE&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24433;&#21709;&#12290;PRECODE&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26799;&#24230;&#38450;&#27490;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#31105;&#29992;&#20854;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04515</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#20102;PRECODE&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24433;&#21709;&#12290;PRECODE&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26799;&#24230;&#38450;&#27490;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#31105;&#29992;&#20854;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#32852;&#21512;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23041;&#32961;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#27844;&#28431;&#26469;&#37325;&#26500;&#26412;&#24212;&#26159;&#31169;&#23494;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#24314;&#27169;&#30340;&#38544;&#31169;&#22686;&#24378;&#27169;&#22359;&#65288;PRECODE&#65289;&#65292;&#20197;&#38450;&#27490;&#26799;&#24230;&#27844;&#28431;&#32780;&#19981;&#25439;&#22833;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#20294;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20043;&#21069;&#65292;&#24050;&#32463;&#35777;&#26126;PRECODE&#25104;&#21151;&#38450;&#27490;&#20102;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#22810;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PRECODE&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20197;&#25581;&#31034;&#20854;&#22522;&#26412;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21464;&#20998;&#24314;&#27169;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#20102;PRECODE&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#32493;&#23618;&#30340;&#26799;&#24230;&#20013;&#12290;&#36825;&#20123;&#23618;&#30340;&#38543;&#26426;&#26799;&#24230;&#38459;&#27490;&#20102;&#36845;&#20195;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#24847;&#22320;&#22312;&#25915;&#20987;&#20248;&#21270;&#36807;&#31243;&#20013;&#30465;&#30053;&#38543;&#26426;&#26799;&#24230;&#65292;&#26469;&#31105;&#29992;PRECODE&#30340;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.04508</link><description>&lt;p&gt;
IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#34701;&#21512;&#22120;&#29992;&#20110;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems. (arXiv:2309.04508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;(IoT)&#20256;&#24863;&#22120;&#22312;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20302;&#25104;&#26412;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#65292;&#20934;&#30830;&#26657;&#20934;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;&#26657;&#20934;&#36807;&#31243;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.04504</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#27010;&#24565;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#36890;&#36807;&#25968;&#30334;&#19975;&#20010;&#22238;&#21512;&#30340;&#35757;&#32451;&#25165;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#19982;&#25351;&#20196;&#30456;&#20851;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#25351;&#20196;&#32452;&#21512;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20799;&#31461;&#21487;&#20197;&#20998;&#35299;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#24182;&#23548;&#33322;&#21040;&#25351;&#23450;&#30340;&#29289;&#20307;&#65292;&#21363;&#20351;&#20182;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#26597;&#35810;&#30340;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;3D&#29615;&#22659;&#65292;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#23398;&#20064;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#12289;BERT&#65289;&#22312;&#26356;&#23569;&#30340;&#22238;&#21512;&#20013;&#23398;&#20064;&#21333;&#35789;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#20195;&#29702;&#22312;&#24418;&#29366;&#25110;&#39068;&#33394;&#27010;&#24565;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#25351;&#20196;&#32452;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38899;&#20048;&#28436;&#22863;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#24773;&#22659;&#25935;&#24863;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#21644;&#21463;&#20247;&#22312;&#35299;&#37322;&#38656;&#27714;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#21463;&#20247;&#24182;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#35299;&#37322;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04491</link><description>&lt;p&gt;
&#38899;&#20048;&#28436;&#22863;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#24773;&#22659;&#25935;&#24863;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Context-Sensitive Approach to XAI in Music Performance. (arXiv:2309.04491v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38899;&#20048;&#28436;&#22863;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#24773;&#22659;&#25935;&#24863;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#21644;&#21463;&#20247;&#22312;&#35299;&#37322;&#38656;&#27714;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#21463;&#20247;&#24182;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#35299;&#37322;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#22914;&#20309;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#36879;&#26126;&#21644;&#21487;&#29702;&#35299;&#30340;&#26041;&#27861;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#19981;&#33021;&#22312;&#25688;&#35201;&#20013;&#24471;&#21040;&#35814;&#23613;&#35299;&#20915;&#65292;&#22240;&#20026;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36941;&#36866;&#29992;&#20110;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29983;&#25104;&#36275;&#22815;&#30340;&#35299;&#37322;&#65292;&#23588;&#20854;&#26159;&#22312;&#33402;&#26415;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#20048;&#28436;&#22863;&#20013;XAI&#30340;&#35299;&#37322;&#23454;&#29992;&#20027;&#20041;&#65288;EP&#65289;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#21644;&#21463;&#20247;&#22312;&#35299;&#37322;&#38656;&#27714;&#30340;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23558;&#35299;&#37322;&#38024;&#23545;&#29305;&#23450;&#21463;&#20247;&#24182;&#26681;&#25454;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#65292;EP&#20026;&#25552;&#39640;&#24191;&#27867;&#30340;&#33402;&#26415;&#24212;&#29992;&#21644;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#38899;&#20048;&#28436;&#22863;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving field of Explainable Artificial Intelligence (XAI) has generated significant interest in developing methods to make AI systems more transparent and understandable. However, the problem of explainability cannot be exhaustively solved in the abstract, as there is no single approach that can be universally applied to generate adequate explanations for any given AI system, and this is especially true in the arts. In this position paper, we propose an Explanatory Pragmatism (EP) framework for XAI in music performance, emphasising the importance of context and audience in the development of explainability requirements. By tailoring explanations to specific audiences and continuously refining them based on feedback, EP offers a promising direction for enhancing the transparency and interpretability of AI systems in broad artistic applications and more specifically to music performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#26234;&#33021;agents&#23454;&#26045;&#24809;&#32602;&#65292;&#20197;&#20445;&#35777;&#20854;&#31526;&#21512;&#35768;&#21487;&#25110;&#20041;&#21153;&#25919;&#31574;&#12290;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;agents&#30340;&#36981;&#20174;&#31243;&#24230;&#23545;&#20854;&#34892;&#20026;&#36827;&#34892;&#24809;&#32602;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31639;&#27861;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#23567;&#24635;&#24809;&#32602;&#30340;&#35745;&#21010;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24809;&#32602;&#19981;&#26381;&#20174;&#21629;&#20196;&#30340;agents&#12290;</title><link>http://arxiv.org/abs/2309.04487</link><description>&lt;p&gt;
&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#23545;&#33258;&#20027;agents&#23454;&#26045;&#24809;&#32602;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Penalization Framework For Autonomous Agents Using Answer Set Programming. (arXiv:2309.04487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#26234;&#33021;agents&#23454;&#26045;&#24809;&#32602;&#65292;&#20197;&#20445;&#35777;&#20854;&#31526;&#21512;&#35768;&#21487;&#25110;&#20041;&#21153;&#25919;&#31574;&#12290;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;agents&#30340;&#36981;&#20174;&#31243;&#24230;&#23545;&#20854;&#34892;&#20026;&#36827;&#34892;&#24809;&#32602;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#31639;&#27861;&#26469;&#36873;&#25321;&#20855;&#26377;&#26368;&#23567;&#24635;&#24809;&#32602;&#30340;&#35745;&#21010;&#12290;&#36825;&#19968;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#24809;&#32602;&#19981;&#26381;&#20174;&#21629;&#20196;&#30340;agents&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#23545;&#19981;&#31526;&#21512;&#35768;&#21487;&#25110;&#20041;&#21153;&#25919;&#31574;&#30340;&#26234;&#33021;agents&#26045;&#21152;&#24809;&#32602;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35745;&#21010;&#20013;&#34920;&#31034;&#21644;&#25512;&#29702;&#24809;&#32602;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#26681;&#25454;agents&#22312;&#25480;&#26435;&#21644;&#20041;&#21153;&#25919;&#31574;&#26041;&#38754;&#30340;&#36981;&#20174;&#31243;&#24230;&#23545;&#20854;&#34892;&#20026;&#36827;&#34892;&#24809;&#32602;&#12290;&#36890;&#36807;&#24847;&#35782;&#21040;&#24809;&#32602;&#65292;agents&#21487;&#20197;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#24635;&#24809;&#32602;&#30340;&#35745;&#21010;&#65292;&#38500;&#38750;&#23384;&#22312;&#20687;&#25937;&#20154;&#19968;&#26679;&#30340;&#32039;&#24613;&#30446;&#26631;&#12290;&#25991;&#31456;&#24471;&#20986;&#32467;&#35770;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24809;&#32602;&#19981;&#26381;&#20174;&#21629;&#20196;&#30340;agents&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework for enforcing penalties on intelligent agents that do not comply with authorization or obligation policies in a changing environment. A framework is proposed to represent and reason about penalties in plans, and an algorithm is proposed to penalize an agent's actions based on their level of compliance with respect to authorization and obligation policies. Being aware of penalties an agent can choose a plan with a minimal total penalty, unless there is an emergency goal like saving a human's life. The paper concludes that this framework can reprimand insubordinate agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#65292;&#20174;&#32780;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04478</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#23454;&#39564;&#27979;&#37327;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties. (arXiv:2309.04478v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#65292;&#20174;&#32780;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#24212;&#29992;&#30340;GPT-4&#31561;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#22312;&#26448;&#26009;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#27169;&#24577;&#30340;&#26448;&#26009;&#25968;&#25454;&#65292;&#27604;&#22914;&#25104;&#20998;&#21644;&#32467;&#26500;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#36824;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22823;&#35268;&#27169;&#35745;&#31639;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23454;&#39564;&#25968;&#25454;&#38598;&#24448;&#24448;&#25968;&#25454;&#26377;&#38480;&#19988;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#26469;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;COmposition-Structure&#21452;&#27169;&#24577;&#32593;&#32476;&#65288;COSNet&#65289;&#26088;&#22312;&#20943;&#23569;&#23545;&#20855;&#26377;&#19981;&#23436;&#25972;&#32467;&#26500;&#20449;&#24687;&#30340;&#26448;&#26009;&#24615;&#36136;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of multimodal machine learning models like GPT-4 has revolutionized various research fields including computer vision and natural language processing. However, its implementation in materials informatics remains underexplored, despite the presence of materials data across diverse modalities, such as composition and structure. The effectiveness of machine learning models trained on large calculated datasets depends on the accuracy of calculations, while experimental datasets often have limited data availability and incomplete information. This paper introduces a novel approach to multimodal machine learning in materials science via composition-structure bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet) is designed to enhance learning and predictions of experimentally measured materials properties that have incomplete structure information. Bimodal learning significantly reduces prediction errors across distinct materials properties 
&lt;/p&gt;</description></item><item><title>SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2309.04077</link><description>&lt;p&gt;
SayNav&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26032;&#29615;&#22659;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments. (arXiv:2309.04077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04077
&lt;/p&gt;
&lt;p&gt;
SayNav&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21160;&#24577;&#35268;&#21010;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#30693;&#35782;&#21644;&#22330;&#26223;&#22270;&#23454;&#29616;&#23545;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#30340;&#39640;&#25928;&#27867;&#21270;&#65292;&#21160;&#24577;&#29983;&#25104;&#25351;&#20196;&#24182;&#26681;&#25454;&#26032;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#25512;&#29702;&#21644;&#21160;&#24577;&#35268;&#21010;&#33021;&#21147;&#23545;&#20110;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#26159;&#20154;&#31867;&#25152;&#20855;&#22791;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SayNav&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23545;&#26410;&#30693;&#22823;&#35268;&#27169;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;SayNav&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25509;&#22320;&#26426;&#21046;&#65292;&#36880;&#27493;&#26500;&#24314;&#19968;&#20010;&#25506;&#32034;&#29615;&#22659;&#30340;3D&#22330;&#26223;&#22270;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#19988;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#39640;&#23618;&#23548;&#33322;&#35745;&#21010;&#12290;&#28982;&#21518;&#65292;&#30001;&#39044;&#20808;&#35757;&#32451;&#30340;&#20302;&#23618;&#35268;&#21010;&#22120;&#25191;&#34892;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#65292;&#23558;&#27599;&#20010;&#35745;&#21010;&#30340;&#27493;&#39588;&#35270;&#20026;&#30701;&#36317;&#31163;&#28857;&#30446;&#26631;&#23548;&#33322;&#23376;&#20219;&#21153;&#12290;SayNav&#22312;&#23548;&#33322;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#19968;&#27493;&#19968;&#27493;&#30340;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#26032;&#33719;&#21462;&#30340;&#20449;&#24687;&#19981;&#26029;&#23436;&#21892;&#26410;&#26469;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#20219;&#21153;&#26426;&#39564;&#35777;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;SayNav&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on a new mul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.01940</link><description>&lt;p&gt;
CodeApex&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. (arXiv:2309.01940v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01940
&lt;/p&gt;
&lt;p&gt;
CodeApex&#26159;&#19968;&#20010;&#21452;&#35821;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#31639;&#27861;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;14&#20010;LLM&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#30340;&#32534;&#31243;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CodeApex&#65292;&#19968;&#31181;&#21452;&#35821;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#27880;&#20110;LLM&#30340;&#32534;&#31243;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;CodeApex&#21253;&#25324;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65306;&#27010;&#24565;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#36339;&#25512;&#29702;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#32534;&#31243;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CodeApex&#21033;&#29992;&#31639;&#27861;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;&#27979;&#35797;&#29992;&#20363;&#26469;&#35780;&#20272;LLM&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;14&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#65292;&#21253;&#25324;&#36890;&#29992;&#21644;&#19987;&#38376;&#21270;&#27169;&#22411;&#12290;GPT&#23637;&#29616;&#20986;&#26368;&#20339;&#30340;&#32534;&#31243;&#33021;&#21147;&#65292;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;&#32422;50%&#21644;56%&#12290;&#32534;&#31243;&#20219;&#21153;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#24076;&#26395;CodeApex&#33021;&#22815;&#20026;&#35780;&#20272;&#32534;&#31243;&#33021;&#21147;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#65292;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01838</link><description>&lt;p&gt;
&#38024;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#39640;&#25928;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#65292;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#20854;&#40657;&#30418;API&#26469;&#31363;&#21462;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#21644;&#20854;&#20182;&#23433;&#20840;&#19982;&#38544;&#31169;&#39118;&#38505;&#12290;&#30446;&#21069;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#24314;&#35758;&#21521;&#39044;&#27979;&#27010;&#29575;&#28155;&#21152;&#25200;&#21160;&#65292;&#20294;&#20854;&#35745;&#31639;&#36739;&#37325;&#19988;&#23545;&#25915;&#20987;&#32773;&#25552;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#36741;&#21161;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#65292;&#22952;&#30861;&#20102;&#35813;&#38450;&#24481;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#12290;&#35813;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#25269;&#24481;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2309.01717</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#30340;&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#20013;&#30340;&#36328;&#23398;&#31185;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#20855;&#26377;&#36873;&#25321;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation. (arXiv:2309.01717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25554;&#20540;&#26469;&#35299;&#20915;&#22312;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#24341;&#36215;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#26696;&#20027;&#39064;&#25512;&#29702;&#30340;&#30446;&#26631;&#26159;&#20174;&#36164;&#21161;&#26426;&#26500;&#23450;&#20041;&#30340;&#23398;&#31185;&#20307;&#31995;&#20013;&#33719;&#21462;&#26368;&#21512;&#36866;&#30340;&#23398;&#31185;&#21010;&#20998;&#65292;&#28982;&#21518;&#26426;&#26500;&#23558;&#26681;&#25454;&#36825;&#31181;&#21010;&#20998;&#20174;&#20854;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#21516;&#34892;&#35780;&#23457;&#19987;&#23478;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#39064;&#25512;&#29702;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#20027;&#39064;&#22635;&#20889;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24357;&#34917;&#36164;&#21161;&#26426;&#26500;&#21644;&#39033;&#30446;&#30003;&#35831;&#20154;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#12290;&#29616;&#26377;&#26041;&#27861;&#23558;&#20854;&#24314;&#27169;&#20026;&#23618;&#27425;&#24615;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36845;&#20195;&#22320;&#25512;&#29702;&#26368;&#21512;&#36866;&#30340;&#20027;&#39064;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#21644;&#38750;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#26696;&#20043;&#38388;&#35268;&#27169;&#24046;&#24322;&#65292;&#23548;&#33268;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#23558;&#36328;&#23398;&#31185;&#25552;&#26696;&#24402;&#31867;&#20026;&#38750;&#36328;&#23398;&#31185;&#65292;&#36896;&#25104;&#22312;&#19987;&#23478;&#20998;&#37197;&#36807;&#31243;&#20013;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#12290;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue und
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.01592</link><description>&lt;p&gt;
&#22823;&#23610;&#24230;&#21644;&#26080;&#31351;&#23485;&#24230;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21202;&#35753;&#28436;&#35762;
&lt;/p&gt;
&lt;p&gt;
Les Houches Lectures on Deep Learning at Large &amp; Infinite Width. (arXiv:2309.01592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#28436;&#35762;&#26159;&#22312;2022&#24180;&#21202;&#35753;&#22799;&#23395;&#23398;&#26657;&#32479;&#35745;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#19978;&#23637;&#31034;&#30340;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#12290;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#35762;&#24072;&#20204;&#35752;&#35770;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65307;&#35757;&#32451;&#36807;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32447;&#24615;&#27169;&#22411;&#65292;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#20123;&#32852;&#31995;&#22312;&#26080;&#31351;&#23485;&#24230;&#30340;&#26497;&#38480;&#19979;&#20986;&#29616;&#65307;&#20197;&#21450;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00417</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#20013;&#30340;&#38754;&#31215;&#35268;&#33539;COBRA
&lt;/p&gt;
&lt;p&gt;
Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#26469;&#35745;&#31639;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#21019;&#24314;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#29983;&#23384;&#26354;&#32447;&#20043;&#38388;&#30340;&#38754;&#31215;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#22238;&#24402;&#35774;&#32622;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#21464;&#37327;&#30340;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#27169;&#25311;&#30740;&#31350;&#65292;&#34920;&#26126;&#25105;&#20204;&#23545;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#25552;&#35758;&#25928;&#26524;&#24456;&#22909;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35828;&#26126;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22269;&#38469;&#27835;&#29702;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#26469;&#35748;&#35777;&#22269;&#23478;&#30340;&#31649;&#36758;&#21306;&#22495;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#65292;&#36827;&#32780;&#31105;&#27490;&#20174;&#26410;&#32463;&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.15514</link><description>&lt;p&gt;
&#22269;&#38469;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;: &#19968;&#31181;&#31649;&#36758;&#35748;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
International Governance of Civilian AI: A Jurisdictional Certification Approach. (arXiv:2308.15514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22269;&#38469;&#27835;&#29702;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#26469;&#35748;&#35777;&#22269;&#23478;&#30340;&#31649;&#36758;&#21306;&#22495;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#65292;&#36827;&#32780;&#31105;&#27490;&#20174;&#26410;&#32463;&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;&#22269;&#38469;&#27665;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27835;&#29702;&#23433;&#25490;&#35774;&#35745;&#20013;&#30340;&#26435;&#34913;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#19968;&#20010;&#26631;&#20934;&#12289;&#35768;&#21487;&#21644;&#36131;&#20219;&#21046;&#24230;&#25193;&#23637;&#21040;&#20840;&#29699;&#33539;&#22260;&#12290;&#25105;&#20204;&#24314;&#35758;&#21508;&#22269;&#24314;&#31435;&#19968;&#20010;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32452;&#32455;&#65288;IAIO&#65289;&#26469;&#35748;&#35777;&#22269;&#23478;&#31649;&#36758;&#21306;&#22495;&#65288;&#32780;&#19981;&#26159;&#20844;&#21496;&#25110;AI&#39033;&#30446;&#65289;&#26159;&#21542;&#31526;&#21512;&#22269;&#38469;&#30417;&#30563;&#26631;&#20934;&#12290;&#21508;&#22269;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#31105;&#27490;&#20174;&#26410;&#32463;IAIO&#35748;&#35777;&#30340;&#31649;&#36758;&#21306;&#22495;&#36827;&#21475;AI&#20379;&#24212;&#38142;&#25152;&#21253;&#21547;&#20135;&#21697;&#30340;&#27861;&#35268;&#26469;&#23454;&#26045;&#36825;&#20123;&#22269;&#38469;&#26631;&#20934;&#12290;&#36825;&#19968;&#26041;&#27861;&#20511;&#37492;&#20102;&#29616;&#26377;&#22269;&#38469;&#32452;&#32455;&#27169;&#24335;&#65292;&#22914;&#22269;&#38469;&#27665;&#29992;&#33322;&#31354;&#32452;&#32455;&#65288;ICAO&#65289;&#12289;&#22269;&#38469;&#28023;&#20107;&#32452;&#32455;&#65288;IMO&#65289;&#21644;&#37329;&#34701;&#34892;&#21160;&#29305;&#21035;&#24037;&#20316;&#32452;&#65288;FATF&#65289;&#12290;&#21508;&#22269;&#36824;&#21487;&#20197;&#23545;&#38750;&#35748;&#35777;&#22269;&#23478;&#37319;&#21462;&#22810;&#36793;&#25511;&#21046;&#25514;&#26045;&#65292;&#20363;&#22914;&#23545;AI&#20135;&#21697;&#36755;&#20837;&#65288;&#22914;&#19987;&#29992;&#30828;&#20214;&#65289;&#30340;&#20986;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes trade-offs in the design of international governance arrangements for civilian artificial intelligence (AI) and presents one approach in detail. This approach represents the extension of a standards, licensing, and liability regime to the global level. We propose that states establish an International AI Organization (IAIO) to certify state jurisdictions (not firms or AI projects) for compliance with international oversight standards. States can give force to these international standards by adopting regulations prohibiting the import of goods whose supply chains embody AI from non-IAIO-certified jurisdictions. This borrows attributes from models of existing international organizations, such as the International Civilian Aviation Organization (ICAO), the International Maritime Organization (IMO), and the Financial Action Task Force (FATF). States can also adopt multilateral controls on the export of AI product inputs, such as specialized hardware, to non-certified
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.15272</link><description>&lt;p&gt;
&#35753;LLM&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#26234;&#33021;&#20219;&#21153;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Empowering LLM to use Smartphone for Intelligent Task Automation. (arXiv:2308.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AutoDroid&#65292;&#19968;&#20010;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#33258;&#21160;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#21644;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#26469;&#23454;&#29616;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#21644;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20813;&#25552;&#29992;&#25143;&#19982;&#26234;&#33021;&#25163;&#26426;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30001;&#20110;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26377;&#38480;&#65292;&#20197;&#21450;&#24320;&#21457;&#20154;&#21592;&#25110;&#32456;&#31471;&#29992;&#25143;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#21162;&#21147;&#30340;&#25163;&#21160;&#24037;&#20316;&#32780;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#24046;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#28608;&#21457;&#20102;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#32479;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20219;&#21153;&#20934;&#22791;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDroid&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#20219;&#20309;Android&#24212;&#29992;&#31243;&#24207;&#19978;&#26080;&#38656;&#25163;&#21160;&#24037;&#20316;&#22788;&#29702;&#20219;&#24847;&#20219;&#21153;&#30340;&#31227;&#21160;&#20219;&#21153;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#21160;&#24577;&#20998;&#26512;&#23558;LLMs&#30340;&#24120;&#35782;&#30693;&#35782;&#19982;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#21151;&#33021;&#24847;&#35782;&#30340;UI&#34920;&#31034;&#26041;&#27861;&#65292;&#26725;&#25509;&#20102;UI&#21644;LLM&#65292;&#22522;&#20110;&#25506;&#32034;&#30340;&#20869;&#23384;&#27880;&#20837;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;ExpCLIP&#36825;&#19968;&#25216;&#26415;&#65292;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#23558;&#25991;&#26412;&#21644;&#38754;&#37096;&#34920;&#24773;&#34701;&#21512;&#65292;&#20351;&#24471;&#39118;&#26684;&#21270;&#35821;&#38899;&#39537;&#21160;&#30340;&#38754;&#37096;&#21160;&#30011;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14448</link><description>&lt;p&gt;
ExpCLIP: &#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#23558;&#25991;&#26412;&#21644;&#38754;&#37096;&#34920;&#24773;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment. (arXiv:2308.14448v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;ExpCLIP&#36825;&#19968;&#25216;&#26415;&#65292;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;&#23558;&#25991;&#26412;&#21644;&#38754;&#37096;&#34920;&#24773;&#34701;&#21512;&#65292;&#20351;&#24471;&#39118;&#26684;&#21270;&#35821;&#38899;&#39537;&#21160;&#30340;&#38754;&#37096;&#21160;&#30011;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#21270;&#35821;&#38899;&#39537;&#21160;&#30340;&#38754;&#37096;&#21160;&#30011;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#21253;&#21547;&#29305;&#23450;&#24773;&#32490;&#34920;&#36798;&#30340;&#21160;&#30011;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#39044;&#20808;&#35774;&#23450;&#30340;&#24773;&#32490;&#26631;&#31614;&#25110;&#38754;&#37096;&#34920;&#24773;&#27169;&#26495;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20934;&#30830;&#20256;&#36798;&#29992;&#25143;&#24847;&#22270;&#25152;&#24517;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#24773;&#32490;&#25552;&#31034;&#26469;&#25511;&#21046;&#20219;&#24847;&#39118;&#26684;&#30340;&#25216;&#26415;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#28789;&#27963;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#25991;&#26412;-&#34920;&#24773;&#23545;&#40784;&#25968;&#25454;&#38598;&#65288;TEAD&#65289;&#65292;&#20854;&#20013;&#27599;&#20010;&#38754;&#37096;&#34920;&#24773;&#37117;&#19982;&#20960;&#20010;&#31867;&#20284;&#25552;&#31034;&#30340;&#25551;&#36848;&#37197;&#23545;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#21160;&#27880;&#37322;&#26041;&#27861;&#65292;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#21152;&#24555;&#25968;&#25454;&#38598;&#26500;&#24314;&#36895;&#24230;&#65292;&#20174;&#32780;&#28040;&#38500;&#25163;&#21160;&#27880;&#37322;&#30340;&#22823;&#37327;&#36153;&#29992;&#12290;&#22312;&#27492;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;TEAD&#26469;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;CLIP&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ExpCLIP&#65292;&#23427;&#23545;&#25991;&#26412;&#21644;&#38754;&#37096;&#34920;&#36798;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of stylized speech-driven facial animation is to create animations that encapsulate specific emotional expressions. Existing methods often depend on pre-established emotional labels or facial expression templates, which may limit the necessary flexibility for accurately conveying user intent. In this research, we introduce a technique that enables the control of arbitrary styles by leveraging natural language as emotion prompts. This technique presents benefits in terms of both flexibility and user-friendliness. To realize this objective, we initially construct a Text-Expression Alignment Dataset (TEAD), wherein each facial expression is paired with several prompt-like descriptions.We propose an innovative automatic annotation method, supported by Large Language Models (LLMs), to expedite the dataset construction, thereby eliminating the substantial expense of manual annotation. Following this, we utilize TEAD to train a CLIP-based model, termed ExpCLIP, which encodes tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.13566</link><description>&lt;p&gt;
MLLM-DataEngine&#65306;&#19968;&#31181;MLLM&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25351;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#29420;&#31435;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;MLLM&#24456;&#38590;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#20154;&#21147;&#25104;&#26412;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23553;&#38381;&#24490;&#29615;&#31995;&#32479;MLLM-DataEngine&#65292;&#23427;&#36830;&#25509;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#27599;&#20010;&#24490;&#29615;&#36845;&#20195;&#20013;&#65292;MLLM-DataEngine&#39318;&#20808;&#26681;&#25454;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#28982;&#21518;&#29983;&#25104;&#21512;&#36866;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#29992;&#20110;&#19979;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#19982;&#22522;&#20934;&#27979;&#35797;&#20998;&#31163;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12714</link><description>&lt;p&gt;
VIGC: &#35270;&#35273;&#25351;&#20196;&#29983;&#25104;&#19982;&#32416;&#27491;
&lt;/p&gt;
&lt;p&gt;
VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VIGC&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21644;&#32416;&#27491;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#39640;&#36136;&#37327;&#35843;&#25972;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25972;&#21512;&#25512;&#21160;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#30340;&#31232;&#32570;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#20027;&#23548;&#33539;&#24335;&#65292;&#22914;LLaVA&#65292;&#20381;&#36182;&#20110;&#20165;&#20351;&#29992;&#35821;&#35328;&#30340;GPT-4&#29983;&#25104;&#25968;&#25454;&#65292;&#36825;&#38656;&#35201;&#39044;&#27880;&#37322;&#30340;&#22270;&#20687;&#26631;&#39064;&#21644;&#26816;&#27979;&#21253;&#22260;&#26694;&#65292;&#23548;&#33268;&#23545;&#22270;&#20687;&#32454;&#33410;&#30340;&#29702;&#35299;&#19981;&#36275;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#38469;&#26041;&#26696;&#26159;&#21033;&#29992;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21069;&#21487;&#35775;&#38382;&#30340;MLLMs&#19981;&#20687;&#23427;&#20204;&#30340;LLM&#23545;&#24212;&#29289;&#37027;&#26679;&#24378;&#22823;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20135;&#29983;&#19981;&#36866;&#24403;&#30340;&#22238;&#24212;&#21644;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#12290;&#20316;&#20026;&#35299;&#20915;&#24403;&#21069;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Visual Instruction Generation and Correction&#65288;VIGC&#65289;&#26694;&#26550;&#65292;&#20351;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#24182;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to ge
&lt;/p&gt;</description></item><item><title>Blending-NeRF&#26159;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#33021;&#22815;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#21407;&#22987;&#23545;&#35937;&#21644;&#30446;&#26631;&#23545;&#35937;&#24182;&#28155;&#21152;&#39118;&#26684;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#36827;&#34892;&#25351;&#23548;&#65292;&#24182;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#28155;&#21152;&#26032;&#23545;&#35937;&#12289;&#20462;&#25913;&#32441;&#29702;&#21644;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#37096;&#20998;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11974</link><description>&lt;p&gt;
Blending-NeRF&#65306;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#23616;&#37096;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields. (arXiv:2308.11974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11974
&lt;/p&gt;
&lt;p&gt;
Blending-NeRF&#26159;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#33021;&#22815;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#21407;&#22987;&#23545;&#35937;&#21644;&#30446;&#26631;&#23545;&#35937;&#24182;&#28155;&#21152;&#39118;&#26684;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#36827;&#34892;&#25351;&#23548;&#65292;&#24182;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#28155;&#21152;&#26032;&#23545;&#35937;&#12289;&#20462;&#25913;&#32441;&#29702;&#21644;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#37096;&#20998;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#23545;&#35937;&#30340;&#23616;&#37096;&#32534;&#36753;&#26159;&#19968;&#39033;&#29305;&#21035;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#23616;&#37096;&#28151;&#21512;&#21407;&#22987;3D&#23545;&#35937;&#19982;&#30446;&#26631;&#26032;&#23545;&#35937;&#21644;&#39118;&#26684;&#25928;&#26524;&#24182;&#19981;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#8212;&#8212;Blending-NeRF&#65292;&#23427;&#30001;&#20004;&#20010;NeRF&#32593;&#32476;&#32452;&#25104;&#65306;&#39044;&#35757;&#32451;&#30340;NeRF&#21644;&#21487;&#32534;&#36753;&#30340;NeRF&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20351;Blending-NeRF&#33021;&#22815;&#27491;&#30830;&#22320;&#32534;&#36753;&#30001;&#25991;&#26412;&#23450;&#20301;&#30340;&#30446;&#26631;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#65292;&#25105;&#20204;&#24341;&#23548;Blending-NeRF&#28155;&#21152;&#20855;&#26377;&#19981;&#21516;&#39068;&#33394;&#21644;&#23494;&#24230;&#30340;&#26032;&#23545;&#35937;&#65292;&#20462;&#25913;&#32441;&#29702;&#65292;&#24182;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Blending-NeRF&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#20135;&#29983;&#33258;&#28982;&#19988;&#23616;&#37096;&#32534;&#36753;&#30340;3D&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11241</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wav2vec2&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;Transformer&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#36824;&#29992;&#20110;&#25972;&#20010;&#35821;&#38899;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#26377;&#25928;&#31471;&#21040;&#31471;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21442;&#25968;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#26377;&#25928;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#38376;&#27744;&#21270;(Temporal Gate Pooling)&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;Conformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;BEST-RQ&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;VoxCeleb1&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#20165;&#26377;28.5M&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20855;&#26377;317.7M&#20010;&#21442;&#25968;&#30340;wav2vec2&#30456;&#24403;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/HarunoriKawano/speaker-identification-with-tgp&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11224</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24615;&#33021;&#27934;&#23519;&#19982;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;LLM&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;LLM&#22312;&#35299;&#20915;&#20960;&#20010;&#22270;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#29702;&#35299;&#33021;&#21147;&#12289;&#27491;&#30830;&#24615;&#12289;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1) LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22270;&#25968;&#25454;&#65292;&#24182;&#25512;&#29702;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;2) GPT&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36923;&#36753;&#21644;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;3) &#25152;&#26377;&#34987;&#26816;&#27979;&#30340;LLM&#22312;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#37117;&#38754;&#20020;&#25361;&#25112;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31561;&#25216;&#26415;&#26174;&#31034;&#20986;&#25928;&#26524;&#19979;&#38477;&#12290;4) GPT&#27169;&#22411;&#22312;&#22810;&#31572;&#26696;&#20219;&#21153;&#20013;&#32463;&#24120;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#65292;&#24341;&#21457;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;5) GPT&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20449;&#24515;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#30699;&#27491;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4&#26174;&#31034;&#20986;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
&lt;/p&gt;</description></item><item><title>PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07327</link><description>&lt;p&gt;
PokerKit: &#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#30340;&#20840;&#38754;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
PokerKit: A Comprehensive Python Library for Fine-Grained Multi-Variant Poker Game Simulations. (arXiv:2308.07327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07327
&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#21464;&#20307;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#65292;&#25552;&#20379;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#25903;&#25345;&#21644;&#28789;&#27963;&#30340;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#65292;&#23545;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PokerKit&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#26088;&#22312;&#20811;&#26381;&#29616;&#26377;&#25169;&#20811;&#28216;&#25103;&#27169;&#25311;&#21644;&#25163;&#29260;&#35780;&#20272;&#24037;&#20855;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#24037;&#20855;&#36890;&#24120;&#21482;&#25903;&#25345;&#23569;&#37327;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#19988;&#22312;&#28216;&#25103;&#29366;&#24577;&#25511;&#21046;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;PokerKit&#36890;&#36807;&#25903;&#25345;&#24191;&#27867;&#30340;&#25169;&#20811;&#21464;&#20307;&#65292;&#24182;&#25552;&#20379;&#28789;&#27963;&#30340;&#26550;&#26500;&#20379;&#29992;&#25143;&#23450;&#20041;&#33258;&#23450;&#20041;&#28216;&#25103;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#36825;&#19968;&#33539;&#22260;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;PokerKit&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#21253;&#25324;&#20854;&#30452;&#35266;&#30340;&#32534;&#31243;API&#65292;&#22810;&#21464;&#20307;&#28216;&#25103;&#25903;&#25345;&#20197;&#21450;&#32479;&#19968;&#30340;&#25163;&#29260;&#35780;&#20272;&#22871;&#20214;&#22312;&#19981;&#21516;&#25163;&#29260;&#31867;&#22411;&#38388;&#30340;&#24212;&#29992;&#12290;PokerKit&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#22312;&#25169;&#20811;AI&#24320;&#21457;&#12289;&#24037;&#20855;&#21019;&#24314;&#21644;&#22312;&#32447;&#25169;&#20811;&#36172;&#22330;&#23454;&#29616;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;PokerKit&#30340;&#21487;&#38752;&#24615;&#36890;&#36807;&#38745;&#24577;&#31867;&#22411;&#26816;&#26597;&#12289;&#24191;&#27867;&#30340;doctest&#21644;&#21333;&#20803;&#27979;&#35797;&#26469;&#30830;&#20445;&#65292;&#36798;&#21040;&#20102;97%&#30340;&#20195;&#30721;&#35206;&#30422;&#29575;&#12290;&#24341;&#20837;PokerKit&#20195;&#34920;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
PokerKit is an open-source Python library designed to overcome the restrictions of existing poker game simulation and hand evaluation tools, which typically support only a handful of poker variants and lack flexibility in game state control. In contrast, PokerKit significantly expands this scope by supporting an extensive array of poker variants and it provides a flexible architecture for users to define their custom games. This paper details the design and implementation of PokerKit, including its intuitive programmatic API, multi-variant game support, and a unified hand evaluation suite across different hand types. The flexibility of PokerKit allows for applications in diverse areas, such as poker AI development, tool creation, and online poker casino implementation. PokerKit's reliability has been established through static type checking, extensive doctests, and unit tests, achieving 97\% code coverage. The introduction of PokerKit represents a significant contribution to the field 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.05734</link><description>&lt;p&gt;
AudioLDM 2: &#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#36827;&#34892;&#25972;&#20307;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38899;&#39057;&#29983;&#25104;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38899;&#39057;&#20013;&#20849;&#20139;&#19968;&#20123;&#20849;&#24615;&#65292;&#27604;&#22914;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#65292;&#20294;&#20026;&#27599;&#31181;&#31867;&#22411;&#35774;&#35745;&#27169;&#22411;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#29305;&#23450;&#30340;&#30446;&#26631;&#21644;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#21487;&#33021;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#30446;&#26631;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23454;&#29616;&#38899;&#39057;&#29983;&#25104;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#35821;&#35328;&#38899;&#39057;&#65288;LOA&#65289;&#8221;&#30340;&#38899;&#39057;&#36890;&#29992;&#34920;&#31034;&#12290;&#20219;&#20309;&#38899;&#39057;&#37117;&#21487;&#20197;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;AudioMAE&#36716;&#25442;&#20026;LOA&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#23558;&#20219;&#20309;&#24418;&#24335;&#30340;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33258;&#28982;&#22320;&#24102;&#26469;&#20102;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;AudioMAE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>Select2Col&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#21327;&#20316;&#24863;&#30693;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#24182;&#25490;&#38500;&#36127;&#38754;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16517</link><description>&lt;p&gt;
Select2Col: &#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#30340;&#21327;&#20316;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception. (arXiv:2307.16517v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16517
&lt;/p&gt;
&lt;p&gt;
Select2Col&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#21327;&#20316;&#24863;&#30693;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#24182;&#25490;&#38500;&#36127;&#38754;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20849;&#20139;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#21327;&#20316;&#22312;&#20811;&#26381;&#23396;&#31435;&#20195;&#29702;&#30340;&#24863;&#30693;&#33021;&#21147;&#38480;&#21046;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21327;&#20316;&#24863;&#30693;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#35821;&#20041;&#20449;&#24687;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#32780;&#24573;&#35270;&#20102;&#26102;&#38388;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#21327;&#20316;&#30340;&#28508;&#22312;&#30410;&#22788;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Select2Col&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#32771;&#34385;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#12290;&#22312;Select2Col&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#37325;&#35201;&#24615;&#65288;IoSI&#65289;&#20197;&#25552;&#39640;&#24863;&#30693;&#24615;&#33021;&#30340;&#21512;&#20316;&#32773;&#36873;&#25321;&#26041;&#27861;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#65292;&#21516;&#26102;&#25490;&#38500;&#37027;&#20123;&#24102;&#26469;&#36127;&#38754;&#24433;&#21709;&#30340;&#21512;&#20316;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;HPHA&#30340;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#21382;&#21490;&#20808;&#39564;&#28151;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration by leveraging the shared semantic information plays a crucial role in overcoming the perception capability limitations of isolated agents. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension. Consequently, the potential benefits of collaboration remain underutilized. In this article, we propose Select2Col, a novel collaborative perception framework that takes into account the {s}patial-t{e}mpora{l} importanc{e} of semanti{c} informa{t}ion. Within the Select2Col, we develop a collaborator selection method that utilizes a lightweight graph neural network (GNN) to estimate the importance of semantic information (IoSI) in enhancing perception performance, thereby identifying contributive collaborators while excluding those that bring negative impact. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid atte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniBriVL&#30340;&#26032;&#22411;&#36890;&#29992;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#38899;&#39057;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#31283;&#20581;&#22320;&#23398;&#20064;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniBriVL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#33021;&#22815;&#20174;&#38899;&#39057;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2307.15898</link><description>&lt;p&gt;
UniBriVL: &#24378;&#22823;&#30340;&#38899;&#39057;&#39537;&#21160;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#34920;&#31034;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models. (arXiv:2307.15898v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniBriVL&#30340;&#26032;&#22411;&#36890;&#29992;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#38899;&#39057;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#12290;&#23427;&#33021;&#22815;&#31283;&#20581;&#22320;&#23398;&#20064;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniBriVL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#33021;&#22815;&#20174;&#38899;&#39057;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#27169;&#22411;&#22240;&#20854;&#22312;&#21508;&#31181;&#24615;&#33021;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#32780;&#34987;&#35748;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;&#23545;&#20110;&#26410;&#26469;&#23454;&#29616;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;UniBriVL&#65292;&#23427;&#22522;&#20110;Bridging-Vision-and-Language&#65288;BriVL&#65289;&#12290;&#36890;&#29992;BriVL&#23558;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#31283;&#20581;&#30340;&#35821;&#35328;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;UniBriVL&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#23450;&#24615;&#35780;&#20272;&#65292;&#36825;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#38899;&#39057;&#20013;&#21019;&#24314;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;UniBriVL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#20174;&#38899;&#39057;&#20013;&#36873;&#25321;&#36866;&#24403;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from au
&lt;/p&gt;</description></item><item><title>CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15453</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#32534;&#31243;&#21040;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Probabilistic Programming to Complexity-based Programming. (arXiv:2307.15453v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15453
&lt;/p&gt;
&lt;p&gt;
CompLog&#26159;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24615;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;Kolmogorov&#22797;&#26434;&#24615;&#26367;&#20195;&#27010;&#29575;&#25512;&#29702;&#65292;&#23454;&#29616;&#35745;&#31639;&#26576;&#31181;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#35268;&#33539;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#25551;&#36848;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CompLog&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#30340;&#20027;&#35201;&#29305;&#28857;&#21644;&#21021;&#27493;&#23454;&#29616;&#12290;CompLog&#20511;&#37492;&#20102;&#27010;&#29575;&#32534;&#31243;&#31995;&#32479;&#65288;&#22914;ProbLog&#65289;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#24182;&#22522;&#20110;Simplicity&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26426;&#21046;&#65292;&#36890;&#36807;ASP&#31243;&#24207;&#30340;min-path&#25628;&#32034;&#35745;&#31639;&#20004;&#31181;Kolmogorov&#22797;&#26434;&#24615;&#65292;&#32780;&#19981;&#26159;&#27010;&#29575;&#25512;&#29702;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#25143;&#33021;&#22815;&#35745;&#31639;&#26576;&#20010;&#24773;&#20917;&#24847;&#22806;&#24615;&#30340;ex-post&#21644;ex-ante&#24230;&#37327;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#21518;&#39564;&#21644;&#20808;&#39564;&#20027;&#35266;&#27010;&#29575;&#12290;&#35745;&#31639;&#22522;&#20110;&#36890;&#36807;&#25551;&#36848;&#24615;&#35859;&#35789;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#25551;&#36848;&#24615;&#20851;&#31995;&#21152;&#26435;&#30340;&#19990;&#30028;&#21644;&#24515;&#26234;&#27169;&#22411;&#30340;&#35268;&#33539;&#12290;&#26412;&#25991;&#36824;&#38416;&#36848;&#20102;&#20960;&#20010;&#24212;&#29992;&#31034;&#20363;&#65306;&#29983;&#25104;&#30456;&#20851;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#23545;&#26512;&#21462;&#21644;&#21542;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the main characteristics and a preliminary implementation of a novel computational framework named CompLog. Inspired by probabilistic programming systems like ProbLog, CompLog builds upon the inferential mechanisms proposed by Simplicity Theory, relying on the computation of two Kolmogorov complexities (here implemented as min-path searches via ASP programs) rather than probabilistic inference. The proposed system enables users to compute ex-post and ex-ante measures of unexpectedness of a certain situation, mapping respectively to posterior and prior subjective probabilities. The computation is based on the specification of world and mental models by means of causal and descriptive relations between predicates weighted by complexity. The paper illustrates a few examples of application: generating relevant descriptions, and providing alternative approaches to disjunction and to negation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10404</link><description>&lt;p&gt;
&#20351;&#29992;PIP-Net&#35299;&#37322;&#21644;&#32416;&#27491;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#26159;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#33258;&#21160;&#35786;&#26029;&#25903;&#25345;&#12290;PIP-Net&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20856;&#22411;&#22270;&#20687;&#37096;&#20998;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;PIP-Net&#30340;&#20915;&#31574;&#36807;&#31243;&#31526;&#21512;&#21307;&#23398;&#20998;&#31867;&#26631;&#20934;&#65292;&#20165;&#25552;&#20379;&#22270;&#20687;&#32423;&#21035;&#30340;&#31867;&#26631;&#31614;&#12290;&#30001;&#20110;PIP-Net&#23545;&#21407;&#22411;&#36827;&#34892;&#20102;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;X&#20809;&#20013;&#30340;&#19981;&#33391;&#25991;&#26412;&#25110;&#26631;&#31614;&#38169;&#35823;&#31561;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#26174;&#31034;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#25163;&#21160;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#23545;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#30456;&#20114;&#21442;&#32771;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03941</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65306;&#28085;&#20041;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions. (arXiv:2307.03941v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#36951;&#24536;&#26435;&#65288;RTBF&#65289;&#26368;&#21021;&#26159;&#30001;&#35895;&#27468;&#35199;&#29677;&#29273;&#19982;&#22467;&#20811;&#26031;&#20869;&#22612;&#32034;&#22996;&#21592;&#20250;(Mario Costeja Gonz\'alez)&#20043;&#38388;&#30340;&#23448;&#21496;&#32467;&#26524;&#32780;&#30830;&#31435;&#30340;&#65292;&#24182;&#19988;&#21518;&#26469;&#34987;&#20316;&#20026;&#27431;&#27954;&#32852;&#30431;&#19968;&#33324;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#19979;&#30340;&#21024;&#38500;&#26435;&#12290;RTBF&#20801;&#35768;&#20010;&#20154;&#21521;&#32452;&#32455;&#35831;&#27714;&#21024;&#38500;&#20010;&#20154;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#65292;&#20010;&#20154;&#21487;&#20197;&#21521;&#32452;&#32455;&#21457;&#36865;&#35831;&#27714;&#65292;&#25490;&#38500;&#20182;&#20204;&#30340;&#20449;&#24687;&#22312;&#26597;&#35810;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21644;&#20854;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;LLM&#21551;&#29992;&#30340;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#25490;&#38500;&#22312;RTBF&#20043;&#22806;&#12290;&#30456;&#27604;&#25628;&#32034;&#24341;&#25806;&#20351;&#29992;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;LLMs&#20197;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#22788;&#29702;&#20449;&#24687;&#65292;&#36825;&#20026;&#31526;&#21512;RTBF&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#23454;&#26045;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20197;&#31526;&#21512;RTBF&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja Gonz\'alez, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of machine unle
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.18365</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#21040;&#24213;&#26377;&#24590;&#26679;&#30340;&#24212;&#29992;&#65311;&#20843;&#20010;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks. (arXiv:2305.18365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#25324; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#36719;&#20214;&#24037;&#31243;&#31561;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;LLM &#26159;&#21542;&#26377;&#33021;&#21147;&#25512;&#21160;&#21270;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#21253;&#21547; 8 &#20010;&#23454;&#38469;&#21270;&#23398;&#20219;&#21153;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21517;&#31216;&#39044;&#27979;&#12289;&#23646;&#24615;&#39044;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#12289;&#21453;&#24212;&#39044;&#27979;&#12289;&#21453;&#21512;&#25104;&#65288;&#20174;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#65289;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#23376;&#35774;&#35745;&#12289;&#20998;&#23376;&#23383;&#24149;&#21644;&#35797;&#21058;&#36873;&#25321;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; BBBP&#12289;Tox21&#12289;PubChem&#12289;USPTO &#21644; ChEBI&#65292;&#26377;&#21147;&#22320;&#35777;&#26126;&#20102; LLM &#22312;&#23454;&#38469;&#21270;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#31034;&#20363;&#20013;&#65292;&#23545;&#19977;&#31181; GPT &#27169;&#22411;&#65288;GPT-4&#12289;GPT-3.5 &#21644; DaVinci-003&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been rapidly applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper,we establish a comprehensive benchmark containing 8 practical chemistry tasks, including 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction prediction, 5) retrosynthesis (prediction of reactants from products), 6)text-based molecule design, 7) molecule captioning, and 8) reagent selection. Our analysis draws on widely recognized datasets including BBBP, Tox21, PubChem, USPTO, and ChEBI, facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Three GPT models (GPT-4, GPT-3.5,and Davinci-003) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#25233;&#21046;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.14298</link><description>&lt;p&gt;
&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Instance Segmentation in the Dark. (arXiv:2304.14298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#40657;&#26263;&#29615;&#22659;&#19979;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#12290;&#20026;&#20102;&#25233;&#21046;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#31561;&#25216;&#26415;&#65292;&#21516;&#26102;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23454;&#20363;&#20998;&#21106;&#25216;&#26415;&#20027;&#35201;&#36866;&#29992;&#20110;&#39640;&#21487;&#35265;&#24230;&#30340;&#36755;&#20837;&#65292;&#20294;&#22312;&#26497;&#20302;&#20809;&#29615;&#22659;&#19979;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#40657;&#26263;&#20013;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#20171;&#32461;&#20102;&#20960;&#31181;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20302;&#20809;&#25512;&#26029;&#20934;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#20302;&#20809;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#20250;&#24341;&#20837;&#39640;&#39057;&#25200;&#21160;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;&#20026;&#20102;&#25233;&#21046;&#36825;&#31181;&#8220;&#29305;&#24449;&#22122;&#22768;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#33258;&#36866;&#24212;&#21152;&#26435;&#19979;&#37319;&#26679;&#23618;&#12289;&#24179;&#28369;&#23450;&#21521;&#21367;&#31215;&#22359;&#21644;&#25200;&#21160;&#25233;&#21046;&#23398;&#20064;&#12290;&#36825;&#20123;&#32452;&#20214;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#19979;&#37319;&#26679;&#21644;&#21367;&#31215;&#25805;&#20316;&#20013;&#30340;&#29305;&#24449;&#22122;&#22768;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#25239;&#25200;&#21160;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#39640;&#20301;&#28145;&#30340;RAW&#22270;&#20687;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#26356;&#20016;&#23500;&#30340;&#22330;&#26223;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
Existing instance segmentation techniques are primarily tailored for high-visibility inputs, but their performance significantly deteriorates in extremely low-light environments. In this work, we take a deep look at instance segmentation in the dark and introduce several techniques that substantially boost the low-light inference accuracy. The proposed method is motivated by the observation that noise in low-light images introduces high-frequency disturbances to the feature maps of neural networks, thereby significantly degrading performance. To suppress this ``feature noise", we propose a novel learning method that relies on an adaptive weighted downsampling layer, a smooth-oriented convolutional block, and disturbance suppression learning. These components effectively reduce feature noise during downsampling and convolution operations, enabling the model to learn disturbance-invariant features. Furthermore, we discover that high-bit-depth RAW images can better preserve richer scene i
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.13000</link><description>&lt;p&gt;
&#20174;&#31354;&#38388;&#20013;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment anything, from space?. (arXiv:2304.13000v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13000
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Segment Anything Model&#65288;SAM&#65289;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#26377;&#25928;&#20998;&#21106;&#33258;&#28982;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#23545;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#39033;&#30740;&#31350;&#25506;&#35752;SAM&#22312;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#39033;&#22522;&#20934;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#35270;&#35273;&#20219;&#21153;&#19987;&#38376;&#24320;&#21457;&#30340;&#31532;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#34987;&#31216;&#20026;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#12290;SAM&#21487;&#20197;&#26681;&#25454;&#31616;&#21333;&#30340;&#36755;&#20837;&#25552;&#31034;&#65288;&#22914;&#19968;&#20010;&#25110;&#22810;&#20010;&#28857;&#12289;&#36793;&#30028;&#26694;&#25110;&#25513;&#30721;&#65289;&#20998;&#21106;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#12290;&#20316;&#32773;&#20204;&#22312;&#22823;&#37327;&#30340;&#35270;&#35273;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;SAM&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#31934;&#24230;&#65292;&#24182;&#21457;&#29616;SAM&#36890;&#24120;&#36798;&#21040;&#20102;&#19982;&#30446;&#26631;&#20219;&#21153;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#20284;&#25110;&#26377;&#26102;&#29978;&#33267;&#36229;&#36234;&#20854;&#35782;&#21035;&#31934;&#24230;&#12290;SAM&#22312;&#20998;&#21106;&#26041;&#38754;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#20174;&#20107;&#33258;&#28982;&#22270;&#20687;&#30740;&#31350;&#30340;&#35270;&#35273;&#30740;&#31350;&#20154;&#21592;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;SAM&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#21542;&#25193;&#23637;&#21040;&#31354;&#20013;&#22270;&#20687;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25351;&#23548;&#31038;&#21306;&#23545;&#20854;&#21457;&#23637;&#30340;&#22238;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#21644;&#24191;&#27867;&#30740;&#31350;&#36807;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;SAM&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;SAM&#36890;&#24120;&#22312;&#31354;&#20013;&#22270;&#20687;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#34920;&#29616;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the first foundation model developed specifically for vision tasks was developed, termed the "Segment Anything Model" (SAM). SAM can segment objects in input imagery based upon cheap input prompts, such as one (or more) points, a bounding box, or a mask. The authors examined the zero-shot image segmentation accuracy of SAM on a large number of vision benchmark tasks and found that SAM usually achieved recognition accuracy similar to, or sometimes exceeding, vision models that had been trained on the target tasks. The impressive generalization of SAM for segmentation has major implications for vision researchers working on natural imagery. In this work, we examine whether SAM's impressive performance extends to overhead imagery problems, and help guide the community's response to its development. We examine SAM's performance on a set of diverse and widely-studied benchmark tasks. We find that SAM does often generalize well to overhead imagery, although it fails in some cases d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10749</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19981;&#20165;&#22240;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21331;&#36234;&#65292;&#32780;&#19988;&#22240;&#20854;&#22825;&#28982;&#36866;&#21512;&#20110;&#38598;&#25104;&#22810;&#23610;&#24230;&#29983;&#29289;&#21487;&#22609;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SNN&#30452;&#25509;&#37319;&#29992;&#25104;&#29087;&#30340;DNN&#32467;&#26500;&#65292;&#24456;&#23569;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29992;&#20110;SNN&#12290;&#20154;&#31867;&#22823;&#33041;&#31070;&#32463;&#27169;&#24335;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27169;&#22359;&#21270;&#30340;&#21306;&#22495;&#32467;&#26500;&#21644;&#20840;&#23616;&#24615;&#30340;&#36328;&#33041;&#21306;&#36830;&#25509;&#26159;&#33258;&#28982;&#36827;&#21270;&#30340;&#20135;&#29289;&#65292;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#22522;&#20110;&#33041;&#30340;SNN&#26550;&#26500;&#30340;&#23436;&#32654;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MSE-NAS&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#20316;&#20026;&#36827;&#21270;&#25628;&#32034;&#31354;&#38388;&#12290; MSE-NAS&#36890;&#36807;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#38388;&#25509;&#26041;&#24335;&#65292;&#36827;&#21270;&#21333;&#20010;&#31070;&#32463;&#20803;&#25805;&#20316;&#65292;&#22810;&#20010;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#20197;&#21450;&#36328;&#27169;&#24335;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.08495</link><description>&lt;p&gt;
&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#32676;&#20307;&#25928;&#29992;&#20248;&#21270;&#65306;&#19968;&#31181;&#31574;&#30053;&#24615;&#21644;&#20247;&#21253;&#24847;&#35782;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Group Utility in Itinerary Planning: A Strategic and Crowd-Aware Approach. (arXiv:2304.08495v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SCAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#65292;&#35299;&#20915;&#34892;&#31243;&#35268;&#21010;&#20013;&#30340;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#31243;&#25512;&#33616;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#22797;&#26434;&#30340;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;&#24403;&#32771;&#34385;&#21040;&#20248;&#21270;&#22810;&#20010;&#29992;&#25143;&#25490;&#38431;&#26102;&#38388;&#21644;&#20154;&#32676;&#27700;&#24179;&#26102;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#35832;&#22810;&#21442;&#25968;&#65292;&#22914;&#26223;&#28857;&#21463;&#27426;&#36814;&#31243;&#24230;&#12289;&#25490;&#38431;&#26102;&#38388;&#12289;&#27493;&#34892;&#26102;&#38388;&#21644;&#33829;&#19994;&#26102;&#38388;&#31561;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38598;&#20013;&#22312;&#21333;&#20154;&#35270;&#35282;&#19978;&#65292;&#26410;&#33021;&#35299;&#20915;&#33258;&#28982;&#20154;&#32676;&#34892;&#20026;&#24341;&#36215;&#30340;&#29616;&#23454;&#38382;&#39064;&#65292;&#22914;&#36138;&#23146;&#36335;&#30001;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25112;&#30053;&#21644;&#20247;&#21253;&#24847;&#35782;&#34892;&#31243;&#25512;&#33616;&#65288;SCAIR&#65289;&#8221;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#20248;&#21270;&#32676;&#20307;&#25928;&#29992;&#12290;&#25105;&#20204;&#23558;&#36335;&#32447;&#25512;&#33616;&#31574;&#30053;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#32534;&#30721;&#26426;&#21046;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#23454;&#29616;&#23454;&#26102;&#35268;&#21010;&#21644;&#20998;&#37197;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#20844;&#22253;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#34892;&#21508;&#31181;&#31454;&#20105;&#24615;&#21644;&#29616;&#23454;&#30340;&#22522;&#32447;&#27979;&#35797;&#65292;&#35777;&#26126;SCAIR&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Itinerary recommendation is a complex sequence prediction problem with numerous real-world applications. This task becomes even more challenging when considering the optimization of multiple user queuing times and crowd levels, as well as numerous involved parameters, such as attraction popularity, queuing time, walking time, and operating hours. Existing solutions typically focus on single-person perspectives and fail to address real-world issues resulting from natural crowd behavior, like the Selfish Routing problem. In this paper, we introduce the Strategic and Crowd-Aware Itinerary Recommendation (SCAIR) algorithm, which optimizes group utility in real-world settings. We model the route recommendation strategy as a Markov Decision Process and propose a State Encoding mechanism that enables real-time planning and allocation in linear time. We evaluate our algorithm against various competitive and realistic baselines using a theme park dataset, demonstrating that SCAIR outperforms th
&lt;/p&gt;</description></item><item><title>ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.04321</link><description>&lt;p&gt;
ARNOLD&#65306;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#23454;&#29616;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes. (arXiv:2304.04321v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04321
&lt;/p&gt;
&lt;p&gt;
ARNOLD&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#65292;&#22312;&#35821;&#35328;&#24341;&#23548;&#19979;&#24110;&#21161;&#26426;&#22120;&#20154;&#23398;&#20064;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#29702;&#35299;&#29289;&#20307;&#30340;&#36830;&#32493;&#29366;&#24577;&#23545;&#20110;&#20219;&#21153;&#23398;&#20064;&#21644;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20551;&#23450;&#30446;&#26631;&#29366;&#24577;&#26159;&#31163;&#25955;&#30340;(&#20363;&#22914;&#20108;&#36827;&#21046;&#29366;&#24577;)&#65292;&#36825;&#32473;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#21644;&#23558;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29366;&#24577;&#31163;&#25955;&#21270;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#26681;&#25454;&#21160;&#20316;&#21644;&#29366;&#24577;&#30340;&#24341;&#23548;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARNOLD&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;&#35821;&#35328;&#24341;&#23548;&#12289;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#30340;&#29616;&#23454;3D&#22330;&#26223;&#20219;&#21153;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;ARNOLD&#30001;8&#20010;&#35821;&#35328;&#26465;&#20214;&#20219;&#21153;&#32452;&#25104;&#65292;&#28041;&#21450;&#29702;&#35299;&#29289;&#20307;&#29366;&#24577;&#21644;&#23398;&#20064;&#36830;&#32493;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#20419;&#36827;&#35821;&#35328;&#24341;&#23548;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#26495;&#29983;&#25104;&#30340;&#35821;&#35328;&#25551;&#36848;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23398;&#20064;&#27169;&#22411;&#26469;&#35780;&#20272;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ARNOLD&#20026;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#30340;&#35821;&#35328;&#24341;&#23548;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#24182;&#21487;&#29992;&#20110;&#35780;&#20272;&#20174;&#27169;&#25311;&#22330;&#26223;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete(e.g., binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present ARNOLD, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. ARNOLD is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results ind
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17155</link><description>&lt;p&gt;
&#22522;&#20110;&#21028;&#21035;&#24615;&#31867;&#26631;&#30340;&#25991;&#26412;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#29255;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#24120;&#24120;&#26080;&#27861;&#25551;&#32472;&#20986;&#24494;&#22937;&#30340;&#32454;&#33410;&#19988;&#26131;&#20110;&#20986;&#38169;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#22312;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#65306;&#65288;i&#65289;&#19982;&#29992;&#20110;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#29228;&#21462;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22270;&#29255;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20250;&#20005;&#37325;&#21463;&#24433;&#21709;&#65292;&#25110;&#65288;ii&#65289;&#36755;&#20837;&#26159;&#30828;&#32534;&#30721;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#33258;&#30001;&#25991;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#24615;&#20449;&#21495;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26082;&#21457;&#25381;&#20102;&#33258;&#30001;&#25991;&#26412;&#30340;&#34920;&#36798;&#28508;&#21147;&#65292;&#21448;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.09026</link><description>&lt;p&gt;
&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#36164;&#28304;&#21463;&#38480;&#21644;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection. (arXiv:2303.09026v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#30340;&#32454;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36793;&#32536;&#35745;&#31639;&#31561;&#36164;&#28304;&#21463;&#38480;&#22330;&#26223;&#19979;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#38024;&#23545;&#20351;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#26102;&#38656;&#35201;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#30340;&#31934;&#20934;&#32454;&#31890;&#24230;&#26816;&#27979;&#38656;&#27714;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#36890;&#35782;&#30693;&#35782;&#36741;&#21161;&#31895;&#31890;&#24230;&#30446;&#26631;&#26816;&#27979;&#22120;&#33719;&#21462;&#31934;&#20934;&#30340;&#32454;&#31890;&#24230;&#26816;&#27979;&#32467;&#26524;&#12290;&#24341;&#20837;&#36890;&#35782;&#30693;&#35782;&#25512;&#29702;&#27169;&#22359;(CKIM)&#22788;&#29702;&#30001;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#32473;&#20986;&#30340;&#31895;&#31890;&#24230;&#26631;&#31614;&#65292;&#20174;&#32780;&#29983;&#25104;&#32454;&#31890;&#24230;&#26631;&#31614;&#12290;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#27169;&#31946;&#35268;&#21017;&#21644;&#28165;&#26224;&#35268;&#21017;&#30340;&#25512;&#29702;&#65292;&#21069;&#32773;&#29992;&#20110;&#22788;&#29702;&#30446;&#26631;&#35821;&#20041;&#26631;&#31614;&#30340;&#27169;&#31946;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#21644;&#26631;&#27880;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider fine-grained image object detection in resource-constrained cases such as edge computing. Deep learning (DL), namely learning with deep neural networks (DNNs), has become the dominating approach to object detection. To achieve accurate fine-grained detection, one needs to employ a large enough DNN model and a vast amount of data annotations, which brings a challenge for using modern DL object detectors in resource-constrained cases. To this end, we propose an approach, which leverages commonsense knowledge to assist a coarse-grained object detector to get accurate fine-grained detection results. Specifically, we introduce a commonsense knowledge inference module (CKIM) to process coarse-grained lables given by a benchmark DL detector to produce fine-grained lables. We consider both crisp-rule and fuzzy-rule based inference in our CKIM; the latter is used to handle ambiguity in the target semantic labels. We implement our method based on several modern DL dete
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#24182;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#22320;&#32534;&#30721;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#25506;&#32034;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#19988;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09893</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#29983;&#25104;&#22120;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Efficient Generator of Mathematical Expressions for Symbolic Regression. (arXiv:2302.09893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09893
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#24182;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#22320;&#32534;&#30721;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#25506;&#32034;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#19988;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23618;&#32423;&#32467;&#26500;&#12290;&#23427;&#23558;&#31616;&#21333;&#30340;&#21407;&#23376;&#21333;&#20803;&#19982;&#20849;&#20139;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20197;&#36882;&#24402;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32423;&#20013;&#30340;&#21508;&#20010;&#33410;&#28857;&#12290;&#32534;&#30721;&#26159;&#33258;&#19979;&#32780;&#19978;&#36827;&#34892;&#30340;&#65292;&#35299;&#30721;&#26159;&#33258;&#19978;&#32780;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;HVAE&#21487;&#20197;&#22312;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#23567;&#35821;&#26009;&#24211;&#19978;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#23558;&#34920;&#36798;&#24335;&#20934;&#30830;&#22320;&#32534;&#30721;&#25104;&#24179;&#28369;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#12290;&#21518;&#32773;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#39640;&#25928;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#30340;&#20219;&#21153;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;HVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#38543;&#26426;&#25628;&#32034;&#27604;&#36890;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#38543;&#26426;&#25628;&#32034;&#25928;&#26524;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#24212;&#29992;&#36827;&#21270;&#31639;&#27861;&#21040;HVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;EDHiE&#31995;&#32479;&#21487;&#20197;&#26356;&#22909;&#22320;&#20174;&#26631;&#20934;&#31526;&#21495;&#22238;&#24402;&#22522;&#20934;&#20013;&#37325;&#24314;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better 
&lt;/p&gt;</description></item><item><title>AudioLDM&#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#23398;&#20064;&#38899;&#39057;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;&#23427;&#22312;TTA&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.12503</link><description>&lt;p&gt;
AudioLDM: &#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#36716;&#38899;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. (arXiv:2301.12503v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12503
&lt;/p&gt;
&lt;p&gt;
AudioLDM&#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#23398;&#20064;&#38899;&#39057;&#30340;&#36830;&#32493;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;&#23427;&#22312;TTA&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#38899;&#39057;&#65288;TTA&#65289;&#31995;&#32479;&#22240;&#20854;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#21512;&#25104;&#36890;&#29992;&#38899;&#39057;&#32780;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;TTA&#30740;&#31350;&#22312;&#29983;&#25104;&#36136;&#37327;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;TTA&#31995;&#32479;AudioLDM&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#28508;&#21464;&#37327;&#23398;&#20064;&#36830;&#32493;&#38899;&#39057;&#34920;&#31034;&#12290;&#39044;&#35757;&#32451;&#30340;CLAP&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#20855;&#26377;&#38899;&#39057;&#23884;&#20837;&#30340;LDM&#65292;&#24182;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#25552;&#20379;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26465;&#20214;&#12290;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;&#20449;&#21495;&#21450;&#20854;&#32452;&#21512;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#24314;&#27169;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;AudioLDM&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;&#22312;&#21333;&#20010;GPU&#19978;&#20351;&#29992;AudioCaps&#36827;&#34892;&#35757;&#32451;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;AudioLDM&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#65288;&#22914;Frechet&#36317;&#31163;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;TTA&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;AudioLDM&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#39057;&#29983;&#25104;&#30340;TTA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided au
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.08374</link><description>&lt;p&gt;
&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Projective Integral Updates for High-Dimensional Variational Inference. (arXiv:2301.08374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#29702;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#36817;&#20284;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31616;&#21270;&#30340;&#21442;&#25968;&#20998;&#24067;&#26469;&#20195;&#26367;&#23436;&#25972;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#20013;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25429;&#25417;&#19982;&#35757;&#32451;&#25968;&#25454;&#19968;&#33268;&#30340;&#27169;&#22411;&#21464;&#21270;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#22266;&#23450;&#28857;&#26368;&#20248;&#21270;&#26041;&#27861;&#65292;&#24403;&#27599;&#20010;&#21487;&#34892;&#30340;&#23545;&#25968;&#23494;&#24230;&#21487;&#20197;&#34920;&#31034;&#20026;&#32473;&#23450;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26102;&#65292;&#35813;&#26041;&#27861;&#29983;&#25928;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#22120;&#25104;&#20026;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#30340;&#19968;&#20010;&#19981;&#21160;&#28857;&#12290;&#24403;&#22522;&#20989;&#25968;&#36328;&#36234;&#27599;&#20010;&#21442;&#25968;&#30340;&#20108;&#27425;&#20989;&#25968;&#26102;&#65292;&#21487;&#34892;&#23494;&#24230;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#20135;&#29983;&#20102;&#20934;&#29275;&#39039;&#21464;&#20998;&#36125;&#21494;&#26031; (QNVB)&#12290;&#20854;&#20182;&#22522;&#20989;&#25968;&#21644;&#26356;&#26032;&#26041;&#27861;&#20063;&#26159;&#21487;&#33021;&#30340;&#12290;&#30001;&#20110;&#36825;&#20123;&#26356;&#26032;&#38656;&#35201;&#39640;&#32500;&#31215;&#20998;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20934;&#38543;&#26426;&#31215;&#20998;&#24207;&#21015;&#29992;&#20110;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, feasible densities are Gaussian and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. As these updates require high-dimensional integration, this work first proposes an efficient quasirandom quadrature sequence for mean-fie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25552;&#39640;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00776</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#23545;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries. (arXiv:2301.00776v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25552;&#39640;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#65292;&#24050;&#24314;&#31435;&#20102;&#35768;&#22810;&#27169;&#22411;&#26469;&#25551;&#36848;&#20854;&#34928;&#20943;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32463;&#39564;&#25110;&#29289;&#29702;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#26377;&#20851;&#34928;&#20943;&#21160;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#36825;&#20123;&#27169;&#22411;&#25152;&#20195;&#34920;&#30340;&#20449;&#24687;&#12290;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#24037;&#20855;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PINN&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#21322;&#32463;&#39564;&#21322;&#29289;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#24314;&#27169;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#34928;&#20943;&#21160;&#24577;&#12290;&#24403;&#23545;&#21160;&#24577;&#20102;&#35299;&#36739;&#23569;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#38544;&#34255;&#29289;&#29702;&#27169;&#22411;&#65288;DeepHPM&#65289;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#20027;&#23548;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#21457;&#29616;&#30340;&#21160;&#24577;&#20449;&#24687;&#19982;&#25366;&#25496;&#30340;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion) batteries, many models have been established to characterize their degradation process. The existing empirical or physical models can reveal important information regarding the degradation dynamics. However, there are no general and flexible methods to fuse the information represented by those models. Physics-Informed Neural Network (PINN) is an efficient tool to fuse empirical or physical dynamic models with data-driven models. To take full advantage of various information sources, we propose a model fusion scheme based on PINN. It is implemented by developing a semi-empirical semi-physical Partial Differential Equation (PDE) to model the degradation dynamics of Li-ion batteries. When there is little prior knowledge about the dynamics, we leverage the data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlying governing dynamic models. The uncovered dynamics information is then fused with that mined by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#25512;&#29702;&#33258;&#36523;&#30693;&#35782;&#12289;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#20132;&#27969;&#12289;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#23558;&#20854;&#19982;IFOL&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#30456;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.07935</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#30340;&#24378;&#20154;&#24037;&#26234;&#33021;&#33258;&#25105;&#30830;&#20999;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Strong-AI Autoepistemic Robots Build on Intensional First Order Logic. (arXiv:2212.07935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#25512;&#29702;&#33258;&#36523;&#30693;&#35782;&#12289;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#20132;&#27969;&#12289;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#20154;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#23558;&#20854;&#19982;IFOL&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#35797;&#22270;&#20197;&#20114;&#34917;&#30340;&#26041;&#24335;&#38598;&#25104;&#31070;&#32463;&#21644;&#31526;&#21495;&#26550;&#26500;&#65292;&#20197;&#24212;&#23545;&#27599;&#31181;&#26550;&#26500;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20197;&#25903;&#25345;&#20855;&#26377;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#35748;&#30693;&#24314;&#27169;&#33021;&#21147;&#30340;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20869;&#28085;&#19968;&#38454;&#36923;&#36753;(IFOL)&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#20154;&#30340;&#31526;&#21495;&#26550;&#26500;&#65292;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#65292;&#24182;&#20351;&#29992;&#33258;&#25105;&#21442;&#29031;&#21644;&#25277;&#35937;&#35821;&#35328;&#23646;&#24615;&#25512;&#29702;&#20854;&#33258;&#36523;&#30693;&#35782;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#26426;&#22120;&#20154;&#20351;&#29992;&#20854;&#31070;&#32463;&#26550;&#26500;&#30340;&#32463;&#39564;&#26469;&#33719;&#24471;&#26426;&#22120;&#20154;&#35821;&#35328;&#30340;&#22522;&#30784;&#65292;&#24182;&#23558;&#36825;&#31181;&#32463;&#39564;&#19982;IFOL&#30340;PRP&#65288;&#23646;&#24615;/&#20851;&#31995;/&#21629;&#39064;&#65289;&#29702;&#35770;&#20013;&#30340;&#38750;&#23450;&#20041;&#35821;&#35328;&#27010;&#24565;&#65288;&#29305;&#23450;/&#20010;&#20307;&#21644;&#26222;&#36941;&#27010;&#24565;&#65289;&#30340;&#25366;&#25496;&#65288;&#24863;&#35273;&#65289;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#32771;&#34385;&#26426;&#22120;&#20154;&#30340;&#22235;&#32423;&#30693;&#35782;&#32467;&#26500;&#65306;&#29305;&#23450;&#33258;&#28982;&#35821;&#35328;&#30340;&#35821;&#27861;&#23618;&#38754;&#65288;&#24847;&#22823;&#21033;&#35821;&#12289;&#27861;&#35821;&#31561;&#65289;&#12289;&#20004;&#20010;&#36890;&#29992;&#35821;&#35328;&#23618;&#38754;&#65306;&#20854;&#35821;&#20041;&#20851;&#31995;&#21450;&#23427;&#30340;&#20869;&#28085;&#36923;&#36753;&#32467;&#26500;&#21644;&#20854;&#23427;&#35821;&#35328;&#27010;&#24565;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust strong AI capable of reasoning, learning, and cognitive modeling. In this paper we consider the  intensional First Order Logic (IFOL) as a symbolic architecture of modern robots, able to use natural languages to communicate with humans and to reason about their own knowledge with self-reference and abstraction language property.  We intend to obtain the grounding of robot's language by experience of how it uses its neuronal architectures and hence by associating this experience with the mining (sense) of non-defined language concepts (particulars/individuals and universals) in PRP (Properties/Relations/Propositions) theory of IFOL.  We consider the robot's four-levels knowledge structure: The syntax level of particular natural language (Italian, French, etc..), two universal language levels: its sem
&lt;/p&gt;</description></item><item><title>DWRSeg&#37325;&#26032;&#24605;&#32771;&#20102;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#20004;&#20010;&#27493;&#39588;&#65292;&#31616;&#21270;&#20102;&#22810;&#36895;&#29575;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#30340;&#35282;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#29305;&#24449;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01173</link><description>&lt;p&gt;
DWRSeg:&#37325;&#26032;&#24605;&#32771;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#39640;&#25928;&#33719;&#21462;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
DWRSeg: Rethinking Efficient Acquisition of Multi-scale Contextual Information for Real-time Semantic Segmentation. (arXiv:2212.01173v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01173
&lt;/p&gt;
&lt;p&gt;
DWRSeg&#37325;&#26032;&#24605;&#32771;&#20102;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#20004;&#20010;&#27493;&#39588;&#65292;&#31616;&#21270;&#20102;&#22810;&#36895;&#29575;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#30340;&#35282;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#29305;&#24449;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35768;&#22810;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22810;&#36895;&#29575;&#30340;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#26469;&#21516;&#26102;&#20174;&#19968;&#20010;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#20013;&#25429;&#33719;&#22810;&#23610;&#24230;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#21487;&#33021;&#30001;&#20110;&#19981;&#21512;&#29702;&#30340;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#32780;&#23548;&#33268;&#38590;&#20197;&#35775;&#38382;&#22810;&#23610;&#24230;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#38477;&#20302;&#33719;&#21462;&#22810;&#23610;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#38590;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;&#21333;&#27493;&#26041;&#27861;&#20998;&#35299;&#20026;&#20004;&#20010;&#27493;&#39588;&#65306;&#21306;&#22495;&#27531;&#20313;&#21270;&#21644;&#35821;&#20041;&#27531;&#20313;&#21270;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#22810;&#36895;&#29575;&#30340;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#22312;&#29305;&#24449;&#25552;&#21462;&#26041;&#38754;&#25198;&#28436;&#20102;&#26356;&#31616;&#21333;&#30340;&#35282;&#33394;&#65306;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#26681;&#25454;&#31532;&#19968;&#27493;&#25552;&#20379;&#30340;&#27599;&#20010;&#31616;&#27905;&#30340;&#21306;&#22495;&#24418;&#24335;&#29305;&#24449;&#26144;&#23556;&#65292;&#23545;&#27599;&#20010;&#26399;&#26395;&#30340;&#24863;&#21463;&#37326;&#36827;&#34892;&#31616;&#21333;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#24418;&#24577;&#28388;&#27874;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many current works directly adopt multi-rate depth-wise dilated convolutions to capture multi-scale contextual information simultaneously from one input feature map, thus improving the feature extraction efficiency for real-time semantic segmentation. However, this design may lead to difficult access to multi-scale contextual information because of the unreasonable structure and hyperparameters. To lower the difficulty of drawing multi-scale contextual information, we propose a highly efficient multi-scale feature extraction method, which decomposes the original single-step method into two steps, Region Residualization-Semantic Residualization.In this method, the multi-rate depth-wise dilated convolutions take a simpler role in feature extraction: performing simple semantic-based morphological filtering with one desired receptive field in the second step based on each concise feature map of region form provided by the first step, to improve their efficiency. Moreover, the dilation rate
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2211.12875</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#32858;&#31867;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26088;&#22312;&#23558;&#22270;&#20013;&#30340;&#33410;&#28857;&#21010;&#20998;&#20026;&#33509;&#24178;&#19981;&#21516;&#30340;&#31751;&#65292;&#26159;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#32508;&#36848;&#35770;&#25991;&#30456;&#23545;&#36739;&#23569;&#65292;&#32508;&#36848;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#21183;&#22312;&#24517;&#34892;&#12290;&#22522;&#20110;&#27492;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20844;&#24335;&#21270;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#22270;&#31867;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#32858;&#31867;&#26041;&#27861;&#31561;&#22235;&#20010;&#19981;&#21516;&#30340;&#26631;&#20934;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#20174;&#22270;&#25968;&#25454;&#36136;&#37327;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#36776;&#21035;&#33021;&#21147;&#21644;&#26410;&#30693;&#31751;&#25968;&#31561;&#20116;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2211.09172</link><description>&lt;p&gt;
&#25991;&#23383;&#23545;&#35805;&#20013;&#30340;&#28145;&#24230;&#24773;&#24863;&#35782;&#21035;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Emotion Recognition in Textual Conversations: A Survey. (arXiv:2211.09172v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#38024;&#23545;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20171;&#32461;&#20102;&#28041;&#21450;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#20197;&#21450;&#25551;&#36848;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35843;&#30740;&#24635;&#32467;&#20102;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#21644;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#24314;&#35758;&#24615;&#30340;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#26032;&#30340;&#24212;&#29992;&#21644;&#23454;&#26045;&#22330;&#26223;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#21033;&#29992;&#23545;&#35805;&#35821;&#22659;&#12289;&#35828;&#35805;&#20154;&#21644;&#24773;&#24863;&#21160;&#24577;&#24314;&#27169;&#65292;&#35299;&#37322;&#24120;&#35782;&#34920;&#36798;&#12289;&#38750;&#27491;&#24335;&#35821;&#35328;&#21644;&#35773;&#21050;&#65292;&#24212;&#23545;&#23454;&#26102;&#24773;&#24863;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#35782;&#21035;&#24773;&#24863;&#21407;&#22240;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#20998;&#31867;&#27861;&#65292;&#22810;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#20197;&#21450;&#35299;&#37322;&#24615;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#19982;&#27492;&#20219;&#21153;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#28982;&#21518;&#65292;&#23427;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#31867;&#27861;&#21644;&#22810;&#31181;&#20351;&#29992;&#35813;&#20998;&#31867;&#27861;&#30340;&#24773;&#24863;&#35782;&#21035;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#23427;&#25551;&#36848;&#20102;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#37325;&#35201;&#30340;&#20316;&#21697;&#65292;&#24182;&#35299;&#37322;&#20102;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#20110;&#26356;&#22909;&#30340;&#26694;&#26550;&#30340;&#24314;&#35758;&#24615;&#24773;&#24863;&#35782;&#21035;&#23454;&#36341;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#22788;&#29702;&#20027;&#35266;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Emotion Recognition in Conversations (ERC) has seen a tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker and emotion dynamics modelling, to interpreting common sense expressions, informal language and sarcasm, addressing challenges of real time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC to interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities pertaining to this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions of the most prominent works in ERC with explanations of the Deep Learning architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in ann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;EmoAug&#65292;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#12290;EmoAug&#21033;&#29992;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35821;&#35843;&#32534;&#30721;&#22120;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#39118;&#26684;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08843</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving Speech Emotion Recognition with Unsupervised Speaking Style Transfer. (arXiv:2211.08843v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EmoAug&#65292;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#35828;&#35805;&#39118;&#26684;&#36716;&#25442;&#26469;&#25552;&#21319;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#27169;&#22411;&#12290;EmoAug&#21033;&#29992;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#35821;&#35843;&#32534;&#30721;&#22120;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#26041;&#24335;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#39118;&#26684;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#21508;&#31181;&#38901;&#24459;&#23646;&#24615;&#65292;&#20363;&#22914;&#37325;&#38899;&#20301;&#32622;&#21644;&#24773;&#24863;&#24378;&#24230;&#65292;&#20197;&#20256;&#36798;&#29305;&#23450;&#30340;&#24773;&#24863;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#33268;&#30340;&#35821;&#35328;&#20869;&#23481;&#12290;&#21463;&#21040;&#36825;&#31181;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmoAug&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#65292;&#26088;&#22312;&#22686;&#24378;&#24773;&#24863;&#34920;&#36798;&#24182;&#35299;&#20915;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;EmoAug&#30001;&#19968;&#20010;&#35821;&#20041;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35821;&#35843;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#20998;&#21035;&#34920;&#31034;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#35299;&#30721;&#22120;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#19978;&#36848;&#20004;&#20010;&#20449;&#24687;&#27969;&#30340;&#26465;&#20214;&#19979;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;EmoAug&#36890;&#36807;&#23558;&#19981;&#21516;&#39118;&#26684;&#36755;&#20837;&#21040;&#35821;&#35843;&#32534;&#30721;&#22120;&#20013;&#65292;&#20016;&#23500;&#20102;&#24773;&#24863;&#35821;&#38899;&#30340;&#34920;&#36798;&#65292;&#21253;&#25324;&#37325;&#38899;&#12289;&#33410;&#22863;&#21644;&#24378;&#24230;&#31561;&#19981;&#21516;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#21516;&#26102;&#65292;EmoAug&#33021;&#22815;&#29983;&#25104;&#27599;&#20010;&#31867;&#21035;&#30456;&#20284;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can effortlessly modify various prosodic attributes, such as the placement of stress and the intensity of sentiment, to convey a specific emotion while maintaining consistent linguistic content. Motivated by this capability, we propose EmoAug, a novel style transfer model designed to enhance emotional expression and tackle the data scarcity issue in speech emotion recognition tasks. EmoAug consists of a semantic encoder and a paralinguistic encoder that represent verbal and non-verbal information respectively. Additionally, a decoder reconstructs speech signals by conditioning on the aforementioned two information flows in an unsupervised fashion. Once training is completed, EmoAug enriches expressions of emotional speech with different prosodic attributes, such as stress, rhythm and intensity, by feeding different styles into the paralinguistic encoder. EmoAug enables us to generate similar numbers of samples for each class to tackle the data imbalance issue as well. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22825;&#32447;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20026;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2211.08796</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#21450;&#20854;&#22312;&#22825;&#32447;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model Based Residual Policy Learning with Applications to Antenna Control. (arXiv:2211.08796v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22825;&#32447;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20026;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21487;&#24494;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#31574;&#30053;&#24191;&#27867;&#24212;&#29992;&#20110;&#25511;&#21046;&#35832;&#22914;&#30005;&#20449;&#32593;&#32476;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#31227;&#21160;&#32593;&#32476;&#22522;&#31449;&#22825;&#32447;&#30340;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#21160;&#24577;&#37197;&#32622;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#35206;&#30422;&#29575;&#21644;&#26381;&#21153;&#36136;&#37327;&#12290;&#21463;&#22825;&#32447;&#20542;&#26012;&#25511;&#21046;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#12290;MBRPL&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19982;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20943;&#23569;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#22522;&#20110;&#27169;&#22411;&#30340;&#22825;&#32447;&#25511;&#21046;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#26159;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as telecommunication networks and robots. Specifically, parameters of mobile network base station antennas can be dynamically configured by these policies to improve users coverage and quality of service. Motivated by the antenna tilt control problem, we introduce Model-Based Residual Policy Learning (MBRPL), a practical reinforcement learning (RL) method. MBRPL enhances existing policies through a model-based approach, leading to improved sample efficiency and a decreased number of interactions with the actual environment when compared to off-the-shelf RL methods.To the best of our knowledge, this is the first paper that examines a model-based approach for antenna control. Experimental results reveal that our method delivers strong initial performance while improving sample efficiency over previous RL methods, which is one step towards deploying these algorithms in real networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#23618;&#27425;&#21270;&#24314;&#27169;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01120</link><description>&lt;p&gt;
&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#27010;&#29575;&#36870;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics. (arXiv:2211.01120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#23618;&#27425;&#21270;&#24314;&#27169;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#20219;&#21153;&#30340;&#22797;&#26434;&#21270;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#23398;&#20064;&#32452;&#25104;&#37096;&#20998;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32463;&#20856;&#30340;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#35201;&#20040;&#26159;&#20855;&#26377;&#28789;&#27963;&#32467;&#26500;&#20294;&#19981;&#38543;&#25968;&#25454;&#20248;&#38597;&#25193;&#23637;&#30340;&#27010;&#29575;&#26680;&#26426;&#22120;&#65292;&#35201;&#20040;&#26159;&#20855;&#26377;&#38480;&#21046;&#21442;&#25968;&#24418;&#24335;&#21644;&#36739;&#24046;&#27491;&#21017;&#21270;&#30340;&#30830;&#23450;&#24615;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#26426;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#27010;&#29575;&#23618;&#27425;&#24314;&#27169;&#33539;&#24335;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#22797;&#26434;&#24615;&#27491;&#21017;&#21270;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23545;&#23616;&#37096;&#22238;&#24402;&#25216;&#26415;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#36890;&#36807;&#19968;&#32452;&#23616;&#37096;&#32447;&#24615;&#25110;&#22810;&#39033;&#24335;&#21333;&#20803;&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#21407;&#21017;&#65292;&#26500;&#24314;&#20102;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#24182;&#19988;&#28508;&#22312;&#22320;&#28085;&#30422;&#26080;&#38480;&#20010;&#27169;&#22411;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-calibrated probabilistic regression models are a crucial learning component in robotics applications as datasets grow rapidly and tasks become more complex. Unfortunately, classical regression models are usually either probabilistic kernel machines with a flexible structure that does not scale gracefully with data or deterministic and vastly scalable automata, albeit with a restrictive parametric form and poor regularization. In this paper, we consider a probabilistic hierarchical modeling paradigm that combines the benefits of both worlds to deliver computationally efficient representations with inherent complexity regularization. The presented approaches are probabilistic interpretations of local regression techniques that approximate nonlinear functions through a set of local linear or polynomial units. Importantly, we rely on principles from Bayesian nonparametrics to formulate flexible models that adapt their complexity to the data and can potentially encompass an infinite nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#37327;&#25968;&#25454;&#30340;&#20840;&#23616;&#20449;&#24687;&#20851;&#32852;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.16719</link><description>&lt;p&gt;
&#22522;&#20110;MLP-Mixer&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#24322;&#24120;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network. (arXiv:2210.16719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#37327;&#25968;&#25454;&#30340;&#20840;&#23616;&#20449;&#24687;&#20851;&#32852;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#26159;&#35768;&#22810;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#24050;&#32463;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#36890;&#24120;&#24378;&#35843;&#27969;&#37327;&#25968;&#25454;&#30340;&#23616;&#37096;&#27169;&#24335;&#65292;&#32780;&#24573;&#35270;&#20102;&#20840;&#23616;&#20449;&#24687;&#30340;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;MLP-Mixer&#32467;&#26500;&#65292;&#36825;&#19982;&#25968;&#25454;&#21253;&#30340;&#32467;&#26500;&#26356;&#21152;&#31526;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#25968;&#25454;&#21253;&#34987;&#20998;&#20026;&#25968;&#25454;&#21253;&#22836;&#21644;&#25968;&#25454;&#21253;&#20027;&#20307;&#65292;&#21152;&#19978;&#19981;&#21516;&#35270;&#22270;&#19979;&#30340;&#27969;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#35774;&#32622;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#22330;&#26223;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#20511;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
Network traffic classification is the basis of many network security applications and has attracted enough attention in the field of cyberspace security. Existing network traffic classification based on convolutional neural networks (CNNs) often emphasizes local patterns of traffic data while ignoring global information associations. In this paper, we propose an MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, one packet is divided into the packet header and the packet body, together with the flow features of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. Taking advantage of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.05918</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20869;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#30340;&#20998;&#26512;&#65306;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#19982;&#23614;&#24179;&#22343;&#30456;&#32467;&#21512;&#26102;&#30340;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19981;&#38656;&#35201;&#20851;&#20110;&#24213;&#23618;&#25237;&#24433;TD&#19981;&#21160;&#28857;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20449;&#24687;&#30340;&#27493;&#38271;&#36873;&#25321;&#19979;&#65292;&#25512;&#23548;&#20102;&#23614;&#24179;&#22343;TD&#36845;&#20195;&#30340;&#21442;&#25968;&#35823;&#24046;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23614;&#24179;&#22343;TD&#20197;&#26399;&#26395;&#36895;&#29575;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20110;&#26368;&#20248;&#30340; $O(1/t)$ &#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#21021;&#22987;&#35823;&#24046;(&#20559;&#24046;)&#30340;&#26356;&#24555;&#34928;&#20943;&#36895;&#29575;&#65292;&#36825;&#26159;&#23545;&#25152;&#26377;&#36845;&#20195;&#30340;&#24179;&#22343;&#20540;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21017;&#21270;&#30340;TD&#21464;&#20307;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
&lt;/p&gt;</description></item><item><title>jsdp&#26159;&#19968;&#20010;Java&#24211;&#65292;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#23545;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2209.09979</link><description>&lt;p&gt;
jsdp: &#19968;&#20010;Java&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
jsdp: a Java Stochastic Dynamic Programming Library. (arXiv:2209.09979v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09979
&lt;/p&gt;
&lt;p&gt;
jsdp&#26159;&#19968;&#20010;Java&#24211;&#65292;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#23545;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35268;&#21010;&#26159;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26159;&#38543;&#26426;&#35268;&#21010;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#37319;&#29992;&#8220;&#20989;&#25968;&#26041;&#31243;&#8221;&#26041;&#27861;&#26469;&#21457;&#29616;&#26368;&#20248;&#31574;&#30053;&#12290;&#36890;&#36807;&#21033;&#29992;Java&#20013;&#23454;&#29616;&#30340;lambda&#34920;&#36798;&#24335;&#12289;&#20989;&#25968;&#25509;&#21475;&#12289;&#38598;&#21512;&#21644;&#32858;&#21512;&#36816;&#31639;&#31526;&#26469;&#25805;&#20316;MapReduce&#26694;&#26550;&#65292;jsdp&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24211;&#65292;&#29992;&#20110;&#24314;&#27169;&#21644;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Programming is a framework for modelling and solving problems of decision making under uncertainty. Stochastic Dynamic Programming is a branch of Stochastic Programming that takes a "functional equation" approach to the discovery of optimal policies. By leveraging constructs - lambda expressions, functional interfaces, collections and aggregate operators - implemented in Java to operationalise the MapReduce framework, jsdp provides a general purpose library for modelling and solving Stochastic Dynamic Programs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#39044;&#27979;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2207.09847</link><description>&lt;p&gt;
&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#39044;&#27979;&#20799;&#31461;&#30340;&#35789;&#35821;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Predicting Word Learning in Children from the Performance of Computer Vision Systems. (arXiv:2207.09847v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#39044;&#27979;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20799;&#31461;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35828;&#65292;&#23398;&#20064;&#19968;&#20010;&#35789;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#23558;&#35813;&#35789;&#19982;&#25551;&#36848;&#30340;&#35270;&#35273;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#20316;&#20026;&#20174;&#35270;&#35273;&#32447;&#32034;&#23398;&#20064;&#19968;&#20010;&#35789;&#30340;&#38590;&#24230;&#30340;&#20195;&#29702;&#26469;&#25506;&#31350;&#35789;&#35821;&#23398;&#20064;&#30340;&#36825;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#33719;&#24471;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#35821;&#30340;&#24180;&#40836;&#19982;&#35270;&#35273;&#20998;&#31867;&#21644;&#23383;&#24149;&#31995;&#32479;&#30340;&#34920;&#29616;&#30456;&#20851;&#65292;&#36229;&#20986;&#20102;&#35789;&#35821;&#39057;&#29575;&#39044;&#26399;&#25928;&#24212;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#34920;&#29616;&#19982;&#35789;&#35821;&#30340;&#20855;&#20307;&#24615;&#30340;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#32780;&#20855;&#20307;&#24615;&#21448;&#26159;&#20799;&#31461;&#35789;&#35821;&#23398;&#20064;&#30340;&#39044;&#27979;&#22240;&#32032;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#35789;&#35821;&#19982;&#35270;&#35273;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
For human children as well as machine learning systems, a key challenge in learning a word is linking the word to the visual phenomena it describes. We explore this aspect of word learning by using the performance of computer vision systems as a proxy for the difficulty of learning a word from visual cues. We show that the age at which children acquire different categories of words is correlated with the performance of visual classification and captioning systems, over and above the expected effects of word frequency. The performance of the computer vision systems is correlated with human judgments of the concreteness of words, which are in turn a predictor of children's word learning, suggesting that these models are capturing the relationship between words and visual phenomena.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;NVIDIA Jetson Nano&#24320;&#21457;&#26495;&#36827;&#34892;&#30446;&#26631;&#20998;&#31867;&#26102;&#30340;&#21151;&#32791;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;YOLOv5n&#27169;&#22411;&#22312;&#27599;&#31186;&#36755;&#20986;&#24103;&#25968;&#21644;&#20302;&#21151;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;YOLOV5&#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2207.06150</link><description>&lt;p&gt;
&#35780;&#20272;&#24322;&#26500;&#35774;&#22791;&#22312;&#36827;&#34892;AI&#25512;&#29702;&#26102;&#30340;&#21151;&#32791;
&lt;/p&gt;
&lt;p&gt;
Estimating the Power Consumption of Heterogeneous Devices when performing AI Inference. (arXiv:2207.06150v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;NVIDIA Jetson Nano&#24320;&#21457;&#26495;&#36827;&#34892;&#30446;&#26631;&#20998;&#31867;&#26102;&#30340;&#21151;&#32791;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;YOLOv5n&#27169;&#22411;&#22312;&#27599;&#31186;&#36755;&#20986;&#24103;&#25968;&#21644;&#20302;&#21151;&#32791;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;YOLOV5&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#27963;&#21463;&#21040;&#36830;&#25509;&#21040;&#20114;&#32852;&#32593;&#30340;&#30005;&#23376;&#35774;&#22791;&#30340;&#39537;&#21160;&#12290;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#36825;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36830;&#25509;&#35774;&#22791;&#30340;&#25968;&#37327;&#20063;&#22312;&#31283;&#23450;&#22686;&#38271;&#12290;&#30001;&#20110;&#35768;&#22810;&#35774;&#22791;&#34987;&#29992;&#20110;&#25191;&#34892;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#22240;&#27492;&#20102;&#35299;&#23427;&#20204;&#30340;&#21151;&#32791;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;NVIDIA Jetson Nano&#24320;&#21457;&#26495;&#22312;&#25191;&#34892;&#30446;&#26631;&#20998;&#31867;&#26102;&#30340;&#21151;&#32791;&#29305;&#24615;&#21644;&#20998;&#26512;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20851;&#20110;&#27599;&#24103;&#30340;&#21151;&#32791;&#21644;&#27599;&#31186;&#36755;&#20986;&#24103;&#25968;&#65288;&#20351;&#29992;YOLOv5&#27169;&#22411;&#65289;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;YOLOv5n&#22312;&#21534;&#21520;&#37327;&#65288;&#21363;&#27599;&#31186;12.34&#24103;&#65289;&#21644;&#20302;&#21151;&#32791;&#65288;&#21363;&#27599;&#24103;0.154mWh&#65289;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;YOLOV5&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern-day life is driven by electronic devices connected to the internet. The emerging research field of the Internet-of-Things (IoT) has become popular, just as there has been a steady increase in the number of connected devices. Since many of these devices are utilised to perform CV tasks, it is essential to understand their power consumption against performance. We report the power consumption profile and analysis of the NVIDIA Jetson Nano board while performing object classification. The authors present an extensive analysis regarding power consumption per frame and the output in frames per second using YOLOv5 models. The results show that the YOLOv5n outperforms other YOLOV5 variants in terms of throughput (i.e. 12.34 fps) and low power consumption (i.e. 0.154 mWh/frame).
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (c-MARL) &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#29366;&#24577;&#25200;&#21160;&#20197;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#39318;&#20010;&#21463;&#23475;&#20195;&#29702;&#36873;&#25321;&#31574;&#30053;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#34987;&#27979;&#35797;&#30340;&#25152;&#26377;&#29615;&#22659;&#20013;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2202.03558</link><description>&lt;p&gt;
&#26356;&#26377;&#25928;&#22320;&#25915;&#20987;c-MARL&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attacking c-MARL More Effectively: A Data Driven Approach. (arXiv:2202.03558v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (c-MARL) &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#29366;&#24577;&#25200;&#21160;&#20197;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#39318;&#20010;&#21463;&#23475;&#20195;&#29702;&#36873;&#25321;&#31574;&#30053;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#34987;&#27979;&#35797;&#30340;&#25152;&#26377;&#29615;&#22659;&#20013;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;c-MARL&#65289;&#24050;&#32463;&#28044;&#29616;&#20102;&#24456;&#22810;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;c-MARL&#20195;&#29702;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#31283;&#20581;&#24615;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; c-MBA &#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272; c-MARL &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#65292;&#20174;&#32780;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21463;&#23475;&#20195;&#29702;&#30340;&#36873;&#25321;&#31574;&#30053;&#21644;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#27599;&#20010;&#22833;&#36133;&#29366;&#24577;&#37117;&#33021;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#26080;&#38656;&#23545;&#24213;&#23618;&#29615;&#22659;&#25317;&#26377;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340; MARL &#27979;&#35797;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#29615;&#22659;&#20013;&#37117;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65306;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25915;&#20987;&#22312;&#25152;&#26377;&#27979;&#35797;&#29615;&#22659;&#20013;&#37117;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named c-MBA. Our proposed formulation can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. In addition, we propose the first victim-agent selection strategy and the first data-driven approach to define targeted failure states where each of them allows us to develop even stronger adversarial attack without the expert knowledge to the underlying environment. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2112.14417</link><description>&lt;p&gt;
Temporal Difference&#23398;&#20064;&#31639;&#27861;&#30340;&#25511;&#21046;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;Temporal Difference (TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;TD&#23398;&#20064;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#19982;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32473;&#23450;&#31574;&#30053;&#30456;&#20851;&#30340;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#23384;&#22312;&#22810;&#31687;&#20851;&#20110;TD&#23398;&#20064;&#29702;&#35770;&#29702;&#35299;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;&#20960;&#24180;&#65292;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#23545;&#20854;&#32479;&#35745;&#25928;&#29575;&#25552;&#20379;&#20855;&#20307;&#20445;&#35777;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#25511;&#21046;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;TD&#23398;&#20064;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#39046;&#22495;&#30340;&#24050;&#26377;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#35770;&#23548;&#20986;&#30340;&#31616;&#21333;&#20998;&#26512;&#24037;&#20855;&#65292;&#20026;TD&#23398;&#20064;&#30340;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#24191;&#38420;&#39046;&#22495;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
&lt;/p&gt;</description></item><item><title>&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#36824;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#35268;&#27169;&#30340;&#22270;&#20687;&#25915;&#20987;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24110;&#21161;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2110.03605</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#26159;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03605
&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#36824;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#35268;&#27169;&#30340;&#22270;&#20687;&#25915;&#20987;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24110;&#21161;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#25991;&#29486;&#36890;&#24120;&#20851;&#27880;&#20687;&#32032;&#32423;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#24448;&#24448;&#24456;&#38590;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#21019;&#24314;&#8220;&#29305;&#24449;&#32423;&#8221;&#23545;&#25239;&#25200;&#21160;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#25506;&#32034;&#21487;&#24863;&#30693;&#12289;&#21487;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#25552;&#20379;&#20102;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26377;&#29992;&#36755;&#20837;&#31867;&#21035;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#23545;&#25239;&#25915;&#20987;&#30340;&#29420;&#29305;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#22312;ImageNet&#35268;&#27169;&#19978;&#20135;&#29983;&#26377;&#38024;&#23545;&#24615;&#12289;&#36890;&#29992;&#24615;&#12289;&#20266;&#35013;&#24615;&#12289;&#29289;&#29702;&#21487;&#23454;&#29616;&#24615;&#21644;&#40657;&#30418;&#25915;&#20987;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#23545;&#25239;&#22270;&#20687;&#29992;&#20316;&#23454;&#38469;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#23545;&#25239;&#25915;&#20987;&#23545;&#29305;&#24449;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#36827;&#34892;&#39044;&#27979;&#65292;&#28982;&#21518;&#36890;&#36807;&#35774;&#35745;&#8220;...
&lt;/p&gt;
&lt;p&gt;
The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2110.00269</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#35789;&#34920;&#31034;&#65292;&#22312;&#32454;&#35843;&#20043;&#21518;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#40065;&#26834;&#24615;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27880;&#20837;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31216;&#20026;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KEPLMs)&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#28145;&#20837;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;KEPLMs&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#20998;&#31867;&#20102;&#29616;&#26377;&#30340;KEPLMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
&lt;/p&gt;</description></item><item><title>MCML&#26159;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#23545;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#20869;&#23384;&#24211;&#26469;&#36319;&#36394;&#20808;&#21069;&#35757;&#32451;&#30340;&#21095;&#38598;&#30340;&#26631;&#31614;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#20174;&#20869;&#23384;&#20013;&#36866;&#24212;&#8221;&#30340;&#26426;&#21046;&#65292;&#20197;&#20811;&#26381;&#23567;&#26679;&#26412;&#27133;&#20301;&#26631;&#35760;&#20013;&#30340;&#8220;&#26679;&#26412;&#36951;&#24536;&#38382;&#39064;&#8221;&#12290;</title><link>http://arxiv.org/abs/2108.11635</link><description>&lt;p&gt;
MCML&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20869;&#23384;&#30340;&#23545;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#23567;&#26679;&#26412;&#27133;&#20301;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11635
&lt;/p&gt;
&lt;p&gt;
MCML&#26159;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#23545;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#20869;&#23384;&#24211;&#26469;&#36319;&#36394;&#20808;&#21069;&#35757;&#32451;&#30340;&#21095;&#38598;&#30340;&#26631;&#31614;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#20174;&#20869;&#23384;&#20013;&#36866;&#24212;&#8221;&#30340;&#26426;&#21046;&#65292;&#20197;&#20811;&#26381;&#23567;&#26679;&#26412;&#27133;&#20301;&#26631;&#35760;&#20013;&#30340;&#8220;&#26679;&#26412;&#36951;&#24536;&#38382;&#39064;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27133;&#20301;&#26631;&#35760;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#8220;&#26679;&#26412;&#36951;&#24536;&#38382;&#39064;&#8221;&#30340;&#20005;&#37325;&#24433;&#21709;&#65292;&#21363;&#27169;&#22411;&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#20165;&#20381;&#36182;&#25903;&#25345;&#38598;&#32780;&#24536;&#35760;&#20102;&#21382;&#21490;&#23398;&#20064;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCML&#65288;Memory-based Contrastive Meta-Learning&#65289;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#8220;&#20174;&#20869;&#23384;&#20013;&#23398;&#20064;&#8221;&#21644;&#8220;&#20174;&#20869;&#23384;&#20013;&#36866;&#24212;&#8221;&#20004;&#20010;&#27169;&#22359;&#65292;&#20998;&#21035;&#22312;&#35757;&#32451;&#21095;&#38598;&#20043;&#38388;&#21644;&#35757;&#32451;&#19982;&#27979;&#35797;&#20043;&#38388;&#24314;&#31435;&#20102;&#20998;&#24067;&#24046;&#24322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21069;&#32773;&#20351;&#29992;&#26174;&#24335;&#20869;&#23384;&#24211;&#26469;&#36319;&#36394;&#20808;&#21069;&#35757;&#32451;&#30340;&#21095;&#38598;&#30340;&#26631;&#31614;&#34920;&#31034;&#65292;&#24182;&#22312;&#24403;&#21069;&#21095;&#38598;&#30340;&#26631;&#31614;&#34920;&#31034;&#19982;&#20869;&#23384;&#20013;&#23384;&#20648;&#30340;&#21382;&#21490;&#21095;&#38598;&#36827;&#34892;&#23545;&#27604;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#8220;&#20174;&#20869;&#23384;&#20013;&#36866;&#24212;&#8221;&#30340;&#26426;&#21046;&#26469;&#23398;&#20064;...
&lt;/p&gt;
&lt;p&gt;
Meta-learning is widely used for few-shot slot tagging in task of few-shot learning. The performance of existing methods is, however, seriously affected by \textit{sample forgetting issue}, where the model forgets the historically learned meta-training tasks while solely relying on support sets when adapting to new tasks. To overcome this predicament, we propose the \textbf{M}emory-based \textbf{C}ontrastive \textbf{M}eta-\textbf{L}earning (aka, MCML) method, including \textit{learn-from-the-memory} and \textit{adaption-from-the-memory} modules, which bridge the distribution gap between training episodes and between training and testing respectively. Specifically, the former uses an explicit memory bank to keep track of the label representations of previously trained episodes, with a contrastive constraint between the label representations in the current episode with the historical ones stored in the memory. In addition, the \emph{adaption-from-memory} mechanism is introduced to learn 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#32593;&#20013;&#30340;GPS&#27450;&#39575;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2005.04513</link><description>&lt;p&gt;
&#30005;&#32593;&#20013;&#26234;&#33021;GPS&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Intelligent GPS Spoofing Attack Detection in Power Grids. (arXiv:2005.04513v1 [eess.SY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.04513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#32593;&#20013;&#30340;GPS&#27450;&#39575;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPS&#23481;&#26131;&#21463;&#21040;GPS&#27450;&#39575;&#25915;&#20987;&#65288;GSA&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;GPS&#25509;&#25910;&#22120;&#30340;&#26102;&#38388;&#21644;&#20301;&#32622;&#32467;&#26524;&#20986;&#29616;&#28151;&#20081;&#12290;&#22312;&#30005;&#32593;&#20013;&#65292;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#65288;PMUs&#65289;&#20351;&#29992;GPS&#26500;&#24314;&#26102;&#38388;&#26631;&#35760;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#22240;&#27492;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;GSA&#12290;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.
&lt;/p&gt;</description></item></channel></rss>